quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Integrability,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:331,wrap,wrapper,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,1,['wrap'],['wrapper']
Integrability,"> Also isnâ€™t it cool that it points exactly to the problematic line?. Currently, I think the line number reported is the number of lines past the `:` in the function definition. It'd be really nice if it could tell you which line number in the file it was (which might be difficult for manipulated doc-strings). Also, from what the error message says, isn't the `any(broken)` check testing the same thing as assert lines[0], `f""{name} needs a single-line summary""`? Isn't the first one sufficient?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519:338,message,message,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-725996519,1,['message'],['message']
Integrability,"> Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?); > ; > A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading. I was not aware of `file`. I think it might be a good solution! `readwrite._download` should make sure that downloads are not incomplete, so just reading the header might be enough",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462:63,depend,dependency,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733750462,2,['depend'],"['dependencies', 'dependency']"
Integrability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, Â¯\\\_(ãƒ„)_/Â¯. > Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1141,interface,interfaces,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,['interface'],"['interface', 'interfaces']"
Integrability,"> Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. I've been wanting to use Xarray in the backend for AnnData, as AnnData objects are like a restricted `Dataset`. This is mainly blocked by not having CSC/ CSR sparse arrays compatible with Xarray, since we use those formats pretty heavily. @tomwhite's sparse wrapper could be a solution to this, as xarray will accept these if an `__array_function__` implementation is added. I tried a simple, broken in many cases, implementation which had promising results inside DataArrays. I'd definitely like to help fill this out a bit more. <details>; <summary><code>__array_function__</code> implementation</summary>. ```python; def __array_function__(self, func, types, args, kwargs):; result = func(*(x.value if isinstance(x, SparseArray) else x for x in args), **kwargs); if issparse(result):; result = SparseArray(result); elif isinstance(result, np.matrix):; result = np.asarray(result); return result; ```. </details>. @mrocklin would it make sense for this SparseArray class to live in pydata/sparse as a pair of CSR/ CSC classes? The internals could gradually be replaced with a more generic n-dimensional representation, but would get two very common use cases into the library.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953:684,wrap,wrapper,684,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557721953,1,['wrap'],['wrapper']
Integrability,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:760,depend,depending,760,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078,1,['depend'],['depending']
Integrability,"> As you can see, it's a pretty trivial wrapper anyway. Yes, makes sense. > Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. Yes, I like this. > I added exaggeration=None, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release. Ah, right, I somehow overlooked that you did add the exaggeration parameter. That's fine then!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515:40,wrap,wrapper,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753621515,1,['wrap'],['wrapper']
Integrability,"> Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Fully agree. > ; > Also: If trying to call a t-test with non-logarithmized data, a warning should be written.; > . Also agree. > The overflow and 0 warnings: are you sure you used logarithmized data, GÃ¶kcen?. Oh true, it wasn't log transformed, true.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325:93,Depend,Depending,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-478377325,1,['Depend'],['Depending']
Integrability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:68,integrat,integrate,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['integrat'],"['integrate', 'integrated']"
Integrability,"> Can scale_factor be removed as an argument to embedding, and instead have that handling occur inside spatial?. no, otherwise I would have to modify the adata or pass a copy, and this would break other functionalities (first that come to mind, categorical colors saved in adata.uns). > Otherwise we assume it's already an array, and make sure it's the right shape. what do you mean by that>? What you are proposing is to pass the adata.obsm as array in question and not as a string basis right? Is that possible now>? Would be happy to do that but don't want to create and pass an adata copy. > Can an image be passed directly into spatial? I'd prefer this as being the ""expert users"" interface for plotting over an image, and think passing an image to embedding could be removed altogether in the future. done, also there is no image in embedding now. Everything is handled by spatial. It is possible to do something like this:. ```python; img = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""images""][""hires""]; scalef = adata.uns[""spatial""][""V1_Adult_Mouse_Brain""][""scalefactors""][; ""tissue_hires_scalef""; ]; sc.pl.spatial(; adata,; color=""leiden"",; scale_factor=scalef,; img=img,; size=100,; basis=""spatial"",; groups=[""0""],; ); ```; ![image](https://user-images.githubusercontent.com/25887487/103009720-7eee5b00-4537-11eb-9bbf-39751493890f.png). I would still like to have this in 1.7 if possible, I can write docs and additional tests real quick tomorrow early morning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610:686,interface,interface,686,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-750338610,1,['interface'],['interface']
Integrability,"> Can we get a weighted graph out of the fit embedding object? . Yes, PyMDE can do that. The longer answer is that it depends on the type of embedding problem you set up --- some are specified using weighted graphs, others are not. But most embedding problems (including all problems specified using the `preserve_neighbors` function, which is the most commonly used recipe) have associated weighted graphs. > I'm also wondering about just how early the package is. I would like to be able to take advantage of any new features, and wouldn't want an early API decision to lock us out of those. Great question. The internals will very likely change over the coming months/year. But the interface to the `MDE` class, which is the central object in PyMDE, will likely be stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062027813:118,depend,depends,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062027813,2,"['depend', 'interface']","['depends', 'interface']"
Integrability,"> Completely agree, GÃ¶kcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself ðŸ˜ƒ), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:175,wrap,wrapper,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,3,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?. Just to clarify what I meant, current API:. ```python; sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', ; var_group_positions=[(7, 8)], var_group_labels=['NK']); ```. My suggestion:. ```python; markers = {'NK': ['GNLY', 'NKG7']}; sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers); ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-493780398:67,wrap,wrapper,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-493780398,1,['wrap'],['wrapper']
Integrability,"> Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily. Many thanks for your quick reply!; Unfortunately , no visible exception... My code is as follows:. ```py; import velocyto as vcy; import numpy as np; import scanpy as sc; import anndata. vlm = vcy.VelocytoLoom(""path of DentateGyrus.loom""); S = vlm.S; S=S.transpose(); adata = anndata.AnnData(S); print(adata.X); print(adata.obs); print(adata.var). sc.pp.neighbors(adata, n_neighbors=100); adata.uns['iroot'] = 0; print(adata.uns); sc.tl.dpt(adata, n_branchings=2); sc.pl.diffmap(adata, color='dpt_pseudotime', projection='2d'); ```. error message (a number of warnings as well, taking up lots of lines and I have no idea of how to include all of them here...) :. <details><summary>numba warnings</summary>. ```pytb; WARNING: Youâ€™re trying to run this on 27998 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py:450: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""make_euclidean_tree"" failed type inference due to: Cannot unify RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none) and RandomProjectionTreeNode(none, bool, array(float32, 1d, C), float64, RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none), RandomProjectionTreeNode(array(int64, 1d, C), bool, none, none, none, none)) for '$14.16', defined at /home/liz3/env/lib/python3.6/site-packages/umap/rp_tree.py (457). File ""env/lib/python3.6/site-packages/umap/rp_tree.py"", line 457:; def make_euclidean_tree(data, indices, rng_state, leaf_size=30):; <source elided>. left_node = make_euclidean_tree(data, left_indices, rng_state, leaf_size); ^. [1] During: resolving callee type: recursive(type(CPUDispatcher(<function make_euclidean_tree at 0x7f822dd05d08>))); [2] Du",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442:26,message,message,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515138442,3,['message'],['message']
Integrability,"> Do you think you could provide me with some example objects that are giving you trouble?. you can take any `sc.datasets.visium_sge` and play around with inverting/not inverting second axis,and plotting using `sc.pl.spatial` or `sc.pl.embedding` where coordinates are in `adata.obsm[""coords""]`. I'll give another summary on current situation and goals:. **Type of spatial data**; 1. data with coordinates centered bottom left and no image (non visium); 2. data with coordinates centered top left and no image (visium); 3. data with coordinates centered top left and image (visium). `sc.pl.spatial` should support all of the above cases. In all cases it wraps embedding but in 1. it uses scatterplot, in 2. and 3. it uses circles (with a specified radius, present only in visium). The inversion is needed in case 2., because in case 3. this is already handled by the image axis plot.; My solution for this is to pass inverted coordinates for y axis in 2. to `embedding`, so that everything can be handled by the function independently. ; What do you think is a good way to go about this ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921:654,wrap,wraps,654,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-740744921,1,['wrap'],['wraps']
Integrability,"> From a first glance, with seurat_v3 requiring count data, it is important that your .X (becoming the layer you refer to as counts) indeed contains counts, otherwise loess quickly runs into stability issues. I would expect there would be a warning here if this were the case, since `check_values` defaults to `True`. But at least this person had the same error caused by passing in normalized values:. * https://www.biostars.org/p/9535944/. The [author of scikit-misc says](https://github.com/has2k1/scikit-misc/issues/6#issuecomment-615304167):. > pass `surface=""direct""`. to the loess solver based only off the error message. So maybe we can enable that. I don't know enough about loess to be able to say why that would fix this. It would be interesting to see the data that caused this error. I would definitely want to have a reproducible case before attempting a fix.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2853#issuecomment-1997957671:620,message,message,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2853#issuecomment-1997957671,1,['message'],['message']
Integrability,"> From my error log it seems the only non-noarch dependency is [h5py](https://beta.mamba.pm/channels/conda-forge/packages/h5py). Thatâ€™s surprising! I think numba is our most complex dependency, and umapâ€™s dependency PyNNDescent is also compiled. I think if this isnâ€™t a mistake and itâ€™s really just about h5py, we can think about it. Trying to install scanpy and following JupyterLiteâ€™s debug instructions gives:. ![image](https://github.com/scverse/scanpy/assets/291575/07a30013-e78d-46af-80fd-fb48af71d45b). ```pytb; ValueError: Can't find a pure Python 3 wheel for: 'umap-learn>=0.3.10', 'session-info', 'numba>=0.41.0'; See: https://pyodide.org/en/stable/usage/faq.html#why-can-t-micropip-find-a-pure-python-wheel-for-a-package; ```. (session-info isnâ€™t a problem, itâ€™s just an old package that doesnâ€™t publish wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731:49,depend,dependency,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1803434731,3,['depend'],['dependency']
Integrability,"> Given the file sizes nowadays and the number of ""groups"", this is getting fairly computationally intensive. It's one of those simple things your biologists will love (""this is so fast now!""). I agree it doesn't harm to have `rank_genes_groups` parallelized (given that it should be straightforward to implement). ; What @ivirshup was referring to though, is that `rank_genes_groups` on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Please take a look at @Zethson's [book chapter](https://www.sc-best-practices.org/conditions/differential_gene_expression.html). . > RE: pertpy; >; > Could does this relate to @davidsebfischer and diffxpy?. Diffxpy is currently being reimplemented. Once it is released, it would likely be included in pertpy as an additional method. I.e. pertpy is more general and strives to provide a consistent interface to multiple methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226:916,interface,interface,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1396521226,1,['interface'],['interface']
Integrability,"> Great!; > ; > I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do.; > ; > On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?. Sure- there you go. I reverted the above and just raised an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017:219,message,message,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726651017,1,['message'],['message']
Integrability,"> Great, thank you!. hi, did you find the ""merge"" or ""integrate"" commond in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223:54,integrat,integrate,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527330223,1,['integrat'],['integrate']
Integrability,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:1053,integrat,integration,1053,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,2,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. Thanks you so much~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527391920,1,['integrat'],['integration']
Integrability,"> Hi @grimwoo,; > ; > The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`. sorry to bother you again. ; I want to merge adata001, adata002, and adata003 into adata.combined, with mark ""001"", ""002"", and ""003"" respectively. I looked into the help-information of ""help(combat)"", but still don't know how to do so. In Seurat (R), it can be done like: ; adata001$Sample <- ""001""; adata002$Sample <- ""002""; adata002$Sample <- ""003""; adata.anchors <- FindIntegrationAnchors(object.list = list(adata001, adata002, adata003), dims = 1:11); adata.combined <- IntegrateData(anchorset = adata.anchors, dims = 1:11)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457:31,integrat,integration,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527731457,2,"['Integrat', 'integrat']","['IntegrateData', 'integration']"
Integrability,"> Hi @sygongcode,; > ; > Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy). Yes, that is what I want to do. Thank you so much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989:206,integrat,integrated,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529218989,1,['integrat'],['integrated']
Integrability,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class â€œdgCMatrixâ€ object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:64,wrap,wrapper,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061,2,['wrap'],['wrapper']
Integrability,"> Hi, please provide the data you use, otherwise this is not reproducible:; > ; > ```; > FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); > ```. Hey, the data is publicly available under this link: https://www.10xgenomics.com/resources/datasets/human-lung-cancer-ffpe-2-standard. I simple copied the `curl` bash script to download all the files and then unzipped the file corresponding to the images to get the ""spatial"" folder",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906:277,message,message,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845048906,1,['message'],['message']
Integrability,"> Hi,; > ; > all things that are not transcriptomics should ideally live in our sister packages such as https://github.com/scverse/muon . If you remove all things related to ATAC-seq in this PR we could consider it, but honestly I think that this might better live outside of scanpy external and much rather in the [scverse ecosystem](https://scverse.org/packages/#ecosystem). Hi,; Thank you for your reply. I could remove the ATAC in the PR, actually, this commit [7f74d8c](https://github.com/scverse/scanpy/pull/2355/commits/7f74d8c47005dd630f691ab5926095f0ff277ce8) is the version without ATAC. Please let me know if this does not work, I will commit another PR.; I think scalex is very suitable to be included in pp.external since it was developed based-on scanpy system, which could provide more choices for scanpy users to do single-cell integration.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2355#issuecomment-1376280885:844,integrat,integration,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2355#issuecomment-1376280885,1,['integrat'],['integration']
Integrability,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:187,depend,depending,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137,1,['depend'],['depending']
Integrability,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:660,depend,depends,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['depend'],['depends']
Integrability,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did.; > ; > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:; > ; > ```shell; > pip install --upgrade numba; > pip install --upgrade umap-learn; > ```; > ; > Then I essentially reinstalled scanpy using the steps in their installation docs.; > ; > ```shell; > conda install seaborn scikit-learn statsmodels numba pytables; > conda install -c conda-forge python-igraph leidenalg; > pip install scanpy; > ```; > ; > I think I then ended up with a version of numpy that was incompatible with numba so I ran; > ; > ```shell; > pip install numpy==1.20; > ```; > ; > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:; > ; > ```shell; > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))""; > ```; > ; > This seemed to fix my problems; I hope it's able to help others!. I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606:1238,message,messagestream,1238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1063184606,2,"['Message', 'message']","['MessageStream', 'messagestream']"
Integrability,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. Thatâ€™s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if itâ€™s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:240,protocol,protocol,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086,1,['protocol'],['protocol']
Integrability,"> I doubt that it would be considered a branch of logic. What do you define as logic here? I was talking about the logic theory that encompasses formal systems and so on. > [Union and intersection are bad names]. I agree, wikipedia enumerates more names, and explains where â€œunionâ€ comes from:. > tagged union, variant, variant record, choice type, discriminated union, disjoint union, or sum type; > â€¦; > Mathematically, tagged unions correspond to disjoint or discriminated unions, usually written using +. Given an element of a disjoint union A + B, it is possible to determine whether it came from A or B. If an element lies in both, there will be two effectively distinct copies of the value in A + B, one from A and one from B. . I think â€œdiscriminated union/intersection of typesâ€ would make sense here. leaving out the â€œdiscriminated/tagged/disjointâ€ here is the problem. in C thereâ€™s actual *untagged* unions, which simply means that C reserves the memory for the largest of the intersected types and you need to keep track yourself of which the type of the value is. In python you can always do `isinstance`, so a more correct name for `Union[A, B]` would be `TaggedUnion[A, B]`. Iâ€™d also like `OneOf[A, B]`, but that ship has sailed. And intersections are basically duck types or structural types (when anonymous) and traits/interfaces (when named). (i.e. `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. So it makes sense for python, itâ€™s just defined more explicitly than by literally intersecting types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140:1336,interface,interfaces,1336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443178140,1,['interface'],['interfaces']
Integrability,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:285,message,messages,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782,1,['message'],['messages']
Integrability,"> I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working. Same here. I assume there is some issue with the implementation of the setter of adata.X, which prevents `adata.X = adata.X.toarray()` from updating X to its densified version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689:39,message,message,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578596689,1,['message'],['message']
Integrability,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:681,integrat,integrated,681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['integrat'],['integrated']
Integrability,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:105,message,messages,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610,1,['message'],['messages']
Integrability,"> I think it would be sufficient for sc.tl.tsne to warn users if the graph it was passed looks unexpected. Yes, warning is a good idea. But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. If you really want to get exactly t-SNE, run the following command instead: ..., but note that it can be slower and the result will likely look around the same"". > From an API point of view, we don't control weights at the sc.tl.umap call, so I think it would be strange to control weights at the sc.tl.tsne call. But we will *have* to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. > I'm also not sure if binarizing the graph would be closer to ""t-SNE"". Maybe not, but; (1) it will be further away from UMAP, so that e.g. UMAP paper does not need to be cited when using such t-SNE. ; (2) There is citeable literature showing that binary affinities yield practically the same result as t-SNE proper. We can cite this in Scanpy docs. I am not aware of any such literature for normalized UMAP affinities in t-SNE. Stepping back, I am not sure I managed to convey my main point here. Which is: Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, *UMAP-independent*, and nearly equivalent replacement for k=90, perplexity=30 affinities. . I agree with Pavlin though that pretty much any decision would be better than the current situation :-)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727:179,message,message,179,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762763727,3,['message'],['message']
Integrability,"> I think this could be done more efficiently by using the index returned from `filter_genes(..., inplace=False)` in `_highly_variable_genes_single_batch` and instead of the whole data frame merging you add in the current version of your changes. I guess that would depend if you want to have a `filter_genes` call in the HVG selection function every time, or whether you only want it in there in a case where `filter_genes` normally doesn't work. You typically use it on the whole dataset, but not per batch. Another issue atm is that if you set the verbosity high, then the `filter_genes()` call gives you an output, which is not really intended as the user can't see the function call.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265:266,depend,depend,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-529560265,1,['depend'],['depend']
Integrability,"> I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common scio package could be a way to achieve that. Agree, also R packages seem to be doing just fine with Seurat/Bioconductor representation?. > Also muon and scvi-tools read in different things from 10x atac data. This is not intentional at all, muon read atac data would work just fine in our package. > A solution to that would be versioned schemata. That could be good, the schema also don't have to be versioned, we can just have a few options and package devs wrap the method with their choice.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725:575,wrap,wrap,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059798725,1,['wrap'],['wrap']
Integrability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push ðŸ˜†. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1692,depend,dependencies,1692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['depend'],['dependencies']
Integrability,"> I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". This makes more sense now. In that case however I would say that having just raw counts in `adata.raw.X` is fine, no? In the end you are distributing a data file. You can have your version of the normalized data in a layer... and you would be distributing your analysis code as well, so it's always clear how people should use this data file that is being deposited, no?. > Might be important for integration?. Integration works better with HVGs typically, so I don't think these super lowly expressed genes are so relevant here... I would often go with `min_cells=20` or even `50` for larger datasets. In the end I reason that this value will be approximately related to the size of the smallest unique cellular identity you expect to find. > This does run into memory usage problems if want do a densifying transform on the data. Don't understand this entirely... and not sure what a block sparse matrix type is... but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask... although i can imagine it might be slower than doing this on dense matrices. Based on above arguments the main issue I see is currently for the case @gokceneraslan mentioned about MT genes or non-coding genes being stored in `.raw`. In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. This clashes with the way raw is used in current scanpy pipeline. I think we could deprecate the way `.raw` is used at the moment, and use a `.layer` for this instead (maybe a designated ""raw"" layer?), but then introduce a new `.frozenraw` or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I would be a bit hesitant to not have a replacement f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449:532,integrat,integration,532,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820336449,2,"['Integrat', 'integrat']","['Integration', 'integration']"
Integrability,"> I wouldnâ€™t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldnâ€™t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called â€“ so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:621,depend,dependency,621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678,1,['depend'],['dependency']
Integrability,"> I'm not sure how you could get any python setup to install R dependencies for you. Maybe a conda package could include dependencies? I think getting a working environment would alleviate a large pain point for this stuff (for example, I currently have no working Seurat install.). Plus making sure packages are up to date for the wrapped functionality. > you'd have to have a a separate data structure that can move been languages. Sort of. The idea is that you could move arrays to R from python without making any copies, they'd just point to the same memory. This is already possible when passing data from R to python. The main idea is making these wrappers faster and take less memory.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132:63,depend,dependencies,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590215132,4,"['depend', 'wrap']","['dependencies', 'wrapped', 'wrappers']"
Integrability,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:855,depend,depends,855,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523,1,['depend'],['depends']
Integrability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:215,depend,depend,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,2,"['depend', 'wrap']","['depend', 'wrapped']"
Integrability,"> I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. Probably for another discussion -- I like jax as much as anyone, but it's not nearly as easy to install as pytorch, especially on windows and m1 mac.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081:91,depend,dependency,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062188081,1,['depend'],['dependency']
Integrability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:729,depend,depending,729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['depend'],['depending']
Integrability,> If our dependencies have dropped support we can too. You mean like anndata? :laughing: . > Is this ready for review? I think it mostly looks good. yeah!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1772221723:9,depend,dependencies,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1772221723,1,['depend'],['dependencies']
Integrability,"> If we need multiple tools in the same container the place to add it would be [BioContainers/multi-package-containers](https://github.com/BioContainers/multi-package-containers). We do make heavy use of optional dependencies, so this might be the way to go regardless. > Curious to know why and if it's something that can be overcome?. ### Practically. * The documentation for bioconda has been incomplete and out of date for years.; * conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated.; * bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on bioconda all our dependents do too â€“ *this could make it extremely painful to do a migration to bioconda*.; * All of our dependencies are on conda-forge; * Fewer channels to search means easier, faster environment solving. ### More philosophically. Why have separate package registries for biology vs everything else? Code for biology isn't particularly special, much of the tooling/ work here is duplicated effort. Why not just put all of bioconda onto conda-forge, but with a special tag saying they are bio packages? All the extra tooling/ maintenance consortiums can be developed orthogonally to the registry. I think there are very clear problems that come out of separate registries. It was a huge pain to install anything from BioJulia until they deprecated the BioJuliaRegistry. If bioconda didn't use it's own build system there wouldn't be out of date docs for that build system. It just seems like a lot of trouble to go through for unclear benefit. I will admit, I think there were more benefits to this model ~a decade ago. But I think these benefits have been mitigated by significantly improved tooling for developing, building, and distributing packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404:213,depend,dependencies,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160555404,4,['depend'],"['depend', 'dependencies', 'dependents']"
Integrability,"> If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. . This isn't always true though, e.g., if you use scArches or seurat (which also seems to use this umap transform). On the other hand, the umap transform visualization can be quite deceiving. It can be the case that it qualitatively appears to have no batch effects even when there definitely has been no integration/correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744:14,integrat,integrated,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1134250744,2,['integrat'],"['integrated', 'integration']"
Integrability,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:106,depend,dependency,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['depend'],['dependency']
Integrability,"> In which I introduced that convention when helping Laleh to make it more efficient. Cool, I didn't know that! Should have made it a lot more efficient! :smile:. > The convention I know is to return two n Ã— k matrices. Right, this is the default in sklearn. But yes, in the end, we want some sort of adjacency matrix for convenience and direct integration with all the graph stuff.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455:345,integrat,integration,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/441#issuecomment-460070455,1,['integrat'],['integration']
Integrability,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?. I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({; library(reticulate); library(SingleCellExperiment); library(glue); library(clustree); }); sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE); H5AD_PATH = args[1]; OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")); print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {; adata <- sc$read_h5ad(h5ad_path). return(adata); }. count_clusterings = function(adata){; # Ryan suggests:; # length(grep(""leiden"",names(adata$obs))). clusterings = c(); for (x in adata$obs_keys()){; if (startsWith(x, ""leiden"")){; clusterings = append(clusterings, x); }; }; ; return(length(clusterings)); }. set_fig_dimensions = function(num_clusterings){; width = 10; height = (0.6 * num_clusterings); ; if (height < 8){; height = 8; }; ; png(width = width, height = height); options(repr.plot.width = width, repr.plot.height = height); ; return(list(width=width,height=height)); }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)); # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(; x=adata$obs,; prefix=""leiden_"",; # suffix = NULL,; # metadata = NULL,; # count_filter = 0,; # prop_filter = 0.1,; # layout = ""sugiyama"",; # layout = ""tree"",; # use_core_edges = FALSE,; # highlight_core = FALSE,; # node_colour = prefix,; # node_colour_aggr = NULL,; # node_size = ""size"",; # node_size_aggr = NULL,; # node_size_range = c(4, 15),; # node_alpha = 1,; # node_alpha_aggr = NULL,; # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409:88,integrat,integrates,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409,1,['integrat'],['integrates']
Integrability,"> It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. I suspect this is because I processed the dataset with `sc.tl.diffmap`. The random seed tends to make only minor differences (+/- 1). As far as I can tell, the resolution parameter really is dataset dependent. But maybe someone with a better knowledge of the actual algorithm can comment on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498161514:382,depend,dependent,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498161514,1,['depend'],['dependent']
Integrability,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1022,message,message,1022,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522,1,['message'],['message']
Integrability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:877,depend,dependencies,877,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['depend'],['dependencies']
Integrability,"> My idea was to have print_header output very little, plus an expandable region (as itâ€™s the one called in notebooks), and to revert print_versions to just copyable text output. Could we deprecate `print_header` and instead suggest a way to call `session_info` for the equivalent?. If all we're doing is calling `session_info.show` with a couple default arguments, I'm not sure it's worth keeping here. I'd like users to call it directly because:. * Users get access to all of the session_info options without us having to mediate that; * If something doesn't work, it's not our problem. > session_info has no file argument. It has `write_req_file`, which is the same intent â€“ right?. I assume `file` was there from a time when this wrote something that you could `pip install` from?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030:524,mediat,mediate,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2089#issuecomment-998837030,1,['mediat'],['mediate']
Integrability,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl?. I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*?. A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618:809,integrat,integrate,809,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653#issuecomment-782699618,2,['integrat'],['integrate']
Integrability,"> Oh interesting, I thought it was clear :) I mean you even contributed to the function, no?; > ; > I think we also discussed why not to use intersection by default in the PR: [#614 (comment)](https://github.com/theislab/scanpy/pull/614#issuecomment-485875031); > ; > If intersection is not used by default, why would we write in the documentation that it acts as a lightweight batch correction method. I'm as surprised as you are :). Yes, I fixed sth and reorganized a bit. I also recall our disc on `highly_variable_intersection`. However, I thought your organization of HVGs was only for the ranking in `highly_variable_nbatches`. Didn't see it's also the default for `highly_variable`. I never really looked at the docs... that would have given a hint... I still feel as though I have sth slightly different though if I recall. Will look more carefully once this benchmarking data integration thing is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-617120764,1,['integrat'],['integration']
Integrability,"> Oh, then you didnâ€™t hear of type theory. Itâ€™s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:1156,depend,dependent,1156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359,1,['depend'],['dependent']
Integrability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:109,depend,depends,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,2,"['depend', 'interface']","['depends', 'interface']"
Integrability,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:389,Depend,Depends,389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,3,"['Depend', 'depend', 'integrat']","['Depends', 'depends', 'integrate']"
Integrability,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone ðŸ˜… . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:415,depend,depend,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815,1,['depend'],['depend']
Integrability,"> Thank you for this, @awnimo! I added a few small comments.; > ; > Could you move the whole code into `scanpy/external/_tools`, please? We'll transition to all wrapper code for external code to be in that directory. Thank you!. Hey @falexwolf ,; I have completed the changes you requestedI.; Please let me know if there are any other issues. Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-471725401:161,wrap,wrapper,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-471725401,1,['wrap'],['wrapper']
Integrability,"> Thanks for opening the issue.; > ; > It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this.; > ; > Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy. Thank you for pointing out the issue with pytables. Tried a couple things and it works now.; I don't know how this matters. I uninstalled pytables > tried importing scanpy > doesn't work (says tables module not found, which is expected I guess). I reinstalled pytables - now it decides to work. I can't see how that makes a difference since I had the same pytables version before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057:105,depend,dependency,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1047851057,1,['depend'],['dependency']
Integrability,"> Thanks! So my understanding is that you are saying that neighbors function is ALREADY too complicated, so we should not complicate it any further (and rather the existing function could be eventually split by taking that gauss out of it, I guess?). Pretty much. I prefer more smaller, simpler functions with common APIs than fewer functions with larger APIs. > and rather the existing function could be eventually split by taking that gauss out of it, I guess?. I think I'd be pro that. I'd probably prefer exposing an interface for computing weights from KNN distances where methods like `gauss` could sit. > I think it's important that the following works and is actually the recommended way to run t-SNE within scanpy. (Using uniform affinities). Couple questions, first scientific: Why would you prefer uniform edge weights as input to your t-sne? I would think the information about relative distance is useful. Second API: I'm not sure I completely agree with this. I think it would be the most clear for `sc.pp.neighbors` to essentially mean ""build umap's connectivity graph"", and functions like `sc.tl.tsne` or `sc.tl.umap` to be ""find a 2d embedding using the passed connectivity graph"". This means whatever affinities you're passing through (e.g. via `connectivities_key`) are the weights that get used. Are there cases you think this disallows?. > One question here is maybe what should other downstream functions like Leiden clustering use, if somebody runs neighbors_tsne (or both neighbors and neighbors_tsne). The graph that's used is provided from arguments like `neighbors_key` or `obsp` from `leiden`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128:521,interface,interface,521,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759335128,1,['interface'],['interface']
Integrability,"> The Leiden algorithm is now [included](https://igraph.org/python/doc/igraph.Graph-class.html#community_leiden) in the latest release of `python-igraph`, version 0.8.0. I believe this alleviates the need to depend on the `leidenalg` packages. The Leiden algorithm provided in `python-igraph` is substantially faster than the `leidenalg` package. It is simpler though, providing fewer options, but I believe the more extensive options of the `leidenalg` package are not necessarily needed for the purposes of `scanpy`. We provide binary wheels on PyPI and binaries for conda are available from the conda-forge channel, also for Windows. I have now done a speed comparison with adata object of 1.85 million cells. igraph on adata as implemented [above](https://github.com/theislab/scanpy/issues/1053#issuecomment-1039424473) ran in **33 minutes** vs `sc.tl.leiden()` which took **~14 hours**",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011:208,depend,depend,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1039999011,1,['depend'],['depend']
Integrability,"> The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. Any thoughts on where the `scipy.sparse` wrapper might live? It currently exposes just enough of the NumPy API for the purposes of ScanPy - i.e. it would need quite a bit more work to be more generally useful. Ditto for the `cupyx.scipy.sparse` wrapper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075:21,wrap,wrapper,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557464075,3,['wrap'],['wrapper']
Integrability,"> The internals will very likely change over the coming months/year. Good to know, thanks! Any hints about what will change here? In particular, I'm wondering if there might be a `jax` implementation as I'm a bit more keen on that as a dependency. > But most embedding problems (including all problems specified using the preserve_neighbors function, which is the most commonly used recipe) have associated weighted graphs. I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. Would this be the right way to retrieve the graphs for the object, or is `distortions` not the right field?. ```python; from scipy import sparse. weights = mde.distortions().cpu().numpy(); edges = mde.edges.cpu().numpy(). graph = sparse.coo_matrix((weights, (edges[:, 0], edges[:, 1])), shape=(mde.n_items, mde.n_items)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274:236,depend,dependency,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274,1,['depend'],['dependency']
Integrability,"> This agrees with what I suspected: that randomized PCA itself should be pretty stable, . No, if you look into how higher PCs vary, you see that they vary drastically depending on the seed or computational platform. That also makes sense, it's a power-method that does the computation, that runs into some unstable stuff. > PCs were similar to within a couple of decimal points,. I'm very sure that you only observed this for the first couple of PCs, which you robustly estimate. Going higher, you end up in some local minima for a subsapce; I believe that it doesn't mean it doesn't capture important variation; it just means that it's a local minimum that the algorithm converges into... something we observe all the time when training models... in the context of Lanzcos and other algorithms powering SVD, PCA etc., it's usually a nuisance that you have that instability when you go higher in iterations; but also there, people just use the results even if they know they don't get the *exact* 50th eigen vector...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937:168,depend,depending,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436041937,1,['depend'],['depending']
Integrability,"> This is strange, i also tried to run the tests multiple times at the time of committing this and they failed every time. Maybe a dependency had a bugged release at the time?. > I am not sure what king of test. I don't want to add another save_and_compare_images test because plots seem to depend on the system at least sometimes. You could instead use `check_same_image`. Check that running `filter_rank_genes_group` then plotting is equivalent to manually passing those genes to `sc.pl.rank_genes_groups_*` plot on an object that hasn't had `filter_rank_genes_group` run on it. You can search the tests for examples of `check_same_image`. > (i have 3 failing plotting tests locally but they run fine here). Could you open an issue for this and note which tests they are? It would be good to make the tests as resilient as possible on other people's systems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649:131,depend,dependency,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878134649,2,['depend'],"['depend', 'dependency']"
Integrability,"> This seems like pretty bad behavior for a development environment. We definitely don't want the dev install to be uninstalled when a new package gets downloaded. Well, scvelo depends on 1.7 and you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your installâ€™s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. Thatâ€™s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there canâ€™t be pluses in there, only that you canâ€™t upload packages with local specifiers in their version to PyPI. Which we donâ€™t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:177,depend,depends,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,"['depend', 'integrat']","['depends', 'integrated']"
Integrability,"> This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes. @ilan-gold , I didn't check that. I will check that and let you guys know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898:328,depend,dependency,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2115403898,1,['depend'],['dependency']
Integrability,"> To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. I disagree very much :) First of all I didn't say that these 10 dotplots are for ""looking at data,"" most of them are indeed for a paper (we can count the number of dotplots in sc papers, if we want to have a more accurate estimate of the time cost) and all of them need to be edited in a pdf editing software. Once the preprint is out, I can write in this post which dotplots I am talking about, how many there are in reality and how much time it took us to manually edit such minor things that can be easily addressed in the plotting code ðŸ˜„ . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:864,depend,dependency,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,2,"['depend', 'interface']","['dependency', 'interface']"
Integrability,"> Two options:; > ; > * bbknn < 1.5.0; > * use `bbknn.bbknn()` instead of the scanpy wrapper. Thank you for your rapid reply, bbknn 1.4 works for me (scanpy 1.7.2). The second option is good.; Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230:85,wrap,wrapper,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872835230,1,['wrap'],['wrapper']
Integrability,"> Unfortunately, I run into; > ; > ```; > __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________; > ; > flavor = 'use_fastpp'; > ; > @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); > def test_scale(flavor):; > adata = pbmc68k_reduced(); > adata.X = adata.raw.X; > v = adata[:, 0 : adata.shape[1] // 2]; > # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; > assert v.is_view; > with pytest.warns(Warning, match=""view""):; > > sc.pp.scale(v, flavor=flavor); > ; > scanpy/tests/test_preprocessing.py:127: ; > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:959,wrap,wrapper,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['wrap'],['wrapper']
Integrability,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:11,wrap,wrap,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715,1,['wrap'],['wrap']
Integrability,"> Well, as that issue says, itâ€™s fixed in [lmcinnes/umap#261](https://github.com/lmcinnes/umap/pull/261), which means itâ€™s in umap 0.3.10. @flying-sheep unfortunately `umap==3.10` does not fix this relative to the latest scanpy version on Bioconda (`1.4.4.post1`). The issue is that the UMAP fix does not address the branch of code that scanpy depends on (specifically the call follows [this branch in the UMAP code](https://github.com/lmcinnes/umap/blob/41205248fb48391d1f6e4effcb974307b7c229ce/umap/umap_.py#L1059)), which still just passes the `init_coords` in as is. . Of course, there has been [a workaround in scanpy since 1.4.5.post1](https://github.com/theislab/scanpy/commit/1400d1e35f908d6f5ab8a8681970ac4aba673565). However, I would caution against the advice in that commit's message, which assumed that once `umap==3.10` was released the workaround could be removed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427:344,depend,depends,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948#issuecomment-595355427,2,"['depend', 'message']","['depends', 'message']"
Integrability,"> Well, scvelo depends on 1.7 and you have a release candidate. `scvelo` depends on `scanpy>=1.5`, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. > If your installâ€™s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. . I'm not sure what you mean by this. Does `flit install -s --deps=develop` not count as reinstalling? Are you counting `flit install -s` as a development install?. > Because development installs in general are nonstandard, and pip install -e in particular uses the deprecated `setup.py`. I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. > No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. My reading of the PR in `flit` and the subsequent discussion in the `pip` and PEP threads suggests to me that the issue is `pip` validating the metadata name against the wheel, while a spec exists saying the wheel can't contain characters that are allowed by version specs. If anything, `pip` suddenly started expecting exact version specifiers in wheel filenames, while a spec exists that says how the filenames should be mangled. `flit` did the mangling, and `pip` now says that's wrong. It looks like the direction the discussion is headed is PEP 427 is wrong, and `pip` is right. I have no idea what sort of timeframe should be expected here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159:15,depend,depends,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-782812159,2,['depend'],['depends']
Integrability,"> What do you define as logic here?. [Logic](https://en.wikipedia.org/wiki/Logic) as the mother of all formal reasoning and its close relative set theory in mathematics. When you say type theory is a branch of logic then 90% of computer science is a branch of logic. In many contexts this might be a valid but not a very useful statement. > Iâ€™d also like `OneOf[A, B]`. I love `OneOf[A, B]`. This also doesn't pretend to be logic or set stuff. > `hasattr(obj, 'foo')` defines an intersection type of all types having that attribute. This is what I meant when I said _intersection of properties of supertypes_. But I still don't know when you'd need such a type in a practical context, given that we just keep overloading functions like crazy and simply treat passed arguments dependent on their type. Any example when intersection types are actually useful? In a function we might see in Scanpy (this was the whole beginning of this discussion; I cannot imagine a case in which we need to label something _intersection type_ in the docs).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973:776,depend,dependent,776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443397973,1,['depend'],['dependent']
Integrability,"> When I run [`conda install python=3.11` and `conda install -c conda-forge scanpy`] I get an error . Yeah no idea how to debug conda conflicts. Iâ€™ve often seen things like this: completely unrelated packages â€œconflictingâ€ containing cryptic symbols like `feature:|@/osx-64::__osx==10.16=0`. No clue what that means. Conda seems to be unable to figure out which user-specified versions are in conflict with each other. Pip seems to do a better job these days:. > Running this with `pip -vv install scanpy` as you suggested indeed gives an error with numba. Exactly, so this is a numba issue. Please follow https://github.com/numba/numba/issues/8304. Once numba supports Python 3.11, youâ€™ll be able to install any dependent project there (including scanpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043:713,depend,dependent,713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1333418043,1,['depend'],['dependent']
Integrability,"> Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the imctools dependency). I have an object that looks like:. thank you for reporting, this is very interesting use case! and thanks for the detailed evaluation. I would also try with different number of PCs to see whether that has an impact. if you open an issue on `pynndescent`, would you mind referencing this issue or pinging me there, would be interested to see what's the proposed solution/bug. @TiongSun let us know about your use case, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797652809,1,['depend'],['dependency']
Integrability,"> You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. https://pypi.org/project/autopep8/. Someone suggested to use it for pre-commit anyways. However, there is no full flake8 autofixer. > As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? . Possible, but this will be an iterative process that will take a long time. If you are willing to do that I can close this PR and enable flake8 for example on the first folder of Scanpy. I cannot however fix all of them manually in like 20 PRs or something. > I'm pretty strongly against this. noqas just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783:985,Depend,Depends,985,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785875783,1,['Depend'],['Depends']
Integrability,"> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=40) ; [42](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=41) Parameters; (...); [49](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=48) The squared euclidean distance between x and y; [50](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=49) """"""; [51](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=50) result = 0.0. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\decorators.py:219, in _jit.<locals>.wrapper(func); [217](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=216) with typeinfer.register_dispatcher(disp):; [218](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=217) for sig in sigs:; --> [219](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=218) disp.compile(sig); [220](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=219) disp.disable_compile(); [221](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/decorators.py?line=220) return disp. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\dispatcher.py:965, in Dispatcher.compile(self, sig); [963](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/dispatcher.py?line=962) with ev.trigger_event(""numba:compile"", data=ev_details):; [964](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:13278,wrap,wrapper,13278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wrapper']
Integrability,"> ```python; > scipy.io.mmwrite; > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869:213,wrap,wrapper,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-1476035869,1,['wrap'],['wrapper']
Integrability,"> `paul15` is downloaded automatically, very practical. Yeah, itâ€™s really cool for interactive use, but not for automated testing / continuous integration I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580:143,integrat,integration,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/80#issuecomment-364372580,1,['integrat'],['integration']
Integrability,"> can we just make finding out about this easier for users?. Thatâ€™s what I feel like most of our plot options are. Kinda like an executable FAQ. Pythonâ€™s `itertools` docs have a [recipes](https://docs.python.org/3/library/itertools.html#itertools-recipes) section instead, which are copyable code for less common use cases. I think going for docs here make sense because weâ€™re not wrapping something complex, just increasing visibility for some useful option. Maybe we can add a table that shows what some of our plot options do with `rcParams` behind the scenes (and also some `rcParams` like this one which arenâ€™t covered by our plot options)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104:381,wrap,wrapping,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792304104,1,['wrap'],['wrapping']
Integrability,"> it's annoy. Sounds like annoy is being â€¦ annoying :smile:. > In future, could you not force push while responding to review?. Okay! Hmm, generally IDK what the best approach is since I now know how I want to rebase the commits but Iâ€™ll probably forget later â€¦ Maybe indicate in the message which commit they â€œfixupâ€?. Also: can we reenable squash/rebase merges soon?. > Doc builds failures do seem related to this, however. The docs failure is ugly to fix, but I did it â€¦. Since very shortly ago, (pypa/pip#9320) pip validates wheels and for some reason decided that pluses in wheel filenames are not valid (I couldnâ€™t find that in any spec). I hope takluyver/flit#388 gets merged soon to circumvent/fix that. If we want to temporarily circumvent that weâ€™d have to tell readthedocs to use pip 20.3.3 version (like I did in the pipelines yaml). And thatâ€™s ugly because weâ€™d have to add a requirements file that contains just `pip==20.3.3`, since readthedocs doesnâ€™t allow to specify a pip version or a literal list of requirements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179:284,message,message,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777397179,1,['message'],['message']
Integrability,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:321,depend,dependencies,321,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,3,['depend'],['dependencies']
Integrability,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packagesâ€™ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees â€œ1.7.0rc2â€ and decides â€œIâ€™ll update this even when not asked to updateâ€. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, thatâ€™s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since weâ€™re past 1.7 now, unless you havenâ€™t git-pulled yet, I assume your dev installâ€™s metadata has gone stale. I guess pip wouldnâ€™t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:9,depend,depends,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,2,['depend'],"['dependencies', 'depends']"
Integrability,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776:319,depend,dependency,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776,1,['depend'],['dependency']
Integrability,"> sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in code format or just plain text?. I'm not sure, I think with math is nicer but not aware of any convention. @ivirshup ?. > I think the .._pca function is missing from the release note. should I add it there?; The ..pca function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. must say I missed those sorry, feel free to add and I'll take a look again tomorrow and wrap it up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944:585,wrap,wrap,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065373944,1,['wrap'],['wrap']
Integrability,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:198,message,message,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,1,['message'],['message']
Integrability,"> there is still a large amount of changes to the dataframe code here. Not really changes: itâ€™s almost all refactoring, because the code was spaghetti with quite some duplication. Iâ€™m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and thatâ€™s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:1475,integrat,integrated,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,1,['integrat'],['integrated']
Integrability,>Could epiScanpy be used as a multi-modal analysis tool ? @falexwolf. I think this is a question that is best asked in the episcanpy forum:; https://github.com/colomemaria/epiScanpy/issues. They have used it for multiple epigenomics modalities. Not sure if integrated though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095:257,integrat,integrated,257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-510453095,1,['integrat'],['integrated']
Integrability,"@Brycealong could you also try this in a new isolated environment, please? There might be some dependency that's interfering. Would be glad to know which one, but it's tricky...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1291812216:95,depend,dependency,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1291812216,1,['depend'],['dependency']
Integrability,"@Khalid-Usman Of course you can use silhouette coefficient for any kind of clustering in principal. you just need to choose the ""best option"" depending on your dataset which is again depending on what you are interested in and then you can validate it by looking into your clusters markers. I am actually very curious to see the T-cells case that was mentioned here....you can also have a look here: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set; Again I would like to mention using such control parameters are mathematical methods to assess your clustering quality it might has nothing to do with the biological side and they can be actually helpful when you have no clue over the number of clusters you would look for so you would reply only on mathematics ! your question is really topic specific that what you look for and in which case you want to assess your data...if you already have an estimation or no.......there are also other ways to look for the quality of the data as grst mentioned but again it really depends on what you look into.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498181103:142,depend,depending,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498181103,3,['depend'],"['depending', 'depends']"
Integrability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:508,wrap,wrapper,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['wrap'],['wrapper']
Integrability,@Koncopd it is the # of celltypes per each cohort or the relative_frequencies per each group:; ![image](https://user-images.githubusercontent.com/23288387/119840907-5ea23e00-bed3-11eb-9738-17b267889bb5.png). is it something researchers looking for? or do you think this not good approach as cells depends on how many cells per sample,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917:297,depend,depends,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849666917,1,['depend'],['depends']
Integrability,"@Koncopd this looks correct to me. For `compute_connectivities_umap`, it could be left in `scanpy/scanpy/neighbors/__init__.py` until UMAP's `fuzzy_simplicial_set` returns distances too. (In other words, the change to `fuzzy_simplicial_set` doesn't have to block depending on UMAP.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-477199838:263,depend,depending,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-477199838,1,['depend'],['depending']
Integrability,"@Koncopd, could we have a meeting or VC on the progress here? Let's discuss via email. ðŸ™‚ . @tomwhite, any updates from @lmcinnes regarding integrating the distributed version of `pynndescent` into UMAP itself? I'm not 100% convinced to mingle with private functions within UMAP that might eventually break (another reason for why I copied over code from UMAP back in January/February 2018). It would be nice to have Leland's OK for adding distributed aspects to the package. Scanpy would then just make use of them... @tomwhite, by this, also many other single-cell packages (many of them use UMAP these days) would profit from the new distributed computing capabilities.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722:139,integrat,integrating,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-476592722,1,['integrat'],['integrating']
Integrability,"@LuckyMD Dear Malter, Thanks, now I got your point. I just filtered my cells and genes again using ; `sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3)`; but I still get the same warning message",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494332488:218,message,message,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494332488,1,['message'],['message']
Integrability,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind.; I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376:185,rout,route,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625#issuecomment-772439376,1,['rout'],['route']
Integrability,"@LuckyMD Mine was a general answer, I agree that at this scale it may not be an issue but it may indeed be for larger atlases. @mxposed about your first question: cell distances depend on the HVG in absolute terms, but the overall structure of your data is more ""relative"". If the kNN graph topology is overall conserved you'll end up with similar populations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346:178,depend,depend,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-738828346,1,['depend'],['depend']
Integrability,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494342325:206,message,message,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494342325,1,['message'],['message']
Integrability,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-515764313:82,depend,depending,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-515764313,1,['depend'],['depending']
Integrability,"@LuckyMD, I definitely agree. . I personally find it kinda hard to work with version management and updating of notebooks, as well as keeping them clean with useful prose. This makes updating tutorials a pain. A lot of this just has to do with the interface, as I find this much easier with `.Rmd`. . I'm thinking this could be alleviated a bit with better automation around tutorials. Namely:. * Running + rendering notebooks through CI; * Save tutorials in a more git friendly format, maybe through something like [jupytext](https://jupytext.readthedocs.io). It looks to me like @michalk8 has set up some more extensive CI for tutorials with notebooks. @michalk8 do you have any recommendations here? How are you finding running CI against notebooks?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668:248,interface,interface,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669097668,1,['interface'],['interface']
Integrability,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:261,integrat,integration,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['integrat'],['integration']
Integrability,"@Mr-Milk I see. I think right now, it's probably not going to go into scanpy pre 2.0.0 (as marked by the milestone). @flying-sheep is leading the new plotting effort so I'll step back. But roughly from my perspective, 1. seems useful perhaps for some internal stuff but 2. seems perhaps a point against integrating directly since; > this design doesn't make much difference compared to directly using Marsilea. Maybe there's some middle ground? Some definte API for integration into 2.0.0 that wouldn't require full integration but could give this sort of `return` functionality?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939:303,integrat,integrating,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2371719939,3,['integrat'],"['integrating', 'integration']"
Integrability,"@Zethson I believe that's an upstream issue. Looks like the docs broke when `sphinx-autodoc-typehints` bumped versions from `1.12.0` to `1.13.0`. I can build the docs locally from `master` and from this branch with `sphinx-autodoc-typehints` v1.12, but not v1.13. (You'll also see an identical error in #2099, despite that just being a dependency bump for pre-commit.). I'll submit a PR to pin `sphinx-autodoc-typehints` to version 1.12.0 shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811:336,depend,dependency,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1005072811,1,['depend'],['dependency']
Integrability,"@Zethson thank you for the consideration and explanation. I am not sure Mellon would pass the criteria since it does not depend on or explicitly use AnnData although we do recommend using AnnData: https://mellon.readthedocs.io/en/latest/notebooks/basic_tutorial.html; Additionally, it relies on [Palantir](https://github.com/dpeerlab/Palantir) which does also not qualify since it does not have a CI yet.; Do you think we should try making a PR to https://github.com/scverse/ecosystem-packages regardless?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656331448:121,depend,depend,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656331448,1,['depend'],['depend']
Integrability,"@Zethson thanks ðŸ’¯ ! looks really good! Tbh I was expecting much worse, the changes with flake8 are pretty conservative imho. > Depends on the view that you have. I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I totally share this view, I think it's fine to use noqa as flag and come back later. I admit you guys are 1000x more experts in this, but from what I can judge I think it's fine to merge this as first attempt and then in subsequent PRs improve and address flake 8 fails. . Maybe worth to merge #1527 first? otherwise there will be potential conflicts more for phil.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462:127,Depend,Depends,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-786522462,1,['Depend'],['Depends']
Integrability,@adamgayoso I don't think it fits under the other preprocessing tool headings of Data integration or Imputation. Maybe add a new one called Call hashing or Sample demultiplexing. @fidelram thoughts? Not sure who else to tag,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260:86,integrat,integration,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1483#issuecomment-722042260,1,['integrat'],['integration']
Integrability,"@aditisk that depends on what you put in `adata.raw` ;). Initially `adata.raw` was used to store the full gene object when `adata.X` was filtered to only include HVGs or remove genes that aren't expressed in enough cells. Now, we just have a boolean mask in `adata.var['highly_variable']` for HVGs and so it's often not used anymore. I typically store my log-normalized expression data there if I do batch correction or regress anything out, as `adata.raw` is used as default to compute `rank_genes_groups` and to show expression values on an embedding plot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284:14,depend,depends,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039#issuecomment-617882284,1,['depend'],['depends']
Integrability,@brianpenghe Hi may I consult how you resolved the problem?. The comment says upgrade anndata to 0.8.0 but mine already is 0.8.0 and the error message remains.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1447928394,1,['message'],['message']
Integrability,"@cartal @SamueleSoraggi ; For some reason I decided to integrate Scrublet using Scanpy's functions where possible, rather than making a simple wrapper. The core functionality is up and running in [this fork](https://github.com/swolock/scanpy), and now I just need to add documentation, make some of the code more Scanpythonic(?), and add an example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457:55,integrat,integrate,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492900457,2,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,"@cdpolt, is there are specific change (""new behavior"") you're referring to?. > Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed . Would the code in the previous message be helpful to understand why that happens and how to fix that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186:229,message,message,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186,1,['message'],['message']
Integrability,"@falexwolf ; Is it caused by the parametter svd_solver ? Acorrding to the tl.pca() document ( svd_solver : str, optional (default: â€˜autoâ€™)SVD solver to use. Either â€˜arpackâ€™ for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or â€˜randomizedâ€™ for the randomized algorithm due to Halko (2009). â€œautoâ€ chooses automatically depending on the size of the problem.; The 'randomized' algorithm will give different result each time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122:188,wrap,wrapper,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/203#issuecomment-405780122,2,"['depend', 'wrap']","['depending', 'wrapper']"
Integrability,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesnâ€™t model zero inflation, itâ€™s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:106,protocol,protocols,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723,1,['protocol'],['protocols']
Integrability,"@falexwolf I agree with you about the `diffxpy` a `scanpy` dependencies, Tensorflow is a very important dependency!. > would you make a PR?. I did it, I pushed the code where I added the parameter `n_components` for `scanpy.tl.tsne` function. > Why not using `diffxpy` Volcano plots right away?. I wrote a function in which you can change the colour of the genes, you can add the names of the genes etc. > How did you write your tests?. I tried them on data coming from the lab in which I am working.; I can write a jupyter notebook using public dataset and push it on my copy of the `scanpy` repository.; Give me a couple of days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471324466:59,depend,dependencies,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471324466,2,['depend'],"['dependencies', 'dependency']"
Integrability,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:480,integrat,integration,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827,1,['integrat'],['integration']
Integrability,@falexwolf thanks very much for the extra information! I've copied the t-SNE API so it should fit much more cleanly into the scanpy interface. Let me know if there's anything else you need me to change. Thanks again!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-386075622:132,interface,interface,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-386075622,1,['interface'],['interface']
Integrability,"@falexwolf, realized I didn't change the AnnData dependency. I'm not totally sure what to do with that, since we've already got a requirement on 0.6.22 due to scipy and statsmodels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-511276816:49,depend,dependency,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-511276816,1,['depend'],['dependency']
Integrability,"@fidelram I guess you are the right person to ask for help with this... I'm struggling to work nicely with `plot_scatter()`. I am trying to generate a plot where density values for non-selected conditions are grey, while density values for the selected condition are on 'YlOrRd' or another color map. It seems this is not ideal with a single `plot_scatter()` call (which I was hoping to use as the facet wrapping is already done there). For the grey values I am using a color value of -1, while the others are between 0 and 1. However, when I define a color map that is symmetric around 0, positive values near 0 are mapped to grey instead of colours... any idea why?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-474979855:404,wrap,wrapping,404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-474979855,1,['wrap'],['wrapping']
Integrability,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754:319,integrat,integrate,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-427909754,1,['integrat'],['integrate']
Integrability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:298,wrap,wrap,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['wrap'],['wrap']
Integrability,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:108,integrat,integrated,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177,1,['integrat'],['integrated']
Integrability,@flying-sheep ; Thank you so much for your reply!; Indeed quite a lot of packages are different between the two environments. ; I'm sorry for making this complicated. . The env on my desktop (where the `scrublet` function stopped) is actually newer and at first I thought that would not create huge problems (I recently switched to mamba instead of conda on my Intel-core desktop. ; I didn't use the yml from my M2-chip laptop to re-create the environment because of some dependency problems between the Intel/M2 computers).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116#issuecomment-2187966638:472,depend,dependency,472,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116#issuecomment-2187966638,1,['depend'],['dependency']
Integrability,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:133,depend,depends,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111,1,['depend'],['depends']
Integrability,@flying-sheep I completely agree that it may not be the best naming convention. That was what it was used for at the time though. @grst The idea of using gene symbols as `.var_names` in scanpy was to make the software as user friendly as possible as far as I'm aware. Typically people care about the gene-level as the highest resolution. This is probably also due to the dominance of 3` enrichment protocols making it hard to detect signals at a higher resolution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219:398,protocol,protocols,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441077219,1,['protocol'],['protocols']
Integrability,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:543,integrat,integrates,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729,1,['integrat'],['integrates']
Integrability,"@flying-sheep Yup, I think we need to at least print a better failure message than just returning -1 for the milestone check. It's not a good experience for people that are not familiar with this. @lazappi thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2657#issuecomment-1719024055:70,message,message,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2657#issuecomment-1719024055,1,['message'],['message']
Integrability,"@flying-sheep yes, it's exactly the same problem, with the exactly same error message that only happens when I (or the function) wanna subset an existing adata object",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134:78,message,message,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458658134,1,['message'],['message']
Integrability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:466,interface,interfaces,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,2,['interface'],['interfaces']
Integrability,"@flying-sheep, do you know of a large package (ideally in our dependencies) which uses the directory structure you're advocating for? I'd ideally like to have another repo to look at/ crib from for test organization strategies.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1090364103:62,depend,dependencies,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1090364103,1,['depend'],['dependencies']
Integrability,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:364,message,message,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,1,['message'],['message']
Integrability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:917,Integrat,Integration,917,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['Integrat'],['Integration']
Integrability,"@hhhh1230511, this PR is not part of any release yet (the latest version `scanpy==1.6` was released August 15, 2020). If you want to have the latest version from GitHub you can follow the instructions for a developer installation [here](https://scanpy.readthedocs.io/en/stable/installation.html) in the documentation, for example. Once a new release is available on `pip`, you can install it via `pip install --upgrade scanpy`; In general, you should avoid modifying the code by e.g. simply copying and pasting. This will either easily cause conflicts when updating the package or cause problems when functions from other files which depend on the content you changed but were not updated accordingly. Hope this helped and clarified things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539:634,depend,depend,634,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1422#issuecomment-734460539,1,['depend'],['depend']
Integrability,"@ilan-gold your minimal example causes the exact same error:. Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32. if you are curious, it spits the error out 14.210 times (71050 lines of error message). EDIT: the random state does not seem to matter btw, also happens with different random states",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034445010:456,message,message,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2034445010,1,['message'],['message']
Integrability,"@ivirshup ""One issue with the enrichment as is, is that gprofiler-official import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?"". I'm not really sure I got it right, but if new version of scanpy includes new version of gprofiler-official, then it should work well. If people have old version of scanpy that uses old version of gprofiler, then it should also work but with data from archived release of gprofiler. . With this kind of updates it is inevitable that some environments break (we have the experience as you can see;)), these just need to be solved case by case if people with problems start to contact. They could be advised to update their packages to solve these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323:126,wrap,wrapper,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-484043323,1,['wrap'],['wrapper']
Integrability,"@ivirshup -- I still can't tell why Travis is failing. For some reason on Travis, loess is outputting a zero for the gene mentioned in the error message, but this doesn't happen locally for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279:145,message,message,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-624831279,1,['message'],['message']
Integrability,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:1502,message,message,1502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223,1,['message'],['message']
Integrability,"@ivirshup @giovp I'm wondering whether you've had the time to look over this. If this PR is maybe too big a change, then perhaps it would make more sense to migrate to openTSNE in a more iterative approach. For instance, we could just replace the t-SNE implementation to openTSNE, ignoring the API discussion and ignoring the precomputed graphs. I think switching to openTSNE, regardless of integration, would make the t-SNE implementation faster. We could then go for tighter integration step by step. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079:391,integrat,integration,391,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-922497079,2,['integrat'],['integration']
Integrability,@ivirshup I assume you caught that in your minimal-dependencies branch already? Maybe itâ€™s a good time to push it!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2733#issuecomment-1798483923:51,depend,dependencies,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2733#issuecomment-1798483923,1,['depend'],['dependencies']
Integrability,"@ivirshup Indeed the problem is `use_raw=True` by default. In the test, I think what happens is that the raw data is being plotted and thus no error appears. The tolerance for the image difference may hide the problem if indeed the test image is correct. To avoid this confusion when plotting a layer I think it is better to override `use_raw`. This is how it was supposed to be working before the changes according to the documentation:. ```; layer : typing.Union[str, NoneType], optional (default: None); Name of the AnnData object layer that wants to be plotted. By default; adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.; If `layer` is set to a valid layer name, then the layer is plotted. `layer`; takes precedence over `use_raw`.; ``` ; The current logic is around this lines https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L744. PS: the `use_raw` has been a source of many confusions for me. Now I know when raw is used by default but for new users this may not be obvious. One solution is to add a warning message everytime that `use_raw` is set to `True` by the code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080:1083,message,message,1083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510785080,1,['message'],['message']
Integrability,"@ivirshup Thank you for your super elaborate response and treatment of the topic. I completely understand that you're going for a more comprehensive solution than something like the simple bokeh wrapper that I pasted above. I'd really be interested in something that combines datashader and bokeh, for instance. If you're creating your own package for that, it would be awesome if it was somehow possible to use it also for Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106:195,wrap,wrapper,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/253#issuecomment-420664106,1,['wrap'],['wrapper']
Integrability,"@ivirshup, yes, your example works. However, I would not consider the issue as resolved as it still exists IMO.; Your example only works as you are working with a sparse matrix. If `X` is a `np.ndarray`, the method still fails:. ```bash; >>> adata = sc.AnnData(; np.ceil(np.abs(np.random.randn(10, 10))).astype('int64'),; dtype=int,; ); >>> sc.pp.log1p(adata); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper; return dispatch(args[0].__class__)(*args, **kw); File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 350, in log1p_anndata; X = log1p(X, copy=False, base=base); File ""/opt/anaconda3/lib/python3.7/functools.py"", line 840, in wrapper; return dispatch(args[0].__class__)(*args, **kw); File ""/opt/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 318, in log1p_array; np.log1p(X, out=X); TypeError: ufunc 'log1p' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-683644812:497,wrap,wrapper,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-683644812,2,['wrap'],['wrapper']
Integrability,"@ivirshup. > The worst case scenario I see here me typing something so poorly a newbie trying to follow the documentation gets horrible numba errors they can't figure out. Well, thatâ€™s an improvement over the current situation of â€œthe freeform text type annotations make me guess what I can pass and I get horrible numba errorsâ€, right?. > `issubclass(np.ndarray, typing.Sequence) == False`. That looks like a bug. The docs to `Sequence` say: â€œConcrete subclasses must override `__new__` or `__init__`, `__getitem__`, and `__len__`â€, and. ```py; >>> np.ndarray.__new__ ; <function ndarray.__new__(*args, **kwargs)>; >>> np.ndarray.__getitem__ ; <slot wrapper '__getitem__' of 'numpy.ndarray' objects>; >>> np.ndarray.__len__ ; <slot wrapper '__len__' of 'numpy.ndarray' objects>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940:651,wrap,wrapper,651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441583940,2,['wrap'],['wrapper']
Integrability,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector?. With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293:403,wrap,wrapper,403,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791871293,1,['wrap'],['wrapper']
Integrability,@kaushalprasadhial We internal discussed adding `scikit-learn-intelex` as a dependency. We came to the conclusion that we dont want it as such. Since this a patch that the user can do regardless we think tath the best thing would be to have a notebook that would show the speedup of the patch. We could host this in a new notebook acceleration category.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571:76,depend,dependency,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279#issuecomment-2429335571,1,['depend'],['dependency']
Integrability,"@kt6k, I think that's worth opening a separate issue for. I'm not too experienced with the velocyto tools, but I suspect the issue might be more appropriate for either [velocyto-team/velocyto.py](https://github.com/velocyto-team/velocyto.py) or [theislab/scvelo](https://github.com/theislab/scvelo) depending on when this filtering is occurring.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/567#issuecomment-489908092:299,depend,depending,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/567#issuecomment-489908092,1,['depend'],['depending']
Integrability,@maarten-hifibio in the mean time if you need it I have just made some GPU wrappers available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. It includes one for leiden that would exactly act like sc.tl.leiden (it is part of my PR referenced here),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321:75,wrap,wrappers,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-1106331321,1,['wrap'],['wrappers']
Integrability,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896:27,message,messages,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478907896,2,['message'],"['message', 'messages']"
Integrability,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz; INFO:root:Transposing matrix; INFO:root:Writing gene-by-gene, without using pandas; INFO:root:Writing 8068 genes in total; INFO:root:Wrote 0 genes; INFO:root:Wrote 2000 genes; INFO:root:Wrote 4000 genes; INFO:root:Wrote 6000 genes; INFO:root:Wrote 8000 genes; INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv; ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ?. Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403:112,message,message,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-478685403,1,['message'],['message']
Integrability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:722,depend,dependencies,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['depend'],['dependencies']
Integrability,"@outlace Curiously, your change causes an error. Without your change I can run the tests correctly without a problem. I remember that I fixed a bug similar to this one that was recently integrated into master (see https://github.com/theislab/scanpy/pull/425/files#diff-b5175ed1415cdbf853646e523cbe8ae0L902). Could it be that you didn't have the latest pull from scanpy and that was causing the error?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072:186,integrat,integrated,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471592072,1,['integrat'],['integrated']
Integrability,@outlace I will check the problem with the test and integrate your changes in a new PR that addresses #512 and #524 if this is OK with you.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638:52,integrat,integrate,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/525#issuecomment-471455638,1,['integrat'],['integrate']
Integrability,"@outlace, whoops, didn't see your message before I posted mine, chatroom model woulda stopped that. It looks to me like it's free to self host and theres (admittedly kinda high) educational price for discourse. I'll check out how easy it is to self host. An alternative for threaded conversations is a good old fashioned google group or a tag on stack overflow. (edit: we could probably even just use biostars). The main reason I'm pushing this, is because the more I think about it, the more how unhelpful most gitters have been for me. It might to do with being in a non-European or American timezone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476058909:34,message,message,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476058909,1,['message'],['message']
Integrability,"@pati-ni ; I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843:121,depend,dependencies,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-653900843,1,['depend'],['dependencies']
Integrability,"@pinin4fjords the tests aren't running since scrublet isn't actually being installed (I'm surprised the build still worked, apparently this is just a warning?). From the travis logs:. ```; 203$ pip install .[dev,test,louvain,leiden,magic,scvi,harmony,skmisc,scrublet]; 204Processing /home/travis/build/theislab/scanpy; 205 Installing build dependencies ... done; 206 Getting requirements to build wheel ... done; 207 Preparing wheel metadata ... done; 208 WARNING: scanpy 0.1.dev67+g3918588 does not provide the extra 'scrublet'; ```. You'll need to add a scrublet entry to `extras_require` here: https://github.com/theislab/scanpy/blob/d56d6beacdd951a010bb6a93078db26e1ac904b0/setup.py#L31-L56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707:340,depend,dependencies,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1476#issuecomment-734643707,1,['depend'],['dependencies']
Integrability,"@pranzatelli, could you open a new issue for this? In that issue, could you also report what versions of the dependencies you're using via `sc.logging.print_versions()`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113:109,depend,dependencies,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1823#issuecomment-983618113,1,['depend'],['dependencies']
Integrability,"@rcannood,. I have the same issue while using PAGA_tree from the dyno wrapper.; Do you know where I can add the chunk of code that @flying-sheep suggested to remove the warnings?; My first guess would be to change the python files in the container built from PAGA_tree, however the latter (from dyno) builds a new container every time it is called. How should I procedd?. Best,; Andy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/688#issuecomment-1180404535:70,wrap,wrapper,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/688#issuecomment-1180404535,1,['wrap'],['wrapper']
Integrability,@sjfleming Is there a GIST or repo url to use this code? Might take time to integrate into scanpy/anndata but people can benefit from the code if it already lives somewhere...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615:76,integrat,integrate,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/950#issuecomment-1117994615,1,['integrat'],['integrate']
Integrability,"@tomwhite, we're now fully depending on UMAP. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487798308:27,depend,depending,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487798308,1,['depend'],['depending']
Integrability,"@vitkl now multiple samples are supported, see [here](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html) for description on how to use the new concat strategy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084:112,integrat,integration-scanorama,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640496084,1,['integrat'],['integration-scanorama']
Integrability,"A numba reimplementation of some of the metrics sounds pretty awesome actually. That's out of scope for `scIB` at the moment. We didn't bother with parallelization for most of the metrics (beyond what was already implemented in `sc.tl.louvain` and the sklearn dependencies) as the slowest ones were in R anyway (and now also C++ with our LISI update). Would really welcome that. I can help where I can, although not so familiar with numba.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920:260,depend,dependencies,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764146920,1,['depend'],['dependencies']
Integrability,"About the API, I still think it makes sense for TSNE weighted neighbor calculation to be separate, especially if it is going to have multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:165,depend,depend,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,3,"['depend', 'integrat']","['depend', 'integrate', 'integrated']"
Integrability,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:41,depend,depending,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600,1,['depend'],['depending']
Integrability,"Agreed. I donâ€™t think we should rush and include everything into scanpy, especially when it would be a simple wrapper of something existing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247:110,wrap,wrapper,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/95#issuecomment-369863247,1,['wrap'],['wrapper']
Integrability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:394,interface,interface,394,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,2,['interface'],['interface']
Integrability,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663:291,inject,injecting,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-788832663,1,['inject'],['injecting']
Integrability,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesnâ€™t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:481,depend,dependency,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434,1,['depend'],['dependency']
Integrability,"Also I'm surprised to see I never left a note on your message @falexwolf : thanks! I'm working on the API now, will send in a PR when it's done or leave a note here if I think the DCA API could do with some modification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-403501395:54,message,message,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-403501395,1,['message'],['message']
Integrability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` â€“ the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:334,depend,depends,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,1,['depend'],['depends']
Integrability,"Also, I don't think I would mind `Pillow` too much as a test dependency (it doesn't have a ton of dependencies, right?). A more lightweight solution would be to call [`file`](https://en.wikipedia.org/wiki/File_(command)) on the path, which should be able to recognize it as a `tiff`. I think this would only look at the head of the file though, and wouldn't check if it was corrupted/ didn't finish downloading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124:61,depend,dependency,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733652124,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Also, set up johnnydep and then do:. `johnnydep --output-format pinned scanpy_scripts; `; and after trundling for a very long time and emitting a lot of messages it gives up with:. ```. Given no hashes to check 0 links for project 'scipy': discarding no candidates; ERROR: Could not find a version that satisfies the requirement scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0rc2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0b1, 1.0.0rc1, 1.0.0rc2, 1.0.0, 1.0.1, 1.1.0rc1, 1.1.0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0); ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084:153,message,messages,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653294084,1,['message'],['messages']
Integrability,"AnnData a large dependency given that the only interaction is; ```python; ad.obs[""mellon_log_density""] = mellon.DensityEstimator().fit_predict(ad.obsm[""DM_EigenVectors""]); ```; I understand the criteria though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656364181:16,depend,dependency,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2577#issuecomment-1656364181,1,['depend'],['dependency']
Integrability,"Apparently, UMAP versions below 0.5 are affected (which is a strict dependency of `scvelo`). Versions 0.5+ are not affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064:68,depend,dependency,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-822498064,1,['depend'],['dependency']
Integrability,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:698,depend,depending,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254,1,['depend'],['depending']
Integrability,"As an alternative, I'd be up for just deprecating raw all together, as I think it causes more problems than it solves. I was talking about this recently with @falexwolf, who has come to a similar conclusion. This could be done on the `anndata` side, and just warn whenever `raw` is set. If no `raw` is present, then none of the weird behavior should come up. > I wonder how important it is to keep genes that are filtered out due to being expressed in too few cells anyway. Might be important for integration? But hopefully this could be solvable by just knowing what annotation was used so you can safely assume the missing values are 0. Also, what level of filtering are you doing here? I've tend to go `min_cells=1`. I think we do need to have a more general solution for having a ""feature-select-ed"" subset of the data, but think this can be done with `mask` argument. E.g. `sc.pp.pca(adata, mask=""highly_variable"")` (I believe we've talked about this before). This does run into memory usage problems if want do a densifying transform on the data, though I have doubts about whether this can be a good representation of the data. This can be technically solved by using a block sparse matrix type, but I'm not sure if any practically usable implementations of this are currently available.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988:497,integrat,integration,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988,1,['integrat'],['integration']
Integrability,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:845,message,message,845,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919,1,['message'],['message']
Integrability,"As discussed, @Koncopd will try to integrate this into scikit-learn itself and not into Scanpy. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298:35,integrat,integrate,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/403#issuecomment-456032298,1,['integrat'],['integrate']
Integrability,"As expected the two other links here were of no use.; I also replictaed the problem in Python (not Jupyter) and get this unhepful message:. scanpy.pp.neighbors(adatas[1]); WARNING: You\u2019re trying to run this on 19151 dimensions of `.X`, if you really want this, set `use_rep='X'`.; Falling back to preprocessing with `sc.pp.pca` and default params.; Segmentation fault (core dumped). I know cpp is like this. But I can not even find the core.dump anywhere. Please help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313461261:130,message,message,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313461261,1,['message'],['message']
Integrability,"As said: `pip install scanpy[leiden]`, and use `scanpy.tl.leiden()` instead. See here for how to install scanpy and its dependencies: https://scanpy.readthedocs.io/en/stable/installation.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248:120,depend,dependencies,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638210248,1,['depend'],['dependencies']
Integrability,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:1502,Depend,Depending,1502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942,1,['Depend'],['Depending']
Integrability,"At the moment we're trying to clean up `scIB` that it becomes easier to use. We're still not certain how to best deal with metrics that rely on R and C++ code though. The current plan is to make a more usable pypi package where some metrics give you a warning on additional requirements/manual C++ compilation. Apologies for the usability mess that a package that also assesses usability has become ^^. I'd prefer to keep it separate for now to facilitate maintenance and citation though. That being said, maybe we could think about an optional requirement for scIB to integrate them? At least when we've cleaned up our side of things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114:569,integrat,integrate,569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763835114,1,['integrat'],['integrate']
Integrability,"Based on my experience setting a single cutoff for all datasets will not work, as I've used a lot of different cutoffs depending on the distributions. I would echo @ivirshup's suggestion of looking at distributions. Joint distributions being a lot more important than individual histograms. There's a small discussion about it in our [best practices paper](https://www.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814:119,depend,depending,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507264814,1,['depend'],['depending']
Integrability,But I think it's a little different. It's probably easier to implement since we still have all dependencies available at collection time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088708723:95,depend,dependencies,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088708723,1,['depend'],['dependencies']
Integrability,"But I think scanpydoc is very confused now for some reason. Documentation build is broken, it's visible in ~all~ some recent PRs too and there is not much we can do without the help of @falexwolf or @flying-sheep or @ivirshup, because we cannot even see the error message. My local builds are just fine ðŸ¤·",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915:264,message,message,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-645460915,1,['message'],['message']
Integrability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:66,integrat,integrate,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,1,['integrat'],['integrate']
Integrability,"Can you call `del adata.uns['cell_ontology_class_colors']`? This should throw a better error message... I can do that soon, I wonder how you managed to produce the error... cannot be anything related to a recent update... Hm.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461:93,message,message,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-439745461,1,['message'],['message']
Integrability,"Can you point to a package whose test organization you would like our tests to emulate?. I find pytests docs rather hard to navigate and would really prefer to see an example of what you're advocating for. From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py). -----------. > Would you accept a PR that simply moves the test utils into private submodules of scanpy.testing. I'd lean towards it, but I fully expect issues like #685 to come up. This is why I'd like to see a working example of what you want to work towards. ------------. > switches the import mode to (future default, drawback-less) importlib?. Is it definitely the future default? It looks like they are walking that back. Current versions of pytest docs say:. > [We intend to make importlib the default in future releases, depending on feedback.](https://docs.pytest.org/en/latest/explanation/pythonpath.html#import-modes). Where it previously said:. > [We intend to make importlib the default in future releases.](https://docs.pytest.org/en/6.2.x/pythonpath.html#import-modes)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096718863,1,['depend'],['depending']
Integrability,"Can you show us the values of `combined_bbknn.obs['scNym']`?. Also, if you create a conda environment, does your problems still occur? I'm wondering if some dependencies like `pandas` could be out of date.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572:157,depend,dependencies,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787864572,1,['depend'],['dependencies']
Integrability,"Caught it. I had forgotten that arguments only get evaluated once, so if you mutate them, there is state which is maintained to other calls. I think the unhelpful `abort` message is from `louvain-igraph` expecting a weight vector of the right shape, which ended up with the error:. ```; libc++abi.dylib: terminating with uncaught exception of type char const*; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370:171,message,message,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419698370,1,['message'],['message']
Integrability,"Changing it to a property throws a different error:. <details>; <summary> from make html </summary>. ```sh; reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot ; Exception occurred:; File ""/usr/local/lib/python3.8/site-packages/sphinx/util/docfields.py"", line 369, in transform; new_list += fieldtype.make_field(fieldtypes, self.directive.domain, items,; TypeError: make_field() got an unexpected keyword argument 'inliner'; The full traceback has been saved in /var/folders/bd/43q20k0n6z15tdfzxvd22r7c0000gn/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:668,message,message,668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['message']
Integrability,"Closing in favor of #1116, where I integrated the commit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624:35,integrat,integrated,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1109#issuecomment-600094624,1,['integrat'],['integrated']
Integrability,"Completely agree, GÃ¶kcen!. How I just thought about dealing with this in the past couple of minutes: could we not make a submodule *rtools*? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful. The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a `scanpy-contrib`: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: `anndata` is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759:169,wrap,wrapper,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381984759,5,"['depend', 'interface', 'wrap']","['dependencies', 'interfaces', 'wrapper', 'wrappers']"
Integrability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:490,Wrap,Wrap,490,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['Wrap'],['Wrap']
Integrability,"Cool! I'm of course happy to work together! :smile: And, of course, a Python implementation is way cooler than an R one. :wink:. The last addition to Scanpy was an interface for [pypairs](https://github.com/theislab/scanpy/blob/master/scanpy/tools/pypairs.py). It's still not in the docs but will be very soon, I'm right now missing a link to a short example, there. I'm guess you're planning to put your own package on PyPI, right? If yes, than it's not so crucial: for Scanpy, I changed from cython to numba. Many people currently do that, if you try numba, you'll realize why. So, I'd recommend to use numba if you need to speedup something.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384310269:164,interface,interface,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384310269,1,['interface'],['interface']
Integrability,"Cool! Very happy to get another pull request for an interface to `mnnpy`. :smile:. Regarding writing it in C: I disagree, numba-boosted Python code is much nicer for these type of ""relatively simple"" algorithms... . Regarding `rtools`. I'll remove it from the `api` but leave it in scanpy as an example for how one could wrap other r packages... No user will notice that...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485:52,interface,interface,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384463485,2,"['interface', 'wrap']","['interface', 'wrap']"
Integrability,"Cool!. Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578:59,interface,interface,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-427044578,6,"['interface', 'wrap']","['interface', 'interfaces', 'wrapper']"
Integrability,"Cool, @Koncopd! Looks good! @tomwhite, thank you for your comments!. @lmcinnes: are the functions like `nearest_neighbors` and `fuzzy_simplicial_set` stable enough to be used externally? They are not re-exported to the user API; a year ago, I was afraid that code that depended on them might break if changes were made, even if it's just renaming parameters etc. Do you announce chances that might break backwards compat for these functions?. @Koncopd ; > But this fuzzy_simplicial_set doesn't calculate distances, only connectivities. What is the right approach to solve this? Writing PR to umap that adds distances as a return value for fuzzy_simplicial_set?. I'd guess that @lmcinnes wouldn't want to blow this up with another return value. Why can't we just use the return value for `nearest_neighbors` and construct the distance graph as it's done right now in Scanpy? In any case, it doesn't block depending on UMAP...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-477325247:269,depend,depended,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-477325247,2,['depend'],"['depended', 'depending']"
Integrability,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/646#issuecomment-492693756:65,wrap,wrapper,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646#issuecomment-492693756,1,['wrap'],['wrapper']
Integrability,Could you suggest some error handling behavior here? I think there could definitely be a more helpful error message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704:108,message,message,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-732724704,1,['message'],['message']
Integrability,"Could you throw a more informative error message for `copy=False`? Maybe:. `NotImplementedError(""Inplace subsampling is not implemented for backed objects"")`. ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691514632:41,message,message,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691514632,1,['message'],['message']
Integrability,"Dear @wflynny. You're completely right, I added to the documentation a note that the whole topic is under debate ([here](http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp)). Generally, Scanpy aims to enable access to different tools via the same data object and consistent interfaces so that users can conveniently try out different tools. The threshold for including an interface in Scanpy is low and only requires that a preprint/paper together with a solid GitHub repository exist.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668:294,interface,interfaces,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-405181668,2,['interface'],"['interface', 'interfaces']"
Integrability,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:890,wrap,wraps,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"Dear professor&nbsp;Philipp A. Thank you of your patience.; I've attached my Anndata and codes in the attachment. Kind regards. ; Original Email; ; . Sender:""Philipp A.""< ***@***.*** &gt;;. Sent Time:2024/6/7 19:28. To:""scverse/scanpy""< ***@***.*** &gt;;. Cc recipient:""FessenSimon""< ***@***.*** &gt;;""Author""< ***@***.*** &gt;;. Subject:Re: [scverse/scanpy] unexpected error in sc.pl.dpt_timeseries anddpt_groups_pseudotime (Issue #3086). ; Please create a fully reproducible example. I canâ€™t help if I donâ€™t have an AnnData object that doesnâ€™t behave like yours.; ; â€”; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;; . 	; 	 		 			ä»ŽQQé‚®ç®±å‘æ¥çš„è¶…å¤§é™„ä»¶ 	; 	 		 				 					 						 							 						 					; 					 						 							repro.zip 							 (321.0MB, 2024å¹´7æœˆ8æ—¥ 16:26) 						 						 							è¿›å…¥ä¸‹è½½é¡µé¢ 							ï¼šhttps://wx.mail.qq.com/ftn/download?func=3&k=ca9c3c356e51f263f8ef48353a6462397a35692138646239104c1e410b56560c535b4b025c050314505550571501005a034e055008575700575252015d546b3947061647574a1850457756363f313de49719e585e08cd2a2c51d8b742d5c&key=ca9c3c356e51f263f8ef48353a6462397a35692138646239104c1e410b56560c535b4b025c050314505550571501005a034e055008575700575252015d546b3947061647574a1850457756363f313de49719e585e08cd2a2c51d8b742d5c&code=5cf58db9&from=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086#issuecomment-2155872130:692,Message,Message,692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086#issuecomment-2155872130,1,['Message'],['Message']
Integrability,"Definitely a heavy dependency, you should see the size of the conda environment you need to test it. I think it'd be useful for playing around with ideas on how you'd like to aggregate and scale the values, since they've already got a bunch of methods implemented. Plus the plots often look pretty good.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479510188:19,depend,dependency,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479510188,1,['depend'],['dependency']
Integrability,"Depends on how we're feeling about semantic versioning. 2.0 for a complete switch, but we could internally switch over with a compatibility layer and deprecation warnings anytime before then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545:0,Depend,Depends,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666331545,1,['Depend'],['Depends']
Integrability,Depends on how you calculate your neighbors graph: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2498#issuecomment-1576510979:0,Depend,Depends,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2498#issuecomment-1576510979,1,['Depend'],['Depends']
Integrability,"Didnâ€™t you rephrase the message?. > scanpy/tests/test_read_10x.py: +3 -1; > ; > This above file has < {thresh} changes to black formatting. Please black format it and afterwards remove it from â€œtool.black.exclude"" in pyproject.toml. Anyway, it should be â€œremove it from â€˜tool.black.excludeâ€™ *and then* black-format itâ€, as black wonâ€™t run on it if itâ€™s excluded.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563:24,message,message,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/989#issuecomment-577155563,1,['message'],['message']
Integrability,Do you also mean minimal dependency versions? Because Rustâ€™s cargo e.g. has `carg update -Z minimum-versions` which allows people to figure out if their minimum version bounds are truthful or lies (i.e. to be raised),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942:25,depend,dependency,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088701942,1,['depend'],['dependency']
Integrability,"Do you get an exception message or something else? If you can also copy paste the error message here, we can debug it more easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872:24,message,message,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-515127872,2,['message'],['message']
Integrability,"Do you mean that If I want to do use `scanpy.tl.louvain`, I can use `scanpy.tl.leiden` instead? I can `pip install scanpy[leiden]` but it will not change the error message in `scanpy.tl.louvain` with option `flavor='vtraag'`. When I try `pip install scanpy[louvain]`, it has the error `legacy-install-failure`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295:164,message,message,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1283#issuecomment-1638255295,1,['message'],['message']
Integrability,"Does this cause the same issue?. ```python; import numpy as np; import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)); ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message?. In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843:183,message,message,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754547843,2,['message'],['message']
Integrability,"Don't know what this ""review"" process is , but basically it is ready. . De: ""Lukas Heumos"" ***@***.***> ; Ã€: ""theislab/scanpy"" ***@***.***> ; Cc: ""Yves Le Feuvre"" ***@***.***>, ""Mention"" ***@***.***> ; EnvoyÃ©: Jeudi 6 Janvier 2022 20:11:35 ; Objet: Re: [theislab/scanpy] Pca loadings n points patch (PR #2075) . [ https://github.com/Yves33 | @Yves33 ] is this ready for review? . â€” ; Reply to this email directly, [ https://github.com/theislab/scanpy/pull/2075#issuecomment-1006847333 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/ACEYIQUT75OTZC3MUGVAT3DUUXSOPANCNFSM5JWG2IZQ | unsubscribe ] . ; Triage notifications on the go with GitHub Mobile for [ https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675 | iOS ] or [ https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub | Android ] . ; You are receiving this because you were mentioned. Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047:1005,Message,Message,1005,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047,1,['Message'],['Message']
Integrability,"Dot sizes now work, this is ready to be merged... assuming you are happy with how I integrated it into the documentation (I added a separate ""density"" subsection for the plotting tools as I felt it shouldn't really be put in the same category as embeddings.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551:84,integrat,integrated,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475595551,1,['integrat'],['integrated']
Integrability,"Emm, the code is just from the scanpy tutorial.; https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html. I run ingest on pbmc dataset, then meet this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578:99,integrat,integrating-data-using-ingest,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951#issuecomment-883853578,1,['integrat'],['integrating-data-using-ingest']
Integrability,Even cooler!. Do you recall whether the memory usage for a million cells was anything prohibitive?. I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812:344,integrat,integration,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495306812,1,['integrat'],['integration']
Integrability,"Everything runs fine on the current master branch, I uploaded the current version of the notebook: https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb. I'll release either 1.3.3 or 1.4 very soon and if there should have been a bug at some point, it seems to have been fixed at some point. Finally, PAGA is also in the continuous integration tests, so no bugs in the future anymore for this. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872:365,integrat,integration,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333#issuecomment-435728872,1,['integrat'],['integration']
Integrability,"Exactly the same error message pops up when inputting `np.int64` data into `sc.pp.log1p()`. This is with the latest scanpy, and using data that has otherwise worked well when not using `sc.pp.downsample_counts()`. I thus wouldn't consider this resolved, although I can open another issue as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475722334:23,message,message,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475722334,1,['message'],['message']
Integrability,"Exactly, this is why I think itâ€™s a great solution to advertise that repo in the scanpy docs and make scanpy provide a wrapper binary. I added ebi-gene-expression-group/scanpy-scripts#24, let me hear what you think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437812095:119,wrap,wrapper,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437812095,1,['wrap'],['wrapper']
Integrability,"Expanding on this a bit, here's an example discourse forum I've found useful: [https://discourse.julialang.org](). Good, relevant threads show up all the time in google searches, while I've spent a lot of time trying to find help on various `pyviz` and `conda` gitters with a pretty low success rate. I think something like gitter/ single slack channel works for about 20 people, but after that it gets a bit rough. Having a single stream of messages means keeping track of a conversation over days becomes a huge task. Topics get dropped not based on the value of the discussion, but because of the conversation's overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-476057005:442,message,messages,442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-476057005,1,['message'],['messages']
Integrability,"FWIW, I stumbled upon a related issue this morning where my kernel just crashes/restarts computing neighbors. . For me it appears to crop up when the number of neighbors is <15, metric doesn't appear to matter. I've been upgrading/downgrading various dependencies, and I'm fairly certain this has to do with the call to [`NNDescent` in `umap.umap_.py`](https://github.com/lmcinnes/umap/blob/b1223505ca56ae104feb35e4196227277d1e8058/umap/umap_.py#L328) as if I import that directly, it raises the same errors. Currently have `numba=0.52` `llvmlite=0.35.0` `scanpy=1.7.1` `pynndescent=0.5.2` `umap-learn=0.5.1`. Rebuilding my environment from scratch and will update with a complete package list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893:251,depend,dependencies,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797603893,1,['depend'],['dependencies']
Integrability,"Faced exactly the same problem with file ""GSE185477_GSM3178784_C41_SC_raw_counts.zip"" from [GSE185477](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE185477). The advice from @hurleyLi helped, thanks a lot! But the error is quite confusing. It would be great to read files without extra actions for the user, but is it possible to at least change the error message? E.g.; ```python; try:; ....; except KeyError:; raise KeyError(""Unexpected error, probably due to Cellranger version. Make sure to unarchive gzipped file coming from Cellranger v2""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392:364,message,message,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916#issuecomment-1296918392,1,['message'],['message']
Integrability,Facing the same issue! Any guidance would be appreciated. Was trying to install using Anaconda Navigator for Windows but i guess I will try the Miniconda route,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751:154,rout,route,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-581828751,1,['rout'],['route']
Integrability,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:670,integrat,integration,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020,2,['integrat'],"['integrated', 'integration']"
Integrability,Fixed https://anaconda.org/conda-forge/legacy-api-wrap,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2966#issuecomment-2023259405:50,wrap,wrap,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2966#issuecomment-2023259405,1,['wrap'],['wrap']
Integrability,"For me sounds interesting, especially if you add circle pathes. But I guess @ivirshup should say if this should be added as scanpy dependency potentially.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145328823:131,depend,dependency,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145328823,1,['depend'],['dependency']
Integrability,"For some reason I don't see the figures here on the Github page (and get an error message when I click on the link), but they showed fine in the email notification I received. Looks good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782:82,message,message,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822057782,1,['message'],['message']
Integrability,"For those interested in using the GPU accelerated functions leiden, draw_graph_fa, I have made them available on the following gist:; https://gist.github.com/LouisFaure/9302aa140d7989a25ed2a44b1ce741e8. I have also included in that code `load_mtx`, which reads and convert mtx files into anndata using cudf. I tested on a 654Mo mtx containing 56621 cells x 20222 genes, I can obtain a 13X speedup (using RTX8000)! . ![image](https://user-images.githubusercontent.com/27488782/164707560-30c0c9fe-6bfe-4fcb-ac2c-0d8a503081b6.png). I expect this to scale even better with higher number of cells. I could also add this wrapper into scanpy once CI is ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960:615,wrap,wrapper,615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106431960,1,['wrap'],['wrapper']
Integrability,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python; from scipy.sparse impor csr_matrix; adata.X = csr_matrix(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048:15,message,message,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645#issuecomment-778224048,1,['message'],['message']
Integrability,"Given the old function now raises an error, could you at least add a; FutureWarning (or np.exceptions.VisibleDeprecationWarning) indicating the; new function to be used? Thanks!. On Wed, 7 June 2023, 05:35 Philipp A., ***@***.***> wrote:. > Closed #2500; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500&g=ZWVjM2FjODk0ZjdmMTI1Nw==&h=N2Y4NmFmODU2ZTBlYjI1NzEzZDVlY2M3ZDQxMmVkMGVkZjY2OGMxZjEzMjZiMjNlODhmMGFhMTkwYjFmNGVjOQ==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>; > as not planned.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23event-9456493371&g=ZGJmZGZhMzNmOTM5ZTgzYQ==&h=Y2JmZjM5MDc2MjMzNjM3MGQwMzk1MDYxZmE3MDZlYzBiNWEzYzdjMTMwNWY5MjgxNTU5YmQ3NDI0ZDBjNWRhZg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>,; > or unsubscribe; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAWZ7ISUHUBPHQDLLCDXKBDOHANCNFSM6AAAAAAY3HAO3E&g=NDU3YTZlZTA4ZDE0MzNhZQ==&h=OWQzOWMxNDgxMjZkZGM3ZWUxMmQ1ZTFlN2UwNjI5ZDI4YjFmMDA3OGVmYjc5MTljZDVkMDlhMTE1YjRiODBmNg==&p=YzJlOmltbXVuYWk6YzpnOjlhYTRmOGNmMDU2NDdjZTQ1ZTI0NjFjZmQ1OTY3NjljOnYxOmg6VA==>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >. -- ; PLEASE NOTE: The information contained in this message is privileged and ; confidential, and is intended only for the use of the individual to whom it ; is addressed and others who have been specifically authorized to receive ; it. If you are not the intended recipient, you are hereby notified that any ; dissemination, distribution, or copying of this communication is strictly ; prohibited. If you have received this communication in error, or if any ; problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580655895:1379,Message,Message,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580655895,2,"['Message', 'message']","['Message', 'message']"
Integrability,"Glad to see this discussion going on. Integrating openTSNE into scanpy should be fairly straightforward but may require some thought. I think Dmitry has already pointed out the most important things such as improved defaults, which other t-SNE implementations are lagging behind in. Apart from that, we package prebuilt binaries, so adding openTSNE as a dependency would be incredibly easy. The main thing we'd have to agree on is how to deal with the KNN graphs. UMAP by default calculates 15 nearest neighbors, and from what I can tell, louvain and leiden clustering both use those 15 neighbors as well by default. t-SNE, on the other hand, calculates 90 nearest neighbors by default. This is every single t-SNE implementation, not just openTSNE. Dmitry suggested using a uniform kernel with 15 neighbors, which would fit elegantly, but then again, this isn't truly t-SNE anymore, but rather something very close. The same goes for the `ingest` functionality. openTSNE does something similar to UMAP for adding new samples to existing embeddings, and then we'd again have to figure out how to calculate nearest neighbors from the new data to the reference data. I don't know how you do this currently for UMAP. I'm not exactly sure how these neighbors are meant to be used in scanpy, since there are several different algorithms that use them. Graph-based clustering uses the KNNG, UMAP uses it, forceatlas2 uses it, PAGA probably as well? Is relying on a single k=15 from UMAP for everything really ok? For example, seurat defaults to using 30 neighbors for clustering.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724:38,Integrat,Integrating,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-637563724,2,"['Integrat', 'depend']","['Integrating', 'dependency']"
Integrability,"Good point! I think the idea might be that theyâ€™re actually different versions of a gene that just have been mapped to the same gene name (and have e.g. different ENSEMBL IDs). But of course that depends on how the data was generated, and your method might be more appropriate for some datasets. Example from the tutorial:. ```py; import pooch; import scanpy as sc. EXAMPLE_DATA = pooch.create(; path=pooch.os_cache(""scverse_tutorials""),; base_url=""doi:10.6084/m9.figshare.22716739.v1/"",; ); EXAMPLE_DATA.load_registry_from_doi(). samples = {; ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",; ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",; }; adatas = {}. for sample_id, filename in samples.items():; path = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); #sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adatas[""s1d1""].var[adatas[""s1d1""].var.index.duplicated(keep=False)]; ```. > | | gene_ids | feature_types | genome | pattern | read | sequence |; > | --- | --- | --- | --- | --- | --- | --- |; > | TBCE | ENSG00000285053 | Gene Expression | GRCh38 | | | |; > | TBCE | ENSG00000284770 | Gene Expression | GRCh38 | | | |; > | LINC01238 | ENSG00000237940 | Gene Expression | GRCh38 | | | |; > | LINC01238 | ENSG00000261186 | Gene Expression | GRCh38 | | | |; > | CYB561D2 | ENSG00000114395 | Gene Expression | GRCh38 | | | |; > | CYB561D2 | ENSG00000271858 | Gene Expression | GRCh38 | | | |; > | MATR3 | ENSG00000280987 | Gene Expression | GRCh38 | | | |; > | MATR3 | ENSG00000015479 | Gene Expression | GRCh38 | | | |; > | LINC01505 | ENSG00000234323 | Gene Expression | GRCh38 | | | |; > | LINC01505 | ENSG00000234229 | Gene Expression | GRCh38 | | | |; > | HSPA14 | ENSG00000284024 | Gene Expression | GRCh38 | | | |; > | HSPA14 | ENSG00000187522 | Gene Expression | GRCh38 | | | |; > | GOLGA8M | ENSG00000188626 | Gene Expression | GRCh38 | | | |; > | GOLGA8M | ENSG00000261480 | Gene Expression | GRCh38 | | | |; > | GGT1 | ENSG00000286070 | Gene Express",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3241#issuecomment-2360283300:196,depend,depends,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241#issuecomment-2360283300,1,['depend'],['depends']
Integrability,"Great to hear from both of you. I'd really love to have better Dask integration with AnnData and am excited to see these progress!. @ryan-williams, it'd great if you could open an issue over on anndata about this! I think that'd be a good place to discuss design considerations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345:68,integrat,integration,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-783235345,1,['integrat'],['integration']
Integrability,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests?. Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857:167,integrat,integration,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519798857,1,['integrat'],['integration']
Integrability,"Great!. I'll replace the dataset in the tests in that case. > It would be good to have tests that actually hit the parts of neighbors where non-pairwise distances are found (>4096 cells I think). We're just completely migrating to a shallow wrapper of umap there, where this is tested. I talked to Leland and he said it should be stable. At some point, we might move to `pynndescent` (when it get's introduced into umap). Long story short, I don't think we need to test the neighbors module within scanpy beyond testing the interface. > I've been pretty successful at speeding up the tests by just running them in parallel. Stuff like this might be good to have in some dev docs. Is there a place for that kind of thing right now?. No, happy to have you put some dev docs in a location that you find sensible. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/581#issuecomment-479472437:241,wrap,wrapper,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/581#issuecomment-479472437,2,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"Great!. I'm not sure what's going on with that conda build, but hopefully one of the maintainers there will know what to do. On the topic of this PR, I do think this case should give a more helpful error message. Would you like to do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381:204,message,message,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1490#issuecomment-726498381,1,['message'],['message']
Integrability,"Great, thank you for summarizing this so neatly. We could even link from the PBMC3k tutorial to this. Quick answer: yes, there are many situations in which `pp.regress_out` can do more harm than good. In most cases, I personally don't correct for mitochondrial gene expression, for instance. Quite a few people will agree with that. Whether a certain processing step makes sense or not depends on the data. You should choose with subject knowledge. Because, of that, I found it hard to come up with a best practice tutorial; what came into life as Benchmark with Seurat, remained Seurat's way of defining best practice. Many papers stick to this. But I'm sure that also many Seurat users won't always regress out mitochondrial genes and number of counts per cell. I'm sure @LuckyMD has a lot to say on this. :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526#issuecomment-471317931:386,depend,depends,386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526#issuecomment-471317931,1,['depend'],['depends']
Integrability,"Great, thank you, @andrea-tango and @Koncopd!. @andrea-tango, would you make a PR? We can then look at how you solved this. In principle, I'm very hesitant to add `diffxpy` as a dependency of Scanpy. It depends on Tensorflow itself, which is a large dependency. What would be OK would be to have a wrapper in `scanpy.external`, but I don't know whether this makes sense. Why not using `diffxpy`s Volcano plots right away?. Regarding the discrepancy between `wilxocon` in `diffxpy` and `scanpy`. There obviously shouldn't be any and there also shouldn't be duplicated code, here, at all. The only reason that Scanpy has its own Wilcoxon implementation was that there was no implementation available that would scale to large sparse data. That's why @tcallies wrote the present implementation about 1.5 years ago. He benchmarked with scipy's Wilcoxon test. @davidsebfischer, can you shed light on why and how you implemented your Wilcoxon? Shouldn't we have a comparison? At the time, @tcallies wrote [this](https://github.com/theislab/scanpy_usage/blob/master/171106_t-test_wilcoxon_comparison/Generic%20Comparison%20T-Test%20Wilcoxon-Rank-Sum%20Test.ipynb) and these [tests](https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py). How did you write your tests?. We were just talking about `log2FC`, which is such a simple quantity and should evidently be properly computed by `rank_genes_groups`. We just had this other PR on it (https://github.com/theislab/scanpy/pull/519). @tcallies, any thoughts from your side?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809:178,depend,dependency,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471322809,4,"['depend', 'wrap']","['dependency', 'depends', 'wrapper']"
Integrability,"Great. Please ping me here when you upload the file to `scanpy_usage` and feel free to close the issue then. I'll update my script to link directly to `scanpy_usage`. > Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. . I don't have much experience with randomized PCA, but this is very disturbing, no? Was your feeling that the PCs themselves changed strongly (as measured, I don't know, by the %% of total captured variance, or maybe angle between subspaces, etc.), or is it rather that clustering outcome is dangerously sensitive to small changes in the data? I think this is something worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047:227,depend,depending,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435797047,1,['depend'],['depending']
Integrability,"Had same problem. It seems it tries to look for all gene names present in the anndata object,; rather than the cell cycle genes that are requested?. I have previously checked that the s_genes and g2m_genes in call; sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); are in the data. use_raw=False ; makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```; Gene_names_in_data:; Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',; 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',; ...; 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',; 'TMLHE-AS1', 'PRKY', 'UTY'],; dtype='object', length=3658). calculating cell cycle phase; computing score 'S_score'; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-82-30815e6b6381> in <module>; 20 print(gdata.var.index); 21 ; ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found); 23 ; 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs); 247 ctrl_size = min(len(s_genes), len(g2m_genes)); 248 # add s-score; --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs); 250 # add g2m-score; 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 128 _adata = adata.raw if use_raw else adata; 129 ; --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata; 131 if issparse(_adata_subset.X):; 132 obs_avg = pd.Series(. ~/.pyenv/version",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350:357,message,message,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-767782350,1,['message'],['message']
Integrability,"Hah, so I wasn't aware of the ecosystem page yet. This looks very cool, and could really be built upon nicely. I think a more clear tutorial integration into the page would be useful.... and I guess some tools don't really have any brief explanations there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703726683,1,['integrat'],['integration']
Integrability,"Happy new year! And thanks for opening this PR @pavlin-policar. -----------------. First a general question. What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ `ingest` functionality happening separately, or would you like to do it all at once?. -----------------. In terms of workflow, I think I'd like it to look similar to UMAP. * One function for calculating the graph/ manifold; * One function for computing the embedding. If possible, I would like it if the user could specify an arbitrary manifold (e.g. the umap weighted one) to pass to the embedding step, but this is icing. > It would also make sense to add a tsne option to sc.pp.neighbors. I would prefer for this to be a separate function, maybe `neighbors_tsne`? This could use the entire neighbor calculating workflow from `openTSNE`. How different are the arguments to the various `affinity` methods? At first glance they look pretty similar. I'd like to have the option of choosing which one, but does it make sense to have all the methods available through one function?. > noticed that sc.tl.umap and now sc.tl.tsne add their parameters to adata.uns. ... Determining which affinity kernel to use would then be as simple as looking into adata.uns to find which parameter value sc.pp.neighbors was called with. +1. Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448:195,integrat,integration,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-758355448,1,['integrat'],['integration']
Integrability,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:; - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient.; - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced).; - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient); while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. ; Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802:29,integrat,integrated,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802,1,['integrat'],['integrated']
Integrability,"Has anyone found a solution for this? I run into segfault with the same message when trying to run `sc.pp.calculate_qc_metrics` on my M2. Latest clean installation. I have the core dump as well, but I don't know how to get useful information from there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076:72,message,message,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1345470076,1,['message'],['message']
Integrability,"Has the format changed for all assays, or is this specific to visium HD?. And is there significant analysis that can be done on these without taking into account extra spatial information?. I would be inclined to say that the visium IO functions in scanpy should be deprecated/ replaced with a light wrapper around: `spatialdata_io.experimental.to_legacy_anndata(spatialdata_io.visium(*args, **kwargs))` or just accessing the AnnData from `spatialdata_io.visium_hd`. But if there is significant burden involved in the dependencies of `spatialdata`, then maybe we could implement something lighter here for now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2033981290:300,wrap,wrapper,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2033981290,2,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"Haven't tried again but I have a suggestion. Since umap (or pynndescent) is a critical component of scanpy, I think it'd be great to run our tests against both ""stable"" and ""development"" branches of umap. However in order for this to happen, umap needs proper naming for the development and stable branches. Right now, there are master, 0.3dev and 0.4dev, therefore the names are version-dependent. . Does it make sense to file a bug report in umap repo? It'd be a lot easier to run test against two major branches of umap without changing the names in every major release. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498:388,depend,dependent,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/779#issuecomment-524128498,1,['depend'],['dependent']
Integrability,"Having run into the limitations of constraining to 10 categories for quite some time, and now seeing @sunnysun515 write a wrapper that brings the functionality and produces meaningful plots: I'd strongly advocate for letting the user decide!. To address concerns raised above, I added a warning to the docstring that enough cells per category are needed for a meaningful estimate: https://github.com/theislab/scanpy/pull/1936/commits/760a967b57b93b0da3296d98430509627f3a80d7. [Imagining a warning showing up whenever the function is called seems more disruptive.]. Otherwise, it'd be great if @LuckyMD and @sunnysun515 could take a look in addition to Isaac: https://github.com/theislab/scanpy/pull/1936",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/719#issuecomment-875362308:122,wrap,wrapper,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-875362308,1,['wrap'],['wrapper']
Integrability,"Hello ; I am also facing the same problem.; I would like to get gene name, log fold change, pval_adj, pts.pts_rest in a single output CSV file but i couldn't able to do that; ` ; sc.tl.rank_genes_groups(adata,""leiden_0.6"", method='t-test',pts=True,corr_method='benjamini-hochberg'); pd.DataFrame(adata.uns['rank_genes_groups']['names']); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; df= pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names','logfoldchanges','pts','pts_rest','pvals','pvals_adj']}); df.to_csv(""/home/Akila/integration/harmony/subset/celltype/find_markergenes.csv"")`; ; Any idea how to get in the single file along with pts??; ; Thanks; Akila",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375:601,integrat,integration,601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1455#issuecomment-1164848375,1,['integrat'],['integration']
Integrability,"Hello, ; I have run this command again in the fresh conda environment. Again I get the same error as before. AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-3-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028:280,depend,dependencies,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-883208028,3,['depend'],['dependencies']
Integrability,"Hello, I am using the folder where I store the raw data for the analysis. There is no error message when I run the command but it does not generate any file or object with this name or any name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376:92,message,message,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817682376,1,['message'],['message']
Integrability,"Hello,. I just type ""import scanpy"", and it still shows the error message. . Here is my code. . ```py; import scanpy; ```. Here is what the computer showed after I ran this code:. ```pytb; AttributeError Traceback (most recent call last); <ipython-input-2-135279188441> in <module>; ----> 1 import scanpy. ~/Documents/scanpy/scanpy/scanpy/__init__.py in <module>; 34 # the actual API; 35 from ._settings import settings, Verbosity # start with settings as several tools are using it; ---> 36 from . import tools as tl; 37 from . import preprocessing as pp; 38 from . import plotting as pl. ~/Documents/scanpy/scanpy/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/Documents/scanpy/scanpy/scanpy/tools/_sim.py in <module>; 23 ; 24 from .. import _utils; ---> 25 from .. import readwrite; 26 from .._settings import settings; 27 from .. import logging as logg. ~/Documents/scanpy/scanpy/scanpy/readwrite.py in <module>; 7 import numpy as np; 8 import pandas as pd; ----> 9 import tables; 10 import anndata; 11 from anndata import (. ~/anaconda3/lib/python3.6/site-packages/tables/__init__.py in <module>; 91 ; 92 # Necessary imports to get versions stored on the cython extension; ---> 93 from .utilsextension import (; 94 get_pytables_version, get_hdf5_version, blosc_compressor_list,; 95 blosc_compcode_to_compname_ as blosc_compcode_to_compname,. tables/utilsextension.pyx in init tables.utilsextension(). ~/anaconda3/lib/python3.6/site-packages/tables/tables/__init__.py in <module>; 122 from .flavor import restrict_flavors; 123 from .description import *; --> 124 from .filters import Filters; 125 ; 126 # Import the user classes from the proper modules. ~/anaconda3/lib/python3.6/site-packages/tables/tables/filters.py in <module>; 27 from tables.req_versions import min_blosc_bitshuffle_version; 28 ;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622:66,message,message,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853#issuecomment-539798622,1,['message'],['message']
Integrability,"Hello,; I am trying to use the wrapper class and I am getting error; RRuntimeError: Error in `[.data.frame`(meta.data, , ii, drop = FALSE) : ; undefined columns selected; Could you please suggest me what should I do; Its on line ro.r('seurat_obj = as.Seurat(adata, counts=""X"",data=NULL)'); Thank you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338:31,wrap,wrapper,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-982525338,1,['wrap'],['wrapper']
Integrability,"Hello,; This command (sc.logging.print_versions()) gives me the error pasted below:; AttributeError Traceback (most recent call last); c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 194 try:; --> 195 mod_version = _find_version(mod.__version__); 196 except AttributeError:. AttributeError: module 'importlib_metadata' has no attribute '__version__'. During handling of the above exception, another exception occurred:. TypeError Traceback (most recent call last); <ipython-input-19-c71c26e11b3b> in <module>; ----> 1 sc.logging.print_versions(). c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\scanpy\logging.py in print_versions(file); 159 try:; 160 buf = sys.stdout = io.StringIO(); --> 161 sinfo(dependencies=True); 162 finally:; 163 sys.stdout = stdout. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in sinfo(na, os, cpu, jupyter, dependencies, std_lib, private, write_req_file, req_file_name, html, excludes); 196 except AttributeError:; 197 try:; --> 198 mod_version = _find_version(mod.version); 199 except AttributeError:; 200 try:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\sinfo\main.py in _find_version(mod_version_attr); 40 return joined_tuple; 41 elif callable(mod_version_attr):; ---> 42 return mod_version_attr(); 43 else:; 44 # print(f'Does not support module version of type {type(mod_ver_attr)}'). TypeError: version() missing 1 required positional argument: 'distribution_name'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246:256,depend,dependencies,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1932#issuecomment-874660246,3,['depend'],['dependencies']
Integrability,"Here an example for the plot with NA:. The used obs has 2 categories (Levitas and none) and for the individual plots the groupby argument were â€žLevitasâ€œ and â€žnoneâ€œ. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ï»¿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. â€”; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354:750,Message,Message,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372299354,1,['Message'],['Message']
Integrability,"Here's a paper that compares various integration methods, with some nice figures showing which ones output corrected counts vs. corrected projections:. https://www.nature.com/articles/s41592-021-01336-8. You could use one of the methods that outputs corrected counts if you need corrected counts for some reason.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673:37,integrat,integration,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240118673,1,['integrat'],['integration']
Integrability,"Here's pretty much what I've tried:. ```python; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(partial(sc.pl.scatter, basis=""pca"")); pca( #tried tab completion here; ```. Tab completion shows:. <img width=""597"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/54574990-8e796f80-4a46-11e9-8ad4-ae5aeead9604.png"">. As opposed to:. <img width=""598"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/54580161-fdf95a00-4a5a-11e9-9b51-8eeec17babfc.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474168347:91,wrap,wraps,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474168347,2,['wrap'],['wraps']
Integrability,"Hey @adamgayoso ,. I really love this HVG method, but sometimes I get the following error from the loess fit:. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-244-764977f87ce6> in <module>; ----> 1 sc.pp.highly_variable_genes(ad_sub, n_top_genes=1500, flavor='seurat_v3', layer='counts'); 2 sc.pp.pca(ad_sub); 3 sc.pp.neighbors(ad_sub); 4 sc.tl.umap(ad_sub); 5 sc.tl.leiden(ad_sub, resolution=2.0). ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/.miniconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 82 x = np.log10(mean[not_const]); 83 model = loess(x, y, span=span, degree=2); ---> 84 model.fit(); 85 estimat_var[not_const] = model.outputs.fitted_values; 86 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'reciprocal condition number 7.4971e-16\n'; ```. This is due to the very low but non-zero variance genes, I think. It goes away when I run `sc.pp.filter_genes(ad_sub, min_cells=5)` but not when I run only `sc.pp.filter_genes(ad_sub, min_cells=1)`. Maybe we can make the variance check more stringent, or we can print a better error message for the users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512:1611,message,message,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-708677512,1,['message'],['message']
Integrability,"Hey @giovp & @ivirshup,; hope you had a good start into 2022! I was getting a twitter request recently asking about when this PR will be merged - are there any news on the timeline yet?. For the PR itself I made suggestions for the few remaining points (see my previous post) - just ping me here if you have feedback on that or if there is anything else to do!. Looking forward to wrap this up :); Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133:381,wrap,wrap,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1030169133,1,['wrap'],['wrap']
Integrability,"Hey @gokceneraslan,. I'm surprised at how you describe the contents of `adata.var['highly_variable']` when `batch_key` is set. I wrote a function that does pretty much exactly the same thing building upon use of `batch_key` for our data integration benchmarking, as I thought this wasn't available in scanpy. I recall looking through the code and thinking this was missing. Maybe we can compare functions for that to see if we're doing exactly the same thing or not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714:237,integrat,integration,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616820714,1,['integrat'],['integration']
Integrability,"Hey @victorlga are you still interested in fixing this? :); I think deleting the line order=keys where catplot is used should do the trick, then also the seaborn dependency could be loosened to allow for `0.13.0`...; What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1791425380:162,depend,dependency,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1791425380,1,['depend'],['dependency']
Integrability,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:1038,integrat,integration,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,2,['integrat'],['integration']
Integrability,"Hey Dmitry, happy New Year's to you too!. > Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? [...] I noticed that you implemented `UniformAffinities ` in here, but isn't it part of openTSNE already?. No, I think we should be able to call the existing machinery. But we'd need to do something like I do with the Uniform affinities here. The reason I had to write separate classes is that the ones in openTSNE calculate the KNNG internally, and don't really offer a way to pass an existing KNNG. In openTSNE that makes sense, since otherwise, the API would be pretty complicated. But here, we have to deal with that. As you can see, it's a pretty trivial wrapper anyway. > How would tsne function know if it should use the uniform kernel or the weights constructed by the neighbors function?. I noticed that `sc.tl.umap` and now `sc.tl.tsne` add their parameters to `adata.uns`. I would imagine `sc.pp.neighbors` probably do the same, and if not, that seems like an easy addition, which is in line with the scanpy architecture. Determining which affinity kernel to use would then be as simple as looking into `adata.uns` to find which parameter value `sc.pp.neighbors` was called with. > I would definitely suggest to add `exaggeration=1` argument to `tsne()`. I added `exaggeration=None`, as is the default in openTSNE. But setting it to 1 instead of None is better, and I should change that in the next release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428:722,wrap,wrapper,722,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753617428,1,['wrap'],['wrapper']
Integrability,"Hey Phil! I'm still a bit hesitant to adopt this option. As discussed before, I'm mainly planning on integrating c++ extensions. Then doing everything via Cython seems overhead. Even the latest Cython distributions recommends **not** doing it. See [here](http://cython.readthedocs.io/en/latest/src/reference/compilation.html) and search for *distributing cython modules*. They provide a solution very similar to the one fom stackoverflow that we have adopted right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803:101,integrat,integrating,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/20#issuecomment-304808803,1,['integrat'],['integrating']
Integrability,"Hey again,. I addressed many of @ivirshup's comments by now and think I am almost done - will finish up the rest next week if all goes well. What is left todo:; - [ ] Make tests faster (re-use results where possible); - [ ] Make tests more code-efficient by code-sharing between functions where possible. Where I could use your / @giovp's input to continue:; - on the keyword/positional argument issue; - on the the ""is median rank a good way to do HVG selection across batches""-issue; - on the question what the final names of the functions should be - your suggestions were:; > `normalize_pearson_residuals` -> `pearson_residuals`; > It's a bit more like log1p; >; > `sc.experimental.pp.highly_variable_genes` -> something else; > I think using an already used function name (highly_variable_genes) and giving it a different API can be confusing. Would calling this pearson_deviant_genes or something like that be better? I do generally dislike how many methods highly_variable_genes wraps already though. Looking forward to the last bits :) ; Cheers, Jan. PS: I'm sorry for the problems that my dirty force-pushing caused before, I hope now everything works fine! I was not aware of the consequences for the comment history back then, but will now take care not to do it again",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395:986,wrap,wraps,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-902985395,1,['wrap'],['wraps']
Integrability,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:444,integrat,integrate,444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490,2,['integrat'],"['integrate', 'integration']"
Integrability,"Hey everyone, thanks for your feedback! In the latest commit, I have tried to include all of your comments, including the more stylistic comments, the references, the numba integration, the unit tests and so on. Have a look and see what you think. I won't be able to work on this any more this year because I am going on holidays. Merry Christmas everyone!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646:173,integrat,integration,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-448304646,1,['integrat'],['integration']
Integrability,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:294,integrat,integration,294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504,1,['integrat'],['integration']
Integrability,"Hey!. You used the data integration methods incorrectly as far as I can see. Please read the documentation for the functions first. `mnnpy.mnn_correct()` takes all the batches as separate anndata objects as positional argument. So you need to do:; `mnnpy.mnn_correct(adata_batch1, adata_batch2, adata_batch3,...)` to run it. For `sc.pp.combat()` you didn't specify where your batch information was stored. And I'm surprised BBKNN didn't work for you. Did you run a `sc.pp.pca()` first?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602:24,integrat,integration,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542686602,1,['integrat'],['integration']
Integrability,"Hey!; > * What methods/ tools?; I am mainly thinking about normalization and data integration methods. For example scran pooling, sctransform, scNorm, Seurat data integration, LIGER... etc. I have most of those already... But anyone is welcome to contribute for anything they regularly use. > * How would you handle R depencies?; So far I've been ignoring this problem and just assuming people have an R environment installed that has the relevant packages. You could just stick a `require(package)` in the function called by `rpy2` and then if would give you an `R` error you can interpret. The plan would be to make this a set of convenience functions, but not a cleanly installable module I guess... I'm not sure how you could get any python setup to install R dependencies for you... > * And (probably hard and definitely not necessary at first) could we use [arrow](https://arrow.apache.org/docs/python/) to speed up data transfer?; This looks interesting... but I don't entirely understand it... you'd have to have a a separate data structure that can move been languages, and be interpreted as an R data structure or `AnnData` depending on where it's used? Most methods are designed to run on a particular class of object. How would this help if you always have to convert to that type of object? So far I've just been using `anndata2ri` to ensure we have an `SCE` object which can be converted to other `R` data structures.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256:82,integrat,integration,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590143256,4,"['depend', 'integrat']","['dependencies', 'depending', 'integration']"
Integrability,"Hey!; I'm a member of g:Profiler (biit.cs.ut.ee/gprofiler) development team and was scanning through web to find services that might depend on us. . We recently went live with an extensive update which might break some of the previous pipelines and wrappers. . All the existing Python and R packages should work, however they are linking to an archived data version and they don't access the most up-to-date data from g:Profiler due to the new API etc. . We have already created a new R package that corresponds to the new API, Python package is still in the progress. . I just wanted to let you know and please feel free to contact me if I can be of any help. All the best,; Liis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205:133,depend,depend,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-466359205,2,"['depend', 'wrap']","['depend', 'wrappers']"
Integrability,"Hey, thanks for your reply!. I looked a bit around, and here is what the Seurat 3.1.4 docs say:. > Choose the features to use when integrating multiple datasets. This function ranks features by the number of datasets they appear in, breaking ties by the median rank across datasets. It returns the highest features by this ranking. from https://www.rdocumentation.org/packages/Seurat/versions/3.1.4/topics/SelectIntegrationFeatures. From this, I'd conclude that the current docs are correct, but in the sorting order of `_highly_variable_genes_seurat_v3` has it the wrong way around. Also, the test for the `_highly_variable_genes_seurat_v3()` method seems to assume that the method sorts the other way around than it currently does:. From within the method:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/preprocessing/_highly_variable_genes.py#L139-L144. From the test:. https://github.com/theislab/scanpy/blob/ca07fc12bbcd87e4cf67da56f52525a1e519711b/scanpy/tests/test_highly_variable_genes.py#L138-L151. So from this it seems save to say that the sorting order should be reversed in `_highly_variable_genes_seurat_v3()`..?!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402:131,integrat,integrating,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802052402,1,['integrat'],['integrating']
Integrability,"Hi @JonathanShor,. you don't need to create a custom API. One point of Scanpy is to provide convenient access via `anndata` to many single-cell packages around. The only thing needed for that is to provide a very simple interface like [this](https://github.com/theislab/scanpy/blob/master/scanpy/tools/phate.py#L8-L145) or [this](https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/mnn_correct.py#L4-L104) or several of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:220,interface,interface,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,1,['interface'],['interface']
Integrability,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:959,integrat,integration,959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,1,['integrat'],['integration']
Integrability,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds â€“ when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation â€“ I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then â€“ my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:308,integrat,integration,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916,2,['integrat'],"['integrated', 'integration']"
Integrability,"Hi @LuckyMD,. Many thanks for your comments. . The route via PCA followed by clustering & embedding (UMAP/tSNE) works perfectly fine for me. I have also got some interesting results from the analysis. Now, I want to try clustering cells with specific gene sets instead of the conventional dimensional reduction. Yes, I tried the following lines before:. ```; adata.obsm['X_geneset1`] = adata[:, ['gene1', 'gene2', 'gene3', 'gene4']].X; ```; It still says, KeyError: 'Indices ""[\'Ada\', \'Mustn1\', \'Mlc1\', \'Gfra\', \'Gm765\', \'Csrp2\', \'Socs2\', \'Dnajb9\']"" contain invalid observation/variables names/indices.'. All of these genes are present in my dataset. I am still trying to figure out why this is happening :/ ; Maybe, I will paste the short code snippet later. . P.S: Sorry for getting off the subject. Is there an alternative normalization step included apart from the log-normalization method? For example, TMM in edgeR & SCnorm- that uses quantile regression to calculate the dependence of read counts on sequencing depth for each gene (when count-depth relationship varies among genes).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552:51,rout,route,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552,2,"['depend', 'rout']","['dependence', 'route']"
Integrability,"Hi @LuckyMD,. Sure, I'll work on it, as time allows. Before however, I have a couple of questions. . 1. Do you want it as a separate .py file in the tools module (similar to _dendrogram.py)?; 2. I also found interesting to look at exclusive expression of one gene and not the other. Would you be interested in adding a function for that as well and if so, should be a separate one or somehow integrated with coexpression?; 3. Turning values into categorical works, however now I have problem that the True (coexpressing) cells are not always plotted on top. Do you know how to do it in scanpy? I tried by setting `pd.Categorical(ordered = True)`, however, that doesn't help. ; 4. Could you elucidate on how you want to implement the imputation methods? I've never used them myself. Is there anything available in scanpy already?. And thanks @flying-sheep for showing how to remove the colourbar. I wanted to do it for some of my other plots, so that really helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560:392,integrat,integrated,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/490#issuecomment-588132560,1,['integrat'],['integrated']
Integrability,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:1301,rout,route,1301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['rout'],['route']
Integrability,"Hi @PauBadiaM,. I have always viewed Dorothea and Progeny as methods to aid in the interpretation of my data. Hence, I would assume this might be most useful as a targeted approach to plot activity of a particular TF or pathway. This is something I would probably find most useful as a function where i can either ask for the activity of a single TF/pathway or to get the activity score that explains most variation/correlates with a particular PC. Hence I would err on the side of storing the activities in `.obsm` and then have some functionality around analysing which activity scores are most useful to a user. It will be hard for users to go through all of the data in the end for further analysis. You can always write a wrapper around things like `sc.tl.rank_genes_groups` where the `.obsm` data is copied into a new `adata_tmp.X` for rank genes groups output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079:727,wrap,wrapper,727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-794274079,1,['wrap'],['wrapper']
Integrability,"Hi @VladimirShitov . Thank you for the help (and the information about leiden vs UMAP). I think the code provided shows something slightly different. You are plotting False vs True here, but we would want something like False vs all. So, the True violin plot would be a little different. Regardless, I am just going to down this route :D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2485#issuecomment-1542320460:329,rout,route,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2485#issuecomment-1542320460,1,['rout'],['route']
Integrability,"Hi @Zethson I am the creator of Cirun.io, ""GPU"" and ""CI"" caught my eye. FWIW I'll share my two cents. I created a service for problems like these, which is basically running custom machines (including GPUs) in GitHub Actions: https://cirun.io/. It is used in multiple open source projects needing GPU support like the following:. https://github.com/pystatgen/sgkit/; https://github.com/qutip/qutip-cupy. It is fairly simple to setup, all you need is a cloud account (AWS or GCP) and a simple yaml file describing what kind of machines you need and Cirun will spin up ephemeral machines on your cloud for GitHub Actions to run. It's native to GitHub ecosystem, which mean you can see logs/trigger in the Github's interface itself, just like any Github Action run. Also, note that Cirun is free for Open source projects. (You only pay to your cloud provider for machine usage)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172:712,interface,interface,712,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1793#issuecomment-881043172,1,['interface'],['interface']
Integrability,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:482,message,messages,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['message'],['messages']
Integrability,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:73,interoperab,interoperability,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723,2,"['integrat', 'interoperab']","['integration', 'interoperability']"
Integrability,"Hi @chansigit,; If you want to store the raw counts before filtering out cells/genes you can also do this in `adata.raw`. We're trying to reduce the use of this... but it will allow you to store data in a different dimension. @ivirshup I guess use of scaling is up in the air. Some people like it, some people don't. I find it can be helpful for data integration/batch correction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943:351,integrat,integration,351,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596466943,1,['integrat'],['integration']
Integrability,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946:295,protocol,protocol,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946,1,['protocol'],['protocol']
Integrability,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-20-662b857d9182> in <module>; 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values; 13 ; ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas); 1908 ; 1909 if any_sparse:; -> 1910 sparse_format = all_adatas[0].X.getformat(); 1911 X = X.asformat(sparse_format); 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900:202,message,message,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-602964900,1,['message'],['message']
Integrability,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:67,integrat,integration,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136,1,['integrat'],['integration']
Integrability,"Hi @flying-sheep! is there a fix for this issue? (besides pinning scanpy version) ; as i understand it, matplotlib went back to legend_handles ([API](https://matplotlib.org/stable/api/prev_api_changes/api_changes_3.9.0.html#:~:text=Legend.legendHandles%20was%20undocumented%20and%20has%20been%20renamed%20to%20legend_handles.)) and now the legacy wrapper is breaking.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3102#issuecomment-2162507912:347,wrap,wrapper,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3102#issuecomment-2162507912,1,['wrap'],['wrapper']
Integrability,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390:124,interface,interface,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-484475390,2,['interface'],['interface']
Integrability,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261:395,integrat,integrate,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-431712261,1,['integrat'],['integrate']
Integrability,"Hi @giovp,; no worries, I hope you had a good TAC meeting! And thanks a lot for picking this up again, fixing the docs and also for starting the new issue on batch integration. I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. If there is anything I should look into, let me know - I have some time for this next week!; Best, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277:164,integrat,integration,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1049902277,2,"['integrat', 'message']","['integration', 'messages']"
Integrability,"Hi @grimwoo,. The data integration methods MNN and BBKNN are implemented in scanpy externals, which you can find [here](https://scanpy.readthedocs.io/en/stable/external/index.html#batch-effect-correction). You can also use combat correction, which is a simpler, linear batch effect correction approach implemented as `sc.pp.combat()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268:23,integrat,integration,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527337268,1,['integrat'],['integration']
Integrability,"Hi @grst, I had a superficial look at the functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Grea",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:912,integrat,integrated,912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['integrat'],['integrated']
Integrability,"Hi @grst,. We've been working on a best-practices workflow/tutorial for scRNA-sesq data for the past year. The tutorial is based on scanpy, but it also integrates tools from R. You can take a look at a case study which implements the workflow [here](https://github.com/theislab/single-cell-tutorial). The revised manuscript should be submitted this week as well, so I hope it will be out soon. If you like, I can send it to you, if you pass me your email address. Regarding regressing out covariates. @falexwolf has already mentioned that this is very dataset dependent. It is also dependent on the downstream analysis. This is especially true for the covariates you suggest. MT gene expression and cell cycle are both biological, rather than technical covariates. Regressing out biological covariates is generally done to isolate particular processes in the data that you are interested in, while losing global structure in the data. This is helpful especially for trajectory inference, but maybe less so for global exploratory analysis with clustering. For example, cell cycle stage can be a major determinant in the difference between two cell types (e.g. stem cells and proliferating cells like transit amplifying cells). Removing this effect, hides the distinction. MT gene expression is also a biological covariate (as well as a technical indicator of cell stress). Higher levels of MT gene expression can indicate increased respiration in e.g., asthmatic conditions. . An additional unwanted effect to regressing out biological covariates is that biological processes are not independent. Thus, regressing out one process, will partially remove effects of other, downstream processes as well. If you're interested, we can discuss this in more detail. Hope this helps a bit,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594:152,integrat,integrates,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526#issuecomment-471488594,3,"['depend', 'integrat']","['dependent', 'integrates']"
Integrability,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:36,depend,depends,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748,1,['depend'],['depends']
Integrability,"Hi @mrocklin, you might be interested in this work, especially the Dask-compatible wrapper around `scipy.sparse`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557095212:83,wrap,wrapper,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557095212,1,['wrap'],['wrapper']
Integrability,"Hi @pinin4fjords! I understand by integration, you mean access under the scanpy api. We try to advance the scanpy environment by modular extensions, which are packages with their own API, that also work on adata instances. This is currently what diffxpy is and there are no plans to collect all scanpy-related packages under `sc.*` as far as I am aware.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935:34,integrat,integration,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-884979935,1,['integrat'],['integration']
Integrability,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:484,integrat,integration,484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157,2,['integrat'],"['integrated', 'integration']"
Integrability,"Hi @saiteja-danda,. I cannot reproduce your code above as I don't have your data. Could you try to generate a minimal reproducible example with data from e.g., `sc.datasets.pbmc68k_reduced()`?. In general, could you check the output of `adata.obs['km'].value_counts()` to check whether the covariate you added was correctly stored? What output are you getting? You didn't share an error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885:387,message,message,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452#issuecomment-707656885,1,['message'],['message']
Integrability,"Hi @sygongcode,. Are you referring to differential expression testing between conditions? You can do that with `sc.tl.rank_genes_groups()` or in a more advanced way using `diffxpy`, which is easily integrated with `scanpy`. You can find it [here](https://github.com/theislab/diffxpy)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147:198,integrat,integrated,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821#issuecomment-529213147,1,['integrat'],['integrated']
Integrability,"Hi Ilan, thanks for your interest. One of the main advantages of Marsilea is the flexibility for layout plots and adding/removing components compared to the pre-defined visualization APIs in Scanpy. From my perspective, there could be two ways of integration: . 1) Reimplement some of the visualization APIs in scanpy using Marsilea, but we don't expose Marsilea to the user. You will always have a plot with known rendered size and fixed layout compared to directly using matplotlib. This could significantly reduce the code base complexities on the scanpy side, so less maintenance work. 2) Exposing the Marsilea API, we can create a visualization object that simulates the Marsilea API but is tailored specifically for `AnnData`. Maybe include some data transformation and aggregation functions that the user could directly apply during visualization. But this design doesn't make much difference compared to directly using Masilea as shown in the [notebook](https://scanpy.readthedocs.io/en/stable/how-to/plotting-with-marsilea.html).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693:247,integrat,integration,247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352637693,1,['integrat'],['integration']
Integrability,"Hi Isaac,. When I try to set n_comps equal to 2 (trying to do a diffmap in 2; components), I get an error message saying that it must be greater than 2.; I was wondering why?. On Sun, Jul 7, 2019 at 4:25 AM Isaac Virshup <notifications@github.com>; wrote:. > I'm not sure what you're asking about here. Could you provide a little; > more context?; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/675?email_source=notifications&email_token=AKIOHVNZFKCE63C4KLO45KTP6GSAPA5CNFSM4HSIFHXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZLG7PQ#issuecomment-508981182>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AKIOHVL7254C36JSP374JOTP6GSAPANCNFSM4HSIFHXA>; > .; >; -- ; Harvard-MIT MD-PhD Student; G1, Biophysics; Lander Lab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/675#issuecomment-508997047:106,message,message,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/675#issuecomment-508997047,1,['message'],['message']
Integrability,"Hi Lukas,. I am sorry, but what is a PR?. Best; Jin. á§. On Fri, Mar 10, 2023 at 5:02â€¯AM Lukas Heumos ***@***.***>; wrote:. > Would you be willing to file a PR for this? Thank you!; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1463561536>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFOKIW345IGREVLPLR7F6BLW3L32JANCNFSM6AAAAAAVQOJ5FE>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2436#issuecomment-1465318104:502,Message,Message,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436#issuecomment-1465318104,1,['Message'],['Message']
Integrability,"Hi Malte and Isaac, many thanks for this! Ah, yes that other issue was; opened after I opened this one. I did search for the error message before I; opened the ticket, but I didn't search again while the ticket was open. The easiest workaround for me is simply to not use .raw anymore, for a; pipeline, it's not really needed anyways. Yes, I can see why it's important for file backed data, I just cannot see a; use case for file backed mode either. Any useful operations on file backed; data will be too slow anyways for practical use, and anyone can get a; high-RAM machine these days on Amazon for a few hours, so I've always; wondered file backed mode exists. (sidenote: File backed data is again a; feature that sounds rather complicated to implement. As a user I love; libraries that are small, stable and don't change a lot, especially for; very foundational things like anndata. I guess it's a matter of development; philosophy here). Also, yes, it's because I don't use scanpy interactively; that I don't see the use case for views. anyhow, thanks again, also for all your work on Scanpy!. On Wed, Jul 31, 2019 at 6:27 AM Isaac Virshup <notifications@github.com>; wrote:. > I've just spent a while trying to replicate, before realizing I've seen; > this issue before over on AnnData (theislab/anndata#182; > <https://github.com/theislab/anndata/issues/182>). I've got some good and; > bad news about this. It's fixed on master, but that fix is slated to be; > release in v0.7, which has intentionally breaking changes.; >; > I find views very useful when dealing with large datasets interactively.; > They're also important for file backed data, since copies are extremely; > expensive in that case.; >; > Unlike numpy, AnnData objects should always return a view when subset. If; > you'd like to get copies, you could add a .copy() to the end of your; > subsetting statement.; >; > â€”; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578:131,message,message,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516740578,1,['message'],['message']
Integrability,"Hi Pavlin, Happy New Year!. Great to see this going forward!. > We'd probably have to do something similar to the gauss option and just overwrite the UMAP weights after the fact. Does this sound reasonable?. Yes, I also thought we'd need to do it similar to the `gauss` option. Can one use openTSNE code for computing perplexity-based weights or would one need to copy the binary search in here? I think it may be possible to use openTSNE's function to compute the affinities and then get the weights out of there?. One question here though: how would `tsne` function know if it should use the uniform kernel or the weights constructed by the `neighbors` function? Can some flag be switched if `neighbors` is run with `mode='tsne'` so that the `tsne()` function later on uses those weights? Not sure what's the best interface here. Alternatively the `tsne()` function could check if the weights conform to what t-SNE expects (sum to 1). Maybe that's better actually. Apart from that, I noticed that you implemented ; ```; class UniformAffinities(openTSNE.affinity.Affinities):; ``` ; in here, but isn't it part of `openTSNE` already?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211:816,interface,interface,816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-753488211,1,['interface'],['interface']
Integrability,"Hi Scott!. Sorry that I didn't see this before writing the following email this moring:. > Sorry about forgetting to mention this in the first email below (just thought I'd send you the link to the most recent wrapper). The tSNE tool is a wrapper, too, and also has an example for a visualization function; > https://github.com/theislab/scanpy/blob/master/scanpy/tools/tsne.py; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/tools/__init__.py#L426-L518; > You'll find the corresponding pair in the docs. If your wrapper parallelizes this, this will make it very easy for Scanpy users to use out PHATE. Scanpy's toplevel API does - on purpose - not feature objects, but convenience functions. If people want to do more complicated stuff, they should directly use the PHATE package, in this case. So, it would be nice if you'd only provide the convenience function - which is already very similar to what I pasted about tSNE. Also, please remove phate from the requirements... and raise a corresponding ImportError in your convenience function. Scanpy has quite some optional dependencies and Phate will be one of them. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811:210,wrap,wrapper,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385959811,4,"['depend', 'wrap']","['dependencies', 'wrapper']"
Integrability,"Hi Sergei,. Thank you very much for your fast reply. Do you mean I can still use the latest version of scanpy but installing a lower version of umap?. I tried different versions of scanpy, including pastiest, stable, 1.4.5, 1.4.5post3, 1.4.4post1... They seem to either have different error messages or packages not compatible. Do you know which version of the scanpy has it fixed?. Thank you for your kind help. Best regards,. Lirong. èŽ·å– Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; å‘ä»¶äºº: Sergei R. <notifications@github.com>; å‘é€æ—¶é—´: Wednesday, April 22, 2020 12:44:36 PM; æ”¶ä»¶äºº: theislab/scanpy <scanpy@noreply.github.com>; æŠ„é€: plrlhb12 <lrpeng@hotmail.com>; Mention <mention@noreply.github.com>; ä¸»é¢˜: Re: [theislab/scanpy] Issue with ingest (#1181). Hi, @plrlhb12<https://github.com/plrlhb12> .; Yes, this is a known problem. The easiest fix for now is to use umap 0.3.9 instead of 0.4.1.; You can also install scanpy from github where it is fixed or just wait for a new scanpy release. â€•; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/1181#issuecomment-617895856>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKHCBZKH4TYT5672QNGFW33RN4NHJANCNFSM4MNX44PA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861:291,message,messages,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1181#issuecomment-617911861,1,['message'],['messages']
Integrability,"Hi Sidney,. Thanks for the pull request. igraph and louvain are kind of heavy dependencies (e.g. takes long time to compile them and they're not easily available via PyPI for all platforms etc.), this is why they are excluded from requirements file. It's written in the [installation document](https://scanpy.readthedocs.io/en/latest/installation.html) that these need to be installed manually. Also, there should be proper error messages stating that these must be installed separately when their functionality is needed for a function in scanpy and cannot be found. Did you get any other error regarding these packages?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101:78,depend,dependencies,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-397543101,2,"['depend', 'message']","['dependencies', 'messages']"
Integrability,"Hi all, thanks for the mention @andrea-tango! If have been making multiple changes in diffxpy and batchglm recently, the following refers to the branch diffxpy dev, I haven' merged all of this into master yet as I am waiting for some last issues to be fixed. . 1. Are diffxpy's tests correct: We are using unit tests to check a) that all tests fullfill the null model. b) for standard tests that do not require model fits (Welch's t-test and rank sum test) we check that we produce the same results as scipy. We vectorise where possible but we do actually directly call scipy in the rank sum test right now. Both of these tests check out on the dev branch for the rank sum test, so this is working correctly. @falexwolf this might be a unit test that you could also introduce? I did not see this in the ones that you linked but I just glanced over. @andrea-tango please use dev right now. 2. Nature of the rank sum test. We previously called the Wilcoxon rank sum test de.test.wilcoxon, note that this is also alternatively named Mann-Whitney U test (MWU). Importantly, MWU is for independent samples, which we always have in scRNAseq, the ""wilcoxon"" test in scipy is for paired samples. We therefore renamed this to de.test.rank_sum to avoid confusion. Which one are you using @falexwolf? . 3. Comparison scanpy vs diffxpy (in unit tests). I would discourage this and compare against scipy because I would consider scipy the gold standard for statistical testing. I can run comparisons if the above comments do not resolve all issues discussed here, which would imply that we might differ in our wrapping in the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471487617:1597,wrap,wrapping,1597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471487617,1,['wrap'],['wrapping']
Integrability,"Hi all,. I am trying to use ScanPy for integrating multiple scRNA-Seq samples (~20). Doing so that I can look at RNA Velocity with SCVelo, and want to use MNN as I got good batch effect removal previously in monocle using MNN. Is it true - as stated above, that the current implementation of mnncorrect with ScanPy is only operating on expression values? I have run through a ScanPy MNN [tutorial ](https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html) provided by NBI Sweden. The results are improved, but it doesn't appear to work as well as in monocle - some separation by batch is still going on. . I'm wondering what the difference might be? Whether it could be due to the difference in PCA (multi-batch), or the actual MNN / batch effect removal step. Alternatively, I could use the corrected expression matrix, and add the UMAP coordinates/clusters from monocle, although I wonder if this is advisable. . If you have any info please let me know, or if I should raise a separate issue etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967:39,integrat,integrating,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734319967,1,['integrat'],['integrating']
Integrability,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:200,depend,dependencies,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,1,['depend'],['dependencies']
Integrability,"Hi! Harmony does not adjust the counts matrix at all, or even read it. Rather, it takes principal components as input, adjusts them to correct for batch, and outputs a modified set of PCs. Here is the paper that explains how it works: https://www.nature.com/articles/s41592-019-0619-0. (I wasn't involved in coming up with this method or implementing it, just making it interoperable with scanpy, for the record)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240108377:370,interoperab,interoperable,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240108377,1,['interoperab'],['interoperable']
Integrability,"Hi! I think we have a different focus here, and not all of what you stated as fact is correct, so Iâ€™ll do my best to clear this up:. 1. There is an advantage for type hints in common Scanpy usage. IPython should use Jedi to create autocompletions since this summer, but they forgot to reenable it. I sent them an issue to do so, ipython/ipython#11503 and a fix in ipython/ipython#11506. Jedi supports type hints, so with `c.Completer.use_jedi = True` now or by default in a month, people will profit from them. Furthermore, people are using scanpy in applications and scripts, not just in notebooks. When you use an IDE (or install the jedi extension in EMACS) you should profit from it. 2. The Jupyter shift-tab help being hard to read in the presence of type hints is what I consider a bug. I reported it in ipython/ipython#11504 and fixed it in ipython/ipython#11505. 3. The numpy is on it (see [here](https://github.com/numpy/numpy-stubs)) and will probably integrate it once there needs to be no Python 2 compat. e.g. scikit-learn waits for numpy: scikit-learn/scikit-learn#11170. I see your concern about entry hurdles, but I donâ€™t agree. Itâ€™s super easy. `Union` is â€œorâ€, `Optional` is â€œor `None`â€. If thereâ€™s questions, they can be answered. (or people click on the links in the docs and read like one sentence of explanation). 4. If you want we can change how all that is rendered. `Union[a, b]` could be done as ``` :class:`a` or :class:`b` ``` But itâ€™s really not hardâ€¦. Honestly I think the `Callable[â€¦]` is much better than the textual description that was there before: Until it was there, people (including me when i was writing that annotation) had to dive into the code to figure out what function signature is *really* expected there. Now they have to be able to parse what that `Callable[[a,b], c]` there means. If they have never encountered it before, they can click on it, read one sentence of explanation and know that `a` and `b` are parameters and `c` the return type. Done in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581:962,integrat,integrate,962,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-440619581,1,['integrat'],['integrate']
Integrability,"Hi! I've tried first the 4th point from @atarashansky and yeah, multiplying the distance, connectivities, or both with a normal distribution with std 0.0001 (which makes roughly a 0.008% of absolute difference in the matrix) and the results are different. . Python code:; ```python; c0 = adata.uns['neighbors']['connectivities']; connect = adata.uns['neighbors']['connectivities'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['connectivities'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. ```python; c0 = adata.uns['neighbors']['distances']; csum = adata.uns['neighbors']['distances'].todense().sum(); connect = adata.uns['neighbors']['distances'].todense(); connect = np.multiply(connect, np.random.normal(1, 0.0001, connect.shape)); adata.uns['neighbors']['distances'] = spr.csr_matrix(connect); print(100*np.sum(np.abs(c0 - connect))/min(np.sum(c0), np.sum(connect))); ```. Before; ![image](https://user-images.githubusercontent.com/35657291/73247960-52f51900-41b2-11ea-937c-c09e301c5e7a.png). After; ![image](https://user-images.githubusercontent.com/35657291/73247968-55f00980-41b2-11ea-9d44-8e4cef0149d5.png). I mean, the results (hopefully) are not drastically different, but there are minor rearrangements and clustering assignments that might make the analysis be rearranged depending on the computer. . I've realized that, for some reason, the differences between points are not associated to different random seeds, that is, after setting the seed, the PCA look equal but are different in less than 0.001 when doing a pairwise comparison. This happens as well to the neighbors graph, so I believe that randomness is not really an issue. However I am puzzled as to why the differences in PCA / neighbour graphs is so small. Shouldn't two different computers do calculations equally at least at 3 or 4 decimals?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312:1410,depend,depending,1410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-579254312,1,['depend'],['depending']
Integrability,Hi! Iâ€™ve wanted to introduce https://github.com/flying-sheep/legacy-api-wrap for some time. do you think that would be sufficient or should we take the deprecation of kwargs into account?. Thereâ€™s also tantale/deprecated#8,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-470910941:72,wrap,wrap,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-470910941,1,['wrap'],['wrap']
Integrability,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:435,depend,depending,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337,1,['depend'],['depending']
Integrability,"Hi!. > > > for normalize_pearson_residual, i think it makes sense to keep normalize in, as it's not the same type of transformation compared to log1p.; > ; > > Isn't this quite similar to what log1p does though? In that it's a transformation of the matrix?; > ; > I think it should stay `normalize_pearson_residuals` because it mirrors `normalize_total`. I agree. > ; > for the rest, I think we are at a good stage, I'd ask @jlause to build docs locally `cd scanpy/docs` and then `make clean` and `make html` see https://scanpy.readthedocs.io/en/stable/dev/documentation.html#building-the-docs and check that:; > ; > * arguments and doc params match; > ; > * typo and other minor issues still present (e.g. difficult phrasing). I started doing that and will finish up tomorrow - there Qs in advance if you happen to look at this before:. - sometimes we have math expressions like var = mean * mean^2 etc. in the docs. Is there a convention for scanpy docs if those should be in `code` format or just plain text? e.g. in the adata docstring the matrix shape is described as `n_obs` x `n_var`, but elsewhere we say ""clipping is done by sqrt(n). I can consistently format them into `code` if you agree.; - I think the `.._pca` function is missing from the release note. should I add it there?; - The `..pca` function also did not use shared docs params yet. I started adding them and can commit tomorrow - is that okay if I just do it like that?. > ; > ; > if this gets approval, before merging to master todo:; > ; > * [x] add release note; > ; > * [ ] go over scanpy_tutorials and re run tutorial and merge. I've looked at that and commented in the respective github :). Very happy we are getting this wrapped up now :); Best! Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395:1701,wrap,wrapped,1701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1065345395,1,['wrap'],['wrapped']
Integrability,Hi!. If you used a neural network approach you could use scArches to leverage transfer learning to map things across without re-integrating (only minimal additional training done there). You could also map into the embedded space using `sc.tl.ingest` for example. But there is always the danger that there is a residual batch effect that cannot be removed without de-novo integration.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384:128,integrat,integrating,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1055535384,2,['integrat'],"['integrating', 'integration']"
Integrability,"Hi!. Thereâ€™s a series of abstract base classes (other languages call them interfaces) that can be used if you know specifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, weâ€™d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. Itâ€™s fine if you donâ€™t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:74,interface,interfaces,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438,2,"['depend', 'interface']","['depending', 'interfaces']"
Integrability,"Hi!; Sorry for that, the command-line interface got a bit behind. I fixed everything, simply pull again.; Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113:38,interface,interface,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/30#issuecomment-322022113,1,['interface'],['interface']
Integrability,"Hi, ; I'm having similar issues in using sc.tl.ingest(adata, adata_ref, obs='louvain').; I have updated my Scanpy 1.4.6 and anndata to 0.7.1.; I'm getting the following error message.; ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-71-27e22cc8f823> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='louvain'). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 119 ; 120 for method in embedding_method:; --> 121 ing.map_embedding(method); 122 ; 123 if obs is not None:. ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in map_embedding(self, method); 407 """"""; 408 if method == 'umap':; --> 409 self._obsm['X_umap'] = self._umap_transform(); 410 elif method == 'pca':; 411 self._obsm['X_pca'] = self._pca(). ~/opt/anaconda3/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _umap_transform(self); 396 ; 397 def _umap_transform(self):; --> 398 return self._umap.transform(self._obsm['rep']); 399 ; 400 def map_embedding(self, method):. ~/opt/anaconda3/lib/python3.7/site-packages/umap/umap_.py in transform(self, X); 2006 try:; 2007 # sklearn pairwise_distances fails for callable metric on sparse data; -> 2008 _m = self.metric if self._sparse_data else self._input_distance_func; 2009 dmat = pairwise_distances(; 2010 X, self._raw_data, metric=_m, **self._metric_kwds. AttributeError: 'UMAP' object has no attribute '_input_distance_func'; ```. Appreciate your comments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541:175,message,message,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1092#issuecomment-623064541,1,['message'],['message']
Integrability,"Hi, ; Thank you very much for such a detailed explanation. It really helps. I've two more questions: . 1). Can we do this gene subsetting with Logistic regression (where no multiple testing correction is involved)? . 2). Since you nicely pointed out sc.tl_rank_genes_groups doesn't tell about the contribution of genes in the clustering- are there tools that can be integrated with ScanPy to do this job? (for example, diffxpy or MAST). I'm really interested in the differential gene testing to predict the markers (from a gene subset used for clustering). . I shall be grateful if you can suggest a method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575:366,integrat,integrated,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515114575,1,['integrat'],['integrated']
Integrability,"Hi, I've now implemented many scanpy plots using Marsilea in the notebook. Please let me know what you think of it and if you have any further questions or requests that could be useful for the community. Looking forward to integrating this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007:224,integrat,integrating,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2084906007,1,['integrat'],['integrating']
Integrability,"Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483:62,wrap,wrapper,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590009483,1,['wrap'],['wrapper']
Integrability,"Hi, please create a [minimal reproducible example][mre], including a complete stack trace of the error. Mentioning which data set you used and the error message alone does not help us to figure out what happened, as we lack the context of both the steps you took that lead up to the error and where exactly in the code the error happened. [mre]: https://stackoverflow.com/help/minimal-reproducible-example",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3261#issuecomment-2376233635:153,message,message,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261#issuecomment-2376233635,1,['message'],['message']
Integrability,"Hi, please provide the data you use, otherwise this is not reproducible:. ```pytb; FileNotFoundError: [Errno 2] Unable to open file (unable to open file: name = '\external/CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845023488:271,message,message,271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778#issuecomment-1845023488,1,['message'],['message']
Integrability,"Hi, thanks for the report!. Note that the plots in the documentation are generated on the fly when building the documentation. The plot you currently see on https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.dotplot.html has therefore been created with scanpy 1.10.1. Must be a dependency issue, Iâ€™ll try to reproduce with the environment you provided. /edit: I can reproduce it with that environment:. <details><summary>environment.yml</summary>. ```yaml; name: scanpy-3062; channels:; - conda-forge; dependencies:; - ipykernel. - python==3.10.10; - anndata==0.10.7; - scanpy==1.10.1; - IPython==8.13.2; - pillow==10.0.0; - astunparse==1.6.3; - backcall==0.2.0; - cffi==1.15.1; - cloudpickle==2.2.1; - colorama==0.4.4; - cycler==0.10.0; - cytoolz==0.12.0; - dask==2023.10.1; #- dateutil==2.8.2; - decorator==5.1.1; - defusedxml==0.7.1; - dill==0.3.6; - entrypoints==0.4; - exceptiongroup==1.1.1; - executing==1.2.0; - fasteners==0.17.3; - gmpy2==2.1.2; - h5py==3.8.0; #- icu==2.11; - python-igraph==0.11.2; - jedi==0.19.1; - jinja2==3.1.2; - joblib==1.2.0; - kiwisolver==1.4.4; - leidenalg==0.10.2; - llvmlite==0.42.0; - lz4==4.3.2; - markupsafe==2.1.2; - matplotlib==3.8.3; - mpmath==1.3.0; #- msgpack==1.0.5; - natsort==8.3.1; - numba==0.59.1; - numcodecs==0.11.0; - numexpr==2.7.3; - numpy==1.26.4; - packaging==23.1; - pandas==1.5.3; - parso==0.8.3; - pexpect==4.8.0; - pickleshare==0.7.5; - plotly==5.14.1; - prompt_toolkit==3.0.38; - psutil==5.9.5; - ptyprocess==0.7.0; - pure_eval==0.2.2; - pyarrow==10.0.1; - pydot==1.4.2; - pygments==2.15.1; - pyparsing==3.0.9; - pytz==2023.3.post1; - scipy==1.13.0; #- session_info==1.0.0; #- setuptools==67.7.2; - six==1.16.0; - scikit-learn==1.2.2; - stack_data==0.6.2; - sympy==1.11.1; - tblib==1.7.0; - texttable==1.6.7; - threadpoolctl==3.1.0; #- tlz==0.12.0; - toolz==0.11.2; #- pytorch==2.1.1; - tqdm==4.65.0; - traitlets==5.9.0; - wcwidth==0.2.6; #- yaml==5.4.1; - zarr==2.14.2; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516:287,depend,dependency,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2114986516,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Hi,. If `adata` is your anndata object, you can get the expression data from `adata.X`. Depending on what you did to the object before, that will contain differently pre-processed data. If you want it for ""Gene A"", then you can use `adata[:,'Gene A'].X`to get the expression value of Gene A in all cells. You can of course put that into a pandas dataframe via `pd.DataFrame()`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/870#issuecomment-541467843:88,Depend,Depending,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870#issuecomment-541467843,1,['Depend'],['Depending']
Integrability,"Hi,. If you solely `pip install scanpy` it will use the latest versions of both, Scanpy and umap-learn. These are certainly compatible. Scanpy 1.7.2 not being perfectly compatible with the latest umap-learn package is an artifact of us not pinning the dependencies too hard and being more on the lenient side. I'd suggest to simply use the latest versions of both packages and you should not run into any problems.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568:252,depend,dependencies,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2101#issuecomment-1005944568,1,['depend'],['dependencies']
Integrability,"Hi,. The issues I was mostly running into were that when saving the anndata; variable as a h5ad file, 'pheno_jaccard_ig' was not compatible with this; action. So, I had to either remove pheno_jaccard_ig from the anndata object; and then save it as h5ad or convert it to a sparse matrix. This also; happened with a few other functions I tried on the anndata object, and I; kept getting the error ""this function is not compatible with COO matrix; format"", always talking about pheno_jaccard_ig. Therefore, since a sparse; matrix object does not have any problems with the functions I was running; on adata, changing pheno_jaccard_ig to a sparse matrix from the start makes; sense to circumvent any of those issues I was getting before. I hope this makes sense.; Thank you,; Deena Shefter. On Wed, Jul 27, 2022 at 6:10 PM Lukas Heumos ***@***.***>; wrote:. > Hi,; >; > could you please provide more details? What issues did you run into?; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/pull/2295#issuecomment-1197424392>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMILUOEK7J64GU3YOR7DB53VWGXULANCNFSM534YT5ZA>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180:1249,Message,Message,1249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180,1,['Message'],['Message']
Integrability,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928:307,integrat,integrated,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928,1,['integrat'],['integrated']
Integrability,"Hi,. different versions of dependencies can lead to such changes. This is nothing major, don't worry about this. The general embedding structure will always be very similar though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2471#issuecomment-1513322610:27,depend,dependencies,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471#issuecomment-1513322610,1,['depend'],['dependencies']
Integrability,"Hi,. thanks for the input and the nice description. Agree that this would be nice to have within scanpy itself. And agree, as youâ€™ve shown in your example, that this should be the case when setting `copy=True`. I have set up a draft PR for the moment, with a suggestion where the error message especially persists when `copy=False`; Iâ€™d suggest to keep it this way:. - Overwriting the backed file seems not to be the expected behaviour to me; - Writing a new file for backing would occur in a rather hidden manner, confusing the user or even unexpectedly further fill the disk at worst over time",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2495#issuecomment-1683604054:286,message,message,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495#issuecomment-1683604054,1,['message'],['message']
Integrability,"Hi,; It looks like this code comes from the [single-cell-tutorial github](https://github.com/theislab/single-cell-tutorial). It might be best to report the issue there. It looks like you haven't filtered out genes that are not expressed in your dataset via `sc.pp.filter_genes()`. If you filter the dataset (maybe with `min_cells` set to 5-50, depending on the size of your dataset), then this shouldn't happen. Here, you have too many genes with the same mean, which is very low.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630:344,depend,depending,344,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560#issuecomment-753982630,1,['depend'],['depending']
Integrability,"Hi,; Your question is actually quite difficult to answer. It depends on how you define cell identity. For example, depending on how you set the resolution parameter for your clustering you can get a very different number of clusters. As clusters often have super- and sub-structure, you cannot easily say when a sub- or superstructure is not meaningful (e.g., T-cells vs CD4+ and CD8+ T-cells). Unfortunately many clustering techniques do not incorporate an assessment of uncertainty to tell you when you are fitting noise. And even when they do, this is based on a model of what a cell cluster should look like, which does not have to conform to the biological reality. The heuristic that tends to be used is that if you can biologically interpret your clusters based on marker genes and other signatures, then they are meaningful. That does however not mean that sub- or superstructures involving these clusters are not also meaningful. Sorry, I can't give a clearer answer than that. In terms of assessing differences between clusters, this is a somewhat related problem. An approach that you could use is to look at the distribution of Euclidean distances in gene- or PCA-space. However, that comes with the caveat that Euclidean distance does not take into account the biological manifold on which the cells lie (and on which distances will be more informative). You can try to use pseudotime as a measure for distance over this manifold, however that would require you to have sampled enough of the manifold to fit it in your data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874:61,depend,depends,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/491#issuecomment-464778874,2,['depend'],"['depending', 'depends']"
Integrability,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:74,integrat,integration,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806,1,['integrat'],['integration']
Integrability,"Hi. While @flying-sheep is right that this is a question for the discourse group, I have a quick response here. Check out the tutorials here:; https://scanpy.readthedocs.io/en/stable/tutorials.html. You can merge anndata objects with `anndata.concatenate()`, and if you're looking for data integration methods, there are a couple of them in `scanpy.external`. For example MNN (works better in `mnnpy` though), and BBKNN. Scanorama also has a nice scanpy interface... and I think `SCVI` also has a scanpy integration tutorial (but can be harder to use than scanorama).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554:290,integrat,integration,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-545962554,3,"['integrat', 'interface']","['integration', 'interface']"
Integrability,"Hm, it is still required to follow the internals of umap to reconstruct UMAP object from anndata info. Also this adds dependency on pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629:118,depend,dependency,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1038#issuecomment-585107629,1,['depend'],['dependency']
Integrability,"Hmm, doesnâ€™t seem to work with `functools.partial`. It works if I do almost the same, but with a lambda:. ```py; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(lambda *args, **kw: sc.pl.scatter(*args, basis=""pca"", **kw)); ```. But I think the custom solution above is better anyway! `wraps` is if you really want to pose as the wrapped function, while we only want to inherit its signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934:156,wrap,wraps,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934,4,['wrap'],"['wrapped', 'wraps']"
Integrability,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:168,integrat,integration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638,1,['integrat'],['integration']
Integrability,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:355,integrat,integrated,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636,1,['integrat'],['integrated']
Integrability,"How is this work going? We'd love to integrate Scrublet into our workflows, which are currently quite Scanpy-centric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991:37,integrat,integrate,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545010991,1,['integrat'],['integrate']
Integrability,"How would you change it? I'd probably just wrap the section that says ""paste here"" with a `<details>` tag.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228:43,wrap,wrap,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666268228,1,['wrap'],['wrap']
Integrability,"How would you suggest doing the API for this? Another `kwarg` for backend?. The additional dependencies aren't so bad. They are `xarray`, `dask`, and `pillow`. But still, I probably wouldn't be up for data shader as a required dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2656#issuecomment-1709960901:91,depend,dependencies,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656#issuecomment-1709960901,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Huh, that is weird. Also weird that it's including every optional dependency by default. Any chance you know if there's a way to not do that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1046710380:66,depend,dependency,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1046710380,1,['depend'],['dependency']
Integrability,"I agree that it's bad behavior to modify state on import. I think it's worse to modify state after a function is called, save a few cases where it's obvious that will happen. I think it takes less time to figure out why my plot suddenly looks different if it's based on imports than which functions were called prior. I think if we could make all of our plots without importing `pyplot` that would be great. I'm not sure how feasible this is. Not only do we use `pyplot` a lot, but libraries we depend on for plots (like `seaborn`) import `pyplot`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-523738527:495,depend,depend,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523738527,1,['depend'],['depend']
Integrability,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031:66,integrat,integration,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614#issuecomment-485875031,1,['integrat'],['integration']
Integrability,"I also experienced this a few times, and took me some time to understand what is going on. I fully agree with @ivirshup, we should improve the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925:149,message,message,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1504#issuecomment-748161925,1,['message'],['message']
Integrability,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:420,message,message,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['message'],['message']
Integrability,"I am more and more convinced about having a single package for the reasons @adamgayoso mentioned. To address a few concerns from above: . ---. > > Who manages the sub-packages?; > ; > Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). Scverse core developers could take turns (e.g. every 6 months) in being ""lead maintainer"", i.e. in charge of releases and first-responders to issues (delegating them to the most appropriate people). This has the additional advantage that everything needs to be documented to a point that there can't be a single point of failure. . ---. > Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. ```; pip install scio[all]; ```. could be broadly advertised in the README. Packages could still use the slimmer version, e.g. in scirpy, I could depend on ; `scio[vdj]`. . ---. > I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult. I think we should aim at having one obvious ""right way"" to represent something with AnnData and MuData. A common `scio` package could be a way to achieve that. . > I know squidpy will be changing its representation and I think muon should have changes to the ATAC representation. Also muon and scvi-tools read in different things from 10x atac data. A solution to that would be versioned schemata. E.g. whatever squidpy uses now is the ""spatial schema `v1`"". When we come up with a better way it becomes the ""spatial schema `v2`"". Old schemata will be deprecated but can stick around for a while. If a schema is experimental and subject to active changes it can be `v0.1`. . ```python; scio.spatial.read_vis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261:916,depend,dependencies,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059727261,1,['depend'],['dependencies']
Integrability,I am not sure what king of test. I don't want to add another `save_and_compare_images` test because plots seem to depend on the system at least sometimes (i have 3 failing plotting tests locally but they run fine here).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310:114,depend,depend,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-878118310,1,['depend'],['depend']
Integrability,I am using 1.4.3. I tried to upgrade to 1.4.4 but I was having a lot of problem with dependencies. Wonder it 1.4.3 is recent enough? Thank you so much.; scanpy 1.4.3 py_0 bioconda,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477:85,depend,dependencies,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-544294477,1,['depend'],['dependencies']
Integrability,"I am using Anaconda/Jupyter in my PC. When I try to downgrade numba, I run into issues of numba dependency packages in Anaconda so I am stuck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957:96,depend,dependency,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341#issuecomment-670191957,1,['depend'],['dependency']
Integrability,"I can apply pretty easily scrublet from the original python package, so I; guess a wrapper should be something fast to implement :) I was thinking; about doing it last week, but I am no expert in this kind of stuff :/. Den tir. 14. maj 2019 kl. 20.18 skrev Alex Wolf <notifications@github.com>:. > I guess, we should ask @swolock <https://github.com/swolock>. ðŸ™‚; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UJZJVVP2VIQXNRHEITPVL7A3A5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVMK5PQ#issuecomment-492351166>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UN3NQVYPI4KPDUAOK3PVL7A3ANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492392283:83,wrap,wrapper,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492392283,1,['wrap'],['wrapper']
Integrability,"I can confirm this as a working workaround. Thank you @michalk8 . > @pati-ni; > I have the similar issue when installing CellRank as `conda install -c bioconda cellrank`. Problem is not all dependencies are on bioconda, some of them are on `conda-forge` - I've tested it and I have the same problem with scanpy.; > Installing it as `conda install -c bioconda -c conda-forge scanpy` works. But @ivirshup is right, seems like conda issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011:190,depend,dependencies,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-662450011,1,['depend'],['dependencies']
Integrability,"I can understand your thought process behind facilitating the integration of anndata into the broader ecosystem and I can also understand the frustration. I don't think the integration is quite as bad as you suggest though. `adata.X` is still a `numpy.ndarray` and can be used as such, exactly as `adata.var` and `adata.obs` are dataframes. The only issue is when you require the object to work as a whole data structure in a particular function. I'm not the most experienced `numpy` user, but from what I've seen, you would typically expect any `numpy` function that you apply to an `anndata` object to be applied to `adata.X` and don't require information in other parts of the object. Or am I missing a use case here? So the only change would then be that `adata = np.srqt(adata)` would need to become `adata.X = np.sqrt(adata.X)`. Furthermore, it's not entirely clear what a `numpy` function applied to an `AnnData` object should do. `np.min()` could be on `adata.X` or any column in `.obs` or `.var`. You can call it on the columns in the `pandas` dataframes already via `pandas` conventions... which makes a bit more sense to me. Regarding the slicing conventions... @ivirshup has mentioned a few reasons why things are sliced as they are in `scanpy`. What would your suggestion look like? `loc` and `iloc` work for `adata.obs` and `adata.var` atm. Would you forbid an `adata['Cell A',:]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922:62,integrat,integration,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584118922,2,['integrat'],['integration']
Integrability,I canâ€™t reproduce this locally and would like to get the dependencies unlocked. Whereâ€™s the bug report at h5py?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/822#issuecomment-577743290:57,depend,dependencies,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/822#issuecomment-577743290,1,['depend'],['dependencies']
Integrability,"I completely agree that including the R/scran requirements will be troublesome and harms user experience. The reason I used a R-py interface is that there's no decent MNN correct on python yet, and scran's implementation is already fast and efficient enough, and I think this is meant to be an optional feature that provides a handy fix for those in need. Personally I would prefer if you guys create a submodule _rtools_, and put wrappers inside. This is going to be awesome to use and easy to maintain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082:131,interface,interface,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002082,2,"['interface', 'wrap']","['interface', 'wrappers']"
Integrability,"I completely understand but Iâ€™m travailing and have no access to my data. but itâ€™s the basic function and you can use any anndata object and the groupby argument on any observation. Sent from my iPad. On 5. Jan 2023, at 15:11, Lukas Heumos ***@***.***> wrote:. ï»¿. By a minimal working example I mean something that we can copy and paste and reproduce your result directly. The easier you make it for us the more likely we can dedicate some time to look at your issue. Thanks. â€”; Reply to this email directly, view it on GitHub<https://github.com/scverse/scanpy/issues/1988#issuecomment-1372263677>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AOGNBODKUC3S3SZC32VD3TLWQ3JBVANCNFSM5DB5VLAQ>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439:767,Message,Message,767,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1988#issuecomment-1372293439,1,['Message'],['Message']
Integrability,"I copied your code to a google colabs instance and ran into a Type Error similar to the one above:; https://colab.research.google.com/drive/1LYxOAuNqaJHGfRjNjyluUHk9BFsmkWa4?usp=sharing . Error message:; ```; TypeError Traceback (most recent call last); <ipython-input-3-9abce68d1753> in <module>(); 4 sc.tl.dpt(adata); 5 sc.tl.paga(adata, groups='paul15_clusters'); ----> 6 sc.pl.paga_path(adata, nodes=['1Ery'], keys=['Gata2', 'Btg2', 'Btg1']). 5 frames; /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in set_data(self, A); 697 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):; 698 raise TypeError(""Invalid shape {} for image data""; --> 699 .format(self._A.shape)); 700 ; 701 if self._A.ndim == 3:. TypeError: Invalid shape (3, 43, 1) for image data; ```. Versions: ; ```; scanpy==1.7.0 ; anndata==0.7.5 ; umap==0.5.0 ; numpy==1.19.5 ; scipy==1.4.1 ; pandas==1.1.5 ; scikit-learn==0.22.2.post1 ; statsmodels==0.10.2 ; python-igraph==0.8.3 ; leidenalg==0.8.3; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671:194,message,message,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/953#issuecomment-778212671,1,['message'],['message']
Integrability,"I created [legacy-api-wrap](https://github.com/flying-sheep/legacy-api-wrap). Only caveat: Scanpy is still 3.5 compatible, and Iâ€™m using f string syntax in it and its dependency [get_version](https://github.com/flying-sheep/get_version), so itâ€™s python 3.6 only (which could be circumvented via [future-fstrings](https://github.com/asottile/future-fstrings) or so)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460:22,wrap,wrap,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441684460,3,"['depend', 'wrap']","['dependency', 'wrap']"
Integrability,I did successfully import all three dependencies and (just to make sure) still cannot import scanpy.; ```; >>> import numba; >>> import pynndescent; >>> import umap; >>> import scanpy; Illegal instruction (core dumped); ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190:36,depend,dependencies,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-985144190,1,['depend'],['dependencies']
Integrability,"I didn't use any filtering before the pipeline, but I read the barcode and gene names from index and columns of the data frame. and i have a cuff-off of the cell for my own. updateâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”; The calculation is depends on module ""patsy"" and it is not in the dependency list of scanpy or I have some problems with installing that package. ; After I reinstalled and updated ""pasty"", problem fixed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918:215,depend,depends,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212#issuecomment-407454918,2,['depend'],"['dependency', 'depends']"
Integrability,"I don't have `pytest` installed locally (will change that), and the plan was to emulate the Travis python 3.5 environment, but I'm not sure what versions of all the dependencies are in there. I've been debugging in a notebook, but it always works there... at least with python 3.6. I'll try just creating a conda python 3.5 env to see what happens when I do that. Chances are it will always work locally as well though... hence my remote debugging. Sorry for that... Previous print statements have shown that the order of covariates is just different sometimes in the recarrays. So I thought it would all be fixed with 5b602f5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140:165,depend,dependencies,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583#issuecomment-479462140,1,['depend'],['dependencies']
Integrability,"I don't know how to fix this in the code, but I do find a workaround to specify a color palette. Turns out both `sc.pl.umap()` and `sc.pl.scatter()` accept a color palette if it's from `seaborn` color palette:. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette='Set3'); ```. throws the above error message. But if I use. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=sns.color_palette('Set3')); ```. if works as expected. Also you can manually assign a color list as :. ```py; sc.pl.scatter(adata, 'n_genes', 'n_counts', color='louvain', palette=[; '#000000', '#575757', '#AD2323', '#2A4BD7', '#1D6914', ; '#814A19', '#8126C0', '#A0A0A0', '#81C57A', '#9DAFFF', ; '#29D0D0', '#FF9233', '#FFEE33', '#E9DEBB', '#FFCDF3', ; '#F2F3F4'; ]); ```. `palette=sc.pl.palettes.zeileis_28` works because `sc.pl.palettes.zeileis_28` is already a list of color. This also works for the `palete` argument in`sc.pl.umap()`, but it changes the `adata.uns['louvain_colors']` column values and will change other plots when using this column for plotting. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885:324,message,message,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438#issuecomment-1641632885,1,['message'],['message']
Integrability,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:514,depend,depending,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460,1,['depend'],['depending']
Integrability,I don't think we should have to worry about dealing with CI for R in this project. This only becomes harder if the intent is to have many other dependencies. I think doing anything like this makes much more sense in seperate dedicated package.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144:144,depend,dependencies,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271#issuecomment-666969144,1,['depend'],['dependencies']
Integrability,"I don't think we're going to get this implemented for sparse dataset per-se, but we have implemented this for dask arrays wrapping the sparse dataset in. * https://github.com/scverse/scanpy/pull/2856",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764#issuecomment-2012543319:122,wrap,wrapping,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764#issuecomment-2012543319,1,['wrap'],['wrapping']
Integrability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:676,depend,dependencies,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,3,"['Depend', 'depend']","['Depends', 'dependencies']"
Integrability,"I feel like the `np.min(adata)` is more emblematic of the issue at hand here, which is how hard we should work to integrate with the rest of the python ML/data science ecosystem, e.g. `matplotlib.pyplot.scatter`. My personal view is nothing that works with a pandas DataFrame shouldn't work with an `AnnData` object; if you make it harder for people to work with AnnData than the most obvious competing data structure, they will simply use that other object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033:114,integrat,integrate,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584219033,1,['integrat'],['integrate']
Integrability,"I figured the one-item-thing out: The emitted code is:. ```rst; :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned.; :type copy: `bool`, optional (default: `False`). :returns: AnnData, None; Depending on `copy` returns or updates `adata` with the corrected data matrix.; ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694:235,Depend,Depending,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484050694,1,['Depend'],['Depending']
Integrability,"I got similar error when I was trying to use .h5 file from cellbender output. I have multiome data. . ```pytb; `>>> adata = scanpy.read_10x_h5(""/sc/arion/projects/hmDNAmap/snHeroin/analysis/ARC_TD005235-354/outs/cellbender/cb_feature_bc_matrix_filtered.h5"", gex_only=False)`; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 183, in read_10x_h5; adata = _read_v3_10x_h5(filename, start=start); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 268, in _read_v3_10x_h5; _collect_datasets(dsets, f[""matrix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidena",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:900,wrap,wrapper,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['wrap'],['wrapper']
Integrability,I guess it has to do with the dependency on `scipy`. Downgrading to a previous setup of packages did the trick for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045:30,depend,dependency,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-690967045,1,['depend'],['dependency']
Integrability,"I had the exact same issue and error message at that step in the tutorial. I installed scanpy using pip, because installing with conda was not working.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558:37,message,message,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1010#issuecomment-578570558,1,['message'],['message']
Integrability,"I have been trying but I am not sure how to do it in an appropriate way. Can you teach me how to do it? I am not super familiar with git stuff since; I am not really using it.; á§. On Mon, Mar 13, 2023 at 5:10â€¯AM Lukas Heumos ***@***.***>; wrote:. > A pull request that fixes this :); >; > See https://scanpy.readthedocs.io/en/stable/dev/index.html; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2436#issuecomment-1465764371>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFOKIW6MR6MSUCKJ2YTVG53W33P75ANCNFSM6AAAAAAVQOJ5FE>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2436#issuecomment-1479764509:670,Message,Message,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2436#issuecomment-1479764509,1,['Message'],['Message']
Integrability,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093:148,integrat,integrate,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573#issuecomment-759496093,1,['integrat'],['integrate']
Integrability,"I have the same issue than @brianpenghe .; I am trying to plot a heatmap using:; `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071; n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: ; The maximal number of iterations maxit (set to 20 by the program); allowed for finding a smoothing spline with fp=s has been reached: s; too small.; There is an approximation returned but the corresponding weighted sum; of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; warnings.warn(message). The heatmap was plotted after few minutes, though:; ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me?; Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-607391574:302,message,message,302,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-607391574,2,['message'],['message']
Integrability,I included the notebook with the residuals above. I'll reattach it to this message:. [benchmarks_PR1066_residuals.ipynb.zip](https://github.com/theislab/scanpy/files/4242812/benchmarks_PR1066_residuals.ipynb.zip),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515:75,message,message,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137515,1,['message'],['message']
Integrability,"I input pip show scipy I get:. Name: scipy; Version: 1.4.1; Summary: SciPy: Scientific Library for Python; Home-page: https://www.scipy.org; Author: None; Author-email: None; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: numpy; Required-by: umap-learn, statsmodels, scikit-learn, scanpy, xgboost, seaborn, mnnpy, loompy, Keras, Keras-Preprocessing, ggplot, gensim, anndata; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. Typing in pip show scanpy returns:; Name: scanpy; Version: 1.5.1; Summary: Single-Cell Analysis in Python.; Home-page: http://github.com/theislab/scanpy; Author: Alex Wolf, Philipp Angerer, Fidel Ramirez, Isaac Virshup, Sergei Rybakov, Gokcen Eraslan, Tom White, Malte Luecken, Davide Cittaro, Tobias Callies, Marius Lange, AndrÃ©s R. MuÃ±oz-Rojas; Author-email: f.alex.wolf@gmx.de, philipp.angerer@helmholtz-muenchen.de; License: BSD; Location: /home/ubuntu/.local/lib/python3.6/site-packages; Requires: packaging, h5py, joblib, legacy-api-wrap, tqdm, seaborn, setuptools-scm, statsmodels, numba, matplotlib, scipy, patsy, networkx, tables, natsort, pandas, umap-learn, scikit-learn, importlib-metadata, anndata; Required-by: ; You are using pip version 18.0, however version 20.2b1 is available.; You should consider upgrading via the 'pip install --upgrade pip' command. I have to use !pip install scanpy --user; when starting my session to have it work properly so I thought maybe it was an issue of being in a different directory but based on the location of each package when I look them up that doesn't appear to be the case? I tried using !pip install scipy -U --user but it tells me that the updated version is already present. sc.logging.print_versions() still shows scipy 1.0.1 as the version so I'm a bit confused. Is scanpy somehow defaulting to a different version for some reason? Is there a way to make it use the correct version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942:1100,wrap,wrap,1100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-635681942,1,['wrap'],['wrap']
Integrability,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:1732,depend,dependency,1732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214,1,['depend'],['dependency']
Integrability,"I just wanted to update that this issue does not depend on scvelo at all, but I can recreate it by just using scanpy. I suspect it is an issue with running umap. I'm using version '0.4.6'. Any help would be much appreciated:. ### Minimal code sample (that we can copy&paste without having any data). ```python; import os; import scanpy as sc; import numpy as np; import pandas as pd; import copy; import anndata; import matplotlib.pyplot as plt; adata_pbmc3k = sc.datasets.pbmc3k_processed(); #del adata_pbmc3k.obsm['X_pca']; #del adata_pbmc3k.obsm['X_umap']; del adata_pbmc3k.obsp['distances']; del adata_pbmc3k.obsp['connectivities']; #sc.pp.pca(adata_pbmc3k, n_comps=50); sc.pp.neighbors(adata_pbmc3k); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-13-02/ipykernel_2124423/1009160698.py in <module>; ----> 1 sc.pp.neighbors(adata_pbmc3k). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 806 # we need self._distances also for method == 'gauss' if we didn't; 807 # use dense distances; --> 808 self._distances, self._connectivities = _compute_connectivities_umap(; 809 knn_indices,; 810 knn_distances,. /hps/software/users/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863:49,depend,depend,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983#issuecomment-903666863,1,['depend'],['depend']
Integrability,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:498,wrap,wrapper,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['wrap'],['wrapper']
Integrability,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:66,wrap,wrapper,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,3,"['depend', 'wrap']","['dependency', 'wrapper']"
Integrability,"I like the idea! Better error messages, and getting our modalities a bit under control is a great goal as well!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2106927009:30,message,messages,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2106927009,1,['message'],['messages']
Integrability,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where.; * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python; from anndata import AnnData; from datetime import datetime; from functools import wraps; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(func):; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; call_start_record = dict(call_id=call_id, called_func=func.__name__); if type(args[0]) is AnnData:; call_start_record[""adata_id""] = id(args[0]); logger.msg(""call"", **call_start_record). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:728,wrap,wraps,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,2,['wrap'],['wraps']
Integrability,"I looked at it a while ago (for one test dataset, probably), and got the impression that `louvain` was faster. That said, they're both very fast. I would note that solutions from either can be pretty unstable, frequently depending on size of the community. @LuckyMD When you say heavy tailed, are you thinking of the unweighted KNN graph case or both?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483207692:221,depend,depending,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483207692,1,['depend'],['depending']
Integrability,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:633,depend,depends,633,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276,1,['depend'],['depends']
Integrability,"I mean, @vtraag is is the person Iâ€™d believe when asked which algorithm is superior, so we could. 1. add `sc.tl.leiden` as an alternative that doesnâ€™t have a flavour argument.; 2. make `leidenalg` a dependency and `louvain-igraph` an optional one.; 3. when calling `sc.tl.louvain` (no matter the flavor used), emit a ``DeprecationWarning('We recommend to use `sc.tool.leiden` instead. Refer to its documentation for details')``. This meets the following goals:. - education: people will learn why we recommend the new function; - ease of use: no weird errors pop up suddenly; - reproducibility: If `louvain-igraph` is installed, the code works exactly as before (with an added warning), else it crashes. we could do the following within `sc.tl.louvain` to help users:. ```py; try:; import louvain; except ImportError:; raise ImportError(; 'The package â€œlouvain-igraphâ€œ is not installed. '; 'Try using `sc.tl.leiden` in case you do not need '; 'to reproduce results produced using `sc.tl.louvain`'; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831:199,depend,dependency,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437039831,1,['depend'],['dependency']
Integrability,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:114,integrat,integrated,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328,1,['integrat'],['integrated']
Integrability,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. ; @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271:773,integrat,integrate,773,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271,1,['integrat'],['integrate']
Integrability,"I only just now got the distinction between types and classes in python. So when they talk about â€œtypesâ€, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thatâ€™s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whatâ€™s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. Thereâ€™s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnâ€™t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:743,protocol,protocol,743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,2,['protocol'],['protocol']
Integrability,"I run into this bug too.; when i run:; ```; import scanpy as sc; import anndata as ad; adata = sc.read(filepath); ```; it turns out:; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[6], [line 8](vscode-notebook-cell:?execution_count=6&line=8); [6](vscode-notebook-cell:?execution_count=6&line=6) if filename.endswith('.h5ad'):; [7](vscode-notebook-cell:?execution_count=6&line=7) filepath = os.path.join(directory, filename); ----> [8](vscode-notebook-cell:?execution_count=6&line=8) adata = sc.read(filepath); [10](vscode-notebook-cell:?execution_count=6&line=10) # Rename columns with periods in `.obs` attribute; [11](vscode-notebook-cell:?execution_count=6&line=11) for col in adata.obs.columns:. File /data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:893,wrap,wrapper,893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['wrap'],['wrapper']
Integrability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:143,message,message,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['message'],['message']
Integrability,"I second this initiative. I had used the code from Brent and works quite; well. Naturally, having it integrated into Scanpy would be great. On Mon, Dec 17, 2018 at 2:18 PM Marius Lange <notifications@github.com>; wrote:. > *@Marius1311* commented on this pull request.; > ------------------------------; >; > In scanpy/preprocessing/combat.py; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>:; >; > > @@ -0,0 +1,161 @@; > +import numpy as np; > +from scipy.sparse import issparse; > +import pandas as pd; > +import sys; > +from numpy import linalg as la; > +import patsy; > +; > +def design_mat(mod, batch_levels):; > + # require levels to make sure they are in the same order as we use in the; > + # rest of the script.; > + design = patsy.dmatrix(""~ 0 + C(batch, levels=%s)"" % str(batch_levels),; >; > thanks, did that!; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/398#discussion_r242142511>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1fZSO-j8m0NwemluQp-0wNEGDHJ9ks5u55mlgaJpZM4ZTmeq>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676:101,integrat,integrated,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-447896676,1,['integrat'],['integrated']
Integrability,I skimmed through the code only briefly yesterday but I think the problem might be that both `x` and `y` need to be specified: The error message seems to indicate so and the two lines seemingly responsible for the problem are the only ones where `x` and `y` are **not** both specified. I'll have a closer look at it in a bit.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984:137,message,message,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694079984,1,['message'],['message']
Integrability,"I started doing feature (gene) selection on scSeq data as well. I also observe dependency between Moran's I and gene expression sparsity. I was thinking that maybe I could regress out the expression effect or select genes per bin.; ![image](https://user-images.githubusercontent.com/47607471/112271293-0c4a6400-8c7b-11eb-953e-a7fcec362401.png); And this was the problem with p-values in squdpy-s Moran's I (plot for first 100 genes in my adata, using 100 permutations - but this should not really lead to low pvalues as I think that the reported pvalues are estimated based on null distn shape - not 100% sure though).; ![image](https://user-images.githubusercontent.com/47607471/112271483-461b6a80-8c7b-11eb-826d-0e93d708088b.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439:79,depend,dependency,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-805574439,1,['depend'],['dependency']
Integrability,"I still don't see 100% why the submodule `rtools` would be so much worse than `scanpy-contrib`. The submodule would be separate from the rest of the package and we could write something on top of its API overview page like: interfaces for R tools, address the maintainers of these tools for help... We need to keep some structure so that things remain clean, but yeah, additions will always be somewhat arbitrary. If someone suggests a ""meaningful addition"", we will accept it, if someone suggests something that does not seem to be of good quality, we will reject it. But this is not a problem only for `rtools` but for wrappers of python packages as well... see, for instance, https://github.com/theislab/scanpy/pull/126. So, I would set up the `rtools` submodule to save us the work of maintaining a different repo and the user the work of installing a `scanpy-contrib` package and figuring out which namespaces to use so that notebooks don't get completely messed up. So, if you don't mind, I'd set up the ""rtools"" submodule...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299:224,interface,interfaces,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344299,2,"['interface', 'wrap']","['interfaces', 'wrappers']"
Integrability,"I sympathize with the problem which I tend to solve by plotting individual scatterplots, each one with its specific vmax. But I will be happy to have a better output. I like the idea of using quantile but I would avoid an increasing list of parameters. Thus @ivirshup suggestion to use `functools.partial` seems better. I like the flexibility it provides and I think we should implement it, but I don't know if this would be difficult to document and explain to the user that just would like to compute the quantile. An idea would be to allow some encoding for vmax as for example `vmax='q99'` which would be interpreted as np.quantile. My suggestion is to . - add vectorized vmax and vmin; - each entry of vmax or vmin would be interpreted depending on the data type. Besides a number, if it is a string then it is interpreted as for example quantile if it starts with 'q' or as a function if the type is `partial`. The following options would then be valid:. ```PYTHON; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[4., 3.]); sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=['q80', 'q90']). from functools import partial; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[partial(np.mean), partial(np.median)]). # combination; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2"", ""gene3""], ; vmax=[4., 'q85', partial(np.percentile, q=90]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044:741,depend,depending,741,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-521550044,1,['depend'],['depending']
Integrability,I think I would rather just bump the h5py dependency since it's been over a year.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2064#issuecomment-1013003401,1,['depend'],['dependency']
Integrability,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208:245,depend,depend,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487916208,1,['depend'],['depend']
Integrability,"I think if you add `show=False` to the `sc.pl.umap` commands it might work? . Depending on the type grid and subplots, it could be easier to plot things manually with matplotlib (to have more control over things like legend placement etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022:78,Depend,Depending,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1802#issuecomment-824423022,1,['Depend'],['Depending']
Integrability,"I think it shows us some strategy, yes. I can't claim to understand what each line you did there does but I have a notebook open now and am trying to step through it. I think part of the issue here is we're wanting to store aggregate values like means within adata so that they don't have to be recomputed every time they are needed. Web interfaces are being driven off these files, so rapid access is preferred over storage concerns. . If, in the example of datasets with technical replicates, we almost always are only interested in the mean values across replicates. So if our input is like this:. ![screenshot from 2018-04-11 15-11-07](https://user-images.githubusercontent.com/330899/38640690-9b7c41a8-3d9a-11e8-9b40-b9763a3df422.png). Simple demo with 4 genes, 2 conditions, and 3 replicates of each condition. Does one make this exact matrix adata.X? . ![screenshot from 2018-04-11 15-16-11](https://user-images.githubusercontent.com/330899/38640934-4a25562c-3d9b-11e8-82eb-eb7811463fa4.png). Or should the means be calculated and stored as adata.X with individual replicates stored as separate matrices of the same size, like adata.uns['rep1'], adata.uns['rep2'], ... etc. This is specifically the part I was asking about regarding conventions, as it seems there must already be a convention for storing technical replicates and their aggregate values (mean, stddev, etc.). I think the examples given with values like 0 and 1 are a bit confusing because I can't tell if you're using that as a proxy for column names or are they index positions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298:338,interface,interfaces,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/106#issuecomment-380582298,1,['interface'],['interfaces']
Integrability,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-495239336:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-495239336,1,['depend'],['dependency']
Integrability,"I think itâ€™s pretty impossible to know if theyâ€™ll render â€“ Depends on the fonts available on the system and the way the font rendering stack falls back to other fonts. My approach would be to check which systems have the problem, and if itâ€™s only some Linux server or some obsolete stuff like e.g. Windows versions up to Vista, ignore it. If itâ€™s a commonly used and still supported desktop OS / Linux distribution, we have to deal with it. The reason I excluded Linux servers is that server admins often set up things minimalistically, excluding â€œGUI stuffâ€ so trying to support those highly heterogenous systems will only bring pain. When people want better fonts, then fontconfig is happy to provide them with the means to do so.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541:59,Depend,Depends,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-528244541,1,['Depend'],['Depends']
Integrability,"I think the `<` should work, it's part of [PEP-508](https://www.python.org/dev/peps/pep-0508/#motivation). At least that's what [this blog post](https://hynek.me/articles/conditional-python-dependencies/) says. Didn't those builds just fail before they got to that requirement?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-505432318:190,depend,dependencies,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-505432318,1,['depend'],['dependencies']
Integrability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:45,interface,interface,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,2,['interface'],['interface']
Integrability,"I think the issue here is that BBKNN only generates an integrated graph, while the tsne computation creates a new graph from some matrix representation of the data. There has been the suggestion of allowing a tsne layout (#1233) to be generated from a precomputed connectivity matrix, but that hasn't been implemented here yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279:55,integrat,integrated,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-678131279,1,['integrat'],['integrated']
Integrability,"I think the level of threading can be dependent on BLAS/ LAPACK etc. However, it should generally be multithreaded. Does the issue persist in a conda environment?. If you still run into issues I'd be interested in seeing some details (e.g. what size and kind of matrix, cpu usage during computation). A counter example of a faster way to compute it could be useful to see too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621:38,depend,dependent,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1865#issuecomment-861467621,1,['depend'],['dependent']
Integrability,"I think this has to do with us relying on UMAP. You can check this yourself in UMAP, but you'll actually end up with n-1 neighbors per node. I believe this has to do with each point being it's own nearest neighbor, but I forget if that's important for nearest neighbor descent algorithm (prevent node from adding itself by already having it in the heap) or UMAP (simplexes??). If I can find a link to where I read this, I'll share it here. Two considerations:. * This is the behaviour of UMAP, which we are fairly integrated with; * This has always been the behavior. I was definitely surprised when I read about this recently, and would be open to changing the behavior. It would effect reproducibility for everyone though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203:514,integrat,integrated,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788812203,1,['integrat'],['integrated']
Integrability,"I think this may be already implemented in https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.ingest.html, however, this function contains extra integration and label transfer steps that are not needed for all applications. It would be great if this could be disentangled to make the umap transform available as a separate function on scanpy umaps. Also, it seems that this function does not use scanpy umap to calculate umap so changes may be needed in how scanpy umap is currently calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704:154,integrat,integration,154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1237340704,1,['integrat'],['integration']
Integrability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:64,message,message,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['message'],['message']
Integrability,"I think to preserve the signature like that we could easily do a subset of what `update_wrapper` (and therefore `wraps`) does:. ```py; def wraps_plot_scatter(wrapper):; wrapper.__annotations__ = {; k: v for k, v in plot_scatter.__annotations__.items(); if k != 'basis'; }; wrapper.__wrapped__ = plot_scatter; return wrapper. @wraps_plot_scatter; def umap(adata, **kwargs):; """"""...""""""; return plot_scatter(adata, basis='umap', **kwargs); ```. but first: what did you try and why did it fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-473907861:113,wrap,wraps,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-473907861,5,['wrap'],"['wrapper', 'wraps']"
Integrability,"I think we sorta already did this:. ```python; sc.tl.leiden(adata); /mnt/workspace/repos/scanpy/scanpy/tools/_leiden.py:144: FutureWarning: Use of leidenalg is discouraged and will be deprecated in the future. Please use `flavor=""igraph""` `n_iterations=2` to achieve similar results. `directed` must also be `False` to work with `igraph`'s implementation.; warnings.warn(msg, FutureWarning); ```. However, I do think the warning could be reworked. - [ ] Stack level should be set so the right line is shown; - [ ] Message should be more like:. > FutureWarning: In the future, the default backend for `leiden` will be `igraph` instead of `leidenalg`. To achieve the future defaults please pass: `flavor=""igraph""` and `n_iterations=2`. `directed` must also be `False` to work with `igraph`'s implementation.; >; > Note that results will be slightly different with the changed backend. - [ ] We should only warn once, instead of every time leiden is called. Here's a util we defined in anndata for this (I think stolen from pandas?). ```python; def warn_once(msg: str, category: type[Warning], stacklevel: int = 1):; warnings.warn(msg, category, stacklevel=stacklevel); # Prevent from showing up every time an awkward array is used; # You'd think `'once'` works, but it doesn't at the repl and in notebooks; warnings.filterwarnings(""ignore"", category=category, message=re.escape(msg)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2935#issuecomment-2015559682:514,Message,Message,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2935#issuecomment-2015559682,2,"['Message', 'message']","['Message', 'message']"
Integrability,"I tried to set `var_names` from gene_symbols, and I get a warning message:; `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:; `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```; File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(); ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184:66,message,message,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184,1,['message'],['message']
Integrability,I uninstalled umap and made sure umap-learn was installed but it did not change anything. . I would guess that the problem comes from modules dependency as I managed to make it work on pycharm.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1978#issuecomment-898539219,1,['depend'],['dependency']
Integrability,"I used the following code once in a time . however , no visible different could be found in the result (`sc.pl.umap(adata, color='batch')`). ```; sc.pp.combat(adata). sce.pp.bbknn(adata, batch_key='batch'). from itertools import cycle; sce.pp.mnn_correct(adata, var_index=None, var_subset=None, batch_key='batch', index_unique='-', batch_categories=None, k=20, sigma=1.0, cos_norm_in=True, cos_norm_out=True, svd_dim=None, var_adj=True, compute_angle=False, mnn_order=None, svd_mode='rsvd', do_concatenate=True, save_raw=False, n_jobs=None); #OR; import mnnpy; mnnpy.mnn_correct(adata). sce.pp.magic(adata). sce.tl.phate(adata); ```. Hereï¼ŒI attach the Integrate result from seurat based on same data. we can that most of the sample 001, 002ï¼Œand 009 were grouped together on left , most part of sample 003 was on the right. ; ![20191016EVE_UMAP_Integrate](https://user-images.githubusercontent.com/49429496/66911577-5f11fc00-f043-11e9-8be2-742a4ffaa7a3.png). or this way (split by batch); ![20191016EVE_UMAP_Integrate_SplitByPatient](https://user-images.githubusercontent.com/49429496/66911623-79e47080-f043-11e9-8d5d-7769437831da.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756:652,Integrat,Integrate,652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-542637756,1,['Integrat'],['Integrate']
Integrability,"I was also investigating how `leiden` got `use_weights=True` by default, and noticed the lack of discussion. Seems like it just sorta happened when `leiden` got added #361?. I think it'd be pretty different from clustering on the embedding, because the embedding has constraints based on things like minimum distance two points can be from each other, and the number of dimensions it's embedded in. On the binarized KNN-graph, I think we've actually talked about this before (#240). I personally think using a weighted graph makes more sense. For example, say you have a cell type of which occurs 15 times in your dataset, but you've set k to 30. With a binarized graph there will be a less clear signal that this is a distinct cell-type. From a slightly more empirical/ anecdotal perspective, on a couple datasets I tested, total degree of the generated graph was sub-linear (looked log-ish) w.r.t. `k` for the weighted umap graph. Here's using one of the bone marrow donors from the hca immune census (y-axis is log scaled so you can still see the total weighted degree increase):. ![image](https://user-images.githubusercontent.com/8238804/56469005-400d2580-6477-11e9-98f1-b9dfe70bd1d7.png). To me, this suggested a stable representation of the dataset was being found. As a connected point, in my experience clustering results seems fairly robust to `k` for weighted graphs above a low threshold (I think dataset dependent, but 30-60 range). Using an unweighted graph, there is a much stronger dependence on `k` and some smaller clusters seem less stable (show up in a smaller proportion of clustering solutions from a parameter space).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638:1417,depend,dependent,1417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-485242638,2,['depend'],"['dependence', 'dependent']"
Integrability,"I was thinking we could go a bit further. We could add `sinfo` as a dependency and make `print_versions` just call: `sinfo.sinfo(dependencies=True)` which will always be comprehensive. <details>; <summary> Example output: </summary>. ```; -----; IPython 7.16.1; scanpy 1.5.2.dev38+g6728bdab; sinfo 0.3.1; -----; IPython 7.16.1; PIL 7.2.0; anndata 0.7.5.dev0+g58886f0.d20200729; asciitree NA; backcall 0.2.0; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.21.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; ipython_genutils 0.2.0; jedi 0.17.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; louvain 0.7.0; matplotlib 3.3.0; monotonic NA; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; numba 0.50.1; numcodecs 0.6.4; numexpr 2.7.1; numpy 1.19.0; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.5.2.dev38+g6728bdab; scipy 1.5.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zarr 2.4.0; -----; Python 3.8.5 (default, Jul 23 2020, 15:50:11) [Clang 11.0.3 (clang-1103.0.32.62)]; macOS-10.15.6-x86_64-i386-64bit; 16 logical CPU cores, i386; -----; Session information updated at 2020-07-30 19:28; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831:68,depend,dependency,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1343#issuecomment-666257831,2,['depend'],"['dependencies', 'dependency']"
Integrability,"I was trying some integration methods between the two pbmc datasets. Maybe, could you add a sc.datasets.pbmc68k_full() where the whole transcriptome is included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078:18,integrat,integration,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1762#issuecomment-808189078,1,['integrat'],['integration']
Integrability,"I was wrong, it worked well... ```python; %matplotlib inline. import scanpy as sc; import matplotlib as mpl. # 2 lines below solved the facecolor problem.; mpl.rcParams['figure.facecolor'] = 'white'; sc.settings.set_figure_params(dpi=80, color_map='viridis', transparent=False). adata = sc.datasets.paul15(); sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata, color='paul15_clusters', legend_loc='on data'); ```; ![screenshot from 2019-02-13 12-42-06](https://user-images.githubusercontent.com/19543497/52685380-d9f2b680-2f8c-11e9-8ca2-692b083116ee.png). Anyway, . > So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`). sounds a good solution. Expliciting that 'white', 'w' or (1,1,1) are also applicable may be kind.; Of course, you can add theme like Seurat's one though I'm not sure how many people are requiring it. - https://satijalab.org/seurat/mca.html; - https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-463049965:683,message,message,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-463049965,1,['message'],['message']
Integrability,"I wasn't aware that you could run `sc.pp.scale` without obtaining mean 0 at the end. Would that just scale the variance per gene then?. As for your question on HVG selection after `sc.pp.regress_out` vs in batches... I think that's an interesting question, but I reckon the two scenarios are actually not that related. I normally wouldn't use `sc.pp.regress_out` to remove batch effects, but rather to regress out continuous covariates like cell cycle scores. Batch effect removal is probably best done with methods that account for the variance contribution of the batch effect as well, such as Combat... or more complex data integration methods (Seuart, MNN, scanorama). Either way, it would be an interesting comparison... just with a caveat ^^.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449:627,integrat,integration,627,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509687449,1,['integrat'],['integration']
Integrability,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192:146,integrat,integrate,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192,2,"['depend', 'integrat']","['depending', 'integrate']"
Integrability,"I went over all the places where we use the `array_type` fixture and thought about your idea to use `@pytest.mark.parametrize` and I came around to it for this case:. For **unfinished** features, itâ€™s great. Everwhere we canâ€™t say â€œwe fully support thisâ€ and gradually build in support, we should use it. It has its disadvantages:. - `@pytest.mark.parametrize(""array_type"", ARRAY_TYPES)` is so long that in practice, itâ€™s hard to see the difference to something like this: `@pytest.mark.parametrize(""array_type"", ARRAY_TYPES_XYZ)`. 	E.g. I donâ€™t like seeing; 	; 	```py; 	@pytest.mark.parametrize(""array_type"", ARRAY_TYPES); 	@pytest.mark.parametrize(""dtype"", [""float32"", ""int64""]); 	```. 	4 times in `test_normalize_total`. If the 3rd test had a different list of values in one of the params, it would be near impossible to see. - Fixtures can depend on other fixtures, but canâ€™t easily have a parameter matrix without that. (`pytest.fixture(params=...)` only accepts a single list of parameters, weâ€™d have to manually use `product` in there for a matrix). Thatâ€™s why I didnâ€™t go away from a fixture in `test_pca.py`. I therefore propose that we use `@pytest.mark.parametrize` for. - things that arenâ€™t heavily reused; - things we donâ€™t fully support. and fixtures for everything where thereâ€™s ~3 or more test functions using the same list of parameter values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2696#issuecomment-1781361678:844,depend,depend,844,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2696#issuecomment-1781361678,1,['depend'],['depend']
Integrability,"I will close this issue because there is an external solution available. We may think about integrating this specific plot into Scanpy at some point, but I don't see it happening anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449:92,integrat,integrating,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1824#issuecomment-953646449,1,['integrat'],['integrating']
Integrability,"I will take a look later to see how we can integrate better the visualizations, the tests and the documentation. I will put back `kwds` also. . Have you consider adding another dataset to the repository? This will be good for showing examples.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301:43,integrat,integrate,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405505301,1,['integrat'],['integrate']
Integrability,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-486991947:73,depend,dependency,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-486991947,1,['depend'],['dependency']
Integrability,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224:107,depend,dependencies,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613#issuecomment-768656224,1,['depend'],['dependencies']
Integrability,"I would like this be to somewhere where it'd also work for CPU. I think we can implement a `__dataframe__` interface that passes either GPU or CPU memory to data shader, then let data shader handle the rest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2656#issuecomment-1711727606:107,interface,interface,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2656#issuecomment-1711727606,1,['interface'],['interface']
Integrability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:448,depend,dependent,448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,1,['depend'],['dependent']
Integrability,"I'd add the line as a `.. note` to `neighbors`, but I'd also be fine with adding pynndescent as a dependency. We'd just need to get rid of the code that works with non-pynndescent search.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263:98,depend,dependency,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1675#issuecomment-783836263,1,['depend'],['dependency']
Integrability,I'd agree with your statement that engineering the representation is more important than the analysis. I view my goal here as allowing more representations as input. Would you mind saying more about why you thing using different metics is less clean (simple?)? I would think that would depend on what representation you're calculating the distances on.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973:286,depend,depend,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416821973,1,['depend'],['depend']
Integrability,"I'd argue a janky dependency is an issue with scanpy's code, as there are interfaces to BioMart which are better behaved. Here's a proof of concept:. ```python; def mitochondrial_genes(org, attrname=""external_gene_name"", host=""www.ensembl.org""):; """"""Mitochondrial gene symbols for specific organism through BioMart. Parameters; ----------; org : {{""hsapiens"", ""mmusculus"", ""drerio""}}; Organism to query. Must be an organism in ensembl biomart.; fieldname : `str`, optional (default: ""external_gene_name""); Biomart attribute field to return. Possible values include ; ""external_gene_name"", ""ensembl_gene_id"", ""hgnc_symbol"", ""mgi_symbol"",; and ""zfin_id_symbol"".; host : {{""www.ensembl.org"", ...}}; A valid BioMart host URL. Returns; -------; An `np.array` containing identifiers for mitochondrial genes.; """"""; try:; from pybiomart import Server; except ImportError:; raise ImportError(; ""You need to install the `pybiomart` module.""); server = Server(host); dataset = (server.marts[""ENSEMBL_MART_ENSEMBL""]; .datasets[""{}_gene_ensembl"".format(org)]); res = dataset.query(; attributes=[attrname], ; filters={""chromosome_name"": [""MT""]},; use_attr_names=True; ); return res[attrname].values; ```. Running it:. ```python; >>> mitochondrial_genes(""hsapiens""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', 'MT-TL2', 'MT-ND5',; 'MT-ND6', 'MT-TE', 'MT-CYB', 'MT-TT', 'MT-TP'], dtype=object); >>> mitochondrial_genes(""hsapiens"", host=""asia.ensembl.org""); array(['MT-TF', 'MT-RNR1', 'MT-TV', 'MT-RNR2', 'MT-TL1', 'MT-ND1',; 'MT-TI', 'MT-TQ', 'MT-TM', 'MT-ND2', 'MT-TW', 'MT-TA', 'MT-TN',; 'MT-TC', 'MT-TY', 'MT-CO1', 'MT-TS1', 'MT-TD', 'MT-CO2', 'MT-TK',; 'MT-ATP8', 'MT-ATP6', 'MT-CO3', 'MT-TG', 'MT-ND3', 'MT-TR',; 'MT-ND4L', 'MT-ND4', 'MT-TH', 'MT-TS2', '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514:18,depend,dependency,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-457039514,2,"['depend', 'interface']","['dependency', 'interfaces']"
Integrability,"I'd like to +1 @ivirshup 's prototype there -- perhaps `scale` is not the best example of a function that people would use on data other than anndata, but this would be a good pattern to follow throughout `scanpy`. Not infrequently I run the following workflow:; ```; adata = scanpy.AnnData(data); dpt = scanpy.tl.dpt(adata); del adata; ```; and it would be awfully nice if `dpt` just accepted my data matrix without me having to wrap it in an AnnData object. This could be true for any scanpy function that doesn't require row/column data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722:430,wrap,wrap,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-609057722,1,['wrap'],['wrap']
Integrability,I'd love to have scVI integration! :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867:22,integrat,integration,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-470977867,1,['integrat'],['integration']
Integrability,"I'll take a look at this to make sure I've not messed up in writing this wrapper (I'm actually doing some more testing myself for production use right now). . But you should know that what we've done here is mirror some of the internals of scrublet, but using Scanpy functions. Scrublet should be supplied with raw counts, but does do its own normalisations internally before doing the actual doublet prediction, which is what we're doing here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422:73,wrap,wrapper,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889126422,1,['wrap'],['wrapper']
Integrability,"I'm a little late to the party, but here's my 0.02$. > What is the scope of this PR? Will this just be single dataset TSNE calculation, with integration/ ingest functionality happening separately, or would you like to do it all at once?. I think we can split it into two PRs, since they're going to touch different parts of the code base, and it should be easier to review them individually. > How different are the arguments to the various affinity methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:141,integrat,integration,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['integrat'],['integration']
Integrability,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR!. A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-450770914:141,wrap,wrappers,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-450770914,1,['wrap'],['wrappers']
Integrability,"I'm having some trouble debugging whatever is going wrong with the notebook tests here. I get the same results if I run `pytest` on my machine, but don't get a failure if I run the code manually. Additionally, I don't get an error (the `abort`) if I *only* run the notebook tests (`pytest -k ""test_pbmc3k""`). Pretty sure the error is happening on the call to louvain in the notebook tests â€“ an `assert False` fails the tests, one after gives current result â€“ but I can't reproduce the abort interactively. Any idea what's going on/ how I can get a more helpful error message here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136:567,message,message,567,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-419695136,1,['message'],['message']
Integrability,"I'm not able to reproduce this. Here's what I tried. * Made a conda environment with `conda create -yn torch-scanpy ""python=3.8""`, and activated it `conda activate torch-scanpy`; * Installed: `pip install scanpy torch`; * Imported: `python3 -c ""import torch; import scanpy""`. IIRC, there has been an issue with the order of importing numba and pytorch due to how they require their LLVM dependency. I would make sure your version of pytorch and numba are up to date (I believe your pytorch is a few versions old) and trying again. If the issue persists, could you check if you run into problems with this?. ```python; import torch; import numba; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990:387,depend,dependency,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1286#issuecomment-646456990,1,['depend'],['dependency']
Integrability,I'm pretty sure it's the pandas 0.23 issue... same error message as the one encountered here: https://github.com/theislab/scanpy/issues/158,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529:57,message,message,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/162#issuecomment-391991529,1,['message'],['message']
Integrability,I'm pretty sure none of you are having the same issue as the original one reported here. Compare @abuchin 's error message of `KeyError: 'dict'` to the original poster's error of `OSError: Can't read data`. The thing you're seeing is a new one stemming from an update to anndata. You're trying to read in a `h5ad` file created with a newer version of the package with your older one. I think the cutoff point is 0.8.0 but I could be mistaken. Upgrade your anndata and you should be ok.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945:115,message,message,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1198015945,1,['message'],['message']
Integrability,"I'm still dubious of the value, especially when we provide different ways of ways of optimizing the score. What would you think of instead having `sc.metrics.modularity` where you match a clustering and a graph returning a modularity score?. It would basically wrap:. ```python; (; igraph.Graph.Weighted_Adjacency(adata.obsp[""connectivities""]); .modularity(adata.obs[""louvain""].cat.codes); ); ```. But you could also generate modularity scores for other labelings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869:261,wrap,wrap,261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869,1,['wrap'],['wrap']
Integrability,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, â€¦). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:39,interface,interfaces,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,1,['interface'],['interfaces']
Integrability,"I'm wondering if we can come to some agreement on a slight modification to this proposal. > > How does this impact users vs. developers?; >; > user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. This seems good. > > Who manages the sub-packages?; >; > the IO subpackage? everyone ðŸ˜…. ðŸ˜… indeed. > For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know. I feel like complicated dependency management was what we were trying to avoid here. Also it's nice when you install a package call a function and it works, less nice to have to start mucking around with dependencies. --------------. ## An alternative: project specific IO. `squidpy_io`, `muon_io`. Packages which read in package specific formats with a minimal set of dependencies. We can keep `muon.read_10x_atac`, so nothing changes for users. We skip out on complicated ownership and complicated dependencies. This should be very low overhead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650:692,depend,dependency,692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059537650,4,['depend'],"['dependencies', 'dependency']"
Integrability,"I've come across a strange behavior related with this issue. Depending on whether or not I save the object I get the same warning as OP. This works as it should:; ```; import scanpy as sc. adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 1, no saving, works as it should; adata=adata.raw.to_adata(); sc.pp.log1p(adata); ##>>> no warning; ```. Saving mid-way does not allow to avoid the warning, even restarting the kernel before reading the data:; ```; import scanpy as sc. ## same as above; adata=sc.read_h5ad(data_dir+'scanpy_QC_sexchrom.h5ad'); adata.raw=adata.copy() #data to save; sc.pp.log1p(adata) # logaritmize. ### Test 2, saving and re-assigning from raw; ### saving object, reading, testing again; ### Doesnt work; adata.write_h5ad(tmp+'scanpy_test.h5ad'); adata=sc.read_h5ad(tmp+'scanpy_test.h5ad'); adata=adata.raw.to_adata(); sc.pp.log1p(adata); ###>>>WARNING: adata.X seems to be already log-transformed.; ```. I'm on scanpy 1.9.1 if it matters",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748:61,Depend,Depending,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-1209486748,1,['Depend'],['Depending']
Integrability,"I've got two main reasons for thinking they should be more visible:. 1. If I'm trying to find what tools are available through scanpy for a certain task, it should be very obvious where that might be available. For example, if I want to know what's available for batch correction, I (the user) am probably not too fussed about whether it's in the scanpy codebase or not.; 2. As a method developer, it'd encourage me to integrate my method if I saw it'd be highly visible and that other people were doing it. Right now there are links, but users still have to go to see the notes with those links, go to a separate page, and scroll for a bit to see any particular method. . Another strategy could be a top level `External API` heading underneath the `API` heading? Then there could be an expandable table of contents (how I typically navigate the site) to get an idea of what's there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101:419,integrat,integrate,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/588#issuecomment-479739101,1,['integrat'],['integrate']
Integrability,"I've gotten complaints from Matplotlib about calling `mpl.use`, to set the backend after importing `pyplot` ([relevant matplotlib docs](https://matplotlib.org/tutorials/introductory/usage.html#what-is-a-backend)). I think it would be unintuitive if packages behaved differently depending on what functions had been called. In general, matplotlib has a lot of state and messing with it has only brought me pain.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-522822622:278,depend,depending,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522822622,1,['depend'],['depending']
Integrability,"I've played around with ABCs for defining interfaces, but had thought the `typing` module was more closely tied with them. Do you understand the rationale for `typing` classes being separate when ABCs and regular classes can be used for annotation?. As far I can tell, it's for subscripted (parametric?) annotations. For many python classes those would have to be a runtime check â€“ e.g. `List[int]` â€“ which `ABC`s don't do. Not that `typing` does these checks, but it allows a way to express those constraints.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445609843:42,interface,interfaces,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445609843,1,['interface'],['interfaces']
Integrability,"I've pretty much just gotta write the docs and rebase on my other PR, and this should be good to go. For adding `top_segment_proportions` and `top_proportions` to preprocessing, should they get a wrapper to work on AnnData objects? Also, I'm not super happy with the name `top_segment_proportions`, and am open to suggestions for a better name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-433284121:196,wrap,wrapper,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-433284121,1,['wrap'],['wrapper']
Integrability,"I've updated the code to store the image path in `adata.uns['spatial'][sample_id]['tif_image_path']`. The test now also checks whether the image file exists. . I can also test whether the image file is a valid tiff image, but for this we'd need to add a tiff reading library like `pillow` as a test dependency to scanpy. If that is ok, I'll update the PR with an additional test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592:299,depend,dependency,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506#issuecomment-733605592,1,['depend'],['dependency']
Integrability,"I've used it a bit, and have gotten nice results. I think I've mentioned it before (#938), but that was on an unrelated issue so it's good to have. The results are nice:. <details>; <summary> Example usage </summary>. ```python; from adjustText import adjust_text. def gen_mpl_labels(; adata, groupby, exclude=(), ax=None, adjust_kwargs=None, text_kwargs=None; ):; if adjust_kwargs is None:; adjust_kwargs = {""text_from_points"": False}; if text_kwargs is None:; text_kwargs = {}. medians = {}. for g, g_idx in adata.obs.groupby(groupby).groups.items():; if g in exclude:; continue; medians[g] = np.median(adata[g_idx].obsm[""X_umap""], axis=0). if ax is None:; texts = [; plt.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items(); ]; else:; texts = [ax.text(x=x, y=y, s=k, **text_kwargs) for k, (x, y) in medians.items()]. adjust_text(texts, **adjust_kwargs). with plt.rc_context({""figure.figsize"": (8, 8), ""figure.dpi"": 300, ""figure.frameon"": False}):; ax = sc.pl.umap(pbmc, color=""Low-level celltypes"", show=False, legend_loc=None, frameon=False); gen_mpl_labels(; pbmc,; ""Low-level celltypes"",; exclude=(""None"",), # This was before we had the `nan` behaviour; ax=ax,; adjust_kwargs=dict(arrowprops=dict(arrowstyle='-', color='black')),; text_kwargs=dict(fontsize=14),; ); fig = ax.get_figure(); fig.tight_layout(); plt.show(); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/100496350-81af9780-31a7-11eb-8b38-2eb7f914c1a1.png). I believe you're also supposed to be able to make the text repel from points, so they don't sit on top of your data, but I had some trouble getting that working at the time. I'm a bit antsy about having this as a required dependency since maintenance [doesn't seem too active](https://pypi.org/project/adjustText/#history). Could be an optional dependency, used with `legend_loc=""adjust_text""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689:1693,depend,dependency,1693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-735051689,2,['depend'],['dependency']
Integrability,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671:102,wrap,wrapper,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-791998671,1,['wrap'],['wrapper']
Integrability,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407:44,integrat,integrate,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625#issuecomment-487553407,1,['integrat'],['integrate']
Integrability,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354:407,integrat,integrated,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471808354,1,['integrat'],['integrated']
Integrability,"In the ""requirement already satisfied"" it looks like ""scikit-misc"" is installed in a different location and not within the `site-packages` folder of the anaconda env listed on the line below for `numpy`. From within the `py38` env you could try to reinstall it with `pip install --user scikit-misc --force` and also delete the other one or remove it from your `$PYTHONPATH`? Installing things in the jupyter notebook might be using a different version of pip than the one in the environment (depending on how your kernels are set up) so I think it's sometimes safer to do these things from the command line.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912:492,depend,depending,492,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-989974912,1,['depend'],['depending']
Integrability,"In the described case, it is expected that `adata.raw` is set. Thus, by default `sc.pl.dotplot` will use the raw data instead of the scaled `adata.X`. In the case in which `adata.raw` is not set, then dotplot will wrongly compute the percentage. The suggestion to uncouple the dot size and the dot color is very good and after the 1.6 relase of scanpy is now possible to do this using the `dot_color_df` (see https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.dotplot.html#scanpy.pl.dotplot). However, this requires that the user prepares a pandas dataframe for the color while the dot size (% of cells) can be computed based on raw, or a layer (depending on the settings for `use_raw` and `layer`). I am open to further suggestions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356:649,depend,depending,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361#issuecomment-674912356,1,['depend'],['depending']
Integrability,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python; adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression""; sc.pl.pca(adata, var_key=""gex""); sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); # This also has the nice feature that it could abstract out the current `use_highly_variable` argument; ```. View based:. ```python; gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]; sc.pp.pca(gex_view) # Calculate pca on gene expression; sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618:131,interface,interface,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618,1,['interface'],['interface']
Integrability,"Install it, itâ€™s an optional dependency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048:29,depend,dependency,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1369#issuecomment-673987048,1,['depend'],['dependency']
Integrability,"Instead of having `self.d` as the central attribute, why not having a central property `self.stats` and that's a DataFrame with a multi-index for the columns. The outer index is the group that you're comparing against a reference, the inner index loops over `gene`, `score`, `pval`, `pval_adj`, etc. Meaning, there is a class for this besides the convenience function:; ```; rg = sc.RankGenes(); ```. And if you just want a dataframe and not store something in `adata`, you can do; ```; rg.compute_stats(...); ```. After this `rg.stats` contains your results. Within the convenience wrapper `sc.tl.rank_genes`, this `stats` attribute will be written to `adata`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080:583,wrap,wrapper,583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-616490080,1,['wrap'],['wrapper']
Integrability,"Interesting! I was coming across this error in #2816 (where there is a fix), but only with older versions of dependencies. Probably worth back porting that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2830#issuecomment-1910515749:109,depend,dependencies,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2830#issuecomment-1910515749,1,['depend'],['dependencies']
Integrability,"Is it fine or need to do anything else before make a pull request ?. Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes!; >; > 1. make sure you have all your modified copy somewhere outside of the; > cloned directory! weâ€™re going to destroy all changes inside of that; > directory!; > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if; > itâ€™s really 1.4 and not e.g. 1.4.1); > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering; > 4. Copy your changes over again.; > 5. Add all files you changed individually (not git add . or git add -A,; > but git add scanpy/file1.py scanpy/file2.py ...); > 6. git commit -m 'your commit message'; >; > Now you can make a new PR with just your changes in it:; > master...Khalid-Usman:weighted-clustering; > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>; >; > Make sure that you only see your changes on the lower part of that page; > before hitting the â€œCreate Pull Requestâ€ button; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492175356:703,message,message,703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492175356,1,['message'],['message']
Integrability,Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368:86,integrat,integrates,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-776593368,1,['integrat'],['integrates']
Integrability,It appears this was an issue related to anndata2ri- scipy 1.0.1 was being installed when installing anndata2ri. Installing scanpy first prevented this issue. ; I use pip install --user for scanpy because otherwise I receive an error message: ; Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; My workaround has been to use --user as a directory and add a path to import scanpy.; I'm sorry for the trouble thank you for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302:233,message,message,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1252#issuecomment-636118302,1,['message'],['message']
Integrability,It causes exactly the same issue when I run:. ```; import numpy as np; import umap; ```; There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053:110,message,message,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-754566053,2,['message'],['message']
Integrability,"It doesnâ€™t. We could also. 1. wait to merge this until `skmisc` has a new release or; 2. throw a special error when people try to use seurat v3 with numpy 2. Sadly(?) Python doesnâ€™t allow packages to add constraint to other packagesâ€™ dependencies, else we could tell the resolver that all currently release skmisc versions are incompatible with numpy 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501:234,depend,dependencies,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3115#issuecomment-2182602501,1,['depend'],['dependencies']
Integrability,"It is difficult to find a one-size-fits-all solution. Depending on the number of legends, the length of the labels and the DPI used the space required will vary. Thus, I think is is better for the user to adjust the plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670:54,Depend,Depending,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1312#issuecomment-662452670,1,['Depend'],['Depending']
Integrability,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:511,rout,routine,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173,1,['rout'],['routine']
Integrability,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python; dof[np.isnan(dof)] = 0		; pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test; ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python; df, denom = stats.stats._unequal_var_ttest_denom(; v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest; ); df[np.isnan(df)] = 0; scores, pvals = stats.stats._ttest_ind_from_stats(; mean_group, mean_rest, denom, df; ); ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them); * Revert change (would bring back issue of genes with variance of 0); * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-488907170:1037,Wrap,Wrap,1037,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-488907170,1,['Wrap'],['Wrap']
Integrability,"It looks to me like the sklearn dependency was update more due to bugs in earlier 0.21.* releases series, see 7716bfdec3cb9bd19923a91180dabc35ffd7709a. We don't promise compatibility with older versions of sklearn, so downgrading is not a good long-term solution. @Koncopd might also be able to give some advice on this, as I believe he has been using pytorch with scanpy, though I'm not sure if this is via conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158:32,depend,dependency,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121#issuecomment-604799158,1,['depend'],['dependency']
Integrability,"It seems you do not always end up with n-1 neighbors, because for n=3, you suddenly get differing number of neighbors:; ```python; import scanpy as sc; adata = sc.datasets.blobs(n_observations=5). for n_neighbors in [1, 2, 3]:; sc.pp.neighbors(adata, n_neighbors=n_neighbors); print(f'n_neighbors = {n_neighbors}:\n', adata.uns['neighbors']['connectivities'].A); ```; Output:; ```; n_neighbors = 1:; [[0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]; [0. 0. 0. 0. 0.]]; n_neighbors = 2:; [[0. 0. 0. 1. 0.]; [0. 0. 1. 0. 0.]; [0. 1. 0. 0. 0.]; [1. 0. 0. 0. 1.]; [0. 0. 0. 1. 0.]]; n_neighbors = 3:; [[0. 0.5849553 0. 1. 0.5849636 ]; [0.5849553 0. 1. 0.5849678 0. ]; [0. 1. 0. 0.58496827 0. ]; [1. 0.5849678 0.58496827 0. 1. ]; [0.5849636 0. 0. 1. 0. ]]; ```; It is been while that I read about UMAP and can't get my head around why this happens right now. Relying on UMAP seems a good idea to me, maybe the corner case `n_neighbors=1` should just be catched with a more meaningful error message?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174:1008,message,message,1008,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1706#issuecomment-788885174,1,['message'],['message']
Integrability,"It would depend on what data was in the `Batch` column. HDF5 store are typed, so we can't store columns with mixed kinds of values. If the column's dtype is `object`, we check to see if it's string values, otherwise we say we don't know how to write it, since it could be any mix of things. What kinds of values did you have in `Batch`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537:9,depend,depend,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866#issuecomment-862113537,1,['depend'],['depend']
Integrability,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:258,interface,interface,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645,1,['interface'],['interface']
Integrability,"Itâ€™s more code we have to maintain. If I had to decide between adding a common dependency, feature regression, or complex code, Iâ€™d go with the first one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858:79,depend,dependency,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666510858,1,['depend'],['dependency']
Integrability,"Iâ€™m not categorically against it, but could you describe what issues you encounter trying to use scanpy on the data you want to use it on? E.g. naively, Iâ€™d think youâ€™d just wrap your matrix in an AnnData object, then run `diffmap`:. ```pycon; >>> import scanpy as sc; >>> adata = sc.AnnData(my_matrix) # shape: (n_observations, n_variables); >>> sc.tl.diffmap(adata); ValueError: You need to run `pp.neighbors` first to compute a neighborhood graph.; ```. Then you just follow that advice and `repr` the object after to see whatâ€™s in there:. ```pycon; >>> sc.pp.neighbors(adata); >>> sc.tl.diffmap(adata); >>> adata; AnnData object ...; uns: diffmap_evals; obsm: X_diffmap; ```. Alternatively you read the docs: The [`diffmap` docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.diffmap.html) point out both how to use `neighbors` â€¦. > The width (â€œsigmaâ€) of the connectivity kernel is implicitly determined by the number of neighbors used to compute the single-cell graph in [`neighbors()`](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors). To reproduce the original implementation using a Gaussian kernel, use `method=='gauss'` in [`neighbors()`](https://scanpy.readthedocs.io/en/stable/api/generated/scanpy.pp.neighbors.html#scanpy.pp.neighbors). To use an exponential kernel, use the default `method=='umap'`. Differences between these options shouldnâ€™t usually be dramatic. â€¦ and where the results are pushed:. > â€¦ Sets the following fields:; > ; > `adata.obsm['X_diffmap']` : [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) (dtype `float`); > ; > > Diffusion map representation of data, which is the right eigen basis of the transition matrix with eigenvectors as columns.; >; > `adata.uns['diffmap_evals']` : [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray) (dtype `float`); > ; > > Array of size (number of eigen vectors).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3054#issuecomment-2114964051:174,wrap,wrap,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3054#issuecomment-2114964051,1,['wrap'],['wrap']
Integrability,"Iâ€™m quite sure it has a resolution parameter, but at this point Iâ€™m also quite sure Iâ€™m messing up with modules and dependencies, both in this thread and on my local installation... about conda, I guess Iâ€™m one of the last around who hasnâ€™t adopted it yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151:116,depend,dependencies,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-440443151,1,['depend'],['dependencies']
Integrability,"Jumping in on this conversation to ask a related question - I'm using Scanorama to integrate some datasets and generate an aligned low-dimensional embedding. I then subset the data to only look at specific clusters and want to re-make the UMAP/t-SNE plot. Do you usually re-do the integration to generate a new low-dimensional embedding matrix with Scanorama for the subsetted data? I know you can technically subset the original low-dimensional embedding matrix, but I thought it's preferable to re-do the embedding when you have a different subset of cells to capture more of the variance between those cells (re-select HVGs, etc). Any advice would be welcome - thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919:83,integrat,integrate,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1059551919,2,['integrat'],"['integrate', 'integration']"
Integrability,"Just came across this - is this still relevant?; Scanpy as is does not feature a neat solution integrating with interactive interfaces which would allow to manually tag/select individual points from a plot - for such tasks, [holoviz](https://holoviz.org/) tools might be considered.; As sidenote, a heads-up about considering 2D representations with caution e.g. [here](https://www.sciencedirect.com/science/article/pii/S2405471223002090?via%3Dihub) - considering metrics instead of visual low-dimensional representations to detect or remove outliers might be considered as a viable alternative here in many cases :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475:95,integrat,integrating,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1992#issuecomment-1798949475,2,"['integrat', 'interface']","['integrating', 'interfaces']"
Integrability,"Just checked using this dockerfile, works flawlessly:. ```dockerfile; FROM continuumio/miniconda. RUN conda install python=3.8; RUN pip install flit>=3.1; RUN git clone https://github.com/theislab/scanpy.git; WORKDIR /scanpy; # Go to the mainline-pip branch if it hasnâ€™t been merged into master yet; RUN git checkout mainline-pip || true; RUN FLIT_ROOT_INSTALL=1 flit install -s --dep=develop # Make development install of scanpy; # Make sure the dist-info folder has a plus in its name; RUN SCANPY_VERSION=$(python -c 'from importlib.metadata import version; print(version(""scanpy""))') && \; echo $SCANPY_VERSION | grep '+' &&; test -d /opt/conda/lib/python3.8/site-packages/scanpy-$SCANPY_VERSION.dist-info; # Install project that depends on scanpy; RUN pip install scvelo; # Make sure itâ€™s still a dev install; RUN test -L /opt/conda/lib/python3.8/site-packages/scanpy; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617:733,depend,depends,733,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1702#issuecomment-788200617,1,['depend'],['depends']
Integrability,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect?. A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971:88,protocol,protocol,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-471439971,2,['protocol'],"['protocol', 'protocols']"
Integrability,"Just for reasons of practicality. I figured it would create a mess to visualize more than 10 plots with different groups. Also, you should have sufficient cells per group to make the kde calculation meaningful. That will depend on the number of cells as well though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/719#issuecomment-507260999:221,depend,depend,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/719#issuecomment-507260999,1,['depend'],['depend']
Integrability,"Just in general: the main route to ""improving the results"" is to make sure that your preprocessing yields a meaningful tSNE for the standard parameters. Then also DPT will perfectly do its job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-310275631:26,rout,route,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-310275631,1,['rout'],['route']
Integrability,"Just my 2cents: ; I made really good experiences with Github actions.; * I find them easy to set-up and they run many (20-40?) jobs in parallel. ; * Really good integration with Github (e.g. upload to PyPI on release) ; * windows testing works well, but it is a pain to setup pycairo, see [here](https://github.com/icbi-lab/scirpy/blob/725664a22e6265643633d89a7f38ea3383ccab48/.github/workflows/test.yml#L34) and [here](https://github.com/pygobject/pycairo/issues/19#issuecomment-638716293). . Here's the [github actions script for scirpy](https://github.com/icbi-lab/scirpy/blob/master/.github/workflows/test.yml).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154:161,integrat,integration,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358#issuecomment-674834154,1,['integrat'],['integration']
Integrability,"Just pushing the updates now @LuckyMD ðŸ˜„. One issue with the enrichment as is, is that `gprofiler-official` import name conflicts with the previous unofficial wrapper. I'm worried that this will break peoples environments if they're not aware of this. @liiskolb, do you have any thoughts on this?. Otherwise, I think this should be alright. I'd like to know if there'd be any interest in moving the utility function `rank_genes_groups_df` (added here) into a more central place. I personally use it anytime I use scanpys differential expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474:158,wrap,wrapper,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-483199474,1,['wrap'],['wrapper']
Integrability,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:932,integrat,integration,932,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611,1,['integrat'],['integration']
Integrability,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051:131,depend,depending,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654#issuecomment-494386051,1,['depend'],['depending']
Integrability,"Like you say, the difference between this and `ingest` is joint PCA calculation vs asymmetric batch integration. This function is the first step in the `fastMNN` function, which I have found in some cases yields very sensible batch correction results. It would be awesome to see `multiBatchPCA` +/- `fastMNN` available in scanpy. I am aware of the python implementation of `mnncorrect`, but I think this still operates on expression values rather than a PCA representation (correct me if I am wrong..). Without going all the way the batch correction, `multiBatchPCA` is useful where different experiments have very different numbers of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353:100,integrat,integration,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-671228353,1,['integrat'],['integration']
Integrability,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```; > from scipy.misc import factorial; E ImportError: cannot import name 'factorial'; ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError; ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166:383,depend,dependency,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-495552166,1,['depend'],['dependency']
Integrability,"Looking at this again, now that I have gone through everything, I think we actually need to check types directly and shouldn't rely on `isbacked` because it is possible to do something like `adata.layers['foo'] = sparse_dataset(g_layer)` and this should also error our with a helpful message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455:284,message,message,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455,1,['message'],['message']
Integrability,Looks super cool... but also like a heavy dependency. Do you think it would be worth using datashader when we are just looking for a simple additional function?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800:42,depend,dependency,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-479455800,1,['depend'],['dependency']
Integrability,"MNN does take fairly long. There is a faster version of it, which runs on PCA I think though, but it's not in scanpy external. Before you didn't integrated anything, as the function thought you just have 1 batch the way you ran it. I would report your issue with bbknn on the BBKNN github repo directly. You may get better suggestions there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157:145,integrat,integrated,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873#issuecomment-543582157,1,['integrat'],['integrated']
Integrability,"Matplotlib 3.4 has dropped 3.6 support. Since matplotlib is our most painful dependency (reliably causes test failures when it updates), it's a great time to drop 3.6.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473:77,depend,dependency,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1697#issuecomment-809011473,1,['depend'],['dependency']
Integrability,Maybe I could throw in another ID mapping tool. [BED](https://f1000research.com/articles/7-195/v1) is pretty good. More comprehensive than Biomart and quicker too. It is however a local implementation that runs in a docker container. The image is updated every month or so. At the moment I run a container internally here... but maybe we could make a webserver out of this which can be directly integrated with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068:395,integrat,integrated,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-458065068,1,['integrat'],['integrated']
Integrability,Maybe it would be a good idea to have a separate repo of `rpy2` and `anndata2ri` wrappers for `R` methods that we want to run in scanpy workflows. Would you be interested in sth like that? I could create a separate repo in theislab github? Something like `www.github.com/theislab/Rforscanpy`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116:81,wrap,wrappers,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590116116,1,['wrap'],['wrappers']
Integrability,"Much of the spatial data is stored in `uns`, which does not get combined by default. There is an example of concatenating visium datasets [in the tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html#Data-integration) and more information on concatenating `.uns` [in the latest anndata docs](https://anndata.readthedocs.io/en/latest/concatenation.html#merging-uns).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317:214,integrat,integration-scanorama,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1254#issuecomment-635107317,2,['integrat'],"['integration', 'integration-scanorama']"
Integrability,"My intuition would be neighbor finding would take more time as dataset size increases. What exact fraction of the time will depend a lot on number of samples, number of features, and possibly distance metric. If you're investigating yourself, I think trying `line_profiler`'s `%lprun` on `umap.UMAP.fit` would be a good bet. I'd also bet that they'd have a better idea over at `UMAP` or Pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146:124,depend,depend,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810#issuecomment-528383146,1,['depend'],['depend']
Integrability,"My main reasoning was that:. * Use dask array/ delayed is more simple; * One can go from dask array/ dask delayed -> `Future` with `client.compute`, but the other way around isn't really possible since `Future`s kick off computation immediately. So I'd like to use the higher level interface as much as possible, and only use lower level when necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332:282,interface,interface,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1980790332,1,['interface'],['interface']
Integrability,"My priority are intuitive semantics so people can add or bump dependencies without 100% understanding the algorithm of the minimum dependency script. So I can think of options:. 1. Each version must be fully specified (`>=1.2.0`, not `>=1.2`). The script installs exactly the specified minimum version. Implementation: Would be quickly done now, just check the job run and change `matplotlib>=3.6` to `matplotlib>=3.6.3` and so on. Effect: whenever we bump something, we probably need to bump more things, which might sometimes be painful. The minimum versions will be more accurate, as we know that the exact versions specified successfully run out test suite. 4. We maintain a list of all dependencies we have together with data about which version segment denotes the patch version (i.e. for semver itâ€™s the third, for calendar ver, itâ€™s nothing), then modify versions based on that knowledge (e.g. semver `>=1.2.3` â†’ `>=1.2.3, <1.3`). Implementation: Each newly added dependency needs to be added to that list. Effect: This would be basically a more powerful (able to specify minimum patch) and obvious version of what youâ€™re doing now (explicit data instead of the presence of a patch version indicating if something is semver or not). In both versions, thereâ€™s no hidden semantics in `>=1.2` that would distinguish it from `>=1.2.0`, which is what Iâ€™m after. What does your experience while implementing this so far say to these? Any other ideas?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240:62,depend,dependencies,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1943497240,4,['depend'],"['dependencies', 'dependency']"
Integrability,"Nice! Tests should also be run by Travis, shouldn't they? Or have we missed out on demanding dependencies and your tests won't run through for that reason? If so, please point me to it and I'll make sure that Travis actually runs the tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/439#issuecomment-456635443:93,depend,dependencies,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/439#issuecomment-456635443,1,['depend'],['dependencies']
Integrability,"Nice! Thanks!. On Mon, 8 Jun 2020, 10:46 giovp, <notifications@github.com> wrote:. > @vitkl <https://github.com/vitkl> now multiple samples are supported, see; > here; > <https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html>; > for description on how to use the new concat strategy; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1158#issuecomment-640496084>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTVZVVWII7Z7Q34ZPTQ3RVSXOVANCNFSM4MEXUAPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513:229,integrat,integration-scanorama,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1158#issuecomment-640632513,1,['integrat'],['integration-scanorama']
Integrability,"No problem, I'll change it to your preferred style. I don't think it's a problem to add the chunking but I'll need to test it for sparse matrices. Just to clarify, what I meant by ""more functional style"" is something like this:. ```; processed_data = raw_data.log1p().normalize(options...).some_other_method(options...); ```. That is, it allows a [functional programming](https://en.wikipedia.org/wiki/Functional_programming) style. Similar to libraries like `scikit-learn` (e.g. `fit()` returns `self` so you can immediately call another method) or `keras` (see the [functional API guide](https://keras.io/getting-started/functional-api-guide/). But as you say, that might be a dramatic change in coding style for your library. I find it can lead to simpler code but that's a personal preference. The above examples are notable because they allow both functional and declarative styles of coding, depending on the user.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179:898,depend,depending,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403242179,1,['depend'],['depending']
Integrability,"No worries, feel free to rename :) Monday, 20 June 2022, 09:11PM +02:00 from Isaac Virshup ***@***.*** :. ***@***.*** , would it be fair to retitle this something like ""Generate BioContainer images"", or should that be a separate issue?; >â€”; >Reply to this email directly, view it on GitHub , or unsubscribe .; >You are receiving this because you were mentioned. Message ID: @ github . com>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160765884:362,Message,Message,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1160765884,1,['Message'],['Message']
Integrability,"No worries. Arguably scanpy should check for common compression formats with V2 (and no compression for V3) while reading the input files, or at least provide a more informative traceback, especially if data on GEO routinely includes these formats",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689:215,rout,routinely,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408#issuecomment-689676689,1,['rout'],['routinely']
Integrability,"No worries. It can be something like:. > Subset of groups, e.g. ['g1', 'g2', 'g3'], for which the list of DE genes should be computed. Each group of cells is always compared to the remaining cells, even if they don't belong to the subset of groups. However, it would be probably more useful to change the API and to implement subsetting of the groups through the 'groups' parameter. I think this would be more intuitive, also together with the use of the 'reference' parameter.; In particular, because retrieving of DE genes for specific groups can be more easily done with the [get](https://scanpy.readthedocs.io/en/stable/api/scanpy.get.rank_genes_groups_df.html#scanpy.get.rank_genes_groups_df) interface.; But there may be other use cases I haven't considered in which the actual implementation may be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315:698,interface,interface,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842#issuecomment-531838315,1,['interface'],['interface']
Integrability,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:19,depend,depends,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621,2,['depend'],"['depending', 'depends']"
Integrability,"No, the matplotlib error message was really confusing... the 'on data' and 'right margin' locations are scanpy features and should be in the error message... wanted to do this anyways. :wink:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686:25,message,message,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/88#issuecomment-366300686,2,['message'],['message']
Integrability,"No, those â€œinvalid instructionâ€ errors pop up sometimes. I think theyâ€™re caused by some dependency being compiled for an instruction set that not all GitHub runners support. Ways to deal with it:. 1. just restart until it works (annoying, but not much work); 2. figure out broken dependency, then; 1. if the wheel on PyPI is broken, raise an issue upstream; 2. if we compile it in the runner ourselves, set a compile flag to make it only use instructions that are compatible with all runners (i.e. not `-m arch=native` but select [an older architecture](https://en.wikipedia.org/wiki/X86-64#Microarchitecture_levels))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814222794:88,depend,dependency,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814222794,2,['depend'],['dependency']
Integrability,"None, there is no problem on our side right? @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015 to wait for matplotlib/matplotlib#14298 to be fixed. It seems to be in matplotlibâ€™s 3.1.2 milestone, so we can maybe just set the dependency to â€œmatplotlib == 3.0.0 or matplotlib >= 3.1.2â€",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/787#issuecomment-531732473:260,depend,dependency,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787#issuecomment-531732473,1,['depend'],['dependency']
Integrability,"Not completely sure if this is doing what I intended. I added the `-U` so dependencies would be upgraded, but numpy still isn't being upgraded as shown by these warnings:. ```; umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.15.4 which is incompatible.; scvi 0.6.6 has requirement numpy>=1.16.2, but you'll have numpy 1.15.4 which is incompatible.; ```. Not sure why this is happening. I'd prefer if we didn't have to manually specify the dependencies of our dependencies. Any ideas @flying-sheep?. ------------------. Updating pip doesn't seem to do anything (maybe it has to do with ""editable mode""?). --------------------. An easy fix is just to add a version requirement on `numpy`, but I really feel like dependency resolution should be dealing with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855:74,depend,dependencies,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659867855,4,['depend'],"['dependencies', 'dependency']"
Integrability,"Note also, when I follow the same protocol for a similar dataset (different timepoint for sequencing), regressing out does not cause this problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-411603192:34,protocol,protocol,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-411603192,1,['protocol'],['protocol']
Integrability,"OK, as I wrote here: https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108. > We have no native code in Scanpy, so we donâ€™t cause segfaults. If thereâ€™s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies. Reinstalling your environment often helps. If not, please give us a way to completely reproduce this just from copyable code, i.e. . 1. a lockfile (environment.yaml or requirements.txt) containing the exact versions of everything in your environment; 2. a block of code that throws the error when run in that environment (code should download the data necessary to reproduce the issue). Then we can help",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1909658183:266,depend,dependencies,266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1909658183,1,['depend'],['dependencies']
Integrability,"OK, issues like this are almost always either memory or dependency problems: Somethingâ€™s miscompiled or compiled for the wrong architecture (e.g. a newer CPU than you have) or simply buggy. We have no native code in Scanpy, so we donâ€™t cause segfaults. If thereâ€™s anything we can mitigate, we will, if someone demonstrates a reproducible problem with up-to-date dependencies",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108:56,depend,dependency,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359#issuecomment-1909651108,2,['depend'],"['dependencies', 'dependency']"
Integrability,"OK, so now the question is: should this become part of legacy-api-wrap?. Iâ€™d rather have the API fixed once than using multiple decorators. I think Itâ€™s clearer to see what the new API is like if you donâ€™t have to think about the order of multiple decorators being applied. Also, I think. ```py; @renamed_args(new=""old""); ```. feels more natural than. ```py; @deprecated_arg_names({""old"": ""new""}); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422:66,wrap,wrap,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/474#issuecomment-471489422,1,['wrap'],['wrap']
Integrability,"Oh damn, I was meant to send a PR from the web interface but didn't notice the default is to commit directly. Sorry about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-519249007:47,interface,interface,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-519249007,1,['interface'],['interface']
Integrability,"Oh, by version numbers I meant not just `anndata` but the dependencies as well. I'm wondering in particular about the version of `h5py`. The output of something like [`sinfo(dependencies=True)`](https://pypi.org/project/sinfo/) would be great. If you try writing to a different path, are you able too? It kind of looks like you're writing to a file that already exists, though that should just overwrite the file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662:58,depend,dependencies,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1275#issuecomment-647275662,2,['depend'],['dependencies']
Integrability,Okay all done. `flit install -s` was getting too messy as some dependency installs scanpy and then things canâ€™t be symlinked â€¦. better leave the `pip install -r` in temporarily,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479:63,depend,dependency,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-778328479,1,['depend'],['dependency']
Integrability,"Okay, this is weird. My current theory is that it's a bug in pip. It looks like if multiple packages depend on a single dependency, pip doesn't necessarily check if all requirements are satisfied. It just checks if one packages version requirements are. It seems non-deterministic which package is used to check. --------------------------------------. I was working on an example, but then I just found this: https://github.com/pypa/pip/issues/8218. There's an unstable feature for this (`--unstable-feature=resolver`), which does work, but I think adding a requirement for numpy is a bit more justifiable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938:101,depend,depend,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1320#issuecomment-659921938,2,['depend'],"['depend', 'dependency']"
Integrability,"One important thing: pip supports self-depending. Iâ€™ve written dep lists like. ```toml; [project]; name = 'myproj'. [project.optional-dependencies]; # myprojâ€™s exported testing tools depend on those:; testing = ['pytest-postgresql']; # to run our packageâ€™s tests, we need:; test = ['pytest', 'myproj[testing]']; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088715295:39,depend,depending,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088715295,3,['depend'],"['depend', 'dependencies', 'depending']"
Integrability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:137,depend,dependencies,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,3,"['bridg', 'depend', 'integrat']","['bridge', 'dependencies', 'integration']"
Integrability,"One other thing. We'd like to have two new convenience functions:. ```; def neighbors_update(adata, adata_new); def umap_update(adata, adata_new); ```. The first maps the new data into the existing neighbor graph based on the chosen latent representation. The second maps the new data into the existing UMAP embedding. For the second function, one just needs to find a good way of wrapping; ```; model = umap.UMAP(seed=1234); model.fit(X); model.transform(new_X); ```; For the first, I'm not quite sure how easy it is easy. I'm using `pynndescent` for it, which will become UMAP's dependency at some point, but isn't yet. Maybe what UMAP does internally is already sufficient, but I don't know. Can you investigate and if it's easy cover in this PR? If it's tricky, let's wait for another PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-479424924:381,wrap,wrapping,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-479424924,2,"['depend', 'wrap']","['dependency', 'wrapping']"
Integrability,"One potential solution is to convert the integrated connectivity matrix, C, into a pseudo-distance matrix (1-C) (this probably won't work for datasets much larger than 10k cells due to memory limitations) and run t-SNE with the 'precomputed' metric on that fake distance matrix. If scanpy's t-SNE wrapper does not allow passing a precomputed distance matrix, I would recommend using the sklearn implementation directly:. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446:41,integrat,integrated,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1370#issuecomment-689005446,2,"['integrat', 'wrap']","['integrated', 'wrapper']"
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 5fc12f4a918e21f0c57937b787d52040db046f01; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1587: Attach failing plots to CI results'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1587-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1587 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128:885,integrat,integration,885,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587#issuecomment-787808128,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 ce508c4084e8df272163f4e17136386cfaec2605; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1768: Fix correlation plot test for new version of matplotlib'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1768-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1768 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499:906,integrat,integration,906,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1768#issuecomment-809014499,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; $ git checkout 1.7.x; $ git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; $ git cherry-pick -m1 f7279f6342f1e4a340bae2a8d345c1c43b2097bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; $ git commit -am 'Backport PR #1679: enables highly_variable_genes_seurat_v3 to accept pseudocounts'; ```. 4. Push to a named branch :. ```; git push YOURFORK 1.7.x:auto-backport-of-pr-1679-on-1.7.x; ```. 5. Create a PR against branch 1.7.x, I would have named this PR:. > ""Backport PR #1679 on branch 1.7.x"". And apply the correct labels and milestones. Congratulation you did some good work ! Hopefully your backport PR will be tested by the continuous integration and merged soon!. If these instruction are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648:913,integrat,integration,913,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1679#issuecomment-814587648,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5c0e89e99dc2461c654c549435a73f547f3573ce; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3339: Add PYI lints'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3339-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3339 on branch 1.10.x (Add PYI lints)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625:881,integrat,integration,881,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3339#issuecomment-2457653625,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5d5d873b1fb0353089569f85580b43437df9c6cd; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3104: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3104-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3104 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624:929,integrat,integration,929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3104#issuecomment-2160085624,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.10.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 8d046ff37e024ae88eadfb22ea8fd142a6b95aa1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #3093: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.10.x:auto-backport-of-pr-3093-on-1.10.x; ```. 5. Create a PR against branch 1.10.x, I would have named this PR:. > ""Backport PR #3093 on branch 1.10.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991:929,integrat,integration,929,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3093#issuecomment-2146729991,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 05dcf68f32ce255447ea804de55babefb3c47c92; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2753: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2753-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2753 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2753#issuecomment-1809942763,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 330a099ffe76286f0f047387701af7e9fd58831a; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2838: Fix pytest 8 compat'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2838-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2838 on branch 1.9.x (Fix pytest 8 compat)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036:888,integrat,integration,888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2838#issuecomment-1923260036,2,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 47664d83a7bc47756356b907e5719076ab187361; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2784: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2784-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2784 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2784#issuecomment-1862463379,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 4f4b1c3a655546d981360bcce625d354a4291385; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2811: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2811-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2811 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2811#issuecomment-1893536608,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 585f58c9e4dd82dd7809a831538c4e230b008818; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2841: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2841-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2841 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2841#issuecomment-1929072209,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 5ccce795b19a5aa59a6b1f1c3552884ed6fc94d1; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2544: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2544-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2544 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2544#issuecomment-1619899808,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 86dc4d5d96eb7547833e7805ea2f7d603bd3ba2d; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2779: Fix anndata warnings'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2779-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2779 on branch 1.9.x (Fix anndata warnings)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974:890,integrat,integration,890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2779#issuecomment-1858121974,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 95206dc54c8bb0d9d478f09f47dff9477a5c58c4; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2704: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2704-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2704 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2704#issuecomment-1776676386,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 b23229f9bfc95ff90a5d6393b4d53d062190d5bb; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2732: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2732-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2732 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2732#issuecomment-1795950835,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 bf5f27aa9e968de6e73fc7abb46a89084ddf6880; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2831: Prepare 1.9.8, stop ignoring citation errors'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2831-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2831 on branch 1.9.x (Prepare 1.9.8, stop ignoring citation errors)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423:938,integrat,integration,938,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2831#issuecomment-1911960423,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c2f706b35d52a5e21ccf84f1cd299b0dadf49668; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2716: Add missing link targets'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2716-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2716 on branch 1.9.x (Add missing link targets)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886:898,integrat,integration,898,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2716#issuecomment-1780921886,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 c410cd123f5487f25c08b421c8d06da50551ff73; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2799: [pre-commit.ci] pre-commit autoupdate'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2799-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2799 on branch 1.9.x ([pre-commit.ci] pre-commit autoupdate)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300:924,integrat,integration,924,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2799#issuecomment-1882962300,1,['integrat'],['integration']
Integrability,"Owee, I'm MrMeeseeks, Look at me. There seem to be a conflict, please backport manually. Here are approximate instructions:. 1. Checkout backport branch and update it. ```; git checkout 1.9.x; git pull; ```. 2. Cherry pick the first parent branch of the this PR on top of the older branch:; ```; git cherry-pick -x -m1 e5d41d4aa58a925f0fa5cfcf580cb975167a71c9; ```. 3. You will likely have some merge/cherry-pick conflict here, fix them and commit:. ```; git commit -am 'Backport PR #2235: Separate test utils from tests'; ```. 4. Push to a named branch:. ```; git push YOURFORK 1.9.x:auto-backport-of-pr-2235-on-1.9.x; ```. 5. Create a PR against branch 1.9.x, I would have named this PR:. > ""Backport PR #2235 on branch 1.9.x (Separate test utils from tests)"". And apply the correct labels and milestones. Congratulations â€” you did some good work! Hopefully your backport PR will be tested by the continuous integration and merged soon!. Remember to remove the `Still Needs Manual Backport` label once the PR gets merged. If these instructions are inaccurate, feel free to [suggest an improvement](https://github.com/MeeseeksBox/MeeseeksDev).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870:910,integrat,integration,910,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1604242870,1,['integrat'],['integration']
Integrability,"PS: You don't need a test for this... it would require installing phate on travis and this would take time... Also, the interface is trivial. You should simply link to your package within the docs to redirect people for bugs and more info.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220:120,interface,interface,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/136#issuecomment-385960220,1,['interface'],['interface']
Integrability,"People are discussing intersections [here](https://github.com/python/typing/issues/213). And I agree with you @ivirshup: Thatâ€™s a great example where an intersection would be needed. Unions are only useful if you accept several things and somewhere switch behavior based on what you got like `if isinstance(...):`. I think that Alex just means that something like that isnâ€™t needed anywhere in scanpy. An aside about switching behavior based on types: Too bad Python hasnâ€™t been designed with destructuring `match`/`switch`. Rust is beautiful because of it. However, Python would probably need to use something like [scalaâ€™s unappy](https://docs.scala-lang.org/tour/extractor-objects.html) which I could never wrap my brain around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443655464:710,wrap,wrap,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443655464,1,['wrap'],['wrap']
Integrability,Phneograph was recently updated and also new wrappers are available in external thanks to @awnimo @Koncopd .; Does this work for you @asmariyaz23 ? I will close this but feel free to reopen,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788:45,wrap,wrappers,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1407#issuecomment-706139788,1,['wrap'],['wrappers']
Integrability,Please ask usage questions here: https://discourse.scverse.org/. You should not integrate normalized and unnormalized counts. Consider getting the raw counts or integrating on the normalized counts,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652:80,integrat,integrate,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2662#issuecomment-1723238652,2,['integrat'],"['integrate', 'integrating']"
Integrability,"Please provide more details. What is `folder` in your case and what is the error message? Follow the issue template, please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727:81,message,message,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817677727,1,['message'],['message']
Integrability,"Possible TODO:. - normalize_pearson_residuals_pca. @ivirshup I reverted the change in a6290ee9e0d1baf0e3483118aa552b6f6dcf02c0 where you changed. ```diff; -X_pca = np.zeros((X.shape[0], n_comps), X.dtype); +X_pca = np.zeros((adata_comp.shape[0], n_comps), adata.X.dtype); ```. the commit message is â€œFix up pca testsâ€, but that change doesnâ€™t seem to impact tests and it takes properties from several different object without reasoning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2272#issuecomment-1807755523:288,message,message,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2272#issuecomment-1807755523,1,['message'],['message']
Integrability,"Project specific IO is interesting but IMO makes it even more complicated in some ways. The current biggest problem we face is that no one knows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:796,depend,dependency,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,2,['depend'],['dependency']
Integrability,"Pytables is in requirements.txt (the PyPI package is called â€œtablesâ€), how did yâ€™all get Scanpy installed without all its dependencies?. https://github.com/theislab/scanpy/blob/f252d3a84200cc76060a786ef0589405fc5c9c12/requirements.txt#L7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198:122,depend,dependencies,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462133198,1,['depend'],['dependencies']
Integrability,"Ran into this today as a coworker wanted to stick a UMAP legend into `lower left` and couldn't. I whipped up a hotfix, which turned out to be exactly the same syntax as you've got going on here, and was on my way to mention it when I found this. Would be nice to see this integrated as the docs imply this is possible, and it seems useful to have on occasion?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426:272,integrat,integrated,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2277#issuecomment-1976241426,1,['integrat'],['integrated']
Integrability,"Re: testing externals, I've tried my best to just test the way it interfaces with scanpy. i.e., if MAGIC silently fails to return the correct output, scanpy tests would pass so long as the output is the right type / shape. If MAGIC throws an error when run from scanpy, this might be something you would like to address (i.e. by contacting the relevant external developer) regardless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189:66,interface,interfaces,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/988#issuecomment-573589189,1,['interface'],['interfaces']
Integrability,"Regarding the other packages: of course, we will also interface those as optional dependencies... But I'd do it from the original Scanpy repo. To me, the whole problem is simply about keeping a clean structure and throwing clear error messages if optional dependencies are not installed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862:54,interface,interface,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382344862,4,"['depend', 'interface', 'message']","['dependencies', 'interface', 'messages']"
Integrability,"Regarding this pull request: I'd merge it, put the `rpy2` dependency in the interface and remove it from the requirements. and put the interface into `rtools`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473:58,depend,dependency,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382345473,3,"['depend', 'interface']","['dependency', 'interface']"
Integrability,Remaining Qs:. * Does this need support for nans?; * This code is based off `sklearn.utils.sparsefuncs.mean_variance_axis`. Do we need a copy of the sklearn license here? Do we already include that since it's a dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/857#issuecomment-537310661:211,depend,dependency,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857#issuecomment-537310661,1,['depend'],['dependency']
Integrability,"Running `0.10.3` here. I had a similar error message, originating in a **mismatch of the indices of the obs-like dataframes** of the `AnnData` object. Although it was not exactly the same (sorry i don't have it here), maybe this can be a hint. Here is how i solved the issue. ```python; ### Have a look at the indices; for key, obs_matrix in adata.obsm.items():; if hasattr(obs_matrix, ""index""):; print(key); print(obs_matrix.index); print(adata.obs_names); print(adata.obs.index). ### If there's a mismatch, you can fix by running something like:; for key, obs_matrix in adata.obsm.items():; if hasattr(obs_matrix, ""index""):; obs_matrix.index = adata.obs_names; ```. I also had a similar error message when there was a mismatch in the _name_ of the index, or if the name of the index was also the _name of a column_. Note that if there is a mismatch, the `adata.write_h5ad` function _does not_ crash. While reading the saved file with `ad.read_h5ad` _will crash_. Suggestion for developers: When the `AnnData.write_h5ad` method is called, check the homogeneity of the indices, and raise an exception if there is a mismatch. Suggestion for users: make sure all the index are the same, have the same name, and that no column has the same name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-2162803279:45,message,message,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-2162803279,2,['message'],['message']
Integrability,"Same error occurred when using the default leiden with weights as well; downgrading python-igraph to 0.9.11 fixed the issue.; leidenalg is dependent on python-igraph (0.10.0 for my conda) and igraph (0.9.10), and I suppose the version discrepancy caused the problem. Or you can replace tl.leiden with leiden algorithm in python-igraph:. adjacency = sc._utils._choose_graph(adata, obsp=None, neighbors_key=None); g = sc._utils.get_igraph_from_adjacency(adjacency); clustering = g.community_leiden(objective_function='modularity', weights='weight', resolution_parameter=0.5); adata.obs['leiden_igraph_weight'] = pd.Series(clustering.membership, dtype='category', index=adata.obs.index). sc.pl.umap(adata, color='leiden_igraph_weight')",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1262134575:139,depend,dependent,139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1262134575,1,['depend'],['dependent']
Integrability,"Same thing: If the line is under-indented, the first line summary canâ€™t be properly extracted. Iâ€™ll make the message more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728:109,message,message,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1492#issuecomment-726053728,1,['message'],['message']
Integrability,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:736,message,messages,736,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,2,"['message', 'wrap']","['messages', 'wrapped']"
Integrability,Shouldnâ€™t we just depend on `requests` if itâ€™s so complicated and we have to resort to code copying?. Basically every Python user should have it installed anyway.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642:18,depend,depend,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1344#issuecomment-666331642,1,['depend'],['depend']
Integrability,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701:64,interface,interface,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-758543701,1,['interface'],['interface']
Integrability,"Since the API being used here is deprecated, as is the wrapper, I'm inclined leave this as is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014:55,wrap,wrapper,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814593014,1,['wrap'],['wrapper']
Integrability,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865:36,depend,depend,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619#issuecomment-487811865,1,['depend'],['depend']
Integrability,"Since the bug happens in mnnpy and isnâ€™t caused by the scanpy wrapper, this is not a scanpy bug: chriscainx/mnnpy#30",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974#issuecomment-573589167:62,wrap,wrapper,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-573589167,1,['wrap'],['wrapper']
Integrability,Since this is an overflow any data set with 1000's of cells I can use for this? I think it is Windows specific crash and how python implements sqrt() on windows which probably is a wrapper of the native math library in C. I may be wrong. So will the regression test work in this case?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013:181,wrap,wrapper,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1061#issuecomment-588274013,1,['wrap'],['wrapper']
Integrability,"So it will only work on non-negative expression values without any pre-process?; I guess that make sense, thank you for the reply. The version of the package:. scanpy==1.4.6 anndata==0.7.1 umap==0.4.0 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0. The AnnData objects were all read through same commands without any modification. sc.read_10x_h5(filepath, gex_only=False). the dataset I used to test them are:. https://support.10xgenomics.com/single-cell-vdj/datasets/2.2.0/vdj_v1_hs_nsclc_5gex; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_protein_v3; https://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/malt_10k_protein_v3. It appears to me that it only works on the v2 nsclc h5 data. I was trying to merge the three data sets and run through SAM to compare with the result of BBKNN, didn't work. So I tried to run each of them individually in the loop. I guess it won't work on CITESeq data without other processing?. I tried removed all the antibody read counts from adata.X and ran it once, still got same error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989:1141,message,message,1141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614976989,1,['message'],['message']
Integrability,"So this would be a reproducible example:. ```py; import gzip; import shutil; from urllib.request import urlopen; from pathlib import Path. from tqdm.notebook import tqdm; import scanpy as sc. url = ""https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE194122&format=file&file=GSE194122%5Fopenproblems%5Fneurips2021%5Fmultiome%5FBMMC%5Fprocessed%2Eh5ad%2Egz"". path = Path(""data/GSE194122_openproblems_neurips2021_multiome_BMMC_processed.h5ad""); if not path.is_file():; with (; urlopen(url) as raw,; tqdm.wrapattr(raw, ""read"", total=int(raw.headers[""Content-Length""])) as wrapped,; gzip.open(wrapped, 'rb') as f_in,; path.open('wb') as f_out,; ):; shutil.copyfileobj(f_in, f_out). adata_atac = sc.read(path); adata_atac.X = (adata_atac.X > 0)*1; sc.pp.highly_variable_genes(adata_atac, n_top_genes=13634); adata_atac = adata_atac[:,adata_atac.var['highly_variable']]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2819#issuecomment-1910369113:500,wrap,wrapattr,500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2819#issuecomment-1910369113,3,['wrap'],"['wrapattr', 'wrapped']"
Integrability,"So, I'm not too surprised to see this, since I don't think much of the distributed stuff has good testing, and I'm not too familiar with it. I believe the `AnnData` constructor is converting the array. You can get around this by assigning X to be a dask array, e.g.:. ```python; a = ad.AnnData(np.ones((1000, 100))); a.X = da.from_array(a.X); type(a.X); # dask.array.core.Array; ```. Better support for dask arrays would be a great feature request and series of additions to anndata. I think this is the endemic numeric python problem of ""these things are all like arrays, so can kinda use the same API, but in practice every type needs to be special cased"". > but there's a lot of other stuff happening before & afterwards in normalize_total() which I haven't looked at much. Yeah, I think this function has built up some cruft. I've opened a PR to streamline this #1667, but will need to check with people more familiar with the code. The private method should handle all of the computation, while the outer wrapper will do more argument handling/ getting data out of the `AnnData`/ assigning it back. > What combinations of inputs to _normalize_data() need to be supported. I believe `counts` should always be generated from `X`, so we don't need to worry about the combinations of types.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190:1010,wrap,wrapper,1010,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-782803190,1,['wrap'],['wrapper']
Integrability,"So, question from a user stand point:. Is it worth it for us to include the really really easy to implement metrics? The ones where we'd basically just be wrapping scikit-learn? I think this fits with the idea of `scanpy`'s contents being curatorial to some extent. > Though I do understand the citation issue. It's definitely good to have a citation in the docstring for each function. For the docs of the metrics module, I think there would be a subsection for ""Integration metrics"" which could definitely point to `scIB` as a more comprehensive package for evaluating integration. > Maybe it's time for a global citation table and each function can add to the table if there is an appropriate citation?! . Are you suggesting that the table would be added to at runtime (when a function is called)? I think this may be better addressed by a broader solution to ""what has been done to this dataset?"". I'm not sure how this could be done without buy in from third party libraries. Also has been discussed a bit previously: #472.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892:155,wrap,wrapping,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-764392892,3,"['Integrat', 'integrat', 'wrap']","['Integration', 'integration', 'wrapping']"
Integrability,"So, we could also not early load `scanpy.testing._pytest` or load `pytest-cov` first?. I would like to keep the `xdist` support and use a similar interface.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175:146,interface,interface,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175,1,['interface'],['interface']
Integrability,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution; - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. ; - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png); **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336:102,depend,depends,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336,2,['depend'],['depends']
Integrability,"Sorry about the late reply to this!. > and it seems odd that the existence of the wrapper (which just runs reduce and adds the result to the input AnnData) should disqualify it. I guess I wouldn't think of it as disqualification. If a wrapper is added to external, it adds maintanence burden to both of us by giving you multiple sets of documentation and code to keep in sync, and us for issue management and CI. Plus all the documentation you can provide through external is a docstring, while you can offer much more on your own repo. To us it just seems easier on both of us, especially since you've already implemented the interface with anndata on your side. We're aiming to make the ecosystem documentation much more visible for the next release as well (and are open to input of improving this further), in case that was your concern. So yes, I would still prefer to have your tool added to the ecosystem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577:82,wrap,wrapper,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-848587577,3,"['interface', 'wrap']","['interface', 'wrapper']"
Integrability,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:142,message,message,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,3,['message'],['message']
Integrability,"Sorry about this; `sc.tl.sim` used to be a separate tool in the beginning and integration into Scanpy was erroneous. For the past months I've only used to produce the two reference datasets linked below. All of the problems you mentioned are fixed in Scanpy 0.3.2. Take a look at:; https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11. Cheers,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857:78,integrat,integration,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/52#issuecomment-348018857,1,['integrat'],['integration']
Integrability,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:; ```; mat_all = sc.read_loom(filename=""RSV.loom""); sc.pp.pca(mat_all); sc.pp.neighbors(mat_all); sc.tl.umap(mat_all); ```; The error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:240,message,message,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['message'],['message']
Integrability,"Sorry for the late response, I was on holidays. I'm happy to merge a pull request for this, if the package appears solid. Would you want to add a file `scanpy/preprocessing/doubletdetection`? We should probably just ask @JonathanShor whether he's interested in an interface for easily accessing his package. If he is, he should also make the pull request, I'd say. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802:264,interface,interface,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398678802,1,['interface'],['interface']
Integrability,Sorry for the slow response! Congrats on the very nice preprint and thanks for thanking me! :wink:. I'll upload the file from dropbox to `scanpy_usage`!. Regarding the result: the high PCs can change drastically depending on the platform and the random seed. I've seen clustering results changing completely after I became aware of it. I'm very sure that non of the preprocessing and other code changed at any point. What do you think?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-435735671:212,depend,depending,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-435735671,1,['depend'],['depending']
Integrability,"Sorry for the super-late response! I just worked through almost 60 issues starting with the most recent, this is the last one... Sorry about that. `paga_path` requires computing a pseudotime before-hand as one needs to order cells at single-cell resolution along the path. I added a more meaningful error message stating that. PS: Now, there is also a test for PAGA [here](https://github.com/theislab/scanpy/blob/master/scanpy/tests/notebooks/test_paga_paul15_subsampled.py), making sure that the canonical use ([here](https://nbviewer.jupyter.org/github/theislab/paga/blob/master/blood/paul15/paul15.ipynb)) remains unchanged.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335:305,message,message,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328#issuecomment-435736335,1,['message'],['message']
Integrability,"Sorry yeah, the branch name might not be accurate when weâ€™re done. What dependency causes those mismatches? Locally the image comparisons succeed unless I create a venv â€¦. > master_clustermap.png; > ; > I believe the difference is just the margin, so we should be good to just change the test image. nah, it also has an ugly white gap between tree and heatmap, probably caused by the same reason as the other one â€¦. /edit: the space seems exactly enough to hold the color barâ€™s x tick labels, that might be the cause. /edit2: also appears without scanpyâ€™s style or anything: mwaskom/seaborn#1953",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169:72,depend,dependency,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1015#issuecomment-581052169,1,['depend'],['dependency']
Integrability,"Sounds like a great idea! Currently, `pip install scanpy` avoids installing the C++ dependencies, which give some users trouble: `louvain` and `python-igraph`. I think that these should still remain optional dependencies: users should be able to do some basic analysis with plotting without having to install C++ dependencies. They can then continue to install optional dependencies, if they like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560:84,depend,dependencies,84,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-354904560,4,['depend'],['dependencies']
Integrability,"Sounds like a great idea. generally the order should be the same as in the signature, but I donâ€™t see a problem in reshuffling the lovain args to match the leiden ones. We have to be careful with details though: e.g. `partition_type` needs to be slightly different for both:. ```rst; Type of partition to use. Defaults to :class:`~louvain.RBConfigurationVertexPartition`.; For the available options, consult the documentation for :func:`~louvain.find_partition`.; ```; ```rst; Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.; For the available options, consult the documentation for :func:`~leidenalg.find_partition`.; ```. @falexwolf do you think we should go ahead with https://pypi.org/project/legacy-api-wrap (and introduce `*` in `louvain`â€™s signature` or do you think we can slightly reshuffle the last few arguments of `louvain` without considering it a backwards compat break?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/570#issuecomment-477971741:750,wrap,wrap,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570#issuecomment-477971741,1,['wrap'],['wrap']
Integrability,"Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to `scvi-tools`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716:135,message,message,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703461716,1,['message'],['message']
Integrability,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:874,depend,depending,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926,1,['depend'],['depending']
Integrability,"Still the error message could be a lot better. Iâ€™ve made the same mistake,; itâ€™s easy to forget to log the data. On Fri 2 Aug 2019 at 23:36, Stephen Fleming <notifications@github.com>; wrote:. > Closed #763 <https://github.com/theislab/scanpy/issues/763>.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/763?email_source=notifications&email_token=AACL4TL6QHUQMHIBKEQT5GLQCSSFFA5CNFSM4IJBAFAKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOS3M3XBA#event-2530851716>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TM5VZDC544TAQPK7NDQCSSFFANCNFSM4IJBAFAA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825:16,message,message,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-517929825,1,['message'],['message']
Integrability,"Sure, I agree with you generally. Iâ€™m just saying that an easy, userfriendly way should make it easy to get beautiful colors, not ugly ones. So in the PR tuning that interface to be nicer, we should do something about that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/596#issuecomment-480746287:166,interface,interface,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/596#issuecomment-480746287,1,['interface'],['interface']
Integrability,Switching README.rst to markdown would also look nice on new pypi interface e.g. https://pypi.org/project/markdown-description-example/,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/116#issuecomment-378952904:66,interface,interface,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/116#issuecomment-378952904,1,['interface'],['interface']
Integrability,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:847,integrat,integration-scanorama,847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,1,['integrat'],['integration-scanorama']
Integrability,"Thank you for this, @awnimo! I added a few small comments. Could you move the whole code into `scanpy/external/_tools`, please? We'll transition to all wrapper code for external code to be in that directory. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/493#issuecomment-471330146:152,wrap,wrapper,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/493#issuecomment-471330146,1,['wrap'],['wrapper']
Integrability,"Thank you! As it stands, this destroys backwards compatibility. But fear not:. 1. Add [`legacy-api-wrap`](https://pypi.org/project/legacy-api-wrap/) to requirements.txt and docs/requirements.txt; 2. Use it to change the API to 1-2 positional parameters (i.e. either `def magic(adata: ..., *, name_list: ... = None, ...)` or with the `*` one parameter to the right); 3. While youâ€™re at it, change `name_list`â€™s type to `Union[Literal['all_genes', 'pca_only'], Sequence[str], None]`. You can do `from .._compat import Literal` or so (maybe a dot less or more)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635:99,wrap,wrap,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896#issuecomment-547361635,2,['wrap'],['wrap']
Integrability,Thank you! If you add a few more details we can fix this quickly: Which call will update the groups but not the color and which call will error out with which stack trace? Please add the the traceback to your comment this:. ````md; ```python; sc.tl.something(adata); ```. ```pytb; XError Traceback (most recent call last); ....; XError: some message.; ```; ````,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480:342,message,message,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833#issuecomment-531482480,1,['message'],['message']
Integrability,"Thanks @falexwolf !! I will test if the feature works. Of course I've tested seurat cca, but it seems to work better when integrating data from different sequencing platforms. As for my own data, several batches generated by 10x, output if the MNN method looks more pleasing...; ![unknown](https://user-images.githubusercontent.com/8361080/39244909-b8d3cb38-48c4-11e8-9cdc-82c78703ceee.png). Plus, I haven't looked into the maths of CCA, but I have for MNN and feel more comfortable using it. Actually, @gokceneraslan 's comments do make sense to me, and I've spent quite some time working on a native implementation of MNN correct on python. Now it's nearly complete and features more complete multicore support than the scran implementation.; ![screen shot 2018-04-25 at 20 25 17](https://user-images.githubusercontent.com/8361080/39245687-0a17319a-48c7-11e8-934b-904ee6d75978.png); I built it to be fully compatible with anndata and scanpy. Now it already runs much faster than the scran version, and I'm planning to add more speedups, eg Cython and CUDA. I'm thinking of creating a full toolbox for scanpy, like scran for scater/sce, in python. Perhaps we could work together? ðŸ˜„. I'm currently writing docstrings and will pack and upload the code to a repository shortly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335:122,integrat,integrating,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384268335,1,['integrat'],['integrating']
Integrability,"Thanks @fidelram, that will run the whole Scrublet workflow so will certainly do the trick. But I'd prefer a more Scanpy-integrated approach, which I think I can see how to do from @swolock's fork.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439:121,integrat,integrated,121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-545324439,1,['integrat'],['integrated']
Integrability,Thanks @stkmrc!. Could you merge master into this PR? It looks like there was a dependency issue that should be fixed now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2175#issuecomment-1068381662:80,depend,dependency,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2175#issuecomment-1068381662,1,['depend'],['dependency']
Integrability,"Thanks a lot @a-munoz-rojas!. For the DE approach, I would go with the same thing you suggest. I would definitely not do DE testing on the corrected data (violation of distributional assumptions, potential overcorrection of background variation leading to false significant results). . Regarding altering the number of HVGs or latent dimensions... this is difficult to say in general. I would normally err on the higher side of the number of HVGs, but the latent dimensions will depend heavily on the complexity of the dataset i would imagine. I don't think it's possible to give a general recommendation there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061642536:479,depend,depend,479,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061642536,1,['depend'],['depend']
Integrability,"Thanks a lot. ---Original---; From: ""James ***@***.***&gt;; Date: Thu, Sep 29, 2022 00:06 AM; To: ***@***.***&gt;;; Cc: ""Sijian ***@***.******@***.***&gt;;; Subject: Re: [scverse/scanpy] sc.tl.leiden(adata,use_weights=False) (Issue#2339). ; I also saw this with python-igraph version 0.10. Downgrading to 0.9.9 fixed the issue.; ; â€”; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2339#issuecomment-1261162964:455,Message,Message,455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2339#issuecomment-1261162964,1,['Message'],['Message']
Integrability,"Thanks everyone! I wonder how this affects one-pipeline-for-everything; portals, like the EBI single cell expression atlas... and standarized; pipelines like cellranger. On Mon, Jul 1, 2019 at 3:29 PM MalteDLuecken <notifications@github.com>; wrote:. > Based on my experience setting a single cutoff for all datasets will not; > work, as I've used a lot of different cutoffs depending on the; > distributions. I would echo @ivirshup <https://github.com/ivirshup>'s; > suggestion of looking at distributions. Joint distributions being a lot; > more important than individual histograms. There's a small discussion about; > it in our best practices paper; > <https://www.embopress.org/lookup/doi/10.15252/msb.20188746>; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/718?email_source=notifications&email_token=AACL4TMTNHMCCFM7MGMIZ73P5IBDPA5CNFSM4H4DUZEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY6D6LQ#issuecomment-507264814>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TKKTTZ4IHBJJDFAPKLP5IBDPANCNFSM4H4DUZEA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593:375,depend,depending,375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/718#issuecomment-507267593,1,['depend'],['depending']
Integrability,"Thanks for bearing with me Isaac ðŸ˜… ðŸ™ took some of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, coul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:334,wrap,wrap,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,1,['wrap'],['wrap']
Integrability,"Thanks for opening a new issue for this, and the info. Could you let me know a bit more about how you've installed scanpy? E.g. what OS, did you use conda or pip, etc. My guess would be that this is numba related (which, from reporting the cpu flags, I'm guessing you suspect too). Are you able to import `numba`? If so, what about `pynndescent` and `umap`? I'm trying to figure out if some code in scanpy is triggering the error, or if it's one of our dependencies. ---------------. Initially mentioned in https://github.com/theislab/scanpy/issues/1823#issuecomment-983551937",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860:453,depend,dependencies,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2062#issuecomment-983814860,1,['depend'],['dependencies']
Integrability,"Thanks for opening the issue. It looks like a problem with pytables, which we are removing as a dependency since it's starting to have problems like this. Are you able to update the installation of pytables? Otherwise, you could try a dev version of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484:96,depend,dependency,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2138#issuecomment-1040349484,1,['depend'],['dependency']
Integrability,"Thanks for opening this @ivirshup. The principle should be that Anndata doesn't change array types to numpy arrays. It may not have done this in the past (and this is one example), but we should fix that. Hopefully with improvements in numpy itself (like https://www.numpy.org/neps/nep-0018-array-function-protocol.html, and `__array_ufunc__`) it should be achievable. (I don't know how much work is needed in Anndata to do that though.). > Also, are `ZappyArray`s always read-only like the `DirectZappyArrays` are?. Yes, the idea is that they are immutable, and they are changed by transformation (e.g. applying a ufunc) to create a new array.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/733#issuecomment-512213981:306,protocol,protocol,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/733#issuecomment-512213981,1,['protocol'],['protocol']
Integrability,"Thanks for reporting @dawe and thanks for updating @WeilerP .; I ran into the same problem with the pip version.; When using **python 3.9** in a fresh virtual enviroment, there's an error related to llvmlite:; <details>; <summary>; error message; </summary>. ```; Building wheel for llvmlite (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: /home/mischko/test/python_virtual/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""'; __file__='""'""'/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/setup.py'""'""';f = getattr(tokenize, '""'""'open'""'""', open)(__file__) if os.path.exists(__file__) else io.StringIO('""'""'from setuptools import setup; setup()'""'""');code = f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-rb92hbao; cwd: /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/; Complete output (15 lines):; running bdist_wheel; /home/mischko/test/python_virtual/bin/python /tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py; LLVM version... 11.1.0; ; Traceback (most recent call last):; File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-u4ja11ve/llvmlite_860b580657d846f1993072c1a58436b0/ffi/build.py"", line 143, in main_posix; raise RuntimeError(msg); RuntimeError: Building llvmlite requires LLVM 10.0.x or 9.0.x, got '11.1.0'. Be sure to set LLVM_CONFIG to the right executable path.; Read the documentation at http://llvmlite.pydata.org/ for more information about building llvmlite.; ; error: command '/home/mischko/test/python_virtual/bin/python' failed with exit code 1; ; ERR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752:238,message,message,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799#issuecomment-830137752,1,['message'],['message']
Integrability,"Thanks for tackling this one @falexwolf. I didn't realise until recently that umap has a copy of pynndescent too. However, I think it would be possible for scanpy to depend on both umap and pynndescent packages, and use the latter for generating the knn graph directly. This would mean new features in pynndescent (like the threading support) could be used from scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522#issuecomment-472497425:166,depend,depend,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522#issuecomment-472497425,1,['depend'],['depend']
Integrability,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:421,wrap,wraps,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241,4,['wrap'],"['wrapper', 'wraps']"
Integrability,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208:364,depend,dependency,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208,1,['depend'],['dependency']
Integrability,"Thanks for the clarification. I don't have a good idea for a middle ground integration now. If possible, I would like to hear what @flying-sheep suggests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2379271488,1,['integrat'],['integration']
Integrability,"Thanks for the fix!. For the `p` vs `q` thing, I was just thinking if the user passes a string that doesn't start with `p`, the error message could be something like `""ValueError: Couldn't understand string value '{passed_val}' for vmax. Percentile cutoffs can be specified like 'p99' (99th percentile).""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496:134,message,message,134,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800#issuecomment-526476496,1,['message'],['message']
Integrability,"Thanks for the information. Very interesting read. From the paper it seems that the Leiden method indeed is superior to the louvain method. However, instead of replacing the louvain function I think is better to explicitly create a leiden clustering function as you have done and you can add the n_iterations parameter. . I would suggest to add a PR with this function as part of tools, but because of the new dependency required I am not sure. @falexwolf any suggestions?. Meanwhile I will give it a try.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-437026529:410,depend,dependency,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-437026529,1,['depend'],['dependency']
Integrability,"Thanks for the quick response, @flying-sheep!. I can confirm that updating _pandas-2.2.2_ does fix this. I totally missed this possibility; it's not clear to me why the dots would change ordering, but the totals wouldn't (maybe _scanpy_ relies on default _pandas_ behaviour that changed between 1.x and 2.x?). That said, _pandas-2.x_ unfortunately breaks some dependencies in our environment, so I'll either pin _scanpy_ or use your workaround. Regarding the ordering and issue title change. Maybe a nit, but it's my understanding that the default ordering is alphabetical (which makese perfect sense as a default!). If this is correct, then I'd suggest that the wrong ordering is not the totals, but the categories themselves. Given this, the workaround that gives me the expected behaviour would be `dp.categories_order = dp.dot_color_df.index.sort_values()`:; <img width=""439"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/5192495/6f7622f5-14b5-4ea5-a44f-288c4507c4f0"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629:360,depend,dependencies,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3062#issuecomment-2115841629,1,['depend'],['dependencies']
Integrability,"Thanks for the quick responses @LuckyMD and @ivirshup.; If `obsm` entries were accessible for plotting functions that would be fantastic. It would really solve all our problems. Once this is implemented I would only need to write a wrapper to model differences of activities between groups and that's it.; Looking forward for this update, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056:232,wrap,wrapper,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724#issuecomment-795050056,1,['wrap'],['wrapper']
Integrability,"Thanks for the response! The core `reduce` function of SCA is not scanpy-based, but I wrote a very simple wrapper called `reduce_scanpy` to make it easier for scanpy users while this pull request is being considered. It would be even easier for scanpy users to access this code natively in `sc.tl.external`, and it seems odd that the existence of the wrapper (which just runs `reduce` and adds the result to the input AnnData) should disqualify it. Although the current pull request implements `sc.tl.external.sca`as an additional wrapper to `reduce_scanpy`, I could easily write it as a wrapper to `reduce`, which would remove the redundancy of having separate scanpy interfaces in the base package and in sc.tl.external. I would then mark `reduce_scanpy` as deprecated in further releases of SCA, and direct the user instead to `sc.tl.external.sca`. Does this seem reasonable? Of course, I'd be happy to be part of `ecosystem` if that's still where you think it belongs!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662:106,wrap,wrapper,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-825877662,5,"['interface', 'wrap']","['interfaces', 'wrapper']"
Integrability,"Thanks for the responses!. > Sounds reasonable, would you want to add anything more to Ecosystem about this? Also, would you like to make a PR with the deprecation message pointing to scvi-tools?. I already made a PR to update the docs (https://scanpy.readthedocs.io/en/latest/ecosystem.html). I can also do the latter. > With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users.; >; > Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?. Is this different than the ecosystem page? Sounds reasonable though",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133:164,message,message,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703693133,2,"['integrat', 'message']","['integrated', 'message']"
Integrability,Thanks for this PR @sjfleming . If you don't mind I will integrate this functionality into a new PR that also covers #512 and #525,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944:57,integrat,integrate,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/524#issuecomment-471454944,1,['integrat'],['integrate']
Integrability,"Thanks for your answerï¼; I will spend time finding where the problem is.; ^-^. ---Original---; From: ""Philipp ***@***.***&gt;; Date: Thu, Nov 16, 2023 16:49 PM; To: ***@***.***&gt;;; Cc: ***@***.******@***.***&gt;;; Subject: Re: [scverse/scanpy] sc.pp.filter_genes runs out of memory (Issue#2754). ; Seems like you ran out of memory. Maybe you only have enough memory to run store your data once, but for calculating this, a second copy has to be made.; ; If you think this is a bug and scanpy uses much more memory than it should here, please tell us and we will reopen this issue.; ; â€”; Reply to this email directly, view it on GitHub, or unsubscribe.; You are receiving this because you authored the thread.Message ID: ***@***.***&gt;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2754#issuecomment-1817861190:710,Message,Message,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2754#issuecomment-1817861190,1,['Message'],['Message']
Integrability,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-425729339:100,depend,depends,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-425729339,1,['depend'],['depends']
Integrability,"Thanks for your comments, I understand the struggle of implementing CI for GPU code!. @Zethson here are my answers to your questions:; 1. Instead of checking if a gpu is available, I would suggest to rather check if the related library is installed (depending on the method, it could be cugraph, cupy or cuml) since each of these libraries always require a GPU at installation and usage, I think using these as check would suffice.; 2. I agree with moving to the usage of 'device' as much as possible. It should be easily possible to rename ""method""/""flavor"" to ""device"" for `tl.draw_graph`, `tl.leiden` and `tl.louvain`, and use only ""cpu""/""gpu"" as choices as theses parameters would have only two choices anyway. In most case this would indeed remove the name of the python backend used, but one could instead mention it in the api/doc. ; `pp.neighbors` is a bit more tricky to handle, running it in gpu mode lead to a combination of distances/neighbors calculation with gpu/cuml backend and then connectivities calculations on cpu/umap backend, this could be solved if maintainers of cuml decide to allow the latter to be computed with cuml: https://github.com/rapidsai/cuml/issues/3123. Since it will take time before CI can be implemented, I can just add the easy small changes proposed on 2. and let the PR open so you decide what to do later!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412:250,depend,depending,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-816665412,1,['depend'],['depending']
Integrability,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:765,wrap,wrapper,765,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,2,['wrap'],['wrapper']
Integrability,Thanks for your thoughts on this!. 1. That seems like a good approach - the size of the grid cells would be adjusted for each plot depending on cell number? ; 3. This probably depends a lot on the kind of dataset and comparison one wants to make: Does one want to know if there are *any* differences at all or also *where* on the grid these difference are? Up to now my approach was to use clusters as 'grid' and calculate the differences in proportion of cells per cluster across two or more conditions. The reason that I like your approach is that it is a very good (qualitative) visualization and much more subtle then just binning the data based on clusters. It seems hard to capture this visual aspect with a statistic.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-478830365:131,depend,depending,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-478830365,2,['depend'],"['depending', 'depends']"
Integrability,"Thanks so much for your contribution @dwnorton ! Looks good. Two questions: 1) is there any way to do without densifying the matrix? it's not a big deal though since the matrix is small. 2) Is the UMAP API for the `umap.distances.pairwise_special_metric` function version-dependent somehow in other words do we need to constrain our umap dependency to a specific version, or is the API stable across all commonly used UMAP versions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813:272,depend,dependent,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1413#issuecomment-697932813,2,['depend'],"['dependency', 'dependent']"
Integrability,"Thanks! Can I ask for two clarifications before replying:. > Right now, we tend to use a connectivity graph built by UMAP ... UMAP uses Pynndescent to construct the kNN graph. So does it mean that you depend on Pynndescent to construct the kNN graph, and then if the user calls UMAP, it's run on this previously constructred kNN graph?. By the way, openTSNE uses Annoy instead of Pynndescent for non-sparse input data and simple metrics that are supported by Annoy (like Euclidean or cosine). It seems to work quite a bit faster. For sparse input data and/or fancy metrics, it uses Pynndescent. > ... but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. What are the use cases here that you thinking of?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550:201,depend,depend,201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633311550,1,['depend'],['depend']
Integrability,"Thanks!. On Wed, Jun 7, 2023 at 9:34â€¯AM Philipp A. ***@***.***> wrote:. > I had the same thought and opened #2505; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2505&g=NmVkM2RiMWY2M2U4YzZhYw==&h=YTlmZWU5MDlhNTJlOWJjMTkxZDczZTg2MGE2ODdiNzU2NmIwYjE2OTMzZTczY2M1ZjNlNzEyM2Q0Mjc1OWM5Yg==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>; > to track that!; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/scverse/scanpy/issues/2500%23issuecomment-1580835422&g=YWNlMjU3YjI5ODM4NTJkYQ==&h=ZTJiNzVlYzQ0NzM5YmY0ZTdiMWEzMDQ2MmQ0MGMwOWZmZTVlOGRhN2JmYjZiYTcxYjg1Nzg3OTRjMzEwZDY3OA==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>,; > or unsubscribe; > <https://checkpoint.url-protection.com/v1/url?o=https%3A//github.com/notifications/unsubscribe-auth/AUHCMAR6XJXLYMBK224NMETXKB7MDANCNFSM6AAAAAAY3HAO3E&g=OGRhMDE2YzcyZWIwNGMxNg==&h=M2I1NTIwM2JlNTIwNjA4MGViYjE3YTRmYjQ0MWM3NzNhYzNkNjBlNzVjYjg1NDUwMGVkMjJhNWFkYmZlZTIxYQ==&p=YzJlOmltbXVuYWk6YzpnOjBhNjA3ZDgxZmY2OGQ1YTVjYWY3YWUzM2MzZGM0MDU3OnYxOmg6VA==>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >. -- ; PLEASE NOTE: The information contained in this message is privileged and ; confidential, and is intended only for the use of the individual to whom it ; is addressed and others who have been specifically authorized to receive ; it. If you are not the intended recipient, you are hereby notified that any ; dissemination, distribution, or copying of this communication is strictly ; prohibited. If you have received this communication in error, or if any ; problems occur with the transmission, please contact the sender.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580840698:1245,Message,Message,1245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2500#issuecomment-1580840698,2,"['Message', 'message']","['Message', 'message']"
Integrability,"Thanks, @LisaSikkema; I figured it out shortly after I sent my message. For now, I only have these solutions, but I don't like any of them:; - Filtering this warning. This is not very clean: I would prefer to really solve the issue. Also, it makes a copy, which is memory expensive, as I sometimes have about `10^7` observations.; - Creating a copy so that I don't use a view. Same issue about the memory.; - Plotting this UMAP on the complete anndata object so that it stores the colors, and then plotting with the view I'm creating. The problem is that, for my use case, I don't want to plot it with the entire dataset, so it makes a useless plot. I think I will go for the first solution, even though I'm not really satisfied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2315#issuecomment-1257146488:63,message,message,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2315#issuecomment-1257146488,1,['message'],['message']
Integrability,"Thanks, and you're welcome! I figure if a scanpy user wants to take the plunge and use the OO interface, they shouldn't have to change the way they interact with their AnnData. Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/139#issuecomment-386322714:94,interface,interface,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/139#issuecomment-386322714,1,['interface'],['interface']
Integrability,"Thanks, that's all very helpful. I'll work on getting these recommendations integrated. One quick question, is this still the most efficient way to get the data for one gene for the violin plot?. selected = adata[:, adata.var_names.isin([gene.gene_symbol,])]. And before all the data was just in relational tables but, of course, the scale was a lot less. EDIT: I just re-read and saw that it seems you can pass the gene name to the violin() call itself. Beautiful magic.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511:76,integrat,integrated,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/85#issuecomment-370200511,1,['integrat'],['integrated']
Integrability,"That being said, maybe this better be reflected in the conda dependencies, so pandas 1.0 is not pulled with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056:61,depend,dependencies,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1028#issuecomment-583439056,1,['depend'],['dependencies']
Integrability,"That problem occurs within h5py (we just wrap the underlying OSError) and isnâ€™t a consequence of how scanpy uses h5py. The relevant part of the traceback is:. ```pytb; OSError: Can't read data (file read failed:; time = Sat Aug 1 13:27:54 2020,; filename = '/path.../filtered_gene_bc_matrices.h5ad',; file descriptor = 47,; errno = 5,; error message = 'Input/output error',; buf = 0x55ec782e9031,; total read size = 7011,; bytes this sub-read = 7011,; bytes actually read = 18446744073709551615,; offset = 0); ```. The reported filename looks weird: `'/path.../filtered_gene_bc_matrices.h5ad'`. Is that file on some network share or colab or so? Because thatâ€™d explain wonky I/O. 18 exabytes (18 quintillion bytes!) read seems really off too!. I assume `self.group[""data""][...]` tries to read all the data for `.X` and some bug or connection problem tells h5py that thereâ€™s 18 exabytes. h5py then asks the OS to give them those 18 exabytes which the OS politely denies. See also:. - https://github.com/googlecolab/colabtools/issues/559; - https://forum.hdfgroup.org/t/errors-accessing-hdf5-over-cifs-and-or-nfs/6341",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196:41,wrap,wrap,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667531196,2,"['message', 'wrap']","['message', 'wrap']"
Integrability,"That's a good point, and it is not:; ```python; reducer = umap.UMAP(min_dist=0.5); embedding = reducer.fit_transform(adata.obsm[""X_scVI""]); adata.obsm[""X_umap""] = embedding; ```; again produces stable results on only 3/4 CPUs. . Ok, let's forget about UMAP. It's only a nice figure to get an overview of the data and I don't use it for downstream stuff. Irreproducible clustering, on the other hand, is quite a deal-breaker, as for instance cell-type annotations depend on it. I mean, why would I even bother releasing the source code of an analysis alongside the paper if it is not reproducible anyway? . I found out a few more things: ; - the leiden algorithm itself seems deterministic on all 4 nodes, when started from a pre-computed `adata.obsp[""connectivities""]`. ; - when running `pp.neighbors` with `NUMBA_DISABLE_JIT=1`, the clustering is stable on all four nodes (but terribly slow, ofc); - when rounding the connectivities to 3-4 digits, the clustering is also stable (plus the total runtime is reduced from 2:30 to 1:50min). ```python; adata.obsp[""connectivities""] = np.round(adata.obsp[""connectivities""], decimals=3); adata.obsp[""distances""] = np.round(adata.obsp[""distances""], decimals=3); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365:463,depend,depend,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946539365,1,['depend'],['depend']
Integrability,"That's great, I'll add Isaac to that project so he can see code (repo is private). Let's discuss whether to integrate in scanpy at next meeting! Thank you Sergei !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228:108,integrat,integrate,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-693400228,1,['integrat'],['integrate']
Integrability,"That's not a bug, that's a feature ;). You can only compute as many PCs as the minimum number of dimensions in n_samples and n_features. Do you feel as though the error message is unclear on this? I feel as though changing the default to match the setting can be dangerous as may not recall how many PCs were used then.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464:169,message,message,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1051#issuecomment-586474464,1,['message'],['message']
Integrability,"That's weird. Why would cuda be a dependency?. I'm not sure who is maintaining the bioconda recipe, so that might just be wrong. Do the new installation instructions work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630:34,depend,dependency,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-608191630,1,['depend'],['dependency']
Integrability,"Thatâ€™s weird, but that might be another issue, please check out #1378. /edit: seems to be a conda bug that only occurs on windows due to flit ([legally](https://www.python.org/dev/peps/pep-0376/#record)) writing windows newlines into the RECORD file, and conda reading them as two newlines each and then crashing. ---. This PR adds instructions on how to integrate with conda, which I screenshotted. It fails for me with this error:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed. ResolvePackageNotFound: ; - loompy[version='>=3.0.5']; ```. But since loompy 3.x isnâ€™t on conda-forge, thatâ€™s correct. Seems that resolving anndataâ€™s dependencies on conda is currently not possible and you need to wait until loompy gets upgraded on conda-forge. Or until Quansight-Labs/beni#3 is resolved and you can specify that you donâ€™t want all deps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209:355,integrat,integrate,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1377#issuecomment-675423209,2,"['depend', 'integrat']","['dependencies', 'integrate']"
Integrability,"The UMAP (actually neighbours in the current implementation) is already now derived from any embedding the user wants, including integration embedding, so this is not an issue in itself.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985:129,integrat,integration,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133920985,1,['integrat'],['integration']
Integrability,"The `scipy.sparse` wrapper is actually interesting. I think it's tricky to add directly to SciPy, but it could be split out as a separate package that users could use and we could link to in the `scipy.sparse` docs. With some context about things like `@` vs `*` users can then make an informed decision to use it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557319885:19,wrap,wrapper,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557319885,1,['wrap'],['wrapper']
Integrability,"The additional cases that would need to be handled (and what I believe the current internal solutions are):. * What if colors haven't been saved to uns yet; * If it's an view, we just return what the colors would be by default; * If it's an ""actual"" we return the colors, but also assign them to the object; * What if there's a different number of colors and categories; * We warn and reassign the colors. If it's a view, we don't modify the object, but probably still warn. Most of the logic for handling this internally is wrapped in `_get_palette`: https://github.com/theislab/scanpy/blob/ed364a887db2eb604d0a09cd72325cb5e1f4e27e/scanpy/plotting/_tools/scatterplots.py#L1192-L1204",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992:525,wrap,wrapped,525,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1881#issuecomment-863776992,1,['wrap'],['wrapped']
Integrability,"The broader issue here is what we do and don't support with `backed` mode. As our dask support grows, that will likely be the recommended route going forward. Thus, it would make sense to come up with concrete recommendations for that so we have ""officially supported"" out of core functionality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894#issuecomment-2012531056:138,rout,route,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894#issuecomment-2012531056,1,['rout'],['route']
Integrability,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495163867:111,depend,dependency,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495163867,1,['depend'],['dependency']
Integrability,"The dependencies of scanpy state that anndata>=0.7 are required, please update:. https://github.com/theislab/scanpy/blob/c255fa10fb75f607780ed7d9afc6683cbcecc38e/requirements.txt#L1. Pip would have fullfilled that requirement if you installed the development version using it, but I assume you cloned and installed it manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921:4,depend,dependencies,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1125#issuecomment-602818921,1,['depend'],['dependencies']
Integrability,"The initial problem is due to the fact that the new 'highly_variable_genes' function does not take numpy arrays anymore: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/highly_variable_genes.py. It's also mentioned in the docs, but we should, of course, have thrown a clear error message. Now it does: https://github.com/theislab/scanpy/commit/a578ced0b2e44b26998fb9e08c5bb0ffb82a7a4b. To return the annotation, one can set `inplace=False`. But the updated plotting function also takes the full `AnnData` object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304:300,message,message,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-445515304,1,['message'],['message']
Integrability,"The issue is that there was a blank line starting with three spaces, and that triggered an error whose message wasn't particularly related to that. I think this test could probably get rewritten. https://github.com/theislab/scanpy/blob/a8ee1e01e6cea1d3b9f5474997508c99497d4fb4/scanpy/tests/test_docs.py#L18-L38. The error came from the `if any(broken)` block. Basically it's checking for lines which aren't 1) empty, 2) start with a four space indent. The error message is specific to the first line. The PR had a three line indent in between paragraphs triggering the failure. @flying-sheep, thoughts on removing this part of the test? Should we have a separate rst linting check?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623:103,message,message,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725894623,2,['message'],['message']
Integrability,"The issue that you mention has been reported to matplotlib 3.1 and the; solution is to downgrade to 3.0*. I just updated the dependencies of; scanpy to be matplotlib 3.0. As soon as this is solved we will update the; dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all; > I would like to project my umap from scanpy in 3d but I have faced the; > following problem:; >; > ValueError: operands could not be broadcast together with remapped shapes; > [original->remapped]: (0,4) and requested shape (816,4); >; > It's very strange because before I update some of my packages, I could run; > it it with no problem with the following packages:; >; > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4; > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760; > louvain==0.6.1; >; > but after updating some of my packages it was not possible due to that; > error!; >; > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1; > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0; > python-igraph==0.7.1+4.bed07760 louvain==0.6.1; >; > Should I roll back to the previous version of annadata or scanpy? has; > anyone ran this feature with my package version with no problems?; >; > Thanks a lot; >; > Here are the packages I use; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076:125,depend,dependencies,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663#issuecomment-496226076,2,['depend'],['dependencies']
Integrability,"The problem is not the collapsible thing. I still don't see the images here but only links instead, and when I click on a link e.g. https://user-images.githubusercontent.com/5758119/115158730-f0768a00-a08f-11eb-939a-1b9c35373fae.png I get an error message that the image cannot be displayed because it contains errors. Odd. Maybe it's something weird on my side.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389:248,message,message,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822059389,1,['message'],['message']
Integrability,"The problem referenced above is that we want to skip some doctests when using pandas<2 since outputs changed slightly. doctestplus doesn't currently solve that problem, since we use `doctest-requires` to skip a block when that block exists in a docstring. `doctest-requires` blocks only work in `.rst` files. For code files there's only the option to skip the whole file via `__doctest_requires__`. https://github.com/scientific-python/pytest-doctestplus?tab=readme-ov-file#doctest-dependencies. I think we can request this feature, but it doesn't exist now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2729#issuecomment-1934516029:482,depend,dependencies,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2729#issuecomment-1934516029,1,['depend'],['dependencies']
Integrability,"The reason I actually noticed it in the first place was a difference in t-SNE and UMAP... the difference wasn't huge, but noticeable. Here's the issue where you can see the UMAP plots (#324). The clustering differences were actually rather small... the more worrying thing was the UMAP at the time as it appeared to show a bridge between two clusters that I didn't really expect to be there and didn't see in the float64 data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-436028541:323,bridg,bridge,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-436028541,1,['bridg'],['bridge']
Integrability,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015:249,depend,dependencies,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661#issuecomment-496144015,1,['depend'],['dependencies']
Integrability,"The reorganization of using the ""external API"" (shallow interfaces) via an `import scanpy.external as sce` and the ""internal API"" as accessible via `import scanpy as sc`, sort of, provided a solution to what bothered people the most: expecting the ""internal API"" to run through at a single install, be properly maintained etc. and the interfaces to external packages be clearly marked. I think this is a sustainable, long-term solution, which scales and is convenient for contributors. @flying-sheep agreed as I understood it. Do you think we need more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977:56,interface,interfaces,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063977,2,['interface'],['interfaces']
Integrability,"The same error in `sc.pp.highly_variable_genes` can pop up also if you forget to `sc.pp.filter_genes(adata, min_cells=0)` before running normalization and logging. Some informative error messages could for sure save some time here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331:187,message,messages,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763#issuecomment-1137813331,1,['message'],['messages']
Integrability,"The short answer is that `flit_core`, which provides the PEP 517 hooks, makes a minimal sdist which should always have the files you need to install the module, but may leave out e.g. tests and docs. The Flit CLI tries to make a 'publication quality' sdist. It's kind of an ugly compromise, because how I approached sdists (before PEP 517) wasn't a good fit for the PEP 517 `build_sdist` hook. I view sdists on PyPI as like a snapshot of the development process, so it should (by default) include everything that you'd get if you checked out the corresponding tag from git (except the git history). But using git assumes that it's something the maintainer makes once and publishes on PyPI. PEP 517 defined a `build_sdist` hook which user tools (like pip) can call. I didn't want this to depend on git, so I gave it a way to make working but minimal sdists. Specifying includes & excludes under `[tool.flit.sdist]` should affect both the Flit CLI and the PEP 517 hooks. So if you want to make the sdists to publish with `python -m build` or similar, you'll need to use those to determine what goes in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324:787,depend,depend,787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874715324,1,['depend'],['depend']
Integrability,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:232,message,message,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943,1,['message'],['message']
Integrability,There is [something](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.coloring.greedy_color.html#networkx.algorithms.coloring.greedy_color) if we want to make Networkx a dependency. Igraph only seems to have graph colouring [in their c library](https://igraph.org/c/doc/igraph-Coloring.html).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2313#issuecomment-1239413303:214,depend,dependency,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2313#issuecomment-1239413303,1,['depend'],['dependency']
Integrability,"There now is a much more powerful differential testing package `diffxpy`, @davidsebfischer, which easily integrates into Scanpy. @a-munoz-rojas Would you consider making a pull request that adds log-fold changes for t-test etc. in `rank_genes_groups`? My bandwidth is limited these days, I will certainly do it at some point, but it's faster if you do it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760:105,integrat,integrates,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-420332760,1,['integrat'],['integrates']
Integrability,"There's a possibility of negative values depending on how careful you are with compensation and whether or not you clip values, but at least in my case the counts matrix was always non-negative. **Edit:** But that shouldn't matter because NNDescent is routinely called on PCA-embedded data which is zero centered, right? . If I can find a small subset of the matrix that produces this error reliably, I will share that with the `pynndescent` repo and link back here. Currently that's challenging given the original size of the matrix (a few million observations).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688:41,depend,depending,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802982688,2,"['depend', 'rout']","['depending', 'routinely']"
Integrability,"Thereâ€™s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. Theyâ€™ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (â€œFunction blah excepted a parameter foo of type Bar, but you passed a foo of type Bazâ€). iâ€™m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:488,integrat,integrated,488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,2,"['integrat', 'message']","['integrated', 'messages']"
Integrability,"They are capitalized, which means they are treated as constants and shouldnâ€™t be modified. They *are* the original default value you want to refer to, so if you change them, you canâ€™t do that anymore. So yes, `style` or the parameters in `def dotplot` are the correct way to do this. If you want a customized version, just do:. ```py; my_style = dict(...) # resusable customizations. sc.pl.dotplot(..., **my_style); ```. or. ```py; from functools import wraps. @wraps(sc.pl.dotplot); def my_dotpot(*args, return_fig: bool = False, **kw):; dp = sc.pl.dotplot(*args, **kw, return_fig=True); dp.style(...) # customize here; if return_fig:; return dp; dp.show(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2708#issuecomment-1803768928:454,wrap,wraps,454,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2708#issuecomment-1803768928,2,['wrap'],['wraps']
Integrability,Think the Travis thing failing is because of the new dependencies,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-438281756:53,depend,dependencies,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-438281756,1,['depend'],['dependencies']
Integrability,"This error message was hard to debug! Indeed it is due to the behavior of `sc.pp.neighbors`. Cells are sometimes given different numbers of neighbors. Sometimes that the errant cells have a number of neighbors greater than zero (unlike as seen in #2244, where the # of neighbors of some cells was 0). My fix builds on the one above and was:; ```; b = np.array(list(map(len, adata_ref.obsp['distances'].tolil().rows))) # number of neighbors of each cell; adata_ref_subset = adata_ref[np.where(b == DEFINED_NEIGHB_NUM-1)[0]] # select only those with the right number; sc.pp.neighbors(adata_ref_subset, DEFINED_NEIGHB_NUM) # rebuild the neighbor graph; ```; ___; @ViriatoII Your comment helped me fix things â€“ but your fix itself didn't work for me. First, when I use `adata_ref = adata_ref[b]`, with `b` defined as above, it interprets `b` not as a boolean mask but as an index, returning a single cell duplicated over and over. I'm not sure what the intended behavior is here. My solution is to use `adata_ref[np.where(b == n_neigh-1)[0]]`. However, this subselection changes the number of neighbors that other cells have. For example, for my data,. ```; b = np.array(list(map(len,adata_ref.obsp['distances'].tolil().rows))); print(""Before subselecting"", np.unique(b, return_counts = True)). adata_ref_subset = adata_ref[np.where(b == DEFINED_NEIGHB_NUM-1)[0]]; c = np.array(list(map(len,adata_ref_subset.obsp['distances'].tolil().rows))); print(""After subselecting"", np.unique(c, return_counts = True)); ```; yields; ```; Before subselecting, (array([13, 14]), array([ 28, 1161359])); After subselecting, (array([10, 11, 12, 13, 14]),; array([ 15, 1, 633, 46, 1160664]))); ```. To solve both problems I needed to rebuild the neighbor graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1382325586:11,message,message,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1382325586,1,['message'],['message']
Integrability,"This has been discussed previously: https://github.com/theislab/scanpy/issues/1451, https://github.com/mwaskom/seaborn/issues/1423. I don't think that this sort of normalization is necessarily invalid or wrong, just situational. I also think it makes sense to mimic prior art, and this is how the argument works in seaborn. I do agree that just `x / max(abs(x))` is useful, and more often what people want to use here (if scaling at all). I like suggestion 2. more for this. I would suggest the following api:. ```python; normalization: Optional[Union[str, Callable[np.ndarray, np.ndarray]] (default: None); Normalization to apply to values. Can be selected from ""z-score"", ""minxmax_scale"", etc. or a Callable.; normalization_axis: Literal[""var"", ""group""] (default: ""var""); If a `normalization` is passed, which dimension of the data to normalize along.; ```. It would be nice if the normalization method was mentioned by default in the legend, but that can be difficult with how matplotlib doesn't really do text wrapping with it's notebook backend. Arguably, for `dotplot` `normalization` should be available for both size and color. What to you think @gokceneraslan @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620:1014,wrap,wrapping,1014,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-804509620,1,['wrap'],['wrapping']
Integrability,"This is caused by an adjustment that tries to keep the legends closer to; the figure compared to the default placing. Clearly, there is some; dependency with the font size that I was not aware of. I will prepare a fix; soon. On Wed, Nov 21, 2018 at 3:23 PM Artemis-R <notifications@github.com> wrote:. > Here is a minimal example to recreate the issue I am describing:; >; > import scanpy.api as sc; > adata = sc.datasets.pbmc68k_reduced(); > sc.tl.pca(adata); > sc.pp.neighbors(adata); > sc.tl.umap(adata); >; > when you plot the umap of the bulk labels contained in adata.obs without; > specifying any further settings (i.e. sc.pl.umap(adata, color =; > ['bulk_labels']) ) everything looks fine.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847064-efe34780-eda0-11e8-8d51-b503d7912d1e.png>; >; > But as soon as you try and adjust the legend font size (sc.pl.umap(adata,; > color = ['bulk_labels'], legend_fontsize = 4)) to a value that is smaller; > than the default font size it selects for your legend, the legend overlaps; > with the right edge of the plot.; >; > [image: image]; > <https://user-images.githubusercontent.com/15019107/48847096-04274480-eda1-11e8-9bac-dceb31aba155.png>; >; > For me this sometimes leads to issues that I can no longer export figures; > with my desired fontsize for presentations, etc. without it overlapping the; > plot in an ugly way.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/374>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VijUK8tuCXIowEtUo1I45iOhNd-ks5uxWHfgaJpZM4YtOB0>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446:142,depend,dependency,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/374#issuecomment-440727446,1,['depend'],['dependency']
Integrability,"This is likely because you have [`scvi-tools`](https://scvi-tools.org/) and this wrapper supports our now deprecated `scvi` package. This wrapper will be removed in the next release, so I recommend using scvi-tools directly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000:81,wrap,wrapper,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1781#issuecomment-814561000,2,['wrap'],['wrapper']
Integrability,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy?. In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730:177,integrat,integration,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754652730,1,['integrat'],['integration']
Integrability,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. ðŸ‘. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:948,wrap,wrapping,948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,3,"['protocol', 'wrap']","['protocol', 'wrapper', 'wrapping']"
Integrability,"This is very strange... Do you have this issue also in the [standard tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb)? Of course, the louvain function produces string-named categories, see [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L135) and has always done so. I'm puzzled that the `.astype('U')`, where the `'U'` stands for unicode-string, seems to have no effect in your version of Scanpy. Do you use the most recent version (0.4.4) and recent dependencies? If not, run `pip install --upgrade scanpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266:575,depend,dependencies,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/94#issuecomment-370140266,1,['depend'],['dependencies']
Integrability,"This looks good! :smile:. Storing the forest in the AnnData is good! It should also be compatible with the updates the @tomwhite plans on UMAP and pynndescent (UMAP will depend on pynndescent) as that should be the most basic object to store when to enable queries later on... But I would not store the ""forest"" in a default neighbors call. Or do you have any estimate on how large it is?. Great work!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737:170,depend,depend,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/576#issuecomment-487035737,1,['depend'],['depend']
Integrability,"This seems reasonable to me @flying-sheep . Does using the patched version change results over the unpatched @ashish615 i.e., for a given random seed, unpatched and patched are the same? If the two are the same for a given seed/state, then I think what @flying-sheep is proposing could be done separately (even if we make the dependency optional IMO). However, if the new version does change results, we will need the handling that @flying-sheep describes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805:326,depend,dependency,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2114818805,1,['depend'],['dependency']
Integrability,"Those packages are optional dependencies, and also aren't installed with `pip install scanpy`. You'll need to specify those separately if you'd like to use the features that require them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267:28,depend,dependencies,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000#issuecomment-953226267,1,['depend'],['dependencies']
Integrability,"To help debug it, can one of you do what I suggested? I donâ€™t have the problem locally and if one of you does see it, it would be much easier to find out which dependency started to import that module.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/862#issuecomment-562545044:160,depend,dependency,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862#issuecomment-562545044,1,['depend'],['dependency']
Integrability,"True,; ---> 20 use_raw=False); 21 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 22 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1038 old_len_x = len(x); 1039 print(x); -> 1040 x = moving_average(x); 1041 if ikey == 0:; 1042 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; -> 2423 return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out); 2424 ; 2425 . ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapfunc(obj, method, *args, **kwds); 56 bound = getattr(obj, method, None); 57 if bound is None:; ---> 58 return _wrapit(obj, method, *args, **kwds); 59 ; 60 try:. ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in _wrapit(obj, method, *args, **kwds); 45 except AttributeError:; 46 wrap = None; ---> 47 result = getattr(asarray(obj), method)(*args, **kwds); 48 if wrap:; 49 if not isinstance(result, mu.ndarray):. ValueError: setting an array element with a sequence. ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543:80925,wrap,wrap,80925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-619031543,2,['wrap'],['wrap']
Integrability,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:304,wrap,wrapping,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,2,['wrap'],['wrapping']
Integrability,Two options:; - bbknn < 1.5.0; - use `bbknn.bbknn()` instead of the scanpy wrapper,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609:75,wrap,wrapper,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1873#issuecomment-872833609,1,['wrap'],['wrapper']
Integrability,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691:697,integrat,integrates,697,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424704691,1,['integrat'],['integrates']
Integrability,"Ultimately I don't think these things should be blockers. Lazy dataframes are now en-route with the xarray PR accepted in principle, and the subsetting operation is what it is. I think it's ok to iterate on these things",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2856#issuecomment-1983064398:85,rout,route,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2856#issuecomment-1983064398,1,['rout'],['route']
Integrability,"Unfortunately, I run into. ```; __________________________________________________________________________________ test_scale[use_fastpp] ___________________________________________________________________________________. flavor = 'use_fastpp'. @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; > sc.pp.scale(v, flavor=flavor). scanpy/tests/test_preprocessing.py:127: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(Non",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:911,wrap,wrapper,911,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,1,['wrap'],['wrapper']
Integrability,"Very good catch! It does indeed look like in the function itself it should be. ```; df.sort_values( ; ['highly_variable_nbatches', 'highly_variable_rank'], ; ascending=[False, True], ; na_position='last', ; inplace=True, ; ) ; ```. However, as the test sorting order was correct (though not testing the code the right way), it would still be great to figure out why there is a discrepancy at all. . For reference, here's the seurat code:. https://github.com/satijalab/seurat/blob/4e868fcde49dc0a3df47f94f5fb54a421bfdf7bc/R/integration.R#L2244-L2308",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791:523,integrat,integration,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1733#issuecomment-802145791,1,['integrat'],['integration']
Integrability,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710:191,wrap,wrapped,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626#issuecomment-773090710,1,['wrap'],['wrapped']
Integrability,We have SCVI working on anndata objects for data integration in our benchmarking study.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617:49,integrat,integration,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/520#issuecomment-553385617,1,['integrat'],['integration']
Integrability,"We have a backwards compatibility wrapper, I have no idea how this error can be possible:. https://github.com/theislab/anndata/blob/41eadb2a76d91ae455faf01afd2382143b9af4b2/anndata/_core/anndata.py#L2137-L2140",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083:34,wrap,wrapper,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027#issuecomment-587178083,1,['wrap'],['wrapper']
Integrability,"We have our [CLI layer for Scanpy](https://github.com/ebi-gene-expression-group/scanpy-scripts), and I could put this integration there, but it'd be a shame to silo code that might be useful to other Scanpy users, so happy to contribute to something in the external API if you guys are willing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122:118,integrat,integration,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-885007122,1,['integrat'],['integration']
Integrability,"We were getting a lot of errors from dask tests because they were relying on test helpers from anndata 0.10. It's a small number of functions, but it depends on the types in the compat module so is difficult to copy out. To work around this I've temporarily bumped the minimum required version of anndata up to 0.10, but we definitely shouldn't actually do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895765916:150,depend,depends,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2816#issuecomment-1895765916,1,['depend'],['depends']
Integrability,"Well, â€œimplementedâ€â€¦ Iâ€™d say â€œwrappedâ€.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350#issuecomment-441213626:30,wrap,wrapped,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350#issuecomment-441213626,1,['wrap'],['wrapped']
Integrability,"Weâ€™re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesnâ€™t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isnâ€™t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:405,depend,dependency,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['depend'],['dependency']
Integrability,"What dependency problems do you have? If you installed everything through conda, you should just be able to update it with condaâ€¦",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616:5,depend,dependency,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871#issuecomment-545908616,1,['depend'],['dependency']
Integrability,"What do you mean? In which way is it incompatible?. In matplotlib/matplotlib#9698 itâ€™s said that. > The goal is to ultimately replace setting `savefig.transparent` by; `figure.facecolor = (0, 0, 0, 0)`. So we should add a `facecolor` parameter and deprecate `transparent` (with a nice message to point people to `set_figure_params(facecolor=(0, 0, 0, 0))`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/473#issuecomment-462694429:285,message,message,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/473#issuecomment-462694429,1,['message'],['message']
Integrability,"What do you mean? Scanpyâ€™s metadata only specifies a lower Python version bound:. https://github.com/scverse/scanpy/blob/d7e13025b931ad4afd03b4344ef5ff4a46f78b2b/pyproject.toml#L13. Which of the following is it?. - Does it refuse to install because some dependency is not Python 3.11 compatible?; - Does it crash when run there?; - Something else?. I assume itâ€™s just that numba is incompatible still (which will always be a recurring problem as long as we depend on it), but please let me know. With installation issues, you can always run `pip -vv install scanpy` to get much more output that could be helpful. Please [use code blocks](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) when including it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462:254,depend,dependency,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1331976462,2,['depend'],"['depend', 'dependency']"
Integrability,What if we had copy-on-write behavior for AnnData? Then we could never modify AnnData inplace but always return a view of an AnnData with references to the objects that were unchanged and only the new data added. . Pandas seems to be going that route: https://github.com/pandas-dev/pandas/blob/57390ada100466dac777e5b66d5a4f2a72700c38/web/pandas/pdeps/0008-inplace-methods-in-pandas.md (HT @bernheder),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1674322622:245,rout,route,245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1674322622,1,['rout'],['route']
Integrability,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:69,message,message,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273,1,['message'],['message']
Integrability,What we could do instead is to add a [extras_require section](http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies) to the `setup()` call in setup.py. then people could do `pip install scanpy[louvain]` or `pip install scanpy[all]` to get the whole thing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375:171,depend,dependencies,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398656375,1,['depend'],['dependencies']
Integrability,"What you describe doesnâ€˜t need to happen, and you can fix this!. 1. go to https://github.com/conda-forge/conda-forge-repodata-patches-feedstock/; 2. make a PR that patches condaâ€™s dependency data to include this constraint; 3. the problem is gone",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3029#issuecomment-2363200158,1,['depend'],['dependency']
Integrability,"When I did `pip install --user scikit-misc` in my shell and then in python tried the line that errored for you `from skmisc.loess import loess`, everything worked fine for me. Also, depending on how conda is setup `pip install --user` might install it in your home directory, rather than the conda env. So you could also try activating the conda env and then running `pip install scikit-misc --force`. . Can you print out the full traceback of what happens when you run `from skmisc.loess import loess`? If that was causing the `ImportError` it might be easier to see outside of the try/except block. You can also try `import skmisc; print(skmisc.__file__)` to see what that returns. I also see some related issues (https://github.com/has2k1/scikit-misc/issues/12), which could indicate that it did not install correctly because it did not install the cython scripts properly on windows. The solution (install the numpy+mkl .whl first) in https://github.com/has2k1/scikit-misc/issues/4 might work?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340:182,depend,depending,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-996270340,1,['depend'],['depending']
Integrability,"When the C core `igraph` version 0.10 is released, including a new release of the Python interface building on this new version, i.e. including 64-bit support, I will also update the `leidenalg` implementation to follow suit. In principle, `leidenalg` is already working with 64-bit integers, but since it builds on `igraph`, it is limited by that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652:89,interface,interface,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1103987652,1,['interface'],['interface']
Integrability,Which package has the `scipy<1.3.0` dependency?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384:36,depend,dependency,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661095384,1,['depend'],['dependency']
Integrability,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:210,integrat,integration,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['integrat'],['integration']
Integrability,"With the scanpy ecosystem growing also with packages outside of scanpy external, it would probably be good to have a page with all packages that are part of this ecosystem. Ideally this would be on the Scanpy page I guess as it would be a first port-of-call for users. . Such a page could have brief explanations of the capabilities of each of these tools, and link to tutorials from all the independent packages that show how these tools are integrated into a standard Scanpy workflow. What do you guys think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988:443,integrat,integrated,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1443#issuecomment-703481988,1,['integrat'],['integrated']
Integrability,"Within that loop, I believe large amounts of memory can be allocated. If it's ""group vs rest"", at least one `X` worth of memory is allocated per loop from matrix subsetting â€“ since there's an `X_group = X[mask]; X_rest = X[~mask]`. If you parallelize over groups, now the max memory usage can be `~ min(n_procs, n_groups) * X` as opposed to `~2X`. For large `X` (probably where you want to see the speed up most), this can make you run out of memory. https://github.com/scverse/scanpy/blob/d26be443373549f26226de367f0213f153556915/scanpy/tools/_rank_genes_groups.py#L164-L178. Another memory related concern comes from `multiprocessing` (mentioned in your email). I think there's recently been some improvement here, but my impression was it's difficult/ impossible to share memory with `multiprocessing` â€“ so everything that goes into or out of a subprocess has to be copied. So while I think we can absolutely make use of more processing power here, I think we need to consider the approach. * The link I mentioned above should reduce copies, and could potentially use a parallelized BLAS for compute; * Much of the loops over ""all of the genes between compared samples"" are already in compiled code, but could be parallelized. > In terms of our use case, an interactive way to run DE via the client is too slow. What is the interface here? Scanpy computes results for all groups at once, but in most interfaces I've used you can only really ""ask"" for one comparison at a time. This could also be much faster, if you can just reduce total computation. ---------. > What @ivirshup was referring to though, is that rank_genes_groups on single cells in general isn't seen anymore as best practice for DE analysis because it doesn't account for pseudoreplication bias. Partially, I'm not sure what comparisons are actually being run. I was also wondering if you'd benefit from something fancy like a covariate. > Diffxpy is currently being reimplemented. . As a heads up, I'm unaware of a timeline here",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486:1720,interface,interface,1720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2390#issuecomment-1397484486,2,['interface'],"['interface', 'interfaces']"
Integrability,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs Ã— n_vars = 68865 Ã— 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:180,depend,dependency,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223,1,['depend'],['dependency']
Integrability,"Yeah, it's not working . Here https://scanpy.readthedocs.io/en/stable/generated/scanpy.pl.violin.html they say; > Wraps [seaborn.violinplot()](https://seaborn.pydata.org/generated/seaborn.violinplot.html#seaborn.violinplot) for [AnnData](https://anndata.readthedocs.io/en/stable/generated/anndata.AnnData.html#anndata.AnnData). but when you add `orient='h'` or `orient='v'` to the `sc.pl.violin` run, it fails wit this error:; ```; TypeError: seaborn.categorical.violinplot() got multiple values for keyword argument 'orient'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2092#issuecomment-1513930164:114,Wrap,Wraps,114,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2092#issuecomment-1513930164,1,['Wrap'],['Wraps']
Integrability,"Yeah, someone creates a package and whenever a new release appears on PyPI, the bot makes a PR that increases the version number in the build recipe. A human then checks if everything works and merges. In this case that human didnâ€™t check the dependencies changing (very understandable, itâ€™s draining to search where theyâ€™re defined and compare manually multiple times per day). You could simply do a quick PR that updates dependencies and build number and Iâ€™m sure theyâ€™ll quickly merge it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170:243,depend,dependencies,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876#issuecomment-545971170,2,['depend'],['dependencies']
Integrability,"Yeah, thatâ€™s true. I think depending on whatâ€™s happening, itâ€™s actually the spacer we add, not the area for the dendrogram, but sometimes we also need that spacer â€¦. Our plotting code is complicated and needs to be overhauled. If anyone feels like diving int",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3320#issuecomment-2437408562:27,depend,depending,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320#issuecomment-2437408562,1,['depend'],['depending']
Integrability,"Yeah, the UMAP plots work alright, and I can recognize many of the genes I get in the message are indeed genes that are expressed in the data, for I can visualize them on my UMAP plots. It is kind of weird, I am wondering if somewhere I messed up the format of those names, but then why would they work on UMAP?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456760052:86,message,message,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456760052,1,['message'],['message']
Integrability,"Yep, but still you need data passed to not only have the same dimensionality, you need dimensions to have the same meaning any time you want to project new data. If you have integrated embeddings (such ash X_pca_harmony) those will change every time you add new data. Using genes to fit a initial UMAP will ensure that you can transform new data, provided you have the same genes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205:174,integrat,integrated,174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133929205,1,['integrat'],['integrated']
Integrability,"Yes, I agree. My proposal would be to integrate this PR as it currently stands (after the testing system is working), as it fixes #1097 (namely, you can't currently set x, y, or color to something that is in `adata.raw.var.index` but not `adata.var.index`, even if `use_raw=True`) and leaves the logic flow for the transposition case as is. It also adds some test coverage to use cases of `sc.pl.scatter()` that were not covered before. Then, I can start working on coming up with a strategy for solving the second problem (`use_raw=True` with transposition) separately, as it's mostly unrelated, and seems to be a bit more complicated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514:38,integrat,integrate,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964309514,1,['integrat'],['integrate']
Integrability,"Yes, I think that would be the best solution for the time during which we rely on packages which do not ship proper wheels... . I agree that in the future, `scanpy` could become the full installation. Why not `scanpy-core`, `scanpy`, `scanpy-full`? I don't think it will bother anyone if we stop supporting `scanpy-full` at some point and only use `scanpy`. Given how Scanpy is set up and used, I could also imagine that, upon growing, it will become in some parts even more a thin wrapper for packages that should be optionally installed (it is already a thin wrapper for `igraph`, `louvain` and `MulticoreTSNE`, where Scanpy simply makes the usage more convenient by unifying visualization etc. and efficient by reusing input parameters that have previously been computed and used in other parts of Scanpy - right now, essentially all the preprocessing, the neighborhood relations and graph stuff). . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559:482,wrap,wrapper,482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355144559,2,['wrap'],['wrapper']
Integrability,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:425,depend,dependence,425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139,1,['depend'],['dependence']
Integrability,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:413,wrap,wrapper,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242,1,['wrap'],['wrapper']
Integrability,"Yes, graph_tool is nice and I'm also using it; but yes, it's installation is even worse than igraph... hence, no option for a dependency...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231:126,depend,dependency,126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370355231,1,['depend'],['dependency']
Integrability,"Yes, it looks like we didn't update our dependency requirements correctly. It looks like the `rmatmat` argument for `LinearOperators` was only added as of `1.4`. I believe using `scanpy 1.5.1` with `scipy>1.4` should fix this. Could you let me know if that solves your problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019:40,depend,dependency,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1246#issuecomment-633451019,1,['depend'],['dependency']
Integrability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:87,integrat,integrated,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,5,"['depend', 'integrat']","['dependency', 'integrated', 'integration']"
Integrability,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692:92,interface,interface,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310#issuecomment-430990692,2,['interface'],['interface']
Integrability,"Yes, the image does not synchronize with flipped spatial dots. ; I find a way to flip the image by changing the image coords:; hires_coord = slide.uns['spatial']['sample1']['images'][""hires""]; slide.uns['spatial']['sample1']['images'][""hires""] = hires_coord[:,:,::-1]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2520#issuecomment-1605898378:24,synchroniz,synchronize,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520#issuecomment-1605898378,1,['synchroniz'],['synchronize']
Integrability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:81,integrat,integration,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['integrat'],['integration']
Integrability,"Yes, this was my impression too. However there is a documented option; ""Switch to Windows containers"" which is available if you right click on the; Docker icon in the taskbar and this allows one to run vms using a Windows; kernel. On Fri, Sep 6, 2024, 3:36â€¯AM Philipp A. ***@***.***> wrote:. > If you want to try it out, I give instructions for how to reproduce the; > error with a Docker container for Windows in the cross-referenced issue; >; > Yes please. Iâ€™m confused how Windows comes into play though since I thougt; > that Docker always runs on a Linux kernel â€“ natively on Linux and in a VM; > on macOS and Windows.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/issues/2969#issuecomment-2333436219>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AH2OS47KNFAVTYUHGAMORILZVFLRXAVCNFSM6AAAAABFM3NQROVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZTGQZTMMRRHE>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2334006260:997,Message,Message,997,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2334006260,1,['Message'],['Message']
Integrability,"Yes. I'm interested in many of the things here. Thank you for pinging me. I'm happy to engage going forward in a variety of ways. Let's start with a few responses. > I tried looking at pydata sparse with Dask, but it ran a lot slower than regular scipy.sparse (which is what Scanpy uses). It would be great to get a slimmed down version of the operations that you're running with pydata/sparse and submit those to the issue tracker there. @hameerabbasi is usually pretty responsive, and I know that he appreciates learning about new use cases of pydata/sparse. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse. Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. > It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it canâ€™t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:576,wrap,wrapper,576,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,3,"['protocol', 'wrap']","['protocol', 'wrapper']"
Integrability,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846:199,depend,dependent,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498066846,1,['depend'],['dependent']
Integrability,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:128,depend,depends,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801,1,['depend'],['depends']
Integrability,"You could use conda ([relevant docs](https://scanpy.readthedocs.io/en/stable/installation.html#bioconda)). Not having a GUI shouldn't matter, but I'm not sure if Tkinter is an installation dependency for `matplotlib`. If you're getting an error related to an interactive backend when you try to plot, you can switch the [matplotlib backend](https://matplotlib.org/faq/usage_faq.html#what-is-a-backend).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396:189,depend,dependency,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/595#issuecomment-480657396,1,['depend'],['dependency']
Integrability,You have already replied [here](https://github.com/theislab/single-cell-tutorial/issues/103#issuecomment-1313299316). Many apologies for missing this message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727:150,message,message,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1467415727,1,['message'],['message']
Integrability,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:886,integrat,integrate,886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,2,"['integrat', 'wrap']","['integrate', 'wrapper']"
Integrability,"],; [-1.145477, 10.185449, 4.414117, ..., -0.087394, -1.327791,...; ```. The second test:. ```; sc.pp.neighbors(adata1, n_pcs=30, use_rep='X_pca_harmony'); sc.pp.neighbors(adata2, n_pcs=30, use_rep='X_pca_harmony'); np.testing.assert_array_equal(adata1.obsp[""connectivities""].data, adata2.obsp[""connectivities""].data); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 268636 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 434492 (61.8%); Max absolute difference: 0.99820393; Max relative difference: 810.4644; x: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179325, 1. ],; dtype=float32); y: array([0.158963, 0.206843, 0.234457, ..., 0.095996, 0.179324, 1. ],; dtype=float32); ```. This is my session_info:. ```; Click to view session information; -----; anndata 0.9.2; loguru 0.7.2; matplotlib 3.8.0; numpy 1.26.0; pandas 1.4.3; scanpy 1.9.6; seaborn 0.12.2; session_info 1.0.0; -----; Click to view modules imported as dependencies; PIL 9.4.0; argcomplete NA; asttokens NA; attr 23.1.0; awkward 2.4.2; awkward_cpp NA; backcall 0.2.0; cffi 1.15.1; comm 0.1.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.8.0; decorator 5.1.1; dot_parser NA; etils 1.4.1; exceptiongroup 1.1.3; executing 1.2.0; get_annotations NA; gmpy2 2.1.2; h5py 3.9.0; harmonypy NA; igraph 0.10.8; importlib_metadata NA; importlib_resources NA; ipykernel 6.25.2; ipywidgets 8.1.1; jax 0.4.20; jaxlib 0.4.20; jedi 0.19.0; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.41.1; ml_dtypes 0.2.0; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; numba 0.58.1; nvfuser NA; opt_einsum v3.0.0; packaging 23.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:3154,depend,dependencies,3154,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,1,['depend'],['dependencies']
Integrability,"```; try:; from bbknn import bbknn; except ImportError:; def bbknn(*args, **kwargs):; raise ImportError('Please install BBKNN: `pip3 install bbknn`'); ```. > I went that way since I didnâ€™t want to make it look like we coded it (with the docs hosted on our page and so on). Do you think thatâ€™s a good solution or would you like it to be done differently?. This is great!. https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.bbknn.html#scanpy.api.pp.bbknn. But I actually don't see any docs there, I don't know why it doesn't find the original docstring... We'd like to have the reference to @ktpolanski preprint in the docstring in the first line together with a short summary of what it does and how it does it, just as for any other function. As this directly uses the implementation from @ktpolanski, we'd also want an explicit statement about that. > pp.bbknn is just an alias for bbknn.bbknn(). Refer to it for the documentation. ... can be removed. That should be evident... In the case of the `tl.leiden` wrapper, I'd also add an explicit statement that this wraps the leiden package of Traag (2018). Otherwise, all of this is great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050:1017,wrap,wrapper,1017,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/361#issuecomment-439841050,2,['wrap'],"['wrapper', 'wraps']"
Integrability,"```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <timed exec> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); 201 ); 202 ; --> 203 output = _pca_with_sparse(X, n_comps, solver=svd_solver); 204 # this is just a wrapper for the results; 205 X_pca = output['X_pca']. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_pca.py in _pca_with_sparse(X, npcs, solver, mu, random_state); 293 return XHmat(x) - mhmat(ones(x)); 294 ; --> 295 XL = LinearOperator(; 296 matvec=matvec,; 297 dtype=X.dtype,. TypeError: __init__() got an unexpected keyword argument 'rmatmat'; ```. I got this error once with the new spare PCA. @atarashansky do we need to write an explicit scipy version as dependency? It might be something weird with my setup too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632:453,wrap,wrapper,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-636055632,2,"['depend', 'wrap']","['dependency', 'wrapper']"
Integrability,"`conda install` (not `pip`). Perhaps, that is due to pytables' conda dependencies (such as hdf).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438:69,depend,dependencies,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462140438,1,['depend'],['dependencies']
Integrability,"`conda install` meant to be related to `pytables`, not `scanpy`. `scanpy` runs easily via `pip`, only the tables dependency complains..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457:113,depend,dependency,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-462261457,1,['depend'],['dependency']
Integrability,`gprofiler` functionality is being added to scanpy? I have a small wrapper for that as well... the main components being a try-catch wrapper around it as it can give an error when there are no results.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/467#issuecomment-463965367:67,wrap,wrapper,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/467#issuecomment-463965367,2,['wrap'],['wrapper']
Integrability,"`log_transformed` disappeared in the last commit here... Better: `pp.log1p` should write an attribute to `.uns`, say simply `.uns['log1p'] = True`. Depending on that attribute, log2fc is computed by rexponaniating or not. Also: If trying to call a t-test with non-logarithmized data, a warning should be written. The overflow and 0 warnings: are you sure you used logarithmized data, GÃ¶kcen?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809:148,Depend,Depending,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/519#issuecomment-477907809,1,['Depend'],['Depending']
Integrability,"`numba` really is the main blocker, see also: . * Tracking issue: https://github.com/pyodide/pyodide/issues/621; * Potential PR: https://github.com/emscripten-forge/recipes/pull/168; * Unfortunately, the author recently founded prefix.dev so may not have time to complete this ðŸ˜¢. `pynndescent` also depends on numba. I am hopeful that numba's new AOT backend may make this easier in the future. Unclear how painful it would be to distribute binaries capable of multithreading though. I think `h5py` would also be pretty reasonable to make optional if we could otherwise run in pyodide, since we wouldn't have a filesystem anyways. Though I think pytables runs in pyodide, so it's probably reasonable to get `h5py` there too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2667#issuecomment-1804146936:299,depend,depends,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2667#issuecomment-1804146936,1,['depend'],['depends']
Integrability,"`pytables` in pip is named `tables`, and scanpy `import tables` accordingly, so you have to separately install it using also pip, not conda. I suggest editing the [installation guide](https://scanpy.readthedocs.io/en/stable/installation.html). Besides, my installing using `conda install -c bioconda scanpy` would always give conflicts with nvidia cuda versions, but changing the version makes nothing change except the conflict message. Is it a Windows feature?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584:429,message,message,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-747217584,1,['message'],['message']
Integrability,"a\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_neighbors=n_neighbors,; 141 knn=knn,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 809 # we need self._distances also for method == 'gauss' if we didn't; 810 # use dense distances; --> 811 self._distances, self._connectivities = _compute_connectivities_umap(; 812 knn_indices,; 813 knn_distances,. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\neighbors\__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:4847,message,message,4847,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['message'],['message']
Integrability,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:954,depend,depends,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,1,['depend'],['depends']
Integrability,"also as an aside, would it be appropriate to include some of @LuckyMD scIB integration metrics here? It would give people easier access and probably expand general use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897:75,integrat,integration,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-763812897,1,['integrat'],['integration']
Integrability,"ar-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21; pandas | 1.1.5; pandocfilters | 1.4.3; parso | 0.8.2; patsy | 0.5.2; pickleshare | 0.7.5; Pillow | 8.4.0; pip | 21.2.2; prometheus-client | 0.11.0; prompt-toolkit | 3.0.20; pycparser | 2.2; Pygments | 2.10.0; pynndescent | 0.5.5; pyOpenSSL | 21.0.0; **pyparsing | 3.0.4**; pyrsistent | 0.17.3; PySocks | 1.7.1; python-dateutil | 2.8.2; python-igraph | 0.9.8; pytz | 2021.3; pywin32 | 228; pywinpty | 0.5.7; pyzmq | 22.2.1; requests | 2.26.0; scanpy | 1.7.2; scikit-learn | 0.24.2; scipy | 1.5.4; seaborn | 0.11.2; Send2Trash | 1.8.0; setuptools | 58.0.4; sinfo | 0.3.4; six | 1.16.0; sniffio | 1.2.0; statsmodels | 0.12.2; stdlib-list | 0.8.0; tables | 3.6.1; terminado | 0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2717,wrap,wrap,2717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['wrap'],['wrap']
Integrability,"aslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1047,integrat,integrate,1047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['integrat'],['integrate']
Integrability,"ass_inst = _pass_registry.get(pss).pass_inst; [340](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=339) if isinstance(pass_inst, CompilerPass):; --> [341](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=340) self._runPass(idx, pass_inst, state); [342](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=341) else:; [343](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=342) raise BaseException(""Legacy pass in use""). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_lock.py:35, in _CompilerLock.__call__.<locals>._acquire_compile_lock(*args, **kwargs); [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=31) @functools.wraps(func); [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=32) def _acquire_compile_lock(*args, **kwargs):; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=33) with self:; ---> [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_lock.py?line=34) return func(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\compiler_machinery.py:296, in PassManager._runPass(self, index, pss, internal_state); [294](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=293) mutated |= check(pss.run_initialization, internal_state); [295](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=294) with SimpleTimer() as pass_time:; --> [296](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/compiler_machinery.py?line=295) mutated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:24165,wrap,wraps,24165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wraps']
Integrability,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3462,wrap,wrappers,3462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['wrap'],['wrappers']
Integrability,"ave multiple weighting options that depend on the `openTSNE` package. If it turns out these methods don't have much in the way of parameters, then it might be reasonable for this to be a part of `sc.pp.neighbors`. How about this, the implementation here should be well factored out into:. 1. Getting nearest neighbors; 2. Weighting the graph; 3. Computing the layout. Once the available parameters are clear I think it'll be easier to make an informed decision about whether neighbor weighting for tsne should occur through `sc.pp.neighbors`. Additionally, I think it'll be easier to integrate cleanly separated code than to separate integrated code. > The weights constructed by UMAP in neighbors are not normalized. So if you run neighbors() and then tsne() then t-SNE should do something in order to be able to use this graph. For passing the umap connectivity matrix to tsne layout, I think I would expect the weights to be used. Something like this should accomplish that:. ```python; class WrappedAffinities(openTSNE.affinity.Affinities):; def __init__(self, neighbors, symmetrize=True, verbose=False):; self.verbose = verbose; P = neighbors; if symmetrize:; P = (P + P.T) / 2; total = P.sum(); if not np.isclose(total, 1.):; P = P / total; self.P = P; ```. That said, I'm not too familiar with the assumptions of tsne, or if this would be appropriate. I think binarizing the edge weights is a bit of a strong assumption unless specifically requested though. With `umap`, we throw a warning if it looks like the passed graph didn't come from `umap`. You could do the same here?. > From an implementation standpoint, the sc.pp.tsne_negihbors will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. I would like nearest neighbor calculation and graph weighting to be split out eventually. Since it's already done this way in `openTSNE`, I think this could help with that goal. > From what I can tell, the standard way of weighing the KNNG",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200:1125,Wrap,WrappedAffinities,1125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-761950200,1,['Wrap'],['WrappedAffinities']
Integrability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:81,integrat,integrating,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['integrat'],['integrating']
Integrability,"closed with #1472 , next time @danielStrobl you could just do `git commit -m ""<commit-message> #1471""` and it would be automatically linked (and then closed if PR is merged)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361:86,message,message,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1471#issuecomment-776889361,1,['message'],['message']
Integrability,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4969,depend,dependency,4969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['depend'],['dependency']
Integrability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2121,integrat,integration,2121,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['integrat'],['integration']
Integrability,"e of your suggestions and here's current status:. - reverted back to look for `library_id` in spatial, but still added the exception that `adata.uns[""spatial""]` does not exist. This is in order to use `sc.pl.spatial` with non-visium data.; - if that's the case, then spatial should simply wrap embedding. This also refers to your point.; 	> I'm not totally sure what this means. The coordinates have been z-score transform across each axis? How is this useful? In particular, how is it useful to completely replace the original coordinates with this?. 	this is very likely to happen for anything that it's not visium. In that case, users will share already processed data that contains coordinates in some type of system, and this is the case for whatever processing they had to undertake (would suggest you to have a look at https://github.com/spacetx/starfish for examples of those processing steps.). Anyway, in short, it's much easier for us to just wrap embedding in that case, and I also think it's more correct cause then is the user to choose whatever heuristics they want for point sizes. - fixed a problem in #1534 , that is that the coordinate systems in non-visium has bottom left origin (whereas in visium is top-left, which makes sense because it's in image pixel coordiantes). For this reason, I added the y coordinate inversion in `sc.pl.spatial`, and only in the case where visium is selected, but with img_key = None. Note that this happens because if an img is plotted (before the spots with `circle`), then the origin automatically swap. But if `img_key` is None, then it reverts to default (bottom left). This made it easier as I could remove it from `def _get_data_points` and from `utils._get_edges`. Also added couple of tests for this case. This should be ready for another review, let me know if logic is clearer or I could add more comments in code. re; > Can the spatial neighbours be based off multiple library ids? If so, could you have:; ```python; uns = {; ""spatial"": ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306:999,wrap,wrap,999,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1512#issuecomment-739863306,1,['wrap'],['wrap']
Integrability,"e, ir.Expr):; --> 626 return self.lower_expr(ty, value); 627 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_expr(self, resty, expr); 1161 elif expr.op == 'call':; -> 1162 res = self.lower_call(resty, expr); 1163 return res. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in lower_call(self, resty, expr); 890 else:; --> 891 res = self._lower_call_normal(fnty, expr, signature); 892 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\lowering.py in _lower_call_normal(self, fnty, expr, signature); 1132 ; -> 1133 res = impl(self.builder, argvals, self.loc); 1134 return res; C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in __call__(self, builder, args, loc); 1189 def __call__(self, builder, args, loc=None):; -> 1190 res = self._imp(self._context, builder, self._sig, args, loc=loc); 1191 self._context.add_linking_libs(getattr(self, 'libs', ())). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\base.py in wrapper(*args, **kwargs); 1219 kwargs.pop('loc') # drop unused loc; -> 1220 return fn(*args, **kwargs); 1221 . C:\ProgramData\Anaconda3\lib\site-packages\numba\cpython\rangeobj.py in range1_impl(context, builder, sig, args); 37 state.start = context.get_constant(int_type, 0); ---> 38 state.stop = stop; 39 state.step = context.get_constant(int_type, 1). C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setattr__(self, field, value); 163 return super(_StructProxy, self).__setattr__(field, value); --> 164 self[self._datamodel.get_field_position(field)] = value; 165 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\cgutils.py in __setitem__(self, index, value); 187 else:; --> 188 raise TypeError(""Invalid store of {value.type} to ""; 189 ""{ptr.type.pointee} in "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x000001FF1D57B970> (trying to write member #1). During handling of the above exception, another exception occurred:. LoweringErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:2675,wrap,wrapper,2675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['wrap'],['wrapper']
Integrability,"e_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 390 # umap 0.5.0; 391 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 392 from umap.umap_ import fuzzy_simplicial_set; 393 ; 394 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). C:\ProgramData\Anaconda3\lib\site-packages\umap\__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . C:\ProgramData\Anaconda3\lib\site-packages\umap\umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. C:\ProgramData\Anaconda3\lib\site-packages\umap\layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . C:\ProgramData\Anaconda3\lib\site-packages\numba\core\decorators.py in wrapper(func); 217 with typeinfer.register_dispatcher(disp):; 218 for sig in sigs:; --> 219 disp.compile(sig); 220 disp.disable_compile(); 221 return disp. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in compile(self, args, return_type); 123 ; 124 def compile(self, args, return_type):; --> 125 status, retval = self._compile_cached(args, return_type); 126 if status:; 127 return retval. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\dispatcher.py in _compile_cached(self, args, return_type); 137 ; 138 try:; --> 139 retval = self._compile_core(args, return_type); 140 except errors.TypingError as e:; 141 self._failed_cache[key] = e. C:\ProgramData\Anaconda3\lib\site-packages\numba\core\disp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325:5672,wrap,wrapper,5672,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-1319286325,1,['wrap'],['wrapper']
Integrability,"ecifically what you want (e.g. `typing.Sequence[T]` if it only needs to be iterable and indexable, but not necessarily a `list`). About your examples, generally I always have to dig into the code to figure such things out. Annoying, but it means that people after me can just use the type annotations instead of wasting their time doing the same. - Currently we only accept `str` for obs_names, so depending on the actual usage in the function, `Iterable[str]` or `Sequence[str]` would be it I guess.; - Usually single integers being passed around should be python `int`s, because they can be any size. But if you need to accept `int32` and `int64`, you can do `Union[int, np.integer]`. > Is there a way to say: ""should behave right if I call np.array on it"". In truth, `array` eats just about anything (with weird results), and for us this is probably a good idea:. ```py; Number = Union[float, int, np.integer, np.floating]; Num1DArrayLike = Sequence[Number]; Num2DArrayLike = Sequence[Num1DArrayLike]; Num3DArrayLike = Sequence[Num2DArrayLike]; NumNDArrayLike = Union[Num1DArrayLike, Num2DArrayLike, Num3DArrayLike]; ```. But if we want to be exact about `array_like`s, weâ€™d need this ABC:. ```py; class ArrayLike(ABC):; """"""An array,; any object exposing the array interface,; an object whose __array__ method returns an array,; or any (nested) sequence.; """"""; @classmethod; def __subclasshook__(cls, C):; if issubclass(C, np.ndarray):; return True; if any('__array_interface__' in B.__dict__ for B in C.__mro__):; return True; if any('__array__' in B.__dict__ for B in C.__mro__):; return True; return Sequence.__subclasshook__(cls, C); ```. ----. Two thoughts here:. 1. Itâ€™s fine if you donâ€™t know exactly. Just use your best guess. The worst case is that someone wastes a second of runtime converting their argument to the type you thought was needed, while the original type would have been OK.; 2. Itâ€™s good if someone thinks about all that because that means things donâ€™t break unexpectedly!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438:1385,interface,interface,1385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441207438,1,['interface'],['interface']
Integrability,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:5088,wrap,wrap,5088,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['wrap'],"['wrap', 'wrap-']"
Integrability,"elta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo; # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352; ```. </details>. <details>; <summary>More complicated example with argument value logging</summary>. ```python; from anndata import AnnData; from copy import copy; from datetime import datetime; from functools import wraps; import inspect; from itertools import chain; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(logged_args=None):; """"""; Params; ------; logged_args : list[str], optional (default: `None`); Names of arguments to log.; """"""; if logged_args is None:; logged_args = []; def logged_decorator(func):; argnames = inspect.getfullargspec(func).args; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):; if type(val) is AnnData:; logged_params[param] = id(val); elif param in logged_args:; logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,; logged_args=logged_params, call_id=call_id). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(; called_func=func.__name__, elapsed=dt,; ); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper; return logged_decorator. # Usage. @logged(logged_args=[""x""]); def foo(adata, x, copy=True):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1, copy=True);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:2869,wrap,wraps,2869,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['wrap'],['wraps']
Integrability,"erwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metricâ€™s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6750,Depend,Depending,6750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['Depend'],['Depending']
Integrability,"es())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should be, or at least start out as, separate from scanpy since there are a number of dependencies I think are useful here, which aren't required for scanpy. Plus being able to iterate quickly would be nice. I've started collecting some notebooks on this [here](https://github.com/ivirshup/notebooks/tree/master/plotting).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:5506,depend,dependencies,5506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345,1,['depend'],['dependencies']
Integrability,"eturn func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo; # 2019-02-13 19:28.03 call_finish call_id=UUID('986f57e4-656a-41b1-9c7c-a7c5ad5b01fc') called_func=foo elapsed=datetime.timedelta(microseconds=505970) returned_adata_id=4940502352; ```. </details>. <details>; <summary>More complicated example with argument value logging</summary>. ```python; from anndata import AnnData; from copy import copy; from datetime import datetime; from functools import wraps; import inspect; from itertools import chain; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(logged_args=None):; """"""; Params; ------; logged_args : list[str], optional (default: `None`); Names of arguments to log.; """"""; if logged_args is None:; logged_args = []; def logged_decorator(func):; argnames = inspect.getfullargspec(func).args; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; logged_params = {}. for param, val in chain(zip(argnames, args), kwargs.items()):; if type(val) is AnnData:; logged_params[param] = id(val); elif param in logged_args:; logged_params[param] = copy(val) # Probably need to consider how these values get logged. logger.msg(""call"", called_func=func.__name__,; logged_args=logged_params, call_id=call_id). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(; called_func=func.__",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:2466,wrap,wraps,2466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['wrap'],['wraps']
Integrability,"fueled by reading passively in the bioconda channel for years now and memorizing this rule of thumb regarding where to put recipes:. Anything bio-specific --> bioconda; Anything else --> conda-forge . If this does not hold true (anymore?), @bgruening , I believe that one could still stay with conda-forge and instead try to maintain own biocontainers (need to check with the folks there if uploading would be fine for them etc pp). . >The documentation for bioconda has been incomplete and out of date for years. It could be better, but most of the points are still valid and with some help from the community recipes are still created fine ;-) . >conda-forge autoupdates recipes. When we make a pip release, a conda-forge release is automatically generated. Bioconda-bot does the same for you ;-) . >bioconda packages can depend on conda-forge packages, but not the other way around (last I checked at least). If we go on >bioconda all our dependents do too â€“ this could make it extremely painful to do a migration to bioconda. Thats not the case: E.g. when you move `scanpy` over, the libraries that are not bio related, can stay on conda-forge. That way, resolving will work. I am really not sure if the resolving will not take other channels into account, unless there is different versions of packages on various channels, e.g. a library both on conda-forge and bioconda which would then be handled by channel priorities. >All of our dependencies are on conda-forge. Thats the case for the majority of bio tools - most rely on general purpose tools ;-) . >Fewer channels to search means easier, faster environment solving. `mamba` can help you here, at least for most of the conda recipes I have used (some have hundreds of dependencies in total, especially in multi-tool environments), I didn't notice that much of a difference between using 1 - 2 channels â“ . And thanks all for the ongoing discussion, still learning things here and also getting new perspectives on the general topic here ðŸ‘ðŸ»",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817:1640,depend,dependencies,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2281#issuecomment-1161394817,2,['depend'],['dependencies']
Integrability,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:966,Integrat,Integration,966,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,3,"['Integrat', 'integrat']","['Integration', 'integrate', 'integration']"
Integrability,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:254,integrat,integration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['integrat'],['integration']
Integrability,"hmm, Iâ€™m not sure if itâ€™s possible to require versions depending on the platform. whereâ€™s the h5py issue about this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454#issuecomment-459034769:55,depend,depending,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454#issuecomment-459034769,1,['depend'],['depending']
Integrability,"hon3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-pa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4912,wrap,wrap,4912,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,how would you avoid the context manager?. itâ€™s either that or wrapping try/catch around every single use of the `writedir`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343:62,wrap,wrapping,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346763343,1,['wrap'],['wrapping']
Integrability,"https://scanpy-tutorials.readthedocs.io/en/latest/integrating-data-using-ingest.html; Hi, this should be relevant.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561:50,integrat,integrating-data-using-ingest,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1847#issuecomment-845420561,1,['integrat'],['integrating-data-using-ingest']
Integrability,"i guess we could go this route:. ```py; mean_filter = 0.01; cv_filter = 2; nr_pcs = 50. # row normalize ; adata = adata.smp_norm(max_fraction=0.05, mult_with_mean=True); # filter out genes with mean expression < 0.1 and coefficient of variance < ; # cvFilter ; adata = adata.filter_var_cv(mean_filter, cv_filter); # compute zscore of filtered matrix ; Xz = zscore(adata.X); # PCA ; Xpca = pca(Xz, nr_comps=nr_pcs); # update dictionary; adata['Xpca'] = Xpca; sett.m(0, 'Xpca has shape', Xpca.shape[0], 'x', Xpca.shape[1]); print(adata.X); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015:25,rout,route,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/4#issuecomment-278579015,1,['rout'],['route']
Integrability,"inutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself ðŸ˜ƒ), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1108,wrap,wrappers,1108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,1,['wrap'],['wrappers']
Integrability,"ioned above, not pip.; hint: See above for details.; Exception information:; Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/operations/build/metadata_legacy.py"", line 64, in generate_metadata; call_subprocess(; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/utils/subprocess.py"", line 224, in call_subprocess; raise error; pip._internal.exceptions.InstallationSubprocessError: python setup.py egg_info exited with 1. The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/base_command.py"", line 160, in exc_logging_wrapper; status = run_func(*args); ^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/cli/req_command.py"", line 247, in wrapper; return func(self, options, args); ^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/commands/install.py"", line 400, in run; requirement_set = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 92, in resolve; result = self._result = resolver.resolve(; ^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 481, in resolve; state = resolution.resolve(requirements, max_rounds=max_rounds); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 373, in resolve; failure_causes = self._attempt_to_pin_criterion(name); ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^; File ""/Users/dang/opt/miniconda3/envs2/test/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py"", lin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:5744,wrap,wrapper,5744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['wrap'],['wrapper']
Integrability,"ipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: python-dateutil>=2.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.8.1); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (2.4.5); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10280,wrap,wrap,10280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ite-packages (from scanpy) (3.3.3); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: h5py!=2.10.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (2.9.0); Requirement already satisfied: seaborn in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.9.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: matplotlib==3.0.* in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.0.3); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: scikit-learn>=0.21.2 in /home/tsundoku/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:7652,wrap,wrap,7652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1257,depend,dependencies,1257,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['depend'],['dependencies']
Integrability,"line 1016, in _send_output; self.send(msg); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 956, in send; self.connect(); File ""/anaconda3/envs/scIB-python/lib/python3.7/http/client.py"", line 1392, in connect; server_hostname=server_hostname); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 412, in wrap_socket; session=session; File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 853, in _create; self.do_handshake(); File ""/anaconda3/envs/scIB-python/lib/python3.7/ssl.py"", line 1117, in do_handshake; self._sslobj.do_handshake(); ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1056). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_utils.py"", line 10, in wrapper; return f(*args, **kwargs); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/datasets/_datasets.py"", line 305, in pbmc3k_processed; backup_url='https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad',; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 122, in read; **kwargs,; File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 694, in _read; is_present = _check_datafile_present_and_download(filename, backup_url=backup_url,); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 970, in _check_datafile_present_and_download; _download(backup_url, path); File ""/anaconda3/envs/scIB-python/lib/python3.7/site-packages/scanpy/readwrite.py"", line 936, in _download; urlopen(Request(url, headers={""User-agent"": ""scanpy-user""})) as resp:; File ""/anaconda3/envs/scIB-python/lib/python3.7/urllib/request.py"", line 222, in urlopen; return opener.open(url, data, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665:1863,wrap,wrapper,1863,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1472#issuecomment-721326665,1,['wrap'],['wrapper']
Integrability,"lling numba from a `ThreadPoolExecutor` isnâ€™t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1206,wrap,wrapper,1206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"lmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial](https://icbi-lab.github.io/scirpy/tutorials/tutorial_3k_tcr.html#Analysis-of-3k-T-cells-from-cancer). But I agree that this deserves an own section in the docs (created https://github.com/icbi-lab/scirpy/issues/110). Currently, we simply add columns to `adata.obs` - but I'm still open to discussion. The data-structure needs slight modifications for BCR data anyway. See also: https://github.com/theislab/anndata/issues/115#issuecomment-579275853. . Cheers, ; Gregor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1716,integrat,integration,1716,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['integrat'],['integration']
Integrability,"lobs""] = pd.Categorical(blobs.obs[""blobs""]); blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]); # LinAlgError: Singular matrix; ```. <details>; <summary> Full traceback </summary>. ```pytb; ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <ipython-input-13-5685c001369c> in <module>; ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a); 544 signature = 'D->D' if isComplexType(t) else 'd->d'; 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular); --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj); 547 return wrap(ainv.astype(result_t, copy=False)); 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag); 86 ; 87 def _raise_linalgerror_singular(err, flag):; ---> 88 raise LinAlgError(""Singular matrix""); 89 ; 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix; ```. </details>. Does this occur in your data? You can check with:. ```python; pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303:1894,wrap,wrap,1894,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606#issuecomment-766480303,1,['wrap'],['wrap']
Integrability,"looks good, let's take one these. but before integrating it, the scatter plot of dpt should invoke `plotting.plot_tool` as well. then it's going to be just a one line change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182:45,integrat,integrating,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3#issuecomment-278376182,1,['integrat'],['integrating']
Integrability,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4909,wrap,wrap,4909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['wrap'],['wrap']
Integrability,"lue, pairwise); 817 # Annotation for other axis; 818 alt_annot = merge_dataframes(; --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge; 820 ); 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/panda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2207,wrap,wrapper,2207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,3,['wrap'],"['wrapper', 'wraps']"
Integrability,"matrix'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-102-4378df4ffefd> in <module>; ----> 1 adpt.write_h5ad('../data/ra19_10_liverprimary_yubin_latest.h5ad.gz'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1844 filename = self.filename; 1845 ; -> 1846 _write_h5ad(; 1847 Path(filename),; 1848 self,. ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 90 elif not (adata.isbacked and Path(adata.filename) == Path(filepath)):; 91 # If adata.isbacked, X should already be up to date; ---> 92 write_attribute(f, ""X"", adata.X, dataset_kwargs=dataset_kwargs); 93 if ""raw/X"" in as_dense and isinstance(; 94 adata.raw.X, (sparse.spmatrix, SparseDataset). ~/miniconda3/envs/scrna/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/miniconda3/envs/scrna/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 189 except Exception as e:; 190 parent = _get_parent(elem); --> 191 raise type(e)(; 192 f""{e}\n\n""; 193 f""Above error raised while writing key {key!r} of {type(elem)}"". NotImplementedError: Failed to write value for X, since a writer for type <class 'scipy.sparse.csr.csr_matrix'> has not been implemented yet. Above error raised while writing key 'X' of <class 'h5py._hl.files.File'> from /.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732:1828,wrap,wrapper,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1670#issuecomment-783799732,1,['wrap'],['wrapper']
Integrability,"mba_c251d9588484449eb116f16ee1b89979/setup.py"", line 51, in <module>; _guard_py_ver(); File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py"", line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; Ã— python setup.py egg_info did not run successfully.; â”‚ exit code: 1; â•°â”€> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3281,message,message,3281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['message'],['message']
Integrability,"methods?. So, if we use the KNNG provided by `sc.pp.neighbors`, these parameters become unnecessary. Both `perplexity` and `k` specify the number of k-nearest neighbors when constructing the KNNG. Here, we assume that the KNNG exists from before, so there is no need for this parameter. > Do you need to know what the affinity method was if you're just calculating an embeddings? Or does that only become important when you want to add new data?. Yes, the affinity model will have to be somehow kept, since when we call `transform`, we need to find the nearest neighbors in the index. I haven't checked how your UMAP functionality does this, but I'm guessing it's similar. Regarding the whole API, I have a few comments. I very much dislike the API `sc.pp.neighbors_tsne(adata)`. scanpy is nice because it's easy to use and the API is dead simple. I can just call `sc.pp.neighbors` followed by clustering, visualization, and whatever else I want using simple function calls. If we went this route, this would mean changing `sc.pp.neighbors` to `sc.pp.umap_neighbors`, and then splitting of yet another `sc.pp.gauss_neighbors`. This would not only make things confusing, it would mean re-calculating the KNNG at each call, which we would inevitably have to do if we wanted different visualizations. It then also becomes quite unclear what to do when I want to do Louvain clustering. Should there be a `sc.pp.louvain_neighbors` as well? Which neighbors should I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:1441,rout,route,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['rout'],['route']
Integrability,"mple_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; url",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1790,wrap,wrap,1790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['wrap'],['wrap']
Integrability,"ms, dtype, name, reduced_meta); 362 try:; --> 363 meta = func(reduced_meta, computing_meta=True); 364 # no meta keyword argument exists for func, and it isn't required. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/dask/array/reductions.py:685, in mean_combine(pairs, sum, numel, dtype, axis, computing_meta, **kwargs); 684 ns = deepmap(lambda pair: pair[""n""], pairs) if not computing_meta else pairs; --> 685 n = _concatenate2(ns, axes=axis).sum(axis=axis, **kwargs); 687 if computing_meta:. TypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'. During handling of the above exception, another exception occurred:. IndexError Traceback (most recent call last); Cell In[19], line 1; ----> 1 result = sc.pp.highly_variable_genes(adata, inplace=False); 2 result. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:702, in highly_variable_genes(***failed resolving arguments***); 699 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 701 if batch_key is None:; --> 702 df = _highly_variable_genes_single_batch(; 703 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 704 ); 705 else:; 706 df = _highly_variable_genes_batched(; 707 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 708 ). File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:274, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 271 else:; 272 X = np.expm1(X); --> 274 mean, var = _get_mean_var(X); 275 # now actually compute t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725:1468,wrap,wraps,1468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725,1,['wrap'],['wraps']
Integrability,"n't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explana",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1543,inject,injects,1543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['inject'],['injects']
Integrability,"n3.8/site-packages/docutils/readers/__init__.py"", line 77, in parse; self.parser.parse(self.input, document); File ""/usr/local/lib/python3.8/site-packages/sphinx/parsers.py"", line 100, in parse; self.statemachine.run(inputlines, document, inliner=self.inliner); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 170, in run; results = StateMachineWS.run(self, input_lines, input_offset,; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2769, in underline; self.section(title, source, style, lineno - 1, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 327, in section; self.new_subsection(title, lineno, messages); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 393, in new_subsection; newabsoffset = self.nested_parse(; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 281, in nested_parse; state_machine.run(block, input_offset, memo=self.memo,; File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 196, in run; results = StateMachineWS.run(self, input_lines, input_offset); File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 241, in run; context, next_state, result = self.check_line(; File ""/usr/local/lib/python3.8/site-packages/docutils/statemachine.py"", line 459, in check_line; return method(match, context, next_state); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2344, in explicit_markup; self.explicit_list(blank_finish); File ""/usr/local/lib/python3.8/site-packages/docutils/parsers/rst/states.py"", line 2369, in explicit_list; newl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:7043,message,messages,7043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['message'],['messages']
Integrability,"n39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign; return self.lower_expr(ty, value); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr; res = self.lower_call(resty, expr); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call; res = self._lower_call_normal(fnty, expr, signature); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal; res = impl(self.builder, argvals, self.loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__; res = self._imp(self._context, builder, self._sig, args, loc=loc); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper; return fn(*args, **kwargs); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl; state.stop = stop; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__; self[self._datamodel.get_field_position(field)] = value; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__; raise TypeError(""Invalid store of {value.type} to ""; TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>; sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:2084,wrap,wrapper,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['wrap'],['wrapper']
Integrability,"ng and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > [â€¦](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1560,message,message,1560,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['message'],['message']
Integrability,"not yet, but we sent the message to them only recently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/542#issuecomment-479554225:25,message,message,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/542#issuecomment-479554225,1,['message'],['message']
Integrability,"nt handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` Ã— `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1568,Depend,Depending,1568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['Depend'],['Depending']
Integrability,"ode 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (7.0.0); Requirement already satisfied: statsmodels>=0.10.0rc2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.10.1); Requirement already satisfied: legacy-api-wrap in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.2); Requirement already satisfied: anndata>=0.6.22.post1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.6.22.post1); Requirement already satisfied: tqdm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (4.40.0); Requirement already satisfied: numba>=0.41.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.46.0); Requirement already satisfied: joblib in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.14.0); Requirement already satisfied: patsy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.5.1); Requirement already satisfied: importlib-metadata>=0.7; python_version < ""3.8"" in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy)(1.1.0); Requirement already satisfied: tables in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.6.1); Requirement already satisfied:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:2231,wrap,wrap,2231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ome/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.21.3); Requirement already satisfied: umap-learn>=0.3.10 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.3.10); Requirement already satisfied: numpy>=1.13.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (1.17.4); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: python-dateutil>=2.6.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2.8.1); Requirement already satisfied: pyparsing>=2.0.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (2.4.5); Requirement already satisfied: six in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from packaging->scanpy) (1.13.0); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: llvmlite>=0.30.0dev0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from numba>=0.41.0->scanpy) (0.30.0); Requirement already satisfied: zipp>=0.5 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (0.6.0); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: decorator>=4.3.0 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from networkx->scanpy) (4.4.1); Requirement already satisfied: kiwisolver>=1.0.1 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from matplotlib==3.0.*->scanpy) (1.1.0); Requirement already satisfied: cycler>=0.10 in /home/tsundoku/anaconda3/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:4752,wrap,wrap,4752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['wrap'],['wrap']
Integrability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1139,synchroniz,synchronization,1139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['synchroniz'],['synchronization']
Integrability,"packages\scanpy\neighbors\__init__.py"", line 139, in neighbors; neighbors.compute_neighbors(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors; self._distances, self._connectivities = _compute_connectivities_umap(; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap; from umap.umap_ import fuzzy_simplicial_set; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>; from .umap_ import UMAP; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>; from umap.layouts import (; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>; def rdist(x, y):; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper; disp.compile(sig); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile; cres = self._compiler.compile(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile; status, retval = self._compile_cached(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached; retval = self._compile_core(args, return_type); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core; cres = compiler.compile_extra(self.targetdescr.typing_context,; File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra; return pipeline.compile_extra(func); File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418:4094,wrap,wrapper,4094,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-1054106418,1,['wrap'],['wrapper']
Integrability,"ple who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself ðŸ˜ƒ), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a reference for the community. For example:. ![image](https://user-images.githubusercontent.com/1140359/38873972-4953977a-4257-11e8-8675-a238738eb558.png). Another question is other single cell Python packages like magic, ZIFA or DCA, for example. There will hopefully be more in the future. A contrib repo might include these, as well i.e. `sc.tl.magic`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1341,wrap,wrappers,1341,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,2,"['interface', 'wrap']","['interfaces', 'wrappers']"
Integrability,"posed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>; > .; >. -- . Fidel Ramirez",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:1620,message,message,1620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['message'],['message']
Integrability,"py?line=1129) res = impl(self.builder, argvals, self.loc); [1131](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/lowering.py?line=1130) return res. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1201, in _wrap_impl.__call__(self, builder, args, loc); [1200](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1199) def __call__(self, builder, args, loc=None):; -> [1201](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1200) res = self._imp(self._context, builder, self._sig, args, loc=loc); [1202](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1201) self._context.add_linking_libs(getattr(self, 'libs', ())). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\base.py:1231, in _wrap_missing_loc.__call__.<locals>.wrapper(*args, **kwargs); [1230](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1229) kwargs.pop('loc') # drop unused loc; -> [1231](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/core/base.py?line=1230) return fn(*args, **kwargs). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\cpython\rangeobj.py:40, in make_range_impl.<locals>.range1_impl(context, builder, sig, args); [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=38) state.start = context.get_constant(int_type, 0); ---> [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=39) state.stop = stop; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/numba/cpython/rangeobj.py?line=40) state.step = context.get_constant(int_type, 1). File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\numba\core\cgutils.py:164, in _StructProxy.__setattr__(self, field, value); [",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:5259,wrap,wrapper,5259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['wrap'],['wrapper']
Integrability,"rmagic.py:943, in RMagics.R(self, line, cell, local_ns); 941 if not e.stdout.endswith(e.err):; 942 print(e.err); --> 943 raise e; 944 finally:; 945 if self.device in DEVICES_STATIC:. File /scratch/work/malonzm1/.conda_envs/R_for_scater/lib/python3.9/site-packages/rpy2/ipython/rmagic.py:923, in RMagics.R(self, line, cell, local_ns); 921 return_output = False; 922 else:; --> 923 text_result, result, visible = self.eval(code); 924 text_output += text_result; 925 if visible:. File /scratch/work/malonzm1/.conda_envs/R_for_scater/lib/python3.9/site-packages/rpy2/ipython/rmagic.py:389, in RMagics.eval(self, code); 386 except (ri.embedded.RRuntimeError, ValueError) as exception:; 387 # Otherwise next return seems to have copy of error.; 388 warning_or_other_msg = self.flush(); --> 389 raise RInterpreterError(code, str(exception),; 390 warning_or_other_msg); 391 text_output = self.flush(); 392 return text_output, value, visible[0]. RInterpreterError: Failed to parse and evaluate line '\n# specify row and column names of data\nrownames(data) = genes\ncolnames(data) = cells\n# ensure correct sparse format for table of counts and table of droplets\ndata <- as(data, ""sparseMatrix"")\ndata_tod <- as(data_tod, ""sparseMatrix"")\n\n# Generate SoupChannel Object for SoupX \nsc = SoupChannel(data_tod, data, calcSoupProfile = FALSE)\n\n# Add extra meta data to the SoupChannel object\nsoupProf = data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data))\nsc = setSoupProfile(sc, soupProf)\n# Set cluster information in SoupChannel\nsc = setClusters(sc, soupx_groups)\n\n# Estimate contamination fraction\nsc = autoEstCont(sc, doPlot=FALSE)\n# Infer corrected table of counts and rount to integer\nout = adjustCounts(sc, roundToInt = TRUE)\n'.; R error message: 'Error in data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), : \n duplicate row.names: TBCE, LINC01238, CYB561D2, MATR3, LINC01505, HSPA14, GOLGA8M, GGT1, ARMCX5-GPRASP2, TMSB15B'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763978277:5800,message,message,5800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763978277,1,['message'],['message']
Integrability,"rol metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1414,Depend,Depending,1414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['Depend'],['Depending']
Integrability,rotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.35.0; loompy 3.0.7; louvain 0.7.0; Markdown 3.3.7; MarkupSafe 2.1.1; matplotlib 3.4.1; matplotlib-inline 0.1.2; mistune 0.8.4; msgpack 1.0.4; multidict 6.0.2; multipledispatch 0.6.0; multiprocess 0.70.11.1; natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; pyth,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:3424,wrap,wrap,3424,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['wrap'],['wrap']
Integrability,"round. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and reorganize.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1820,depend,depend,1820,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['depend'],['depend']
Integrability,"rror: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.5+mkl which is incompatible.; Successfully installed numpy-1.21.5+mkl; ```; Step7: check anaconda Python38 environment, numpy-1.21.5+mkl is successfully installed; ![image](https://user-images.githubusercontent.com/75048821/147306587-eb94c188-5c18-40f0-add0-3a899872d786.png). Step8: Scanpy import error. Numpy>v1.20 is conflicted with Scanpy; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600); ; import scvelo as scv; scv.settings.verbosity = 3; scv.settings.presenter_view = True; scv.logging.print_versions(). import cellrank as cr; cr.settings.verbosity = 3; cr.logging.print_versions(). import matplotlib.pyplot as pl; from matplotlib import rcParams; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykerne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:4861,depend,dependency,4861,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['depend'],['dependency']
Integrability,"ry familiar with Scanpy (which seems like a fantastic library!), so please bear with me if some of the things I mention are not relevant to Scanpy. PyMDE (documentation here: https://pymde.org/) has a few benefits:; - as @adamgayoso mentioned, PyMDE supports computing embeddings on GPU. This makes it possible to compute very large embeddings quickly (often 4-10x faster than CPU).; - PyMDE is a very general embedding library. It is based on a general framework for embedding, and this framework includes many well-known methods --- such as UMAP, PCA, Laplacian embedding, multi-dimensional scaling, and more --- as special cases. This makes it easy to compare different methods using a single framework.; - PyMDE also supports creating entirely new types of embeddings, as custom instances of our framework.; - PyMDE provides ways to reason about how much an embedding distorts the original neighborhood graph. There are some comparisons to UMAP & openTSNE in the third part of our manuscript, which has been published in Foundations & Trends in Machine Learning and is available here: https://web.stanford.edu/~boyd/papers/pdf/min_dist_emb.pdf; - on CPU, UMAP and PyMDE are comparable in speed, with UMAP often having a slight edge; on GPU PyMDE can be much faster; - unlike UMAP/openTSNE, PyMDE allows users to fit constrained embeddings. Right now the supported constraints are standardization (zero mean, unit covariance; this forces embeddings to spread out, but not too much, and as a result standardized embeddings are typically similarly scaled), centering, and anchoring (pre-specifying the coordinates of a subset of the items); - PyMDE allows for more types of embeddings, in addition to UMAP-style embeddings. On the other hand, PyMDE is young software. If you do depend on it, I would recommend including it as an optional dependency, not a required one. Happy to chat more, to answer any questions, and to help with integration, if that is something you are ultimately interested in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627:1903,depend,depend,1903,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051103627,3,"['depend', 'integrat']","['depend', 'dependency', 'integration']"
Integrability,"s 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/_utils.py in <module>; 16 from numpy import random; 17 from scipy import sparse; ---> 18 from anndata import AnnData, __version__ as anndata_version; 19 from textwrap import dedent; 20 from packaging import version. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/anndata/__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnDat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18608,interface,interface,18608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['interface'],['interface']
Integrability,"s of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = np.arcsinh(df / cofactor); return T_cytof. def geometric_transform(df):; '''; implements the scanpy transform originating from ivirshup:multimodal; '''; from scipy.stats.mstats import gmean; T_geometric = np.divide(df, gmean(df + 1, axis=0)); return T_geometric. #optionally, for each of these, similar to some cytof workflows, ; #anchor 1-99% quantiles to 0-1, to rescale distribution within a standardized range; #use quantiles as a simple heuristic, due to extreme signal outliers that throw off the scale; #can also floor/ceil, depending on whether values beyond 0-1 are compatible or meaningful; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:2533,depend,depending,2533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['depend'],['depending']
Integrability,"scTransform is easily usable if you use rpy2 and anndata2ri. I use directly; the vst R function at this address to make it work; https://github.com/ChristophH/sctransform/blob/master/R/vst.R. Den sÃ¸n. 23. feb. 2020 kl. 00.44 skrev MalteDLuecken <; notifications@github.com>:. > Hi, It's not available in scanpy at the moment, but I wrote a wrapper for; > it via rpy2 and anndata2ri which is available here:; >; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1068?email_source=notifications&email_token=ACC66UMYH2ZHSMFFQS35FRLREG2ENA5CNFSM4KZJFJP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEMVNJCY#issuecomment-590009483>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC66UJ2GVSPUTR4WLWM2V3REG2ENANCNFSM4KZJFJPQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395:340,wrap,wrapper,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-590049395,1,['wrap'],['wrapper']
Integrability,"scvelo/preprocessing/neighbors.py in neighbors(adata, n_neighbors, n_pcs, use_rep, use_highly_variable, knn, random_state, method, metric, metric_kwds, num_threads, copy); 161 warnings.simplefilter(""ignore""); 162 neighbors = Neighbors(adata); --> 163 neighbors.compute_neighbors(; 164 n_neighbors=n_neighbors,; 165 knn=knn,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds); 748 # we need self._distances also for method == 'gauss' if we didn't; 749 # use dense distances; --> 750 self._distances, self._connectivities = _compute_connectivities_umap(; 751 knn_indices,; 752 knn_distances,. ~/.conda/envs/rpy/lib/python3.9/site-packages/scanpy/neighbors/__init__.py in _compute_connectivities_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:2746,message,message,2746,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['message'],['message']
Integrability,"sed that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem?; > [â€¦](#); > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> .; > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142:1042,message,message,1042,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-491103142,1,['message'],['message']
Integrability,seems like the `--deps` flag can be used to select dependencies. Default is all dependencies. ```bash; $ beni --deps production pyproject.toml; channels:; - conda-forge; dependencies:; - pip:; - flit; - python>=3.7; - pip; - anndata>=0.7.4; - numpy>=1.17.0; - matplotlib-base>=3.1.2; - pandas>=0.21; - scipy>=1.4; - seaborn-split; - h5py>=2.10.0; - pytables; - tqdm; - scikit-learn>=0.22; - statsmodels>=0.10.0rc2; - patsy; - networkx>=2.3; - natsort; - joblib; - numba>=0.41.0; - umap-learn>=0.3.10; - packaging; - sinfo; name: scanpy; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811:51,depend,dependencies,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2144#issuecomment-1055433811,3,['depend'],['dependencies']
Integrability,"similar error when I was trying to use .h5 file from cellbender output. I have multiome data. . ```pytb; `>>> adata = scanpy.read_10x_h5(""/sc/arion/projects/hmDNAmap/snHeroin/analysis/ARC_TD005235-354/outs/cellbender/cb_feature_bc_matrix_filtered.h5"", gex_only=False)`; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 183, in read_10x_h5; adata = _read_v3_10x_h5(filename, start=start); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 268, in _read_v3_10x_h5; _collect_datasets(dsets, f[""matrix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:971,wrap,wrapper,971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['wrap'],['wrapper']
Integrability,"so you would want three installations, right?. - full (by manually installing all optional dependencies); - uncomplicated but limited; - super barebones. since `scanpy` is already the second version, we donâ€™t lose anything this way. in the future maybe we can achieve that `scanpy` becomes the full installation (once the C++ dependencies start shipping wheels)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416:91,depend,dependencies,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355115416,2,['depend'],['dependencies']
Integrability,"sure!. as long as the last version of `scanpy-full` does nothing but depend on `scanpy`, nobody will suffer any consequences if it becomes obsolete one day.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/59#issuecomment-355146641:69,depend,depend,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/59#issuecomment-355146641,1,['depend'],['depend']
Integrability,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:5526,message,message,5526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,1,['message'],['message']
Integrability,"t fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/umap_.py in <module>; 30 import umap.distances as dist; 31 ; ---> 32 import umap.sparse as sparse; 33 ; 34 from umap.utils import (. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/sparse.py in <module>; 10 import numpy as np; 11 ; ---> 12 from umap.utils import norm; 13 ; 14 locale.setlocale(locale.LC_NUMERIC, ""C""). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/umap/utils.py in <module>; 38 ; 39 @numba.njit(""i4(i8[:])""); ---> 40 def tau_rand_int(state):; 41 """"""A fast (pseudo)-random number generator.; 42 . ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:2392,wrap,wrapper,2392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['wrap'],['wrapper']
Integrability,"t_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1347,wrap,wrapper,1347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [â€¦] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1753,interface,interface,1753,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['interface'],['interface']
Integrability,"tblib 1.7.0 py_0 ; terminado 0.9.4 py38h06a4308_0 ; testpath 0.4.4 pyhd3eb1b0_0 ; textdistance 4.2.1 pyhd3eb1b0_0 ; threadpoolctl 2.1.0 pyh5ca1d4c_0 ; three-merge 0.1.1 pyhd3eb1b0_0 ; tk 8.6.10 hbc83047_0 ; tktable 2.10 h14c3975_0 ; tokenize-rt 4.1.0 pyhd8ed1ab_0 conda-forge; toml 0.10.2 pyhd3eb1b0_0 ; toolz 0.11.1 pyhd3eb1b0_0 ; tornado 6.1 py38h27cfd23_0 ; tqdm 4.59.0 pyhd3eb1b0_1 ; traitlets 5.0.5 pyhd3eb1b0_0 ; triku 1.3.1 pypi_0 pypi; tslearn 0.5.0.5 pypi_0 pypi; typed-ast 1.4.2 py38h27cfd23_1 ; typing 3.10.0.0 py38h06a4308_0 ; typing-extensions 3.10.0.0 pypi_0 pypi; typing_extensions 3.7.4.3 pyha847dfd_0 ; tzlocal 2.1 py38_0 ; ujson 4.0.2 py38h2531618_0 ; umap-learn 0.5.1 py38h578d9bd_0 conda-forge; unicodecsv 0.14.1 py38_0 ; unixodbc 2.3.9 h7b6447c_0 ; urllib3 1.26.4 pyhd3eb1b0_0 ; vendorize 0.2.1 pypi_0 pypi; watchdog 1.0.2 py38h06a4308_1 ; wcwidth 0.2.5 py_0 ; webencodings 0.5.1 py38_1 ; werkzeug 1.0.1 pyhd3eb1b0_0 ; wheel 0.36.2 pyhd3eb1b0_0 ; widgetsnbextension 3.5.1 py38_0 ; wrapt 1.12.1 py38h7b6447c_1 ; wurlitzer 2.1.0 py38h06a4308_0 ; xlrd 1.2.0 pypi_0 pypi; xlsxwriter 1.3.8 pyhd3eb1b0_0 ; xlwt 1.3.0 py38_0 ; xz 5.2.5 h7b6447c_0 ; yaml 0.2.5 h7b6447c_0 ; yapf 0.31.0 pyhd3eb1b0_0 ; yarl 1.6.3 pypi_0 pypi; zeromq 4.3.4 h2531618_0 ; zict 2.0.0 pyhd3eb1b0_0 ; zipp 3.4.1 pyhd3eb1b0_0 ; zlib 1.2.11 h7b6447c_3 ; zope 1.0 py38_1 ; zope.event 4.5.0 py38_0 ; zope.interface 5.3.0 py38h27cfd23_0 ; zstd 1.4.5 h9ceee32_0 ; ```. </Details>. <Details>; <Summary>ImportError traceback</Summary>. ```python; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-2-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 3 from ._metadata import __version__, __author__, __email__; 4 ; ----> 5 from ._utils import check_versions; 6 ; 7 check_versions(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:18220,wrap,wrapt,18220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['wrap'],['wrapt']
Integrability,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1980,integrat,integration,1980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['integrat'],['integration']
Integrability,"ties_umap(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 353 # umap 0.5.0; 354 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 355 from umap.umap_ import fuzzy_simplicial_set; 356 ; 357 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/__init__.py in <module>; ----> 1 from .umap_ import UMAP; 2 ; 3 # Workaround: https://github.com/numba/numba/issues/3341; 4 import numba; 5 . ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/umap_.py in <module>; 52 from umap.spectral import spectral_layout; 53 from umap.utils import deheap_sort, submatrix; ---> 54 from umap.layouts import (; 55 optimize_layout_euclidean,; 56 optimize_layout_generic,. ~/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py in <module>; 37 },; 38 ); ---> 39 def rdist(x, y):; 40 """"""Reduced Euclidean distance.; 41 . ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/decorators.py in wrapper(func); 219 with typeinfer.register_dispatcher(disp):; 220 for sig in sigs:; --> 221 disp.compile(sig); 222 disp.disable_compile(); 223 return disp. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, sig); 907 with ev.trigger_event(""numba:compile"", data=ev_details):; 908 try:; --> 909 cres = self._compiler.compile(args, return_type); 910 except errors.ForceLiteralArg as e:; 911 def folded(args, kws):. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in compile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:3583,wrap,wrapper,3583,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['wrap'],['wrapper']
Integrability,"too; long as is was supposed to reduce the plotting time. I would not wait for; more than 5 minutes to see a plot. How many genes were you planning to plot?. The background is that when plotting a heatmap, the matplotlib; visualization will randomly drop genes because the resolution of the; screens is not high enough. Thus, when the number of genes is large, I was; trying to find a compromise by fitting a line before the plotting and then; only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you; find an example that can reproduce the problem?. On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>; wrote:. > I was trying to plot a heatmap using this command:; > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',; > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,; > dendrogram=True, save='ClusterMap.png'); >; > And it didn't finish running after an overnight, with the following; > warning message:; > WARNING: Gene labels are not shown when more than 50 genes are visualized.; > To show gene labels set show_gene_labels=True; > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:; > UserWarning:; > The maximal number of iterations maxit (set to 20 by the program); > allowed for finding a smoothing spline with fp=s has been reached: s; > too small.; > There is an approximation returned but the corresponding weighted sum; > of squared residuals does not satisfy the condition abs(fp-s)/s < tol.; > warnings.warn(message); >; > I don't understand why this is taking this long because seaborn was able; > to finish plotting within 30 minutes. Do you know why?; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/633>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>; > .;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870:1072,message,message,1072,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633#issuecomment-490792870,1,['message'],['message']
Integrability,"umap expects a list as group, so it will work if you do:. ```python; sc.pl.umap(adata, color='blobs', groups=['Zero']); ````. the improvement that I would consider is to automatically convert a string into a list to avoid the error message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960:232,message,message,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/231#issuecomment-414236960,1,['message'],['message']
Integrability,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:996,Integrat,Integration,996,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,2,"['Integrat', 'integrat']","['Integration', 'integrate']"
Integrability,"venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2748,wrap,wrapper,2748,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['wrap'],['wrapper']
Integrability,"workx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.0.1; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.7; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.0; pylab NA; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; requests 2.23.0; requests_cache 0.5.2; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; socks 1.7.1; soupsieve 2.0.1; statsmodels 0.12.0; storemagic NA; tables 3.6.1; terminado 0.8.3; tornado 6.0.4; traitlets 5.0.4; urllib3 1.25.10; wcwidth 0.2.5; wrapt 1.12.1; xlsxwriter 1.3.3; zmq 19.0.2; -----; IPython 7.18.1; jupyter_client 6.1.7; jupyter_core 4.6.3; jupyterlab 2.2.8; notebook 6.1.4; -----; Python 3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) [GCC 7.5.0]; Linux-4.4.0-142-generic-x86_64-with-glibc2.10; 64 logical CPU cores, x86_64; -----; Session information updated at 2020-09-16 11:03; ```. Here is the error message:. ```; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-37-b22ada65a1cd> in <module>; 1 # Create Concatenated anndata object for all timepoints; 2 #alldays = e125.concatenate(e135, e145, e155, uns_merge=""unique""); ----> 3 alldays = e125.concatenate(e135). ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1696 all_adatas = (self,) + tuple(adatas); 1697 ; -> 1698 out = concat(; 1699 all_adatas,; 1700 axis=0,. ~/miniconda3/envs/env4sc_velo_scannpy/lib/python3.8/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise); 799 [dim_indices(a, axis=1 - axis) for a in adatas], join=join; 800 ); --> 801 reindexers = [; 802 gen_reindexer(alt_indices, dim_indices(a, axis=1 -",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875:1857,message,message,1857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-693478875,1,['message'],['message']
Integrability,"y:363, in partial_reduce(func, x, split_every, keepdims, dtype, name, reduced_meta); 362 try:; --> 363 meta = func(reduced_meta, computing_meta=True); 364 # no meta keyword argument exists for func, and it isn't required. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/dask/array/reductions.py:685, in mean_combine(pairs, sum, numel, dtype, axis, computing_meta, **kwargs); 684 ns = deepmap(lambda pair: pair[""n""], pairs) if not computing_meta else pairs; --> 685 n = _concatenate2(ns, axes=axis).sum(axis=axis, **kwargs); 687 if computing_meta:. TypeError: _cs_matrix.sum() got an unexpected keyword argument 'keepdims'. During handling of the above exception, another exception occurred:. IndexError Traceback (most recent call last); Cell In[19], line 1; ----> 1 result = sc.pp.highly_variable_genes(adata, inplace=False); 2 result. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:702, in highly_variable_genes(***failed resolving arguments***); 699 del min_disp, max_disp, min_mean, max_mean, n_top_genes; 701 if batch_key is None:; --> 702 df = _highly_variable_genes_single_batch(; 703 adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 704 ); 705 else:; 706 df = _highly_variable_genes_batched(; 707 adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor; 708 ). File /mnt/workspace/repos/scanpy/scanpy/preprocessing/_highly_variable_genes.py:274, in _highly_variable_genes_single_batch(adata, layer, cutoff, n_bins, flavor); 271 else:; 272 X = np.expm1(X); --> 274 mean",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725:1415,wrap,wrapper,1415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1906001725,1,['wrap'],['wrapper']
Integrability,"yes!. 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory!; 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1); 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`; 4. Copy your changes over again.; 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk.; 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`); 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files.; 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630#issuecomment-492124539:777,message,message,777,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630#issuecomment-492124539,1,['message'],['message']
Integrability,"yes, I'll get to it next week. It didn't seem there was a straightforward way to integrate with the existing implementation given the filtering criterion is different, but I'll try my best.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573:81,integrat,integrate,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615957573,1,['integrat'],['integrate']
Integrability,"you have a release candidate of that one installed, so what happened is the only correct behavior: It uninstalled an incompatible version to install a compatible one. If your installâ€™s metadata was outdated and it was in fact a compatible one, then you forgot to refresh the metadata by reinstalling it. Thatâ€™s annoying but necessary as editable installs are nonstandard and therefore not well integrated into how package metadata works. > Why not just use `pip install -e` here?. Because development installs in general are nonstandard, and `pip install -e` in particular uses the deprecated `setup.py`. Tasks; -----. > - Exclude setup.py from sdist using the standard way, not via .gitignore. sounds good!. > I'm a bit concerned that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes.; > ...; > - flit mangles the build version part of wheel filenames, in a way that pip just started checking for. . No, as far as I can see, pip arbitrarily decided to not allow local version specifiers in wheel filenames. AFAIK nothing says there canâ€™t be pluses in there, only that you canâ€™t upload packages with local specifiers in their version to PyPI. Which we donâ€™t do here, so pip should chill. If flit decides to work around that quirk, or pip relaxes, we can unpin pip. > - flit symlinked packages seem to be overwritten if a new package is installed which has the symlinked package as a dependency. Seee above. Has nothing to do with flit. What made you thing that anyway?. > - There is a fairly large workaround to make the package version available if the dependencies are not installed. Is it possible to use something more standard like versioneer here?. No. Either we hardcode a string constant in the `__init__.py` or we leave it like it is until flit allows an alternative. Thatâ€™s the only disadvantage flit has IMHO, but we discussed that at length in the past and found it to not be a problem as the hack is robust and well documented.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443:1639,depend,dependency,1639,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-781992443,2,['depend'],"['dependencies', 'dependency']"
Integrability,"â€™t intended to be packages, so you shouldnâ€™t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if thereâ€™s a `__init__.py` somewhere in your `tests` directory when using pytest, youâ€™re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what theyâ€™re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think itâ€™s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as thereâ€™s only one `tests` directory with a structure that doesnâ€™t neatly map to your module hierarchy, having it outside is cleaner because people canâ€™t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects donâ€™t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every userâ€™s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `tests` for actual tests. We can import the reusable fixtures in `conftest.py`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:1967,depend,depending,1967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['depend'],['depending']
Modifiability," ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs Ã— n_vars = 14775 Ã— 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata.n_vars, n_comps)); > ; > ValueError: no field of name X_pca. ```; print(adata2); print(adata); ```. > View of AnnData object with n_obs Ã— n_vars = 14775 Ã— 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs Ã— n_vars = 14775 Ã— 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I do the pca on the original AnnData object for the highly variable genes, it works:; ```; sc.tl.pca(adata, use_highly_variable=True, svd_solver='arpack'); print(adata); ```. > AnnData object with n_obs Ã— n_vars = 14775 Ã— 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; > uns: 'pca'; > obsm: 'X_pca'; > varm: 'PCs'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:2555,variab,variable,2555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['variab'],['variable']
Modifiability," Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory â€“ just not on your machine. Does this address your conce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:1481,config,configuring,1481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['config'],['configuring']
Modifiability," Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **knn** : bool, optional (default: True). If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor. **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metricâ€™s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:6385,adapt,adaptive,6385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaptive']
Modifiability," Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneType]=None, knn:bool=True, random_state:Union[int, mtrand.RandomState, NoneType]=0, method:str='umap', metric:Union[str, Callable[[numpy.ndarray, numpy.ndarray], float]]='euclidean', metric_kwds:Mapping[str, Any]={}, copy:bool=False) -> Union[anndata.base.AnnData, NoneType]; Docstring:; Compute a neighborhood graph of observations [McInnes18]_. The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,; which also provides a method for estimating connectivities of data points -; the connectivity of the manifold (`method=='umap'`). If `method=='diffmap'`,; connectivities are computed according to [Coifman05]_, in the adaption of; [Haghverdi16]_. :Parameters:. **adata** : AnnData, optional (default: <class 'inspect._empty'>). Annotated data matrix. **n_neighbors** : int, optional (default: 15). The size of local neighborhood (in terms of number of neighboring data; points) used for manifold approximation. Larger values result in more; global views of the manifold, while smaller values result in more local; data being preserved. In general values should be in the range 2 to 100.; If `knn` is `True`, number of nearest neighbors to be searched. If `knn`; is `False`, a Gaussian kernel width is set to the distance of the; `n_neighbors` neighbor. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwis",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:4818,adapt,adaption,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['adapt'],['adaption']
Modifiability," _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; return dispatch(args[0].__class__)(*args, **kw); scanpy/preprocessing/_simple.py:888: in scale_anndata; X, adata.var[""mean""], adata.var[""std""] = do_scale(; ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; error_rewrite(e, 'typing'); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); issue_type = 'typing'. def error_rewrite(e, issue_type):; """"""; Rewrite and raise Exception `e` with help supplied based on the; specified issue_type.; """"""; if config.SHOW_HELP:; help_msg = errors.error_extras[issue_type]; e.patch_message('\n'.join((str(e).rstrip(), help_msg))); if config.FULL_TRACEBACKS:; raise e; else:; > raise e.with_traceback(None); E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); E non-precise type pyobject; E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); E ; E File ""scanpy/preprocessing/_simple.py"", line 763:; E def do_scale(X, maxv, nthr):; E <source elided>; E # t0= time.time(); E s = np.zeros((nthr, X.shape[1])); E ^ ; E ; E This error may have been caused by the following argument(s):; E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>. ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; ```. When trying to use the new flavor with the existing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183:1713,Rewrite,Rewrite,1713,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1533308183,3,"['Rewrite', 'config']","['Rewrite', 'config']"
Modifiability," _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; > ```; > ; > When trying to use the new flavor with the existing test. Hi @Zethson ,; We are not able t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1893,config,config,1893,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['config'],['config']
Modifiability," ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3878,variab,variable,3878,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variable']
Modifiability, conda-forge; numpy 1.24.4 py310ha4c1d20_0 conda-forge; numpyro 0.12.1 pyhd8ed1ab_0 conda-forge; openh264 2.1.1 h780b84a_0 conda-forge; openjpeg 2.5.0 hfec8fc6_2 conda-forge; openpyxl 3.1.2 py310h2372a71_0 conda-forge; openssl 3.1.1 hd590300_1 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; optax 0.1.5 pyhd8ed1ab_0 conda-forge; ordered-set 4.1.0 pyhd8ed1ab_0 conda-forge; orjson 3.9.2 py310h1e2579a_0 conda-forge; packaging 23.1 pyhd8ed1ab_0 conda-forge; pandas 2.0.3 py310h7cbd5c2_1 conda-forge; parso 0.8.3 pyhd8ed1ab_0 conda-forge; patsy 0.5.3 pyhd8ed1ab_0 conda-forge; pcre2 10.40 hc3806b6_0 conda-forge; pexpect 4.8.0 pyh1a96a4e_2 conda-forge; pickleshare 0.7.5 py_1003 conda-forge; pillow 9.4.0 py310h023d228_1 conda-forge; pip 23.2 pyhd8ed1ab_0 conda-forge; pkginfo 1.9.6 pyhd8ed1ab_0 conda-forge; pkgutil-resolve-name 1.3.10 pyhd8ed1ab_0 conda-forge; platformdirs 3.9.1 pyhd8ed1ab_0 conda-forge; poetry 1.5.1 linux_pyhd8ed1ab_0 conda-forge; poetry-core 1.6.1 pyhd8ed1ab_0 conda-forge; poetry-plugin-export 1.4.0 pyhd8ed1ab_0 conda-forge; pooch 1.7.0 pyha770c72_3 conda-forge; prompt-toolkit 3.0.39 pyha770c72_0 conda-forge; prompt_toolkit 3.0.39 hd8ed1ab_0 conda-forge; psutil 5.9.5 py310h1fa729e_0 conda-forge; pthread-stubs 0.4 h36c2ea0_1001 conda-forge; ptyprocess 0.7.0 pyhd3deb0d_0 conda-forge; pure_eval 0.2.2 pyhd8ed1ab_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pydantic 2.0.3 pyhd8ed1ab_1 conda-forge; pydantic-core 2.3.0 py310hcb5633a_0 conda-forge; pygments 2.15.1 pyhd8ed1ab_0 conda-forge; pyjwt 2.8.0 pyhd8ed1ab_0 conda-forge; pynndescent 0.5.10 pyh1a96a4e_0 conda-forge; pyopenssl 23.2.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.1.0 pyhd8ed1ab_0 conda-forge; pyproject_hooks 1.0.0 pyhd8ed1ab_0 conda-forge; pyro-api 0.1.2 pyhd8ed1ab_0 conda-forge; pyro-ppl 1.8.4 pyhd8ed1ab_0 conda-forge; pysocks 1.7.1 pyha2e5f31_6 conda-forge; python 3.10.12 hd12c33a_0_cpython conda-forge; python-build 0.10.0 pyhd8ed1ab_1 conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 co,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:17396,plugin,plugin-export,17396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['plugin'],['plugin-export']
Modifiability," datasets, but I also used the data from here, https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb, and got the same error. ```; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_list; ```. ```; [View of AnnData object with n_obs Ã— n_vars = 2267 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs Ã— n_vars = 1976 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs Ã— n_vars = 1663 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs Ã— n_vars = 2356 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs Ã— n_vars = 2422 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts',; View of AnnData object with n_obs Ã— n_vars = 1773 Ã— 12818; obs: 'sample', 'region', 'donor', 'n_counts', 'log_counts', 'n_genes', 'mt_frac', 'size_factors'; var: 'gene_id', 'n_cells'; uns: 'log1p'; layers: 'counts']; ```. ```; import mnnpy; corrected = mnnpy.mnn_correct(*adata_list, batch_key=""sample""); ```. ```; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-35-7ad830fcd907> in <module>; 1 import mnnpy; ----> 2 corrected = mnnpy.mnn_correct(*adata_list, batch_key=""samp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537:575,layers,layers,575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367#issuecomment-674176537,6,['layers'],['layers']
Modifiability, filelock 3.0.12 pyhd3eb1b0_1 ; flake8 3.9.0 pyhd3eb1b0_0 ; flask 1.1.2 pyhd3eb1b0_0 ; fontconfig 2.13.1 h6c09931_0 ; freetype 2.10.4 h5ab3b9f_0 ; fribidi 1.0.10 h7b6447c_0 ; fsspec 0.9.0 pyhd3eb1b0_0 ; funcargparse 0.2.3 pypi_0 pypi; future 0.18.2 py38_1 ; future_fstrings 1.2.0 py38h32f6830_2 conda-forge; gcc_impl_linux-64 7.3.0 habb00fd_1 ; gcc_linux-64 7.3.0 h553295d_15 ; geosketch 1.2 pypi_0 pypi; get_terminal_size 1.0.0 haa9412d_0 ; get_version 2.1 py_1 conda-forge; gevent 21.1.2 py38h27cfd23_1 ; gfortran_impl_linux-64 7.3.0 hdf63c60_1 ; gfortran_linux-64 7.3.0 h553295d_15 ; glib 2.63.1 h5a9c865_0 ; glob2 0.7 pyhd3eb1b0_0 ; gmp 6.2.1 h2531618_2 ; gmpy2 2.0.8 py38hd5f6e3b_3 ; google-api-core 1.27.0 pypi_0 pypi; google-auth 1.30.0 pypi_0 pypi; googleapis-common-protos 1.53.0 pypi_0 pypi; gpustat 0.6.0 pypi_0 pypi; graphite2 1.3.14 h23475e2_0 ; graphtools 1.5.2 pypi_0 pypi; graphviz 2.40.1 h21bd128_2 ; greenlet 1.1.0 py38h2531618_0 ; grpcio 1.37.1 pypi_0 pypi; gsl 2.4 h14c3975_4 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb453b48_1 ; gxx_impl_linux-64 7.3.0 hdf63c60_1 ; gxx_linux-64 7.3.0 h553295d_15 ; h5py 3.2.1 nompi_py38h9915d05_100 conda-forge; harfbuzz 1.8.8 hffaf4a1_0 ; harmonypy 0.0.5 pypi_0 pypi; harmonyts 0.1.4 pypi_0 pypi; hdf5 1.10.6 nompi_h3c11f04_101 conda-forge; heapdict 1.0.1 py_0 ; hiredis 2.0.0 pypi_0 pypi; html5lib 1.1 py_0 ; icu 58.2 he6710b0_3 ; idna 2.10 pyhd3eb1b0_0 ; igraph 0.7.1 h2166141_1005 conda-forge; imageio 2.9.0 pyhd3eb1b0_0 ; imagesize 1.2.0 pyhd3eb1b0_0 ; importlib-metadata 3.10.0 py38h06a4308_0 ; importlib_metadata 3.10.0 hd3eb1b0_0 ; iniconfig 1.1.1 pyhd3eb1b0_0 ; intel-openmp 2021.2.0 h06a4308_610 ; intervaltree 2.1.0 pypi_0 pypi; ipykernel 5.3.4 py38h5ca1d4c_0 ; ipython 7.22.0 py38hb070fc8_0 ; ipython_genutils 0.2.0 pyhd3eb1b0_1 ; ipywidgets 7.6.3 pyhd3eb1b0_1 ; isort 5.8.0 pyhd3eb1b0_0 ; itsdangerous 2.0.1 pyhd3eb1b0_0 ; jbig 2.1 hdba287a_0 ; jdcal 1.4.1 py_0 ; jedi 0.17.2 py38h06a4308_1 ; jeepney 0.6.0 pyhd3eb1b0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:7925,plugin,plugins-base,7925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['plugin'],['plugins-base']
Modifiability," natsort 7.1.1; nbclient 0.6.4; nbconvert 6.5.0; nbformat 5.4.0; nest-asyncio 1.5.5; networkx 2.5; notebook 6.4.11; numba 0.52.0; numexpr 2.7.3; numpy 1.19.5; numpy-groupies 0.9.17; numpyro 0.9.2; oauthlib 3.2.0; openpyxl 3.0.10; opt-einsum 3.3.0; optax 0.1.2; packaging 20.9; pandas 1.2.0; pandocfilters 1.5.0; parso 0.8.2; pathos 0.2.7; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; Pillow 9.1.1; pip 21.1.1; pox 0.2.9; ppft 1.6.6.3; prometheus-client 0.14.1; prompt-toolkit 3.0.18; protobuf 3.19.0; protobuf3-to-dict 0.1.5; ptyprocess 0.7.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycosat 0.6.3; pycparser 2.20; pyDeprecate 0.3.1; Pygments 2.9.0; pyOpenSSL 20.0.1; pyparsing 2.4.7; pyro-api 0.1.2; pyro-ppl 1.8.1; pyrsistent 0.18.1; PySocks 1.7.1; python-dateutil 2.8.1; python-igraph 0.9.1; pytorch-lightning 1.5.10; pytz 2021.1; PyWavelets 1.3.0; PyYAML 6.0; pyzmq 22.0.3; requests 2.25.1; requests-oauthlib 1.3.1; rich 12.4.4; rpy2 3.4.2; rsa 4.8; ruamel-yaml-conda 0.15.80; ruamel.yaml 0.17.21; ruamel.yaml.clib 0.2.6; s3transfer 0.4.2; sagemaker 2.39.0.post0; scanpy 1.6.1; scikit-image 0.19.2; scikit-learn 0.24.2; scikit-misc 0.1.4; scipy 1.6.0; scrublet 0.2.3; scvi-tools 0.16.2; seaborn 0.11.1; Send2Trash 1.8.0; setuptools 59.5.0; setuptools-scm 6.0.1; sinfo 0.3.1; six 1.15.0; smdebug-rulesconfig 1.0.1; soupsieve 2.3.2.post1; spectra 0.0.11; statsmodels 0.12.2; stdlib-list 0.8.0; tables 3.6.1; tensorboard 2.9.0; tensorboard-data-server 0.6.1; tensorboard-plugin-wit 1.8.1; terminado 0.15.0; texttable 1.6.3; threadpoolctl 2.1.0; tifffile 2021.11.2; tinycss2 1.1.1; toolz 0.11.2; torch 1.11.0; torchmetrics 0.9.0; tornado 6.1; tqdm 4.60.0; traitlets 5.2.2.post1; typing-extensions 4.2.0; tzlocal 2.1; umap-learn 0.4.6; urllib3 1.26.4; wcwidth 0.2.5; webencodings 0.5.1; Werkzeug 2.1.2; wheel 0.36.2; widgetsnbextension 3.6.0; yarl 1.7.2; zipp 3.4.1; Note: you may need to restart the kernel to use updated packages."". </details>. Has anyone found any solution to work around this issue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:5138,plugin,plugin-wit,5138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['plugin'],['plugin-wit']
Modifiability," object with n_obs Ã— n_vars = 29322 Ã— 19860. ```python; >>> tiss[tiss.obs['cell_ontology_class']=='B cell']; ```. ```pytb; IndexError Traceback (most recent call last); <ipython-input-269-28b4524131cb> in <module>(); ----> 1 tiss[tiss.obs['cell_ontology_class']=='B cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 7 is out of bounds for axis 1 with size 7; ```. even though it's part of the set:; ```py; >>> set(tiss.obs['cell_ontology_class']); {'B cell',; 'NA',; '",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/226#issuecomment-438879520:1024,layers,layers,1024,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/226#issuecomment-438879520,1,['layers'],['layers']
Modifiability," of either cargo culting it or becaue they know that makes setuptoolsâ€™ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what itâ€™s for. Iâ€™d probably judge that we donâ€™t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that weâ€™re dealing with python packages/modules here, but thatâ€™s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, â€¦ apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1362,config,configured,1362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['config'],['configured']
Modifiability," plot_boxplot_cell_fraction(adata, gene, label, title, ax, show); 1 def plot_boxplot_cell_fraction(adata, gene, label, title, ax, show=True):; ----> 2 gene_vals = np.asarray(adata[:, gene].X).flatten(); 3 ; 4 labels = ['3m','24m']; 5 # labels = list(set(adata.obs[label])). ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 691 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 692 # hackish solution here, no copy should be necessary; --> 693 uns_new = deepcopy(self._adata_ref._uns); 694 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 695 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. ~/anaconda3/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. ~/anaconda3/lib/python3.6/copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy); 278 if state is not None:; 279 if deep:; --> 280 state = deepcopy(state, memo);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170:1651,layers,layers,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-442366170,1,['layers'],['layers']
Modifiability," return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; > --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); > 308; > 309 elif plot_type == 'tracksplot':; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); > 819 if isinstance(var_names, str):; > 820 var_names = [var_names]; > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); > 822; > 823 if 'color' in kwds:; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); > 1983 matrix = adata[:, var_names].layers[layer]; > 1984 elif use_raw:; > -> 1985 matrix = adata.raw[:, var_names].X; > 1986 else:; > 1987 matrix = adata[:, var_names].X; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); > 510; > 511 def __getitem__(self, index):; > --> 512 oidx, vidx = self._normalize_indices(index); > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; > 514 else: X = self._adata.file['raw.X'][oidx, vidx]; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); > 538 obs, var = super(Raw, self)._unpack_index(packed_index); > 539 obs = _normalize_index(obs, self._adata.obs_names); > --> 540 var = _normalize_index(var, self.var_names); > 541 return obs, var; > 542; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); > 270 raise KeyError(; > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; > --> 272 .format(index)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910:2259,layers,layers,2259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910,1,['layers'],['layers']
Modifiability," setuptoolsâ€™ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what itâ€™s for. Iâ€™d probably judge that we donâ€™t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that weâ€™re dealing with python packages/modules here, but thatâ€™s not true. The way pytest works is pretty simple:. 1. it collects all test modules (`test_*.py` files, no directories) and determines which `conftest.py` files, plugins, â€¦ apply to which test module; 2. it collects all tests in those modules and checks which fixtures they need; 3. it sets up and tears down fixtures according to the needs of each test and executes the tests. accepting that makes it easier to reason about how our test suite works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:1476,Config,Config,1476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,3,"['Config', 'config', 'plugin']","['Config', 'config', 'plugins']"
Modifiability," was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png). ```python; sc.pl.heatmap(; pbmc,; var_names=[""LDHB"", ""LYZ"", ""CD79A""],; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png). What do you think about that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:1505,variab,variables,1505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315,1,['variab'],['variables']
Modifiability,""", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 242 in <lambda>; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 341 in from_call; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 241 in call_and_report; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 132 in runtestprotocol; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 113 in pytest_runtest_protocol; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 362 in pytest_runtestloop; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 337 in _main; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 283 in wrap_session; File ""<venv>/lib/python3.12/site-packages/_pytest/main.py"", line 330 in pytest_cmdline_main; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""<venv>/lib/python3.12/site-packages/_pytest/config/__init__.py"", line 201 in console_main; File ""<venv>/bin/pytest"", line 10 in <module>; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:7291,config,config,7291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,2,['config'],['config']
Modifiability,"""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; > # Imaginary implementation:; > sc.pl.heatmap(; > pbmc,; > var_names=""LDHB"",; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:1611,variab,variables,1611,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variables']
Modifiability,"## Pytest architecture. `tests` directories and `test_*.py` collections arenâ€™t intended to be packages, so you shouldnâ€™t import from there. Just think about a module-level `pytest.importorskip(...)` or so. Also if thereâ€™s a `__init__.py` somewhere in your `tests` directory when using pytest, youâ€™re doing something wrong. Yes, that means that all packages except for numba are doing it wrong, because numba also has importable test utils in there, and the others just misunderstand how pytest works. I blame setuptools, because `find_packages` finds directories that have `__init__.py`, so people just cargo-cultily started adding it without knowing what theyâ€™re doing. Fixture visibility is hierarchical, so a conftest.py in a subfolder is able to override fixtures from higher-up.; So e.g. for testing a package/app that uses celery, you just define your own `celery_config` fixture, then start using the `celery_app` fixture, which will use your config automatically. ## `tests` in the package. I think itâ€™s a good idea to have it in there if you are a huge project and like to physically split up tests into multiple directiories that are close to the source code they test. (like numpy does it). But as long as thereâ€™s only one `tests` directory with a structure that doesnâ€™t neatly map to your module hierarchy, having it outside is cleaner because people canâ€™t accidentally import from there. And as you can see from your links and our contributors importing stuff from `test_*` collections, a lot of projects donâ€™t know how to use pytest, so making misuse harder is beneficial. Also we have test data, which wastes space on every userâ€™s machine and bandwidth for people installing scanpy. ## `tests` and `testing`/`test_utils`. Given the above points, I think we should move things out to keep everything clean, and the package small. I also like the separation of concerns: `test_utils` or `testing` (like numpy does) for reusable stuff that other projects depending on you might use and `te",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290:950,config,config,950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1528#issuecomment-738776290,1,['config'],['config']
Modifiability,"### symlink installs being uninstalled **(most important)**. > No idea why it sees â€œ1.7.0rc2â€ and decides â€œIâ€™ll update this even when not asked to updateâ€. Maybe raise this issue with pip?. Yeah, this is super weird. I think it's also blocking for adopting `flit` as recommended way to install scanpy to a dev environment. I also raised this on the call yesterday, and I don't think anyone disagreed with this assessment. I see two paths forward here:. * You're able to solve this in this PR; * We merge mostly as is, but we add back `pip install -e` as a way to make a development environment, and add a `.. note` to the flit installation instructions warning people about this behaviour. I would also want a commitment from you to look into this issue. ### Pinning Pip on CI. > Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. I still have the concern that pinning pip to an old version could lead to problems, especially while pip is going through a lot of changes. But we can leave this for now. If getting this wheel issue solved drags on for multiple pip versions, we may need to reconsider. ### PEP stuff. > I see you already commented in `pypa/pip#9628`. I think that conversation is happening in multiple places, so might be hard to track. ### Installing from the repo. As it stands:. ```python; conda create -n scanpyenv python=3.8; https://github.com/theislab/scanpy.git; cd scanpy; pip install .; ```. Will error, unless the commit at the tip of master happens to be tagged with a release version. Right now I don't think this is an issue since I wouldn't expect anyone to install from github unless they were setting up a development environment. And if they are setting up a dev environment, they should be using `pip install -e` or `flit install -s`. . I'm not 100% confident this isn't an issue, and it would be good to get more opinions on this. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659:937,adapt,adapted,937,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783849659,1,['adapt'],['adapted']
Modifiability,"'; obsm: 'X_pca', 'X_umap', 'X_tsne'; varm: 'PCs'. tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-82-428532769794> in <module>(); ----> 1 tiss_facs[tiss_facs.obs['cell_ontology_class']=='keratinocyte stem cell']. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1299 def __getitem__(self, index):; 1300 """"""Returns a sliced view of the object.""""""; -> 1301 return self._getitem_view(index); 1302 ; 1303 def _getitem_view(self, index):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1303 def _getitem_view(self, index):; 1304 oidx, vidx = self._normalize_indices(index); -> 1305 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1306 ; 1307 def _remove_unused_categories(self, df_full, df_sub, uns):. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 713 raise KeyError('Unknown Index type'); 714 # fix categories; --> 715 self._remove_unused_categories(adata_ref.obs, obs_sub, uns_new); 716 self._remove_unused_categories(adata_ref.var, var_sub, uns_new); 717 # set attributes. ~/anaconda3/lib/python3.6/site-packages/anndata/base.py in _remove_unused_categories(self, df_full, df_sub, uns); 1318 uns[k + '_colors'] = np.array(uns[k + '_colors'])[; 1319 np.where(np.in1d(; -> 1320 all_categories, df_sub[k].cat.categories))[0]]; 1321 ; 1322 def rename_categories(self, key, categories):. IndexError: index 6 is out of bounds for axis 1 with size 6; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-440038619:1907,layers,layers,1907,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-440038619,1,['layers'],['layers']
Modifiability,"); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number of counts for variabes in ; `qc_vars`.; * `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""pct_counts_mito"". Proportion of total counts for a cell which ; are mitochondrial. Variable level metrics include:. * `total_{expr_type}`; E.g. ""total_counts"". Sum of counts for a gene.; * `mean_{expr_type}`; E.g. ""mean counts"". Mean expression over all cells.; * `n_cells_by_{expr_type}`; E.g. ""n_cells_by_counts"". Number of cells this expression is ; measured in.; * `pct_dropout_by_{expr_type}`; E.g. ""pct_dropout_by_counts"". Percentage of cells this feature does ; not appear in.; . Example; -------; Calculate qc metrics for visualization. >>> adata = sc.datasets.pbmc3k(); >>> sc.pp.calculate_qc_metrics(adata, inplace=True); >>> sns.jointplot(adata.obs, ""log1p_total_counts"", ""log1p_n_genes_by_counts"", kind=""hex""); """"""; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:2046,variab,variabes,2046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,2,"['Variab', 'variab']","['Variable', 'variabes']"
Modifiability,"* How about a `var_type` argument which defaults to `""genes""`? Could even replace occurrences of `cells` with a `obs_type` variable; * Agreed, `expr_type` it is.; * I'd considered an option of a suffix argument, but I think there are some issues with it being a string. If `suffix=None, expr_type=""counts""` is it still `total_counts`? If `suffix=""_by_counts""` is it still `total_counts`? A possible solution to this is making suffix a boolean argument, where `True` adds `_by_{expr_type}` as appropriate.; * Additionally `n_genes` in particular is already an overloaded term, as it's already used with multiple different meanings in the codebase (typically number of genes to consider, or number of columns in the data matrix). ; * Could you say a bit more about the conditioning? I'm not sure I follow how it would be confusing here.; * `control_variables` definitely makes more sense here than `variables`, I was just wondering if there's a better word than `control`.; * If there was an equivalent for observations (something like `mean_counts_in_donor1`), what would be a good name for that argument?; * I think I'm happy with `n_...` for some things, `total_...` for others.; * On other convention I'm unsure about, should the range of the `pct_...` values be 0-100 or 0-1?. ---------------. Here's what I'm thinking it'll look like right now (pending `suffix` decision). For obs:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{control_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{control_var}`|. For var:. | current | proposed |; | ------- | -------- |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986:123,variab,variable,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436498986,2,['variab'],"['variable', 'variables']"
Modifiability,"**Tests**. For the present changes, these tests are sufficient. Only since recently, Scanpy is becoming properly tested via the example notebooks (previously, I always ran everything manually). So, if you want to test this properly, come up with an interesting use case that uses clustering on weighted graphs, make a pull request for a notebook on `theislab/scanpy_usage`, link to it in the docs (https://scanpy.readthedocs.io/en/latest/examples.html) and commit a corresponding test in `tests/notebooks`. That's a lot of work and I think too much for this present case. Eventually, most of Scanpy's functionality should be tested this way. A lot will also be covered by extending existing notebooks. But I don't think it's possible to meaningfully add the weighted graphs to the present standard Louvain clustering example. **Multiple network representations**. I don't mind as long as the standard user still uses a single one - otherwise people will get confused. But let's not call these grahs ""networks"". The notion ""network"" suggests that nodes ""interact"". The current graphs in Scanpy, however, are all proxies for manifolds. Edges only represent similarity and, through that, have a topological interpretation. I'd really like to reserve the notion ""network"" for cases where nodes possibly or actually interact. Hence, a gene regulatory network or a network of cells with cell-cell interactions. Both are completely different things compared to the ""neighbors"" stuff in Scanpy. **Allow more choice of partition method for louvain-igraph package**. Fine for me as long as the default stays unchanged. I'm not sure whether it adds a lot of value but it will also not be harmful. ---. Could you also fix the docs for the `flavor` parameter and specify the default? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395:672,extend,extending,672,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/248#issuecomment-418079395,1,['extend'],['extending']
Modifiability,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```; np.any(adata.X.sum(axis=0) == 0); np.any(adata.X.sum(axis=1) == 0); ```; both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667#issuecomment-519987040:124,variab,variable,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667#issuecomment-519987040,1,['variab'],['variable']
Modifiability,"/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. But it seems to work no matter what server I choose in R, here's the test code. ```R; library(biomaRt). wwwmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""www.ensembl.org""); asiamart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""asia.ensembl.org""); useastmart = useMart(biomart=""ensembl"", dataset=""hsapiens_gene_ensembl"", host=""useast.ensembl.org""). wwwresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=wwwmart); asiaresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=asiamart); useastresult = getBM(attributes=c(""hgnc_symbol"", ""chromosome_name""), mart=useastmart). assertthat::assert_that(all(wwwresult == asiaresult)); assertthat::assert_that(all(useastresult == asiaresult)); ```. I don't think the issue is with ensembl's servers. Is it possible you have a config for this `bioservices` module you've done anything with?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067:3778,config,config,3778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-416814067,1,['config'],['config']
Modifiability,"0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; > ```; > ; > When trying to use the new flavor with the existing test. Hi @Zethson ,; We are not able to see this issue with the latest commit. Can you please retry with the latest commit in scale branch.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:2022,config,config,2022,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['config'],['config']
Modifiability,"2`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1171,layers,layers,1171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,1,['layers'],['layers']
Modifiability,"; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2277,config,config,2277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['config'],['config']
Modifiability,"> ## Question; > ; > Does this interact with group colors and dendrograms at all?. Dendrogram and colors seem not affected. > ## Test change; > ; > All of the plots will start failing because this will change the output for every test. I have a few concerns here:; > ; > * I'm worried about repo bloat from the plotting tests. Ideally we could just store the reference images outside of git (git lfs maybe?). Updating all the plots with `tight_layout` would increase repo size by 10%; > ; > * Is `tight_layout` deterministic ([matplotlib/matplotlib#11809 (comment)](https://github.com/matplotlib/matplotlib/issues/11809#issuecomment-432726600))? Also, is matplotlib trying to replace it with [`constrained_layout`](https://matplotlib.org/stable/tutorials/intermediate/constrainedlayout_guide.html)?; > ; > * Does globally adding `tight_layout` add to test times? My impression was that it basically rendered the plot, fixed the borders, then rendered it again.; > ; > ; > Proposed solution:; > ; > Can we just explicitly extend the borders for this test? At a later point we can move plots to a different storage system, then have much more freedom in making changes to how they render. Sounds great, I think I managed to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772:1021,extend,extend,1021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1735#issuecomment-796812772,1,['extend'],['extend']
Modifiability,"> 79 characters); scanpy/preprocessing/_highly_variable_genes.py:713:80: E501 line too long (88 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:735:80: E501 line too long (82 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:737:80: E501 line too long (83 > 79 characters); scanpy/preprocessing/_highly_variable_genes.py:742:80: E501 line too long (80 > 79 characters). ```. `git status` and `git diff` show the automatic changes pre-commit makes:. ```; jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git status; On branch pearson_residuals_1.7; Changes to be committed:; (use ""git reset HEAD <file>..."" to unstage). 	modified: _highly_variable_genes.py. Changes not staged for commit:; (use ""git add <file>..."" to update what will be committed); (use ""git checkout -- <file>..."" to discard changes in working directory). 	modified: _highly_variable_genes.py. Untracked files:; (use ""git add <file>..."" to include in what will be committed). 	../../.pre-commit-config.yaml. jlause@8b38045532aa:~/libs/scanpy/scanpy/preprocessing$ git diff _highly_variable_genes.py ; diff --git a/scanpy/preprocessing/_highly_variable_genes.py b/scanpy/preprocessing/_highly_variable_genes.py; index 03b01940..e2851f50 100644; --- a/scanpy/preprocessing/_highly_variable_genes.py; +++ b/scanpy/preprocessing/_highly_variable_genes.py; @@ -15,7 +15,8 @@ from ._utils import _get_mean_var; from ._distributed import materialize_as_ndarray; from ._simple import filter_genes; ; -#testedit; +# testedit; +; ; def _highly_variable_genes_seurat_v3(; adata: AnnData,; @@ -98,7 +99,9 @@ def _highly_variable_genes_seurat_v3(; else:; clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape); np.putmask(; - batch_counts, batch_counts > clip_val_broad, clip_val_broad,; + batch_counts,; + batch_counts > clip_val_broad,; + clip_val_broad,; ); ; if sp_sparse.issparse(batch_counts):; @@ -173,6 +176,7 @@ def _highly_variable_genes_seurat_v3(; df = df.drop(['highly_variable_nbatche",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562:8658,config,config,8658,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794148562,1,['config'],['config']
Modifiability,"> > ooh, this time the benchmark shows really nicely how much faster it is!; > ; > Looks like preprocessing_log.time_regress_out('pbmc68k_reduced') , regress out those variables that is not inside it. It should regress_out ['n_counts', 'percent_mito'] instead of [""total_counts"", ""pct_counts_mt""]. For the both commit it fails so report the same time. @flying-sheep , can you look at the this benchmark test?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2181072184:168,variab,variables,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2181072184,1,['variab'],['variables']
Modifiability,"> @WeilerP, do you think this would be more appropriate in `scvelo`? (Side note, I have thought that tutorial; > of going from BAMs through `scvelo` would be quite useful). Hm, not sure if the functionality would match the expectation. In `scvelo`, we'd store only unspliced and spliced counts (spliced both in `adata.X` and `adata.layers`). Based on the proposed code snippet in [alexdobin/STAR#774 (comment)](https://github.com/alexdobin/STAR/issues/774#issuecomment-850477636), the expected output would be to read all of the available information?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253:332,layers,layers,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-873990253,1,['layers'],['layers']
Modifiability,"> @flyingsheep I can assure you, that's the normal case in academic HPC systems. I agree that this is a huge and common problem in many HPC systems. I usually install conda and R packages to non-home directories with bigger space to avoid issues on servers. One can fill up hundreds of MB by just installing a single package e.g. human genome from Bioconductor ðŸ˜„ . > Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. There is a user home and the cache is `~/.cache` and $XDG_CACHE_HOME is undefined (at least in my case). Some pip wheel files are there for example. . Although it's painful to work in such systems, I believe it's user's responsibility to fix this. One idea might be to print a warning when the cache directory is created for the first time along with the path itself to inform the user about where files are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878:526,variab,variable,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878,1,['variab'],['variable']
Modifiability,"> @zhangguy, adding on to some thoughts from your PR [#2055 (comment)](https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001); > ; > From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:; > ; > ```python; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > ; > sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); > ```; > ; > ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG); > ; > Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments.; > ; > I think you'd be able to get an output close to what you would currently like with:; > ; > ```python; > import scanpy as sc, pandas as pd, numpy as np; > ; > pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); > pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); > df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]); > ; > summarized = df.pivot_table(; > index=[""louvain"", ""sampleid""],; > values=""LDHB"",; > aggfunc=[np.mean, np.count_nonzero]; > ); > color_df = summarized[""mean""].unstack(); > size_df = summarized[""count_nonzero""].unstack(); > ; > # I don't think the var_names or groupby variables are actually important here; > sc.pl.DotPlot(; > pbmc,; > var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; > dot_color_df=color_df, dot_size_df=size_df,; > ).style(cmap=""Reds"").show(); > ```; > ; > I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:; > ; > ```python; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:287,variab,variable,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,3,['variab'],['variable']
Modifiability,"> AFAICT, I think the parallelization you're seeing will be due to the underlying calls in statsmodels. If you turn down the number of threads blas can use, do you see the same utilization?. FYI more n_jobs seems to be slower for regress_out if I don't disable the BLAS multi threading:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=1); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:04:05); ```; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:07:41); ```. I'm using scanpy 1.5.2.dev104+g8611dba1. Indeed after disabling BLAS multi threading sc.pp.regress_out will only use one core if setting n_jobs = 1. But it has to be disable by exporting these environmental variables before starting python ..., but I guess it is not a good thing because other scanpy functions may be affected?; ```; export MKL_NUM_THREADS=1; export NUMEXPR_NUM_THREADS=1; export OMP_NUM_THREADS=1; ```; More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:; ```; sc.pp.regress_out(adata, ['percent_mito'], n_jobs=24); ```; ```; regressing out ['percent_mito']; sparse input is densified and may lead to high memory use; finished (0:00:23); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610:868,variab,variables,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-684103610,1,['variab'],['variables']
Modifiability,"> About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. As a general point about this PR: to me, the fair amount of the work of turning on flake8 deciding on the rules. Perhaps we should start with a subset of files then? I realize you did not come to the meeting where we talked about this, so perhaps there is a difference of expectations here, but we agreed to be conservative about the rules we turned on in `pre-commit`. Going through everything to make sure changes are correctly reverted is also takes a lot of time for me as the reviewer. I'd also like to limit that. ----------------. You said you used some automated tools to get faster compliance. What were these? In general, I would prefer to have a formatter that automatically ran than a tool that told me I formatted something wrong. -----------------. > `@ivirshup` I would keep the noqas. They are very easily searchable across the whole project and can be fixed later. I'm pretty strongly against this. `noqa`s just look like the formatter/ linter was wrong, and I'm not accepting that having no plan to address bugs. I think this should be a discussion with a broader set of the team. > ""Whats up with removing leading #s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. Yes, lets ignore this. >> ""I don't like replacing x == False with not x in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this.""; >; > This should be covered by tests. In any case it is not good style and a violation. I will try and take a closer look at these changes. I'm particularly concerned that there will be cases where possible values are `None`, `True`, and `False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670:1491,variab,variable,1491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785871670,1,['variab'],['variable']
Modifiability,"> Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch.; > ; > I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. Thanks a lot, I rebased and changed the PR target to `master` so I hope everything is on track now! ; The pre-commit style checks were working as expected now (auto-edits only in the files / parts I edited). > Side note: We're considering separating the highly_variable_genes interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function. Sounds good!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189:188,config,config,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-795469189,1,['config'],['config']
Modifiability,"> An alternative solution to this would be `pyplot.imshow(..., origin='lower')`; > https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L301; > by default `pyplot.imshow(..., origin='upper')` which makes y-axis flipped compared to the scatterplot. Good point, but if `img_key=None` shouldn't we assume that there is no image? So nothing to `pl.imshow`. ; Now the point would be:; * passing a dummy array as ""image"" and then plot the spots as `circles`, so keeping the normal behavior; * plot the spatial data as a simple scatterplot. . My reasoning is that, since there is no image to show:; * we probably should assume that also `scalefactors` are empty.; * the correct size of the spots (given by `scalefactors`, and plotted with `circles`) does not matter anymore, since their size is only important in the presence of an image in the background, so a normal scatterplot would do.; * If the user does not want an image in the background, but wants to retain the size of the spots (because `scalefactors` are not empty), then `alpha_img=0` should do the job. . Because of these reasons, I would go for the second option above, passing the `coords` as basis to the scatterplot fun. Probably an even better option would be to plot the spots as hexagons, as originally suggested by @flying-sheep , but would wait for that after the plotting module is refactored. > @giovp conditionally doing that would probably be cleaner and wouldnâ€™t involve flipping the data right?. The reason for flipping is that the coords from space ranger are given with upper origin. ```python; sc.pl.embedding(adata_spatial, basis = ""coords""); ```; ![image](https://user-images.githubusercontent.com/25887487/79198789-55bbed80-7e34-11ea-9db6-66da7d700cd2.png). Happy to discuss and change the behavior, I could have missed a crucial point.; And thank you @vitkl for feedback, we are in dire need of spatial transcriptomics scanpy users !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777:1383,refactor,refactored,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1149#issuecomment-613283777,1,['refactor'],['refactored']
Modifiability,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, Â¯\\\_(ãƒ„)_/Â¯. > Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:1227,config,config,1227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,2,['config'],"['config', 'configs']"
Modifiability,"> As a general approach to this kind of problem, I write functions like this:; > ; > ```python; > def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; > if layer is not None:; > getX = lambda x: x.layers[layer]; > else:; > getX = lambda x: x.X; > if gene_symbols is not None:; > new_idx = adata.var[idx]; > else:; > new_idx = adata.var_names; > ; > grouped = adata.obs.groupby(group_key); > out = pd.DataFrame(; > np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; > columns=list(grouped.groups.keys()),; > index=adata.var_names; > ); > ; > for group, idx in grouped.indices.items():; > X = getX(adata[idx]); > out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); > return out; > ```; > ; > Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`.; > ; > At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type. Thanks. But need to make a tiny amendment to make it work now:; ```python; out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)).tolist(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078:216,layers,layers,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-1871723078,1,['layers'],['layers']
Modifiability,"> Awesome, Gokcen, thank you! ðŸ˜; >. Thank you!; ; > Also, adding an export utility for Gephi was on the list already before. Cool that you found a simple solution for this.; > . Ah ok, didn't know that. Here is what I used so far for gephi:. ```python; # python-igraph from master branch is required; # see https://github.com/igraph/python-igraph/issues/115; from igraph.remote.gephi import GephiConnection, GephiGraphStreamer. sc.tl.draw_graph(adata); # would be also nice have access to igraph object right after sc.tl.draw_graph; g = sc.utils.get_igraph_from_adjacency(adata.uns['data_graph_norm_weights']). # then install latest Gephi and the streaming plugin:; # https://gephi.org/plugins/#/plugin/graphstreaming; # and start the Gephi master server; streamer = GephiGraphStreamer(); conn = GephiConnection(workspace=1). # igraph cannot serialize numpy float32 to json, so it must be converted to float64; g.es['weight'] = [float(x) for x in g.es['weight']]; g.vs['groups'] = adata.obs['louvain_groups'].tolist(); streamer.post(g, conn); ```. Here is the Yifan Hu layout for 3K PBMC:. ![image](https://user-images.githubusercontent.com/1140359/34961174-384c5658-fa0c-11e7-8597-db4e77cbf4e3.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075:657,plugin,plugin,657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/68#issuecomment-357787075,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers.; > ; > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:; https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881:211,plugin,plugins,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-509063881,2,['plugin'],['plugins']
Modifiability,"> Can we call this epi_sc_expression_atlas instead of expression_atlas?. Definitely agree it's good to specify it's from EBI. Would `ebi_expression_atlas` be alright? `ebi_sc_expression_atlas` feels a little verbose for me. I think it's implied it's single cell, plus it's explicit in the doc-string. > For the time being, can we make this settings.datasetsdir instead of settings.dataset_dir and add it here:. Changed the name. It looks like the main docs aren't being generated from `scanpy/scanpy/api/__init__.py`, but from `docs/api/index.rst` instead. Which is correct?. > Can we point it to the home directory by default, I'd say ~/scanpy-datasets/?. Changed. Does this mean config changes should also happen in this PR? I think this may cause trouble (HPC environments with small `~` allocations) without allowing default configuration at the same time. I had previously figured that setting up a config file could be factored out to a separate PR. To be able to put off adding the config for now, we could temporarily special case a `SCANPY_DATASETDIR` environment variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478414057:681,config,config,681,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478414057,5,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"> Can you point to a package whose test organization you would like our tests to emulate?. - pytest: https://github.com/pytest-dev/pytest/tree/main/testing; - loompy: https://github.com/linnarsson-lab/loompy/tree/master/tests. The others have their tests in the package, and just have that useless `__init__.py` in the tests directory because of either cargo culting it or becaue they know that makes setuptoolsâ€™ `discover_packages` or so find it. > From your description above I had thought you didn't want to emulate [pandas use of conftest](https://github.com/pandas-dev/pandas/blob/main/pandas/conftest.py?rgh-link-date=2022-04-12T13%3A19%3A30Z). What do you mean specifically?. Pandas are defining special pytest functions/variables there and fixtures, which is what itâ€™s for. Iâ€™d probably judge that we donâ€™t need all those fixtures for our complete test suite and move some of them to a smaller scope (e.g. `tests/io/conftest.py` or so). > I'd lean towards it, but I fully expect issues like https://github.com/scverse/scanpy/pull/685 to come up. This is why I'd like to see a working example of what you want to work towards. Actually I think we can fix that: [the docs for `pytest_addoption`](https://doc.pytest.org/en/latest/reference/reference.html#pytest.hookspec.pytest_addoption) say it has to be defined at the *tests root directory* which can be configured using the [`rootpath`](https://doc.pytest.org/en/latest/reference/reference.html?highlight=root#pytest.Config.rootpath) config option. > Is it definitely the future default? It looks like they are walking that back. The question is if they remove the others or not, I think: https://github.com/pytest-dev/pytest/issues/7245. ---. My intention here is to make clear which code lives under which laws. Pytest world is very different from Python module world. The presence of `__init__.py` fools people into thinking that weâ€™re dealing with python packages/modules here, but thatâ€™s not true. The way pytest works is pretty simple:.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986:728,variab,variables,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225#issuecomment-1096900986,1,['variab'],['variables']
Modifiability,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:315,variab,variable,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,1,['variab'],['variable']
Modifiability,"> Could any noqas added in this PR get something searchable added to them (like # noqa: {rule} TODO: fix me) so we know why it was added?. The noqas already state what they are ignoring. I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. > Document how to turn off these checks in dev docs. What do you mean? How to ignore a single line? How to fully ignore whole checks? I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. > Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config. I wish it were that easy. autopep8 does not take its configuration from the flake8 config file nor can it fix all pep8 violations nor do Black and autopep8 always work nicely together. Black is an **opiniated** formatter. It formats consistently, but not necessarily compatible with other tools. I would not add autopep8, since I do not see any further benefit to the Black & flake8 combination, only more potential for issues and confused developers. I agree with your other comments and will take care of them as soon as I got your answers :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021:838,config,config,838,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787424021,3,['config'],"['config', 'configuration']"
Modifiability,"> FIXED: Updating adata.X to a scipy csr sparse matrix using `adata.X = scipy.sparse.csr_matrix(adata.X)` fixed this error.; > ; > I still get `RuntimeWarning: invalid value encountered in sqrt std = np.sqrt(var)` when running `sc.pp.scale(adata, max_value=10)` even after forcing to a csr matrix, but doesn't seem to affect downstream results... hi Rebecca, I have been trying to process scRNA (converted seurat to h5ad format) in python (processing like QC, normalisation, scaling, high variables, clustering etc) and have been getting stuck at the highly variable genes. Can you please help me out with it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/391#issuecomment-1149911935:489,variab,variables,489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/391#issuecomment-1149911935,2,['variab'],"['variable', 'variables']"
Modifiability,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here?. ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189:93,layers,layers,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-1413256189,2,['layers'],['layers']
Modifiability,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, itâ€™ll break docstrings with indentation (code blocks, lists, â€¦). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (itâ€™s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We donâ€™t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i donâ€™t think itâ€™s possible to get bugs this way: weâ€™re just adding type annotations, we donâ€™t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:871,rewrite,rewrite,871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,2,['rewrite'],['rewrite']
Modifiability,"> Hey, sorry about the late response!. No worries!. > Would you mind separating out the bug fix and feature addition? That was the bug fix can be released more quickly. OK, will do- see https://github.com/theislab/scanpy/pull/2023, https://github.com/theislab/scanpy/pull/2025. > Question about the main idea here: what kind of batches are you expecting to handle here?; > ; > If they were from completely separate sequencing experiments, would you want to have variable expected doublet rates between batches?; > ; > If the batches are multiple samples that were barcoded and multiplexed, would you want to allow for the possibility of multiplets across batches?. So, really, I just want to be able to follow best practice as per the [Scrublet docs](https://github.com/swolock/scrublet#best-practices), to be able to run Scrublet on cells from different samples separately, perhaps batches is the wrong term. Do you have a preferred alternative, or should I just clarify the help text?. > Could you also merge master into this? CI should be fixed now. Done",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277:462,variab,variable,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1965#issuecomment-953614277,1,['variab'],['variable']
Modifiability,"> Hi, It's not available in scanpy at the moment, but I wrote a wrapper for it via `rpy2` and `anndata2ri` which is available here:; > https://github.com/normjam/benchmark/blob/master/normbench/methods/ad2seurat.py. Hi,. I have been trying to use this wrapper, but seems like there's some error during the conversion process:. RRuntimeError: Error in validObject(.Object) : ; invalid class â€œdgCMatrixâ€ object: 1: invalid object for slot ""i"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 2: invalid object for slot ""p"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 3: invalid object for slot ""Dim"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""integer""; invalid class â€œdgCMatrixâ€ object: 4: invalid object for slot ""x"" in class ""dgCMatrix"": got class ""array"", should be or extend class ""numeric"". Any pointers to get around this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061:495,extend,extend,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-866121061,4,['extend'],['extend']
Modifiability,"> How about a var_type argument which defaults to ""genes""? Could even replace occurrences of cells with a obs_type variable. Sorry about the late response. The week passes so quickly and depending on how it goes, I can only go back to Scanpy during the weekends these days... I like your new table much better than the previous one. If you'd put that forward in a PR, I'd be super happy!. PS: I think `control_variables` is fine, maybe `focus_variables` better reflects that it's not only about ""control"", but just a subset of variables (or observations) of interest. Let's not move forward on the suffix argument, your imposed structure will make things cleaner for people! :smile: `pct` is ok if in 0 - 100. No `obs_type` needed right now...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137:115,variab,variable,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437734137,2,['variab'],"['variable', 'variables']"
Modifiability,"> How do we xfail stuff from dev?. [`pytest.mark.xfail`](https://docs.pytest.org/en/6.2.x/reference.html#pytest-mark-xfail) takes a condition:. ```py; xfail_if_dev_tests = pytest.mark.xfail(; os.environ.get(""DEPENDENCIES_VERSION"", ""latest"") == ""pre-release"",; reason=""..."",; ). @xfail_if_dev_tests; def test_xzy(): ...; ```. You probably need to change the tests so it makes the CI variable visible as an env variable, Iâ€™m not an Azure expert so I donâ€™t know if it already is. > Codecov, I think, is outright wrong aklthough that might have to do with the failing dev test. Yeah, maybe, letâ€™s see once everything passes. Iâ€™m also OK with lowering the percentage, I just set it to 75% to have some indication if codecov is broken or working. (Before it would report 20% for a PR and there would be no visual indication that thatâ€™s a problem)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2114691148:382,variab,variable,382,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2114691148,2,['variab'],['variable']
Modifiability,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:233,variab,variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438,1,['variab'],['variable']
Modifiability,"> I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. Thatâ€™s what ABCs are for. `isinstance(thing, Mapping)` works beautifully for everything that has the `Mapping` protocol, no matter if itâ€™s a `dict` or not.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086:71,polymorphi,polymorphism,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444816086,1,['polymorphi'],['polymorphism']
Modifiability,"> I don't understand this, why would this belong on sc.pp.neighbors? The graph weighing should go into the sc.tl.tsne call. Are the UMAP weights assigned to the graph in sc.pp.neighbors?. Yes, this is my understanding of how it works in scanpy. See https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.neighbors.html:. ```; method : {â€˜umapâ€™, â€˜gaussâ€™, â€˜rapidsâ€™}, None (default: 'umap'). Use â€˜umapâ€™ [McInnes18] or â€˜gaussâ€™ (Gauss kernel following [Coifman05] with ; adaptive width [Haghverdi16]) for computing connectivities. Use â€˜rapidsâ€™ for the; RAPIDS implementation of UMAP (experimental, GPU only).; ```. > If I understand option 2 correctly, we would normalize the 15 neighbors to essentially perplexity=5. That's not what I meant. I meant taking UMAP's weights for k=15 and normalizing the matrix so that it sums to 1, as in t-SNE. That said, I would also prefer option 1 because I don't want anything that sounds like it's a UMAP/t-SNE hybrid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742:464,adapt,adaptive,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748685742,1,['adapt'],['adaptive']
Modifiability,"> I figured it'd be good to with everything being consistent. Plus documentation for the settings is now available through ?sc.settings.{setting}!. Of course, it's much better to be consistent. I was just suggesting a quick solution that wouldn't have been worse than the current one... ;). A complete automatic documentation is now also available from ; https://scanpy.readthedocs.io/en/latest/api/scanpy._settings.ScanpyConfig.html; under; https://scanpy.readthedocs.io/en/latest/api/index.html#settings. scanpy config: At some point almost 2 years ago, I removed a lot of stuff that I didn't think was essential to clean up the project. It was just an element of that. I'm very happy to introduce it again; these days, the project is much more major and indeed has many config options (there were only few at the time) and hence it would indeed merit having a config file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479750272:514,config,config,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479750272,3,['config'],['config']
Modifiability,"> I for now would not try to differentiate between noqas that we want to keep and noqas that we want to get rid of. We want to get rid of all of them in the follow up issue and only when examining all of them we will figure out which ones we want to keep. After this merges new ignore messages can be added for reasons like ""this rule is generally good, but not in this specific case"". Each of these will go through PR review, so will be vetted. The ones added here largely have not been vetted, and are just being added so we don't get a failure. I would like to be able to distinguish between these cases. Once more `noqa` cases are added, it gets more complicated to find cases that haven't been vetted if they don't have some associated label. --------------------------. > What do you mean? How to ignore a single line? How to fully ignore whole checks?. How to disable flake8 errors for a line or file. > I would always refer to the flake8 documentation, because it will certainly maintained better than the dev documentation. A link to the section of the flake8 docs on this would be great. -------------------------. > I wish it were that easy. autopep8 does not take its configuration from the flake8 config file . `autopep8` says it does this: https://github.com/hhatto/autopep8#configuration. > It formats consistently, but not necessarily compatible with other tools. I would like changes that are automatically applicable to be automatically applied. I'm thinking of things like white space in docstrings. Is there another way to automate these you can suggest?. ---------. BTW, I've added a few more points to the checklist above. I would recommend trying to build the package and build the docs in the directory you're working in to see what files get generated so they can be added to the `ignore`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782:1180,config,configuration,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787426782,3,['config'],"['config', 'configuration']"
Modifiability,"> I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels. Thanks, this also worked for me!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917:112,config,config,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-762368917,3,['config'],['config']
Modifiability,"> I have got what I want with the following code adapted from dotplot():; > ; > gene_ids = adata.raw.var.index.values clusters = adata.obs['louvain'].cat.categories obs = adata.raw[:,gene_ids].X.toarray() obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']) average_obs = obs.groupby(level=0).mean() obs_bool = obs.astype(bool) fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count() average_obs.T.to_csv(""average.csv"") fraction_obs.T.to_csv(""fraction.csv""). Love this! Thanks a lot!! ; Just one question, is there a way to get the average expression in different cell types (cluster label 1 ) in different sample (cluster label 2 ) from an integrated object?? ; to get something roughly like this:. Gene 1 Gene 2 ; sample1 sample2 sample3 sample1 sample2 sample3 ..... ....... ....; T-cell; B-cell ; .....; ..... I am not sure if this makes sense, but I have been trying to do this for a while and nothing worked!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-1334674713,1,['adapt'],['adapted']
Modifiability,"> I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. In this case, I think it should be fine. It might not happen too soon if we're left to our own devices, so a PR is welcome. > I could just see a standalone package being widely used and community driven. What formats that aren't in `anndata` would you see in this package? I'm trying to get an idea of the kind of scope you're thinking of here. I think there are formats where there isn't one obvious ""right way"" to represent them as an AnnData object (e.g. visium), so having a canonical reading/ writing function is difficult.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582:69,refactor,refactor,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-683586582,1,['refactor'],['refactor']
Modifiability,"> I know all that. :wink:. And I know that you know! I just like to be comprehensive when presenting my arguments!. > But numpy, pandas, scikit learn, tensorflow, seaborn all have the comma-separated list as a convention and I'd really like to stick to that convention. OK. Iâ€™d prefer â€œa, b, or câ€, but Iâ€™ll concede. It would also be no problem to change it later since all will be automated :+1: . > No, the optional keyword always means that a parameter has a default. Very often, people forget to append ""or None"" (, None) to the list of possible types. Well, when I open scanpy in PyCharm and someone forgot that in a type annotation, it highlights that fact to me. Pretty nice. > As mentioned before, there is no point in using set-theoretic/logical notions like union or intersection as the topic is so simple that it doesn't need it (no need for an intersection, it's not even clear what that would mean; if you're stringent about it, it's also not clear for union). Oh, then you didnâ€™t hear of type theory. Itâ€™s a branch of logic: Type systems are formal systems, and in most of them the terms I used are well defined. The kinds of composite types I mentioned are:. - `Union` of types / Sum Type / [Tagged Union](https://en.wikipedia.org/wiki/Tagged_union): Variables with one of those have one of several fixed types.; - Subtype / [Intersection Type](https://en.wikipedia.org/wiki/Type_system#Intersection_types): Variables have all the properties of the supertypes.; - `Tuple` / [Product Type](https://en.wikipedia.org/wiki/Product_type): Variables contain multiple entries that each have one corresponding type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106:1266,Variab,Variables,1266,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-442007106,3,['Variab'],['Variables']
Modifiability,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust â€“ I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:579,variab,variable,579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033,1,['variab'],['variable']
Modifiability,"> I might be missiong something, though. I personally have only had problems with the intel backends on my MacBook. I see that pinning the parallel backend would be a good way to make the code work for more people. Especially if this used to be worse. I think the capabilities of the different threading layers have also been tweaked over time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744:304,layers,layers,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874674744,1,['layers'],['layers']
Modifiability,"> I see them more as TODOs for later since the ""bad code"" is in master at the moment anyways. But if you want to discuss that - sure. I think it's hard to tell the difference between a `noqa` that was added because: ""most of the time, this rule is right, this time it is wrong"" vs. ""added to get rid of warning"". Could any `noqa`s added in this PR get something searchable added to them (like `# noqa: {rule} TODO: fix me`) so we know why it was added?. In a future meeting we can discuss with the whole team how we will actually fix these. I think it's a good task for a hackathon/ sprint. ------------. Looking at this again, I think this is pretty close to done. Just a few documentation/ minor rule changes left. - [x] Document how to turn off these checks in dev docs; - [x] Document the rules that are turned off (similar to [pandas](https://github.com/pandas-dev/pandas/blob/879cd22dd58b0574cdcaa7a26e396d5ec71a615a/setup.cfg#L71-L79)); - [x] Add autopep8 to precommit. If things can be fixed automatically, they should be. autopep8 should be able to get it's rules from the flake8 config.; - [x] Add annotation to `noqa`s.; - [x] Also add `TODO` annotation to `except Exception`s that have been added.; - [x] Turn off E731; - [x] Turn off the rule that doesn't allow multiple leading `#` in comments; - [x] Turn off F811 for tests (rule violated by using fixture as test argument); - Possible solution: add noqa for this to all files under `tests`, separate check that all files under tests have this.; - [x] Ignore `build` `docs/_build` directories",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099:1089,config,config,1089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787412099,1,['config'],['config']
Modifiability,"> I think we should have a 'cookbook' where we can keep this and other information. I've been trying to be organized about keeping notebooks around for this ([here](https://github.com/ivirshup/notebooks/tree/master/plotting)). Of course, I rarely get the notebooks clean enough to push ðŸ˜†. > > In the end it's about showing which cells are represented per pixel/pixel bin.; >; > I would argue that this would be fair. In the end it's about showing which cells are represented per pixel/pixel bin. Is it fair if coloring by batch and one dataset had fewer samples? Wouldn't you want to know that multiple batches were showing up in this region? I'm fairly convinced there is no good way to show this in one plot, other than telling users some information is hidden. > We could do a quick fix based on random order for now. I'm trying to think of the simplest way to implement this. I would like to keep the behaviour of `sort_order=False` just using the order from the anndata object. Some options:. * `sort_order=""random""`, this would make the order random, but we might need to add a seed argument. Also, do we still plot over null values?; * `sort_order=order_array` where `order_array: np.ndarray[1, int]`. Basically, the user can pass whatever order they like. For random order it would be `np.random.choice(adata.n_obs, adata.n_obs, repeat=False)`. This is pretty flexible since it allows whatever order you want to be used without sorting the object. > larger update that would have to do with updating scanpy plotting to larger cell numbers?. I think this might be worth a separate package, at least to start out. At least with how I'm handling it now, there would be a large number of dependencies. Plus, I think overplottting like this is an unsolved problem, so freedom to experiment in important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895:1368,flexible,flexible,1368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-761745895,1,['flexible'],['flexible']
Modifiability,"> I wasn't really expecting this feature PR to also include such a large refactor. It would have been necessary for the Dask Dataframe version. Now I 1. did the work and 2. improved readability, so it would be counter productive to undo it. > I'm still not 100% convinced the behaviour here is exactly the same as before. I have done a few tests, which have been okay, but I haven't tried much parameterization. I'm ~80% convinced the results should be the same. If you have any specific things in mind, you should probably make a PR that adds tests for the properties you think we should preserve. We can then merge that one, update this one, and see if it actually breaks something. I canâ€™t check for speculative differences if I have no idea where those could be. > I would note that the dataframe returned when inplace=False has a different index than it did previously. Yup, now it actually matches instead of discarding the original Index and replacing it with a RangeIndex for no reason. > Apart from the comments, can we get a regression test for ""cell_ranger"" (e.g. generate results with an older version)? I don't think we have one in the test suite. Sure! Thatâ€™s a concrete thing I can do. Iâ€™ll do that on thursday, I did the rest of what you asked today",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931:73,refactor,refactor,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1930104931,2,"['parameteriz', 'refactor']","['parameterization', 'refactor']"
Modifiability,"> I wouldnâ€™t call that situation fine, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. It was fine in that it made UMAP work more places than it would have otherwise. > This means our solution for the second shouldnâ€™t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. I don't really want to have to touch it, especially since I believe it's immutable after any parallel code is called â€“ so we can't actually control in. Our hands are a little tied by having a dependency do this. I would like to know why this is still considered necessary for pynndescent.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678:109,config,configurable,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874661678,2,['config'],['configurable']
Modifiability,"> I'd like the following sequence of commands ... to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. ; > I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications. I share the same worry, but am not qualified to answer when something becomes ""t-SNE"". I think it would be sufficient for `sc.tl.tsne` to warn users if the graph it was passed looks unexpected (or if it could tell it was generated by a different method). > What you suggest (t-SNE on normalized UMAP affinities) could maybe achieve that. From an API point of view, we don't control weights at the `sc.tl.umap` call, so I think it would be strange to control weights at the `sc.tl.tsne` call. I'm also not sure if binarizing the graph would be closer to ""t-SNE"". ----------------------. About `sc.pp.neighbors` vs `sc.pp.neighbors_tsne`. > This is just a question of API, and is less important for me personally. I agree that it could be better to have neighbors() compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR. I think for backwards compatibility I would like to keep neighbors pretty much as is. I think new functions like `distance_neighbors`, `umap_neighbors`, `tsne_neighbors` could be reasonable to add. It's also possible we could add a `""tsne""` method to `neighbors`, but I think the implementation can look very similar to having a `tsne_neighbors` function, so this can be kicked down the road a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475:1058,refactor,refactoring,1058,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762595475,1,['refactor'],['refactoring']
Modifiability,"> I'm curious about how much the backend changes the runtime and results of nearest neighbors methods. You can see some quick comparisons between Pynndescent and Annoy here: https://github.com/pavlin-policar/openTSNE/issues/101#issuecomment-597178379. But I have not investigated it very thoroughly. Anyway, returning to the main conversation:. I think switching to openTSNE makes sense even if nothing else that we are discussing is implemented. It's A LOT faster than Mutlicore t-SNE for large datasets: https://opentsne.readthedocs.io/en/latest/benchmarks.html. It is also more flexible, actively supported, conveniently packaged/distributed, etc. I don't see any possible disadvantage. You could potentially keep all the default parameters as you have now in scanpy (even though I would not recommend it, see below). However, what I said about using pre-build kNN graph requires some thinking. T-SNE uses perplexity=30 by default and uses kNN graph with k=3*perplexity, so that's 90 by default. UMAP uses k=15 and that's what you use in scanpy by default too. I can see three options here:. i) Let openTSNE do its own thing and ignore the kNN graph built in scanpy. Advantage: that's what you do now. Disadvantage: not very consistent architecture IMHO. . ii) Use the kNN graph built in scanpy and query() it to get 90 neighbors. Disadvantage: can be a bit slow. But I think it's better than (i). iii) Run t-SNE using 15 neighbors. Turns out, t-SNE with uniform affinities across 15 neigbours is *extremely* similar to t-SNE with perplexity 30. Evidence: https://twitter.com/hippopedoid/status/1232698023253303298. So you could run this version of t-SNE with uniform kernel. This will be very fast. Regarding default parameters: learning rate = 1000 that you use by default is simply not enough for large data (sample size in millions), as shown in that Nat Comms paper in detail. If you want to keep it for compatibility reasons, that's your choice, but be aware that you are getting suboptimal t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833:581,flexible,flexible,581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-633735833,1,['flexible'],['flexible']
Modifiability,"> I'm not sure I know what the pytest execution model is like, but does it ever start new processes for different tests?. Apparently [it needs a plugin](https://pypi.org/project/pytest-parallel/) for that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199:145,plugin,plugin,145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/724#issuecomment-513132199,1,['plugin'],['plugin']
Modifiability,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:846,flexible,flexible,846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['flexible'],['flexible']
Modifiability,"> I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:540,variab,variable,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499,1,['variab'],['variable']
Modifiability,"> I've observed that it may introduce unexpected changes (not shown here) in the shape of the distribution that is different from all of the other transforms mentioned, so these need to be validated biologically against some ""ground truth"". Here's some selected examples (skipping the raw and geometric mean for reasons stated earlier) of the additional aspect introduced by CLR, beyond linearization of the signal, which illustrate how one might want to decide on a case by case basis which is biologically true:. Some *potential* artifacts:; - discreteness at low values (reflected in the histograms earlier), and a ""kink"" near there in the contour that doesn't match with a 2D-gaussian; - skewing of the ""absence"" of a marker depending on presence of another marker; - a weird double-positive tail that extends along the diagonal. These types of effects are reminiscent of [flow cytometry artifacts](https://docs.flowjo.com/flowjo/graphs-and-gating/gw-transform-overview/gw-transform-digital/). However, without proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:806,extend,extends,806,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['extend'],['extends']
Modifiability,"> If I understand the .raw removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with .layers?. Pretty much every function where you would want to use `highly_variable`. > It seems to me that adding masking like this would be quite a large endeavour, no?. I think a similarly sized endeavor to adding `highly_variable`, except we can use the `highly_variable` code where it's been implemented. I would expect this to be less effort than supporting `raw`, which is a constant maintenance burden, especially for `anndata`. I think this logic could be added to the `_get_obs_rep`, and `_set_obs_rep` functions. --------------. > If you assume anything filtered out was removed because it was predominantly 0. I'm not sure I like having this assumption. Especially when a collaborator asks ""what about gene X"", but it just wasn't in the table I received. Maybe it's an annotation issue, maybe it wasn't expressed, or maybe it wasn't expressed globally at a high enough level â€“ but could have been expressed in the cells of interest. > you can assume it would not be in the HVG intersection for that dataset and if you add it,. Is intersection the way to go? If you have cell types which are only present in some datasets, wouldn't you want to take the union?. > Typically there is sufficient gene-gene covariance that you still keep this signal somehow. I would agree that it is unlikely that this would have a huge effect on analyses like PCA or UMAP. When it comes time to do differential expression or show expression on an embedding, then it starts to be an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305:150,layers,layers,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822937305,1,['layers'],['layers']
Modifiability,"> In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?. Just the data will be only scaled by stds, the means wouldn't be subtracted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921:100,variab,variables,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1370694921,1,['variab'],['variables']
Modifiability,"> Is there a widely used processing pipeline which does not adhere to this file naming?. STARsolo generates cell-ranger compatible output, and when multiple multi-mapper resolution strategies are enabled, it will write multiple matrix.mtx.gz files, with different names. e.g: `STARsolo ... --soloMultiMappers Unique EM PropUnique Rescue Uniform` yields:. ```; barcodes.tsv.gz; features.tsv.gz; matrix.mtx.gz; UniqueAndMult-EM.mtx.gz; UniqueAndMult-PropUnique.mtx.gz; UniqueAndMult-Rescue.mtx.gz; UniqueAndMult-Uniform.mtx.gz; ```. Each of these `*.mtx.gz` files matches the same format as `matrix.mtx.gz` and can be read in the same way. (They all share the `*.tsv.gz` files). . A 3-parameter version of the `read_10x_mtx()` function would be my vote as the most flexible option.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593:763,flexible,flexible,763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-2002523593,1,['flexible'],['flexible']
Modifiability,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:501,variab,variables,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980,1,['variab'],['variables']
Modifiability,"> It looks like running adata.raw.to_adata() does not preserve the var table, so that won't work either:. That makes sense though right? Since it's a different set of variables. That said, I do suspect there is some weirdness about handling of `use_raw` here that doesn't actually make sense when you think about it, but could break some code to remove.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470:167,variab,variables,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964290470,1,['variab'],['variables']
Modifiability,"> Iâ€™m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot!; > ; > @davidsebfischer itâ€™s maintained, right?; > ; > Iâ€™m going to close this unless Iâ€™m wrong and we want to enhance `rank_genes_groups` after all. Hi Philipp, I will try diffxpy. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640417629:298,enhance,enhance,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640417629,1,['enhance'],['enhance']
Modifiability,"> Letâ€™s pay attention to not include any new function without * anymore, OK?. OK!. > Oh itâ€™s definitely OK! I meant that it needs a bit of imagination to put other things in there than actual gene symbols, but I feel like itâ€™s faster to see!. Yes, but I hardly imagine that you need two layers of annotations for variables outside the specific case of ENSids and gene symbols. So it's good to be verbose about this specific case and people wont miss such functionality in other cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874:287,layers,layers,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/376#issuecomment-441598874,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"> Looks good, thanks for all the work!; > ; > We should add a release note for this at some point, I'm just not sure where yet, probably a section for dev practices. Could you suggest a line for that?; > ; > I was unsure about the variable naming for PAGA, so I've decided to revert that. I couldn't get flake8 to call it a redefinition. :tada: ; Maybe ""Enabled flake8 (https://flake8.pycqa.org/en/latest/) pre-commit to run code style checks""?; Everything else might just be details that people will uncover anyways since the workflows might complain :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494:231,variab,variable,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-801763494,1,['variab'],['variable']
Modifiability,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:110,refactor,refactor,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,1,['refactor'],['refactor']
Modifiability,"> My opinion would be that you need to write `adata.raw = adata.copy()` if you want a copy to be made, since almost all assignments do not create a copy of the assigned object in anndata. But we should look into whether this is a change that was made deliberately or not. That makes python-sense. This is absolutely a change in convention though, see:. 1. The [original scanpy tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html); 2. The [scVI tutorial](https://docs.scvi-tools.org/en/0.6.8/tutorials/scanpy.html) (where they discuss needing to retain counts in raw); 3. Most notably in the [anndata API](https://anndata.readthedocs.io/en/latest/generated/anndata.AnnData.raw.html). In addition, both sc.pl.umap and sc.pl.paga_path() come to mind as functions that default to using the .raw layer. > If we don't change it, we could maybe warn if we're mutating `adata.X` and `adata.raw.X` also refers to the same thing?. I think that's a good idea. In general, it would be _very_ helpful to preserve in the anndata structure some record of the major transformations to .X (or any layer). > Overall, I would recommend that you use `adata.layers[""counts""] = adata.X.copy()` instead of using `.raw` at all though. This seems like good practice and the workaround we'll apply for now. I do wonder if some change was made after [this conversation](https://github.com/scverse/scanpy/issues/1798) which you were a part of. Thank you by the way, this package is an amazing tool.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2151178325:1157,layers,layers,1157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2151178325,1,['layers'],['layers']
Modifiability,"> Oh, then you didnâ€™t hear of type theory. Itâ€™s a branch of logic:. Indeed, I have never heard of that. But I doubt that it would be considered a branch of logic. 1. `Union type` is a pretty bad descriptor for a variable that can take _one_ of a set of fixed types. A union usually denotes a composition of multiple sets giving rise to a new set that contains all elements from these sets.; 2. `Subtype` is a great descriptor for a type that has properties of supertypes.; 3. `Intersection type` is an insanely bad descriptor for a variable that denotes the intersection of _properties_ of supertypes; the concept of such a subtype might be something useful in some languages and some cases and it might deserve a special name as it's the converse behavior of subclassing. But I have no idea how such a type would be useful in Python and in all cases that I've encountered. The [example on Wikipedia](https://en.wikipedia.org/wiki/Type_system#Intersection_types) already constructs a highly artificial case, whose relevance is opaque to me even though Scanpy features it in many instances: functions that overload parameters and have different overloading-dependent return types (standard example is passing an array instead of an AnnData, which triggers the automatic return of the computed annotation). What do you think about 3, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359:212,variab,variable,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443072359,2,['variab'],['variable']
Modifiability,"> Or should we call it col_groups as you did in your sc.pl.heatmap pseudo code?. That could be up to you. It depends on what the user is trying to achieve, which makes more sense. For instance, I'm not sure if it makes sense to allow splitting the columns by both variables and groups, or if that's the wrong abstraction. > I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions are getting really complex- it took me some time to understand _dotplot and _baseplot :). This code could definitely be a lot more simple. Would definitely appreciate help here! I think some of the concepts used in `seaborn` could be quite useful here, though it looks like they're under heavy refactoring at the moment ([relevant seaborn branch](https://github.com/mwaskom/seaborn/tree/skunkworks/features)). Maybe a good first step would be to fix how so the dotplot would look right if the user provides the dot size and dot color dataframes? Would make these plots possible, and gives an interface to try later approaches with.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524:264,variab,variables,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988956524,2,"['refactor', 'variab']","['refactoring', 'variables']"
Modifiability,"> Scipy is actually under ~/.cache on my mac, Â¯\\_(ãƒ„)_/Â¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:779,config,configs,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['config'],"['config', 'configs']"
Modifiability,"> So I guess basic difference is that this one can convert starfish to anndata ""online"" without having to read from disk the anndata object. Sure. I was also wondering why use literal values here instead of the constant variables that starfish uses (e.g. `""cells""` vs. `Features.CELLS`). > this could also be fixed by sending a PR there and modifying what is saved in anndata of the expression matrix. I like the idea of having just one version of conversion, instead of them implementing a `to_anndata` method and us implementing an independent `from_starfish` method. Are either of you already in communication with the starfish developers? . Additionally, it seems like having a `to_anndata` method would be a pretty trivial modification of their code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754:220,variab,variables,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1362#issuecomment-671896754,1,['variab'],['variables']
Modifiability,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of [â€¦]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:629,config,configured,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['config'],['configured']
Modifiability,"> Splitting off new modules; Finally, the idea of having IO functions go into their own package. I think this is a much bigger change, and I'd like to see a more fleshed out case for it. This would add a fair bit of complexity to development, so I'd want to be sure it's worth it. Some general questions I have:. > What are the advantages/ disadvantages of having smaller sub-packages?. method developer would just depend on those instead of (multiple) analysis package. > How does this impact users vs. developers?. user none, as the analysis package would ofc have the IO as dep. developer would be impacted by a leaner dep tree. > Is IO special, or should more parts go into sub-packages?. it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). > What gets re-exported from ""main"" modules?. didn't get this sorry. > Who manages the sub-packages?. the IO subpackage? everyone ðŸ˜… . so beside being in favour, it might also be that other issues arise. For instance, for modality-specific formats we'd have to rely on specific external libraries which would then have to be lazily imported (as pointed out before). Would this create the premise of exponential growing of modality-specific lazy import libraries? probably yes. Is this best practice? I don't know.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815:900,refactor,refactoring,900,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059346815,1,['refactor'],['refactoring']
Modifiability,"> Thanks for the update. Now is clear.; > ; > We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want.; > ; > Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?. Thanks for your attention. Yes it would be nice if I could compare two .obs categories with regard to expression distributions of a list of genes I supply. . Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433:312,variab,variable,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707563433,1,['variab'],['variable']
Modifiability,"> That's a great idea. It might require some reorganization, though, because currently use_raw is checked two places: once in sc.pl.scatter(), because it needs to know whether to look for variables in raw or not when deciding how to call _scatter_obs(), and again in _scatter_obs() itself. Would it be possible to not call it again in `scatter_obs`? E.g. could `_scatter_obs` not even need to know about the `raw` field?. > On another note, some pytests that are in files I did not edit are now failing because they can't find anndata.tests to import. I'm not sure if I messed something up by adding tests to test_plotting.py or whether this is a different issue. Aww crap, I think that was me making a new release. On the plus side it means our build system is now working as it's supposed to.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124:188,variab,variables,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964279124,1,['variab'],['variables']
Modifiability,"> This is not a pleasant way to navigate files:. <details>; <summary>You can fix that with the .ignore plugin!</summary>. Right click the Project view and select â€œHide Ignored Filesâ€. ![grafik](https://user-images.githubusercontent.com/291575/113685021-1c743180-96c6-11eb-8f73-eb7630857009.png). </details>. > I would be happy to have docs.scanpy.org/en/latest/api/dotplot.html be the canonical url, but not if it requires mixing generated and source files. That would be perfect if itâ€™s possible!. > Since (AFAIK) the generated html page has to have the same name as the rst file, this would break links (as you mentioned above). While we could do a redirect from here, what do we redirect too?. We create manual redirects for the 2 APIs where we failed to add an underscore to the function name, and donâ€™t do that again. The non-domain parts of URLs *are* case sensitive, therefore having a redirect `.../dotplot.html` â†’ `.../dot_plot.html` works perfectly fine in the presence of `.../DotPlot.html`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380:103,plugin,plugin,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1753#issuecomment-813954380,1,['plugin'],['plugin']
Modifiability,"> We have original radius dimension but it can be handy to modify it according to cropping/zooming, or simply for visualization purposes. Cropping/zooming wonâ€™t make a difference if you plot circles in data space. So thereâ€™s our problem: We have the original radius in data space, but youâ€™re plotting markers, whose size is in figure space (i.e. their center position in the final figure is determined and then theyâ€™re plotted as circles right into the graphic). So you need to switch from `ax.scatter` to a `circles` function that does what we need: https://stackoverflow.com/questions/9081553/python-scatter-plot-size-and-style-of-the-marker/24567352#24567352. We can just adapt that one (throw out what we donâ€™t need), make it so the `scatter(...)` calls in â€œembeddingâ€ work with it, and do `scatter = ax.scatter if img_key is None else partial(circles, ax=ax)`. This means that we donâ€™t have to do difficult math when cropping/zooming, as the spots will always just be the correct size. We can also get rid of `spot_size` and make `size` a scale factor in the image case (1=normal size, 0.8=slightly smaller than in the data, 1.2=slightly larger than in the data)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894:675,adapt,adapt,675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-580144894,1,['adapt'],['adapt']
Modifiability,"> Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Yes, now that you made everything a property, I would have expected it to take much longer than 10 minutes. It's great that you did!. I'll make some tiny additions and merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479736498:53,variab,variables,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479736498,1,['variab'],['variables']
Modifiability,"> What I noticed was that if I didn't have the same ID columns in my adata.var when setting adata.raw I couldn't use gene_symbols. Ah, if you're using the values in `raw` for differential expression the column used for `gene_symbols` should be in `raw.var` this is because `raw` can have a different set of variables than the main object. So this case, at least, is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446:307,variab,variables,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1758#issuecomment-816352446,1,['variab'],['variables']
Modifiability,"> What do you think of using the | separator to describe adata.X | adata.layers[layer] e.g. [here](https://icb-scanpy--2742.com.readthedocs.build/en/2742/generated/scanpy.pp.regress_out.html)?. works for me!. > the inconsistent and mixed use of inplace and copy (effort: lot of work). yeah, we definitely need to do this â€¦ in a dedicated PR. > Might raise smaller issues in the future for these specific things rather than bloating this purpose-driven PR up?. Yeah! please add them here: https://github.com/orgs/scverse/projects/18/views/1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814024532:73,layers,layers,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814024532,1,['layers'],['layers']
Modifiability,"> Why did we ever configure black?. because Alex liked `'` more. I think at the time we didnâ€™t know of `pre-commit-hooks`â€™ `double-quote-string-fixer`, or it didnâ€˜t exist yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701#issuecomment-1772240411:18,config,configure,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701#issuecomment-1772240411,1,['config'],['configure']
Modifiability,"> Why is this PR getting a build if there is no `pr` trigger entry in the yaml?. See 3 paragraphs down:. > If no pr triggers appear in your YAML file, pull request validations are automatically enabled for all branches, as if you wrote the following pr trigger. This configuration triggers a build when any pull request is created, and when commits come into the source branch of any active pull request.; > ; > ```; > pr:; > branches:; > include:; > - '*' # must quote since ""*"" is a YAML reserved character; we want a string; > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275:267,config,configuration,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737862275,1,['config'],['configuration']
Modifiability,"> Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py; > ; > Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future. I have the data for 120 clusters, is it possible that I can have 120 colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-496532324:192,extend,extend,192,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-496532324,1,['extend'],['extend']
Modifiability,"> adata.uns['log1p'][""base""] = None. Thank you. I also had this error when calculating highly variable genes `sc.pp.highly_variable_genes(Adult,batch_key='batch')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1184556213:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1184556213,1,['variab'],['variable']
Modifiability,> copy paste. please never say those words when speaking about code again :stuck_out_tongue_winking_eye: . No but seriously: Thereâ€™s at least 6 reasons not to do that and to introduce a second (temporary) variable instead: https://github.com/theislab/scanpy/pull/557#issuecomment-476512533,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476513549:205,variab,variable,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476513549,1,['variab'],['variable']
Modifiability,"> if we imagine a notebook with e.g. five sc.pl.dotplot calls. To me, italicizing the genes is more of a ""customizing plots for publication"" than ""looking at data"", so I think it's okay for it to have some extra steps as it happens less often. I personally have used illustrator for this kind of step. Alternatively, I tend to have functions like:. ```python; def customized_dotplot(adata, ..., dotplot_kwargs={}):; ... # Preprocess data, maybe add some fields; ret = sc.pl.dotplot(adata, show=False, ..., **dotplot_kwargs); ... # do additional things; ```. That said, I'm not against also having a global config for it, I would just want to see the structure for handling this global config for extra setting options sketched out in more detail. Also a PR implementing it ðŸ˜‰. > Last suggestion is like theme(axis_text_x=element_text(angle=90, hjust=1)). I do think it would make sense to have this outside of `set_figure_params`. `set_figure_params` does some global setting through the matplotlib config. Were you thinking of this as `DotPlot(...).theme(...)` or `sc.settings.theme(...)`? Maybe both, with the method overriding the config?. 1. I don't really like that `set_figure_params` modifies plots not generated by scanpy; 2. matplotlib doesn't have a concept of the ""variable axis"", so we'd have to handle this ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647:606,config,config,606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873735647,5,"['config', 'variab']","['config', 'variable']"
Modifiability,"> it kind of is imho, it's all about having whatever data there is in an anndata/mudata shape. I must say that I'd also think plotting could be it's own separate package but it would probably require a lot of refactoring across packages (thinking about duplication of scanpy/scvelo code). @giovp and I are synced on these ideas :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278:209,refactor,refactoring,209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059349278,1,['refactor'],['refactoring']
Modifiability,"> like pip install .[dev,test$(test_extras))], and run things once with test_extras='' and once with test_extras=',leiden,magic,harmony,scrublet,scanorama,skmisc'. Yeah, I was thinking something like this. Except we could just reduce `test` to include the barebones needed to make tests run, and separately have optional dependencies. The hard part here is structuring the tests so they can run without optional dependencies being present. We'd need to establish patterns for optional dependencies in fixtures and parameterized tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539:514,parameteriz,parameterized,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088721539,1,['parameteriz'],['parameterized']
Modifiability,"> ooh, this time the benchmark shows really nicely how much faster it is!. Looks like preprocessing_log.time_regress_out('pbmc68k_reduced') , regress out those variables that is not inside it. It should regress_out ['n_counts', 'percent_mito'] instead of [""total_counts"", ""pct_counts_mt""]. For the both commit it fails so report the same time.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2177815754:160,variab,variables,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2177815754,1,['variab'],['variables']
Modifiability,"> scvelo depends on scanpy>=1.5, so I don't think this is the cause. Do you know why this is happening, and can you provide counter examples where it doesn't happen?. Ah weird. Pip tries to resolve the dependencies, and for that purpose gets all the installed packagesâ€™ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees â€œ1.7.0rc2â€ and decides â€œIâ€™ll update this even when not asked to updateâ€. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, thatâ€™s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since weâ€™re past 1.7 now, unless you havenâ€™t git-pulled yet, I assume your dev installâ€™s metadata has gone stale. I guess pip wouldnâ€™t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts sp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:307,config,configuration,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['config'],['configuration']
Modifiability,"> sparse_indicator doesnâ€™t have its weights branches hit at all, maybe we should remove that? Or will this be used at some point?. I think it will be used at some point, but also happy to remove. I think parameterizing `test_aggregate_axis_specification` is overkill for what the test does.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1953840990:204,parameteriz,parameterizing,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1953840990,1,['parameteriz'],['parameterizing']
Modifiability,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:409,variab,variables,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,1,['variab'],['variables']
Modifiability,"> there is still a large amount of changes to the dataframe code here. Not really changes: itâ€™s almost all refactoring, because the code was spaghetti with quite some duplication. Iâ€™m doing nothing more than. 1. I introduce helper functions so code gets more readable, e.g. a clean `disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)` instead of a large inline code block that has to be decyphered line by line to figure out that it finds the nth highest value. This is especially necessary for the huge main pile of spaghetti that used to be the `if flavor == ""seurat"":`/`elif flavor == ""cell_ranger"":` branches. I simply put their contents into a `_get_mean_bins` helper and two helpers `_stats_seurat` and `_stats_cell_ranger` (while deduplicating shared code); 2. Making sure pandas indices match up while removing `.to_numpy()`. That way instead of having `.to_numpy()` potentially copy and and convert data in extension arrays, the series are just used directly. Not to mention that three `.to_numpy()` per line make things hard to read.; 3. refactor the 5 cutoff parameters into one value `cutoff` for clarity, less inline code, and better type information (we either have either `n_top_genes: int` or a `_Cutoffs` instance, never both. This way, the type system knows). and thatâ€™s it. <ins>potentially</ins> faster, much more maintainable, and almost dask-compatible. After my changes, it should be easier to further refactor things so the seurat_v3 flavor is integrated into the overall structure instead of doing its own thing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140:107,refactor,refactoring,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2809#issuecomment-1929321140,4,"['maintainab', 'refactor']","['maintainable', 'refactor', 'refactoring']"
Modifiability,"> this is not a test of `filter_rank_genes_groups`. To me the point of the test is that `filter_rank_genes_groups` followed by `.pl.rank_genes_groups_*` returns the right image by comparing it to a reference. I mainly prefer that the reference is generated by as orthogonal a method as possible, since then we're closer to testing functionality than implementation. I also think that this strategy forms a more flexible implementation that can be extended to test other properties. For example, it's easy to modify this to check that the `sc.pl.rank_genes_groups_*(..., min_logfoldchange=)` argument plays well with `filter_rank_genes_groups`. I also like having an example of an equivalent implementation in the test suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601:411,flexible,flexible,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1942#issuecomment-880411601,2,"['extend', 'flexible']","['extended', 'flexible']"
Modifiability,">=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package joblib conflicts for:; scanpy -> joblib; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package tk conflicts for:; python=3.7 -> tk[version='>=8.6.7,<8.7.0a0|>=8.6.8,<8.7.0a0']; P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:11199,flexible,flexible,11199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['flexible'],['flexible']
Modifiability,"@Koncopd Currently breaking test for me:. ```pytb; $ pytest -k test_ingest; ===================================================== test session starts =====================================================; platform darwin -- Python 3.7.6, pytest-5.3.5, py-1.8.0, pluggy-0.12.0; rootdir: /Users/isaac/github/scanpy, inifile: pytest.ini, testpaths: scanpy/tests/; plugins: pylama-7.7.1, parallel-0.0.10, cov-2.7.1, black-0.3.7, hypothesis-5.6.0; collected 393 items / 389 deselected / 4 skipped . scanpy/tests/test_ingest.py ...F [100%]. ========================================================== FAILURES ===========================================================; _______________________________________________ test_ingest_map_embedding_umap ________________________________________________. def test_ingest_map_embedding_umap():; adata_ref = sc.AnnData(X); adata_new = sc.AnnData(T); ; sc.pp.neighbors(; adata_ref, method='umap', use_rep='X', n_neighbors=4, random_state=0; ); sc.tl.umap(adata_ref, random_state=0); ; ing = sc.tl.Ingest(adata_ref); ing.fit(adata_new); ing.map_embedding(method='umap'); ; reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4); reducer.fit(X); umap_transformed_t = reducer.transform(T); ; > assert np.allclose(ing._obsm['X_umap'], umap_transformed_t); E assert False; E + where False = <function allclose at 0x119616b00>(array([[16.566338, 20.174282],\n [15.368203, 20.291983]], dtype=float32), array([[16.502459, 20.157679],\n [15.581459, 20.302881]], dtype=float32)); E + where <function allclose at 0x119616b00> = np.allclose. scanpy/tests/test_ingest.py:140: AssertionError; ---------------------------------------------------- Captured stderr call -----------------------------------------------------; computing neighbors; finished: added to `.uns['neighbors']`; 'distances', distances for each pair of neighbors; 'connectivities', weighted adjacency matrix (0:00:00); computing UMAP; finished: added; 'X_umap', UMAP coordinates (adata.obsm) (0:00:00); ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073:361,plugin,plugins,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1036#issuecomment-599469073,1,['plugin'],['plugins']
Modifiability,"@Koncopd has looked at refactoring the `rank_genes_groups` methods, but in the big picture we don't really love the output format that `rank_genes_groups` uses. Maybe an easier path forward would be to be able to directly pass values into the various plotting functions? You can already generate mostly similar plots from `sc.pl.rank_genes_groups_{plot_func}` and `sc.pl.{plot_func}` apart from using logfc and pvalues. If we allowed passing those in, it would be simple enough to make the same plots/ add a wrapper that generates the plots into `diffxpy`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954:23,refactor,refactoring,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1955#issuecomment-886408954,1,['refactor'],['refactoring']
Modifiability,@Koncopd it is a correlation between two continuous variables as celltypes are continuous and age is also continuous. how to correlate X with continuous variables stored in .obs ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263:52,variab,variables,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848118263,2,['variab'],['variables']
Modifiability,"@Koncopd pre-commit doesnâ€™t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess itâ€™s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort!. Also we should get #1527 in before doing any big restructuring: Itâ€™s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537:41,config,configured,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757907537,1,['config'],['configured']
Modifiability,"@LisaSikkema, current behavior just changes the groups which are tested (I'd call this the ""left hand side"" in `group vs reference`) not what they are tested against. That is controlled by the `reference` argument. I agree this could be more clear. It would also be nice if `reference` was more flexible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259:295,flexible,flexible,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-743963259,1,['flexible'],['flexible']
Modifiability,"@LuckyMD . I can replicate that with:. ```python; import scanpy as sc; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.write(""tmp.h5ad""); fromdisk = sc.read(""tmp.h5ad"") # Do we read okay; fromdisk.write(pbmc) # Can we round trip; ```. Some context around this, and my current thinking on a solution:. * h5py doesn't do fixed length unicode strings; * h5py does do variable length unicode strings, pretty much anywhere; * zarr doesn't do variable length strings in structured arrays; * We probably don't actually want to use fixed length unicode strings much. Bytestrings, more likely.; * We can probably just add another element type to allow special handling for these. I think it'd be fine to not do `np.str_` type arrays.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438:359,variab,variable,359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545345438,2,['variab'],['variable']
Modifiability,@LuckyMD . Thank you for the whole in-depth discussion. It makes a lot of sense! :smile:. To your question: Scanpy has used Welch's adaption of Student's t-test from the very beginning. @davidsebfischer . Thank you! I guess it would be nice to have a single-cell tutorial in diffxpy that shows higher sensitivity by accounting for technical covariates.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261:132,adapt,adaption,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-450025261,1,['adapt'],['adaption']
Modifiability,@LuckyMD Can you maybe check if the new `highly_variable_nbatches` variable in `adata.var` is useful for finding the intersections? . `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == len(adata.obs.batch.cat.categories)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614#issuecomment-486188826:67,variab,variable,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614#issuecomment-486188826,1,['variab'],['variable']
Modifiability,@LuckyMD should be easy to extend the `Plot` class to add this. I will think about it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910:27,extend,extend,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607236910,1,['extend'],['extend']
Modifiability,"@SamueleSoraggi you are right, the layers contain the same genes as the adata.X matrix. I assume that in your case, you did a highly variable gene selection which affects both adata.X and adata.layers but not adata.raw. The solution is to mark highly variable genes without removing the other less variable genes. This functionality was added some few months ago and may not be properly reflected on the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-459668085:35,layers,layers,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-459668085,5,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"@VolkerBergen on the top of my head here are some features that were added recently:; * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary); * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` ; * support for layers; * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ); * added the option to outline the plot using the parameter `add_outline` #794 ; * added vmin and vmax as percentiles #794 ; * when using the `groups` parameter, plot those groups on top #891",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-555927399:429,layers,layers,429,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-555927399,1,['layers'],['layers']
Modifiability,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:78,variab,variable,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157,1,['variab'],['variable']
Modifiability,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks!. few questions before opening the PR:; - do you think it's worth it to allow users to add other variables (beside log_umi) ?; - do you think it would be useful to add other models other than poisson?; - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them?; - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730:219,variab,variables,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-783547730,2,['variab'],"['variable', 'variables']"
Modifiability,"@biskra Just my two cents on (3), I mostly use histograms or hex bins for viewing qc metrics. I've been meaning to write this up as a tutorial for a while, but I think you can lean on whole pyviz ecosystem pretty heavily here. Something static and pretty:. ```python; # Joint distribution with marginal histograms:; sns.jointplot(; x=""log1p_total_counts"",; y=""log1p_n_genes_by_counts"",; data=adata.obs,; kind=""hex""; ). # For a single variable; sns.distplot(adata.obs[""log1p_total_counts""]). # To facet the distplot by some columns of obs, like ""batch"":; g = sns.FacetGrid(adata.obs, row=""batch"", height=1.7, aspect=3) ; g.map(sns.distplot, ""pct_counts_mito"") # Sadly, this doesn't work for joint grids; # I do have a PR for a ridge plot based on this in the works; ```. If you'd like it to be interactive:. ```python; import hvplot.pandas. adata.obs.hvplot.hexbin(""log1p_total_counts"", ""log1p_n_genes_by_counts""); adata.obs.hvplot.hist(""log1p_total_counts""). # Faceting; adata.obs.hvplot.hist(""pct_counts_mito"", by=""batch"", subplots=True).cols(1); ```. Or, if you want to play with cool most-biggest-data toys:. ```python; adata.obs.hvplot.scatter(; ""log1p_total_counts"",; ""log1p_n_genes_by_counts"",; datashade=True,; dynspread=True; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-473506882:434,variab,variable,434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-473506882,1,['variab'],['variable']
Modifiability,"@carmensandoval, there's no implicit copying so one should make a copy explicitly. . Please see the following code for more details:. ```py; import numpy as np; from anndata import AnnData; from jax import random. adata = AnnData(X=np.array(random.normal(random.PRNGKey(0), (100, 10)))). print(id(adata.X)); # => 5393766064. adata.layers[""X""] = adata.X; adata.layers[""X_copy""] = adata.X.copy(). for layer in (""X"", ""X_copy""):; print(f""{layer}: "", id(adata.layers[layer])); # => X: 5393766064; # => X_copy: 5393773552. print(adata.X[0, 0]); # => -1.5721827. adata.X[0, 0] = 0.0; for layer in (""X"", ""X_copy""):; print(f""{layer}: "", adata.layers[layer][0, 0]); # => X: 0.0; # => X_copy: -1.5721827; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413733462:331,layers,layers,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413733462,4,['layers'],['layers']
Modifiability,"@cdpolt, is there are specific change (""new behavior"") you're referring to?. > Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed . Would the code in the previous message be helpful to understand why that happens and how to fix that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186:97,layers,layers,97,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2071131186,2,['layers'],['layers']
Modifiability,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877:186,variab,variability,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877,1,['variab'],['variability']
Modifiability,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:987,variab,variability,987,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073,4,['variab'],['variability']
Modifiability,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:170,layers,layers,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114,1,['layers'],['layers']
Modifiability,"@dawe What if you use the 'batch' field in `adata.obs` together with the index that might contain duplicates to get the uniqueness that your're missing. If that's not enough. I would do the same thing as [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), namely generating a multi-index. This would be just a one-line edit for `AnnData.concatenate()` and we should do this; we simply need to make sure that the indexing behavior of AnnData as a whole remains consistent with the following. @flying-sheep, could you do this? ; ```; import pandas as pd; s1 = pd.DataFrame({'v1': ['a', 'b'], 'v2': [2, 3]}); s2 = pd.DataFrame({'v1': ['c', 'd'], 'v2': [3, 4]}); s = pd.concat([s1, s2], keys=['s1', 's2']); print('... the concatenated annotations'); print(s); print('... all observations corresponding to ""s1""'); print(s.loc['s1']); print('... a single observation'); print(s.loc['s1', 0]); print('... values of a single observation'); # this is what we do not want in AnnData, only the behavior of the next line; # that is, a multi-index should be indexed with lists or tuples; print(s.loc['s1', 0].values); print('... single observation and a single variable'); print(s.loc[['s1', 0], 'v1']); print('... single observation and a single variable'); print(s.loc[('s1', 0), 'v1']); ```; gives; ```; ... the concatenated annotations; v1 v2; s1 0 a 2; 1 b 3; s2 0 c 3; 1 d 4; ... all observations corresponding to ""s1""; v1 v2; 0 a 2; 1 b 3; ... a single observation; v1 a; v2 2; Name: (s1, 0), dtype: object; ... values of a single observation; ['a' 2]; ... single observation and a single variable; s1 0 a; 1 b; Name: v1, dtype: object; ... single observation and a single variable; a; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240:1188,variab,variable,1188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55#issuecomment-354903240,4,['variab'],['variable']
Modifiability,"@fabianrost84 The 'layers' were not added to anndata until recently and had not been implemented in all functions. However, adding this functionality to the different scatter plot functions is straightforward. Let me prepare a quick PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476095057:19,layers,layers,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476095057,1,['layers'],['layers']
Modifiability,@falexwolf @fidelram how to we change the color palette for numerical variables? currently setting `palette = 'Oranges'` only works for the categorical ones,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-441815667:70,variab,variables,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441815667,1,['variab'],['variables']
Modifiability,"@falexwolf I added `_utils_clustering.py` since I think it's the more maintainable way to do it (e.g. if in the future, new clustering methods are added).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-480280193:70,maintainab,maintainable,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-480280193,1,['maintainab'],['maintainable']
Modifiability,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827:49,plugin,plugin,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423915827,4,['plugin'],"['plugin', 'plugins']"
Modifiability,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:1019,variab,variability,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,1,['variab'],['variability']
Modifiability,"@falexwolf any update on the plug-ins idea for scanpy?. On Sun, Feb 3, 2019 at 4:50 PM Alex Wolf <notifications@github.com> wrote:. > Merged #457 <https://github.com/theislab/scanpy/pull/457> into master.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/457#event-2114366554>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1auPPM2VVhh2E_5Gwd8djTqP9ltAks5vJwVLgaJpZM4agJQ1>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532:29,plug-in,plug-ins,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/457#issuecomment-460063532,1,['plug-in'],['plug-ins']
Modifiability,"@falexwolf thanks for the feedback. :). I agree with your comments on the `sc.tl.dendrogram`. Similar reasoning originally motivated me to separate and expose the implementation of the function. I expect that now, is easier to extend the creation of a correlation matrix to other methods and groupings as you suggest. Currently, by default `sc.tl.dendrogram` uses PCA by recycling the function used by `sc.tl.neighbors` (`tools._utils.choose_representation()`). Any other embedding in `.obsm` can be used (as is the case by `sc.tl.neighbors`. Also, any group of genes can be given as parameter . What tl.dendrogram does not do is to use the underlying network to compute a distance matrix as I think seurat does and apparently you also do in PAGA. . For me, what is important is that the plotting functions get the dendrogram data from `.uns` and thus the generation of the hierarchical clustering is separated and can be computed by any other method.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730:227,extend,extend,227,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456065730,1,['extend'],['extend']
Modifiability,"@falexwolf we just tried the solution you posted and it reveals a bug: when `ax` is not `None` you don't create the variable `axs` and thus throw an error here: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/anndata.py#L634. Should be a simple fix (I think):. ```python; if ax is None:; axs, _, _, _ = setup_axes(ax=ax, panels=['x'] if groupby is None else keys, show_ticks=True, right_margin=0.3); else:; axs = [ax]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154:116,variab,variable,116,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/137#issuecomment-413354154,1,['variab'],['variable']
Modifiability,"@fidelram Thanks a lot! I was getting confused why my dot sizes weren't working (only sometimes, strangely...). If the dot sizes are sorted according to the `color` variable it should hopefully work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543#issuecomment-475287691:165,variab,variable,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543#issuecomment-475287691,1,['variab'],['variable']
Modifiability,"@fidelram Thanks, no further changes. I agree about the arbitrariness but I think it's good to provide a configurable implementation with reasonable defaults. This, I believe, is also what we do in other functions like pp.neighbors and pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/538#issuecomment-474786478:105,config,configurable,105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/538#issuecomment-474786478,1,['config'],['configurable']
Modifiability,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:101,plugin,plugins,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['plugin'],['plugins']
Modifiability,"@fidelram, I really like your plotting gallery! Would be cool to have that as part of the tutorials or even integrated in the main documentation (enhance each plotting function with an example image?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177:146,enhance,enhance,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369#issuecomment-441069177,1,['enhance'],['enhance']
Modifiability,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` â†’ `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:512,adapt,adapting,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['adapt'],['adapting']
Modifiability,"@flying-sheep That does look useful, though I definitely see a lot of toolkits using something like `~/.{toolkitname}` on my mac. Personally, I'd look under my home directory first. I think if things are going to move, there should be a sensible default, but the location should be user configurable (probably through an environment variable). For example, the main hpc I'm on only gives me about 1gb of space where `appdirs` would put these files. @maximilianh Currently I don't get coordinates along with some other computed values like SC3 clustering. I also believe it's only a subset of the metadata. I think there's more through the array express api, but that was more of a pain to navigate. Was there anything in particular you wanted to see included?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476555082:287,config,configurable,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476555082,2,"['config', 'variab']","['configurable', 'variable']"
Modifiability,"@flying-sheep To answer your question. Honestly, I am not so familiar with Plotnine, Plotly or Altair. However, after a quick revision I would say that Altair seems quite interesting and possibly were I could had reused/extended some code. Yet, at the moment in scanpy we use matplotlib extensively and I didn't even think about the other APIs. Looking closely at Altair I realized that I have a lot to catch up regarding Vega, Vega-lite and the idiosyncrasies specific to Altair before I could start using it. . Thus, the current effort only integrates the idea of 'chaining' seen in Altair (or in other context in Pandas). In Plotly or Plotnine the 'chaining' is achieved differently but I don't find it as nice or straightforward:. **Plotly:**; ```PYTHON; import plotly.graph_objects as go; fig = go.Figure(; data=[go.Bar(x=[1, 2, 3], y=[1, 3, 2])],; layout=go.Layout(; title=go.layout.Title(text=""A Bar Chart""); ); ); fig.show(); ```; **Plotnine:**; ```PYTHON. from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap; from plotnine.data import mtcars. (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')); + geom_point(); + stat_smooth(method='lm'); + facet_wrap('~gear')); ```; **Altair:**; ```PYTHON; import altair as alt; from vega_datasets import data. source = data.cars(). alt.Chart(source).mark_circle(size=60).encode(; x='Horsepower',; y='Miles_per_Gallon',; color='Origin',; tooltip=['Name', 'Origin', 'Horsepower', 'Miles_per_Gallon']; ).interactive(); ```; The current solution, although using method chaining, is very *ad hoc* for a specific type of graphs that have predetermined features, like 'dendrogram' or totals for categories or 'brackets' to highlight features.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729:220,extend,extended,220,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607888729,1,['extend'],['extended']
Modifiability,@flying-sheep suggested to write a hatch-plugin that can automatically create the respective @overload type hints,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664197260:41,plugin,plugin,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664197260,1,['plugin'],['plugin']
Modifiability,"@flying-sheep, I'm pretty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properti",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:204,polymorphi,polymorphism,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['polymorphi'],['polymorphism']
Modifiability,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:542,variab,variable,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,1,['variab'],['variable']
Modifiability,@giovp I want to make correlation plot between cell types and the continuous variables stored in .obs,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091:77,variab,variables,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848077091,1,['variab'],['variables']
Modifiability,"@gokceneraslan A few thoughts on this:. 1. In general, I don't like messing with global configurations for other packages.; 2. I generate many plots, but only a few are actually going to go to a manuscript. I'm not sure it's justified to increase all PDF sizes so that they're all editable. Maybe it should be opt-in? And do we need to mess with global settings to do this, or can we just make finding out about this easier for users?. What if there was an ""Plotting output options"" tutorial, which went over saving of figures? This could include a section on how to export for publication.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673:88,config,configurations,88,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1720#issuecomment-792236673,1,['config'],['configurations']
Modifiability,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:353,layers,layers,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900,1,['layers'],['layers']
Modifiability,"@gokceneraslan I like the idea, that would save quite some space. However, as a partial color blind person, I tend to avoid legends based on color because I can not map them reliably back to the figure. But otherwise, I think it is easy to adapt the current code to add such feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951:240,adapt,adapt,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/228#issuecomment-411039951,1,['adapt'],['adapt']
Modifiability,"@gokceneraslan If I follow you correctly, the idea would be to add a palette argument to heatmap. That indeed may solve the issue and should not be difficult to achieve. I will put it on my list of future enhancements.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/548#issuecomment-477997099:205,enhance,enhancements,205,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/548#issuecomment-477997099,1,['enhance'],['enhancements']
Modifiability,"@gokceneraslan Thanks for pointing this out! The `relative_luminance` function required for this can be easily imported from seaborn, therefore, I imagine that the code can be adapted easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250:176,adapt,adapted,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-648000250,1,['adapt'],['adapted']
Modifiability,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` â€“ `~/scikit_learn_data`; * `seaborn` â€“ `~/seaborn-data`; * `NLTK` â€“ `~/nltk_data`; * `keras` and `tensorflow` â€“ `~/.keras/datasets`; * `conda` â€“ `~/miniconda3/`; * `intake` â€“ `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages â€“ same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:832,variab,variable,832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,1,['variab'],['variable']
Modifiability,"@gokceneraslan, how about a tuple like `({obsm_key}, {obsm_value_column})`? It's more verbose, but I think more flexible, particularly if `obsm` starts to allow more kinds of values. Here's an example (using some stuff on personal branches):. <img width=""703"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/56583661-6c62a680-661d-11e9-8314-ca0b0bf8a309.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/613#issuecomment-485800110:112,flexible,flexible,112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613#issuecomment-485800110,1,['flexible'],['flexible']
Modifiability,"@gokceneraslan. > I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"". As a point about this, I don't think `raw` completley solves this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:285,variab,variables,285,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,2,['variab'],['variables']
Modifiability,"@ivirshup . I'd recommend these:; - `mypy` for type checking (I've caught a few bugs using it); - `autoflake` to remove unused variables/imports (by default removes only imports from Python's standard library); - `yesqa` to automatically remove unused `# noqa` for flake8; - `check-yaml` and `pretty-format-yaml` I also found useful; - `requirements-txt-fixer` to sort the reqs.; - `check-added-large-files` have prevented my from commiting some `.h5ad` files that shouldn't be there; - `rstcheck` to check the syntax of .rst files; - some flake8: `pytest-style` (enforces some pytest conventions, like non-returning fixtures beginning with `_`), `blind-except` (no blanket exceptions) and `comprehensions` (helps you rewrite comprehensions). Here's also an exhaustive list from which I picked the ones I use: https://pre-commit.com/hooks.html. As for any problems, some of them came from `rstcheck` as; `docs/source/classes.rst:9: (INFO/1) No directive entry for ""autoclass"" in module ""docutils.parsers.rst.languages.en"".`, that's why `.rstcheck.cfg` might be necessary. Also fixing types for `mypy` takes a while, I'd do it as last.; Also `flake8-comprehensions` forces you to abandon `dict(foo=..., bar=...)` calls, unless you disable it.; And `[tool.isort]` in `pyproject.toml` should have `profile = ""black""`. I had also planed to use `pylint` to enforce naming conventions, etc., but it has large overlap with `flake8` + the config file can be rather large, see https://gist.github.com/dkatz23238/08d79a76911ddaaa6171dff9cddd5772",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509:127,variab,variables,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754065509,3,"['config', 'rewrite', 'variab']","['config', 'rewrite', 'variables']"
Modifiability,"@ivirshup ; Yeah, it was the same data as the privious plot. I tried calling sc.tl.umap(sp, init_pos=""paga"") but meet an error. I just use the get_init_pos_from_paga function to solve this error as mention in #769 .Thanks!; ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f90e19f58c8>)); [2] During: typing of call at /datc/dh_data/.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py (797). File ""../../../../.conda_env/scrna/lib/python3.6/site-packages/umap/umap_.py"", line 797:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please report the error message; and traceback, along with a minimal reproducer at:; https://github.com/numba/numba/issues/new",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223:519,parameteriz,parameterized,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555516223,1,['parameteriz'],['parameterized']
Modifiability,"@ivirshup @dkobak I've fixed up this PR, so now it implements what I mentioned in my comment above. I've left a couple of comments on the code, commenting on anything noteworthy. The tests and everything will fail until I release a new version of openTSNE, which I'll do in the coming days. But please look through the changes and let me know if there's anything you'd like me to change, so we can get this merged. Also, I haven't updated the docstrings at all. The most glaring thing is `neighbors_tsne`. Over 90% of the code here is identical to `neighbors`. Really, the only difference is that I changed the `n_neighbors` parameter to `perplexity`. But there was no elegant way to incorporate that into `neighbors`. I've also tried refactoring the duplicated code that saves the settings into `adata.uns`, but doingt that would also make the code pretty messy. Obviously, it's not a good idea to have duplicated code like this. What do you think would be the best way to handle this?. Functionally, this now works as agreed. Let me know how you want to proceed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944:735,refactor,refactoring,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-822033944,1,['refactor'],['refactoring']
Modifiability,"@ivirshup @flying-sheep @falexwolf . I would like to merge this branch soon and I would like your opinion on the following:; * Extended functionality and fine tuning of the plots is achieved by using the new plot objects. However, I don't know how they can be documented in readthedocs (or if we want that). Note: the object methods are well documented and can be accessed, for example in Jupyter notebooks. * I am using `return_fig` as an argument to return the plot object in `sc.pl.dotplot` etc. Is this a good name choice?. As mentioned previously, the current PR tries to keep the current functionality with minimal changes to the way functions like `sc.pl.dotplot` are called. On the background, the code was refactored to remove much repetition as possible and allow new functionally. . The current progress on the visualization tutorial is here: https://nbviewer.jupyter.org/github/fidelram/scanpy-tutorials/blob/marker_genes_vis_tutorial/visualizing-marker-genes.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104:127,Extend,Extended,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210#issuecomment-636014104,2,"['Extend', 'refactor']","['Extended', 'refactored']"
Modifiability,"@ivirshup @pavlin-policar I'd like to get back to this issue. I think maybe we should postpone discussing `ingest` until later (as well as ""recipes"" based on our Nat Comms paper, and other topics raised above) and focus on switching to openTSNE first. Scanpy's architecture is to compute kNN graph by calling `sc.pp.neighbors` and then run dimensionality reduction (UMAP) and clustering (Leiden) on this graph. I think this is very neat and makes everything consistent and other functions should follow this approach as much as possible. So IMHO if it's possible to run t-SNE on the kNN graph with k=15, then that's what we should do. And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is *very* similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. Admittedly, this is not the ""vanilla"" t-SNE. But it's very close. And I think advantages outweigh the disadvantages. Actually this is quite a bit faster than the standard t-SNE, because it only uses k=15 instead of k=90 (3 times perplexity=30). Moreover, we could make the standard t-SNE available by extending `sc.pp.neighors` with `method=""tsne""` (there are several `method`s there already). What I mean is that . ```; sc.pp.neighors(); sc.tl.tsne(); ```; would use let's say uniform kernel on k=15 kNN neighbor graph (and maybe print a warning about it, but I am not even sure it's needed), while. ```; sc.pp.neighbors(method=""tsne"", perplexity=30); sc.tl.tsne(); ```; would construct k=90 weighted kNN graph as standard t-SNE does and then use that. Either way, `sc.tl.tsne()` runs openTSNE with the pre-defined affinity matrix. Thoughts?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070:1361,extend,extending,1361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748543070,1,['extend'],['extending']
Modifiability,"@ivirshup Do you mind adding as part of the PR an extended description of the function? I don't think that everyone is familiar with `calculateQCMetrics` and thus, the output of this function is unclear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-453959469:50,extend,extended,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-453959469,1,['extend'],['extended']
Modifiability,"@ivirshup I don't think so, unless there's work towards https://github.com/scverse/anndata/issues/244. To follow the ideas in https://github.com/scverse/anndata/issues/706, seems like the steps would be:. - [ ] add an attribute `._X_layer` to store which layer `.X` references;; - [ ] use `.X` to reference `.layers[._X_layer]`;; - [ ] add `in_layer=` and `out_layer=` arguments to scanpy's `.pp` functions;; - [ ] these functions will also alter `._X_layer`. The second to last point can actually be implemented irrespective of the AnnData change as `in_layer=None` will mean taking `.X`. ; The question is, should we consider changing the defaults right away, e.g. `in_layer=""counts"", out_layer=""lognorm""`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1157056525:309,layers,layers,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1157056525,1,['layers'],['layers']
Modifiability,"@ivirshup I hope that I caught all of your comments.; The flake8 configuration is now minimal and pretty much only contains black violations or what you requested. I added and then removed autopep8 again, because it has other opinions on formatting than the opinionated formatter black. Yes, even with the flake8 configuration file. Black formatted the code then autopep8 and this cycle continues forever.; Added TODOs to exceptions and noqas are still easily searchable and mention what rule they ignore anyways. I want to get this merged asap since the merge conflicts will just pile up.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430:65,config,configuration,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-799425430,2,['config'],['configuration']
Modifiability,@ivirshup The test failures are a bug exposed by the fixture refactoring. The tests were relying on `adata['uns']['pos']` being left over from a previous test run. Can you help me fix it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2235#issuecomment-1099069308:61,refactor,refactoring,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2235#issuecomment-1099069308,1,['refactor'],['refactoring']
Modifiability,@ivirshup are you looking for default colors for boolean variables?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165:57,variab,variables,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-1156714165,1,['variab'],['variables']
Modifiability,"@ivirshup coloring by boolean values `(True, False)` is now possible:. 1) The solution is based on casting the boolean columns to string columns, so that they can be colored in a categorical way. Actual columns in the anndata object are not modified. 2) I was thinking about the case where True is 1 and False is 0. Current behaviour: colorbar is plotted, since 0 and 1 are treated like continuous variables. Does it make sense to handle this case in scanpy?. 3) Tests fail due to new pandas version, they do not fail locally though. . What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671:398,variab,variables,398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460#issuecomment-1500883671,1,['variab'],['variables']
Modifiability,"@ivirshup this looks fine to me, but i just took the cell ranger version for the refactored `highly_variable_genes ` from the last version, so also not an expert. But again, this looks right.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705#issuecomment-506889693:81,refactor,refactored,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705#issuecomment-506889693,1,['refactor'],['refactored']
Modifiability,"@ivirshup, @JBreunig using the absolute counts isn't a problem per se. It's simply that the scvelo paper used the spliced counts in `adata.X` based on which the highly variable genes are selected and PCA, neighbor graph and UMAP embedding are calculated.; @JBreunig, shouldn't be a problem to put spliced into `adata.X` as the dimensions of spliced and total counts are the same.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735:168,variab,variable,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860#issuecomment-878300735,1,['variab'],['variable']
Modifiability,"@lazappi. > Users should have control over what type things are (unless this is required for computational reasons). I'm pretty sure I took your position back when this behaviour was added. The closest issue I could find was https://github.com/theislab/anndata/issues/115, but I remember having a longer discussion with @flying-sheep about this. However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). > Functions shouldn't have side effects (i.e. I expect the highly_variable_genes() function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in obs/var). I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. . I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter? Maybe there is a better implementation of categorical values out there in the `R` ecosystem? Does `stringr` play nicer with factors?. -------------------------. @grst, that's bug, we definitely allow null values in categoricals, but it looks like they're being lost in conversion from strings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445:624,variab,variable,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801601445,2,['variab'],['variable']
Modifiability,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:185,config,configuring,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,2,['config'],"['configurations', 'configuring']"
Modifiability,"@moqri bit difficult to answer this question. What are you referring to?. if you are referring to this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.filter_genes_dispersion.html. then the docs are pretty clear: ; ```; The normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301:453,variab,variable,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-826766301,1,['variab'],['variable']
Modifiability,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:228,Variab,Variable,228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692,4,"['Variab', 'variab']","['Variable', 'VariableFeatures', 'variable']"
Modifiability,"@sjfleming I am currently facing the same issue. I was able to load the h5 output with this input function you stated here https://lightrun.com/answers/broadinstitute-cellbender-read_10x_h5-error-in-scanpy-191. But after further analysis and I wanted to save the adata object with the write function to h5ad format, I am not able to read that saved h5ad object with scanpy again with error ; **test.h5ad contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp']**. For some reason, the genome ""GRCh38"" is not showing up in the available genomes options. And when I tried to use command test = sc.read_10x_h5 ('test.h5ad', genome = ""GRCh38), this error shows up again ; **Could not find genome 'GRCh38' in 'test.h5ad'. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp'].** . Do you know if this error is related to this pull request? and is there any fix to it so that I can save processed h5ad after cellbender and able to read it again? Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051:546,layers,layers,546,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051,2,['layers'],['layers']
Modifiability,"@stefanpeidli's code gives this error. `ValueError: Cannot take a larger sample than population when 'replace=False'`. If a group has less than required number observations, it shouldn't subsample. ```python; target_cells = 1000; cluster_key = ""cell_type"". grouped = adata.obs.groupby(cluster_key); downsampled_indices = []. for _, group in grouped:; if len(group) > target_cells:; downsampled_indices.extend(group.sample(target_cells).index); else:; downsampled_indices.extend(group.index). adata_downsampled = adata[downsampled_indices]; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295:402,extend,extend,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1397060295,2,['extend'],['extend']
Modifiability,"@wflynny `hashsolo` allows you to set a prior for your expected rate negatives, singlets, and doublets, which helps quite a bit with the issue you described despite modeling the log CMO counts as a normal distribution. Additionally, you can also add cell types if you've done cell-type annotation or even leiden clustering labels to help with cell type variability with CMO counts. This helped me quite a bit in kidney where NK cells had far fewer CMO counts than other cells despite being apparently live cells, e.g. good gene diversity and low mitochondrial gene percentage. @fidelram I'd be happy to add a visualization tool like you suggested if you have the code laying around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072:353,variab,variability,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759575072,1,['variab'],['variability']
Modifiability,"@xie186, are the variable names within each of your objects are unique?. I would guess that would be the problem. Here's a simple case that would throw this error:. ```python; import anndata as ad, numpy as np, pandas as pd. a = ad.AnnData(np.ones((3, 2)), var=pd.DataFrame(index=[""a"", ""a""])); b = ad.AnnData(np.ones((3, 3)), var=pd.DataFrame(index=[""a"", ""b"", ""c""])). a.concatenate(b); ```. I think our merge operation for the variables is only well defined if variable names are unique with each of the objects. I'm not sure there's a good default result here other than throwing an error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604:17,variab,variable,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1409#issuecomment-694683604,3,['variab'],"['variable', 'variables']"
Modifiability,"@zhangguy, adding on to some thoughts from your PR https://github.com/theislab/scanpy/pull/2055#issuecomment-987012001. From my reading of that PR, you added a boolean argument `groupby_expand` which, when `True`, assumed `group_by` had two values: a grouping variable for the rows of the plot and a grouping variable for the columns of the plot. It also assumed `var_names` was a single variable which would be used to fill cell in the plot. As an example:. ```python; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2). sc.pl.dotplot(pbmc, var_names='LDHB', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. ![tmpdm8256t1](https://user-images.githubusercontent.com/8238804/144899323-c439785d-5d57-4a18-b6e5-2b12412465f8.PNG). Instead of having an argument which changes the interpretation of the earlier arguments, I would prefer more orthogonal arguments. I think you'd be able to get an output close to what you would currently like with:. ```python; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```. I think this functionality could be more generic, and inspired by the `pd.pivot_table` function. This could end up looking like:. ```python; # Imaginary implementation:; sc.pl.heatmap(; pbmc,; var_names=""LDHB"",; row_groups=""louvain"",; col_groups=""sampleid""; ); ```. ![ima",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315:260,variab,variable,260,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-987049315,3,['variab'],['variable']
Modifiability,"A few things!. 1. What version is UMAP and numba?. 2. Can you post your full workflow? How are you generating the AnnData objects in `adatalist`? The convergence scores in your SAM output are suspicious. They should be something like 0.4, 0.1, 0.001,... Usually I get convergence scores like yours when I have an issue with the preprocessing of the data.; ; 3. The tricky bit about applying SAM to batch-corrected data is that batch correction results in negative values in the data, whereas SAM requires non-negative data to calculate dispersion. In order to account for this, I use `adata.X` to calculate the manifold, and `adata.layers['X_disp']` to calculate the dispersions. The 'X_disp' layer should store the nonnegative expression values. If you want to run SAM on batch corrected data, make sure that you set `adata.layers['X_disp']` to be the original non-negative expression values. If you don't assign `adata.layers['X_disp']` yourself, then it is automatically assigned to be equal to `adata.X`. If `adata.X` has negative expression values due to batch correction, then that'll cause problems for SAM. Not sure if it's related to this specific bug, but it's good to know anyway.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185:632,layers,layers,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1157#issuecomment-614700185,3,['layers'],['layers']
Modifiability,"About `use_raw` with `sc.get.var_df`: I didn't include this because the semantics differ significantly from `sc.get.obs_df`, as `raw` can have a different number of variables. I think it makes more sense for a user to call `sc.get.var_df(adata.raw, ...)`, since it's much more explicit that `adata.raw.var` and `adata.raw.varm` will be used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174:165,variab,variables,165,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-730348174,1,['variab'],['variables']
Modifiability,"About the commit process: That's far far too much work to do it like you suggested. I don't have the time for this. . About the rules: . 1. ""I don't like replacing `x == False` with `not x` in all cases. Sometimes a variable could be a container, and an error should be thrown. I think cases have to be evaluated for this."" . This should be covered by tests. In any case it is not good style and a violation. 2. ""Whats with changing from single letter variables inside expressions? Seems fine to me."". They are redefinitions of earlier variables and trip up flake8. We can call them whatever we want as long it s not `l` again. . 3. ""`lambda's also are generally fine."". See comment at the section. 4. ""Whats up with removing leading `#`s from comments?"" Not my choice either. What we have now is pep8 and flake8 compliant. If you're not happy with this we can ignore the rule. 5. ""So, some of the things you've adding a `# noqa` to look like bugs. I think we need to have a plan in place for doing something about these. Do you have any suggestions?"". The noqa ignore a rule for a specific line. I did not want to ""fix"" these things myself since Python is a dynamic language and you never know what happens :) Ideally we eventually get rid of all noqas, but not in this PR and not by me. I don't know the internals well enough to know whether this could have any side effects.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068:216,variab,variable,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785831068,3,['variab'],"['variable', 'variables']"
Modifiability,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495:0,Adapt,Adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484432495,1,['Adapt'],['Adapted']
Modifiability,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494742140:45,variab,variability,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494742140,2,['variab'],['variability']
Modifiability,"Agreed! . On another note, we currently lack a method for HVG selection that works on scaled/regressed out data. Could rewrite `regress_out` to not just output the residuals, but also add the intercept again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615137108:119,rewrite,rewrite,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615137108,1,['rewrite'],['rewrite']
Modifiability,"Ah I think I see the issue! Feature branches should be based off `master` and directing the pull request there! I think what's happening is that a pre-commit hook was installed, but the config only exists on the `master` branch. I think this should largely be manageable by rebasing onto master (e.g. `git rebase --onto master 1.7.x`) and changing the branch the PR is targeting via the github interface:. <img width=""300"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/110570131-9093e600-81a9-11eb-9223-5b7bc233d75c.png"">. --------------. Side note: We're considering separating the `highly_variable_genes` interface into multiple functions, since the arguments to the different methods don't always overlap in meaningful or intuitive ways. There's nothing you need to do about this right now, but just a heads up to keep the logic for this method separate from the main function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768:186,config,config,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-794790768,1,['config'],['config']
Modifiability,"Ah, it looks like `use_raw` was being set to True, even if `layers` was passed. . https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/plotting/_tools/scatterplots.py#L74-L76. Any idea why this didn't trigger this test?. https://github.com/theislab/scanpy/blob/c748b3558b38e908f00b16b0c18e2846d3599e5c/scanpy/tests/test_plotting.py#L294-L298. Edit: I'm guessing vmin.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988:60,layers,layers,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730#issuecomment-510467988,1,['layers'],['layers']
Modifiability,"Ahh, see what you mean though. Think I'd mis-interpreted the [docs](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html), where talks about normalising layers:. ```; Layer to normalize instead of X. If None, X is normalized.; ```. Implies that .X is the only thing that gets normalised, if you don't specify layers. . **Edit:** . Actually this is just due to references I think:. ```; # Done as per current code; >>> ad = sc.read(""read.h5ad""); >>> ad[1:5,1:5].X.todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); >>> ad.layers['raw'] = ad.X; >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); ; # If we actually copy the matrix; >>> ad = sc.read(""read.h5ad""); >>> ad.layers['raw'] = ad.X.copy()); >>> sc.pp.normalize_total(ad); >>> ad[1:5,1:5].X.todense(); matrix([[0.07691469, 0.30765876, 0. , 0. ],; [0. , 0. , 0. , 0.16940302],; [0. , 0.10096962, 0. , 0. ],; [0. , 0. , 0. , 0. ]], dtype=float32); >>> ad[1:5,1:5].layers['raw'].todense(); matrix([[1., 4., 0., 0.],; [0., 0., 0., 2.],; [0., 1., 0., 0.],; [0., 0., 0., 0.]], dtype=float32); ```. Shall I add the fix as part of what I'm doing in https://github.com/theislab/scanpy/pull/1965 @ivirshup ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596:181,layers,layers,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1957#issuecomment-889156596,6,['layers'],['layers']
Modifiability,"All of this is correct, I think you didnâ€™t take into account that shuffling `var` means shuffling `var.index` and therefore renaming the variables. You should do this instead:. ```py; rna_ann = rna_ann[:, (-rna_ann.var['nCount']).argsort()]; ```. it reorders both `.X` and `.var`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3106#issuecomment-2426582045:137,variab,variables,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3106#issuecomment-2426582045,1,['variab'],['variables']
Modifiability,"All right, fair points. > Poetry is great! But i remember two problems:; > ; > no good way to editably install into some env: python-poetry/poetry#34; > doesnâ€™t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140; > . I also stumbled upon the editably install issue. This is not an issue that Poetry can solve at the moment as explained in the thread. I do however understand that this is an issue for scanpy (considering the strong anndata dependency etc). Regarding plugins - they are on the roadmap and should appear at some point. Considering that the community is very active whereas the main developer is not anymore this may solve the editable install with some hack as well. I agree with your points and Poetry is not yet the solution that we should currently use, but I think it is the proper solution that we should aim for. > I'd also be worried using poetry would hamper contributions from people unfamiliar with it, and I don't think bioinformaticians are going to be familiar with it. I don't think that anybody is familiar with flit either, but it is slightly less intrusive and does not fundamentally change so many things like Poetry does. However, many things that Poetry does change make a lot of sense and solve other issues that we did not discuss here yet. So yeah, I personally would wait for Poetry to get it's plugin system and for the editable install issue to get a proper PEP. But if you don't feel like waiting Flit might be fun :). Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434:169,plugin,plugins,169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-765253434,3,['plugin'],"['plugin', 'plugins']"
Modifiability,Already fixed in 6c3e92924ea09ef288e422b283c6e03410d64a0b. > This may result in passing an empty `adj_tree` to the `_compute_pos()` function. empty? you mean an unset variable.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/483#issuecomment-463602100:167,variab,variable,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483#issuecomment-463602100,1,['variab'],['variable']
Modifiability,"Alright, if you would like to achieve reproducibility the next things to play around with would probably be [these CPU feature flag variables](https://numba.pydata.org/numba-doc/dev/reference/envvars.html#compilation-options) for numba. In particular: `NUMBA_CPU_NAME=generic`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555:132,variab,variables,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946714555,1,['variab'],['variables']
Modifiability,"Also `groups_names` was undefined in the same function, I changed it to `groups_order` but I'm not sure if it's the correct variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/93#issuecomment-367725475:124,variab,variable,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/93#issuecomment-367725475,1,['variab'],['variable']
Modifiability,"Also two api thoughts:. For `sc.metrics.gearys_c(a: ""array"", b: ""array"")`, where `b` is 2d is expected to have a shape like: `(variable, number_of_cells)` â€“ the ufunc shape signature would be: `(m,m)(n,m)->(n,)`. This is because it needs fast access to each variable, so they correspond to rows. Also the length of the returned array depends on the first axis of the passed input. Is this intuitive, or should the input be transposed?. Second, for `confusion_matrix`, I'm thinking I should make it singly dispatched on the first argument. This way if a dataframe is passed, the next two arguments could correspond to keys in that dataframe. Otherwise, vectors can be passed directly. Under that, these calls would be equivalent:. ```python; sc.metrics.confusion_matrix(adata.obs, ""sample_labels"", ""leiden""); sc.metrics.confusion_matrix(adata.obs[""sample_labels""], adata.obs[""leiden""]); ```. Right now it has the seaborn style argument handling shown at the top of this PR. I'm not sure that's really caught on in other packages or fits with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610:127,variab,variable,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915#issuecomment-559928610,2,['variab'],['variable']
Modifiability,"Also, I just found time to read the links you posted @davidsebfischer. Shouldn't we always use a welch t-test instead of a t-test in marker gene detection according to your second stackexchange link? They state that If you don't have a good reason to assume equal variances in the groups, then use the Welch correction... if we have a `group` vs `rest` type of setup as we do in `rank_genes_groups()` at the moment, then we would definitely not expect a single cluster to have an equal variance to the combination of all other cells in other clusters. I think the default is currently `t-test-overestimate-var`... being oblivious to exactly how that works, might it not be better to adapt that to a `welch-t-test-overestimate-var` or something like that @falexwolf?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857:683,adapt,adapt,683,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-449358857,1,['adapt'],['adapt']
Modifiability,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:361,variab,variable,361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"And in terms of the `sc.pp.highly_variable_genes` function. We typically don't use the `max_mean` and `disperson` based parametrization anymore, but instead just select `n_top_genes`, which avoids this problem altogether. That being said, there is a PR with the VST-based highly-variable genes implementation from Seurat that will be added into scanpy soon. If you would like to reproduce an updated pbmc3k tutorial from Seurat using scanpy functions, that would be very welcome of course!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348:279,variab,variable,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665746348,1,['variab'],['variable']
Modifiability,"And, in which step should I execute MNN batch effect correction ? Is it still necessary to regress out some variables ( n_counts, percent_mito et al.,) when I execute MNN ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-395337961:108,variab,variables,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395337961,1,['variab'],['variables']
Modifiability,"Another option is to set fewer components to use in sc.tl.pca, option n_comps should be set to at most number of variable genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/432#issuecomment-499561060:113,variab,variable,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/432#issuecomment-499561060,1,['variab'],['variable']
Modifiability,Apologies for the bug but this didn't seem like an enhancement and strikes me as something close to a bug.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839#issuecomment-1917273580:51,enhance,enhancement,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839#issuecomment-1917273580,1,['enhance'],['enhancement']
Modifiability,"Apologies for the late response @hawaiiki! I married and moved to the US with twin babies last week. And in between, I spilled something over my laptop... Yes, unfortunately, there were two half-cooked anndata releases out there. ðŸ˜’ All these issues are fixed on GitHub and in anndata 0.6.10. anndata is now able to fully handle loom's layers, which it wasn't before and hence gained quite some additional functionality, thanks to @Koncopd and @VolkerBergen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347:335,layers,layers,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247#issuecomment-418059347,1,['layers'],['layers']
Modifiability,"Are celltypes really continuous? How does this variable look like?; for continuous you can do; `from scipy.stats import pearsonr`; `r, _ = pearsonr(adata.obs[""celltypes""], adata.obs[""age""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102:47,variab,variable,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-849646102,1,['variab'],['variable']
Modifiability,"Are the variable names in your anndata object unique? If not, there could be an issue with ambiguous gene names being used (e.g. two genes are given the same name, so we can't tell which column to use). If this is the issue, making the variable names unique with `adata.var_names_make_unique()` should solve the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891:8,variab,variable,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1862#issuecomment-861376891,2,['variab'],['variable']
Modifiability,Are these layers for the velocyto implementation? I.e. spliced and unspliced count data layers. And is this then also usable for different layers of data processing?. Or am I missing the point entirely here?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952:10,layers,layers,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414600952,3,['layers'],['layers']
Modifiability,"Are you sure that the genes are in `adata.var_names` in the gene symbol format that you are using to subset the object? In other words, is `'Ada' in adata.var_names` `True`? I'd just like to check whether you don't have e.g., Ensembl IDs as your variable names by chance. Regarding normalization... there are other normalization methods. I believe a method was recently added to scanpy to use only a particular fraction of genes to calculate size factors (avoiding genes that make up >5% of the total counts). Otherwise, we have recently compiled a best practices pipeline in the group, which uses Scran's pooling strategy to normalize the data. This is implemented in R, but can easily be used in a python-based workflow via [`anndata2ri`](www.github.com/flying-sheep/anndata2ri). A case study using the best practices (with scran and anndata2ri) is available [here](www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785:246,variab,variable,246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488011785,1,['variab'],['variable']
Modifiability,"As a general approach to this kind of problem, I write functions like this:. ```python. def grouped_obs_mean(adata, group_key, layer=None, gene_symbols=None):; if layer is not None:; getX = lambda x: x.layers[layer]; else:; getX = lambda x: x.X; if gene_symbols is not None:; new_idx = adata.var[idx]; else:; new_idx = adata.var_names. grouped = adata.obs.groupby(group_key); out = pd.DataFrame(; np.zeros((adata.shape[1], len(grouped)), dtype=np.float64),; columns=list(grouped.groups.keys()),; index=adata.var_names; ). for group, idx in grouped.indices.items():; X = getX(adata[idx]); out[group] = np.ravel(X.mean(axis=0, dtype=np.float64)); return out; ```. Swapping out the last 8 lines or so depending on what I'm calculating. To use a set of marker genes I'd call it as `grouped_obs_mean(adata[:, marker_genes], ...)`. At some point we might have `groupby` for `AnnData`s, but that'll require figuring out how to be consistent about the returned type.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254:202,layers,layers,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/181#issuecomment-534867254,1,['layers'],['layers']
Modifiability,"As an update, I've been using this helper function to consistently handle this:. ```python. def _choose_obs_rep(adata, *, use_raw=False, layer=None, obsm=None, obsp=None):; """"""; Choose array aligned with obs annotation.; """"""; is_layer = layer is not None; is_raw = use_raw is not False; is_obsm = obsm is not None; is_obsp = obsp is not None; choices_made = sum((is_layer, is_raw, is_obsm, is_obsp)); assert choices_made <= 1; if choices_made == 0:; return adata.X; elif is_layer:; return adata.layers[layer]; elif use_raw:; return adata.raw.X; elif is_obsm:; return adata.obsm[obsm]; elif is_obsp:; return adata.obsp[obsp]; else:; assert False, (; ""That was unexpected. Please report this bug at:\n\n\t""; "" https://github.com/theislab/scanpy/issues""; ); ```. This could use support for variable masks like `use_highly_variable`. Also the error message should be better. I think a collection of helper functions like this should go in to a utils module (`sc.utils.argutils`?) which could be public so it's easier to use in `scanpy`-like packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919:495,layers,layers,495,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828#issuecomment-560072919,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"As for `randomized_svd`, looking at [this](https://github.com/scikit-learn/scikit-learn/blob/7fe3413475bf50683f821d296c2ca6cb525a7714/sklearn/utils/extmath.py#L120) it seems that is should work properly if a class for lazy evaluation inherits from standard sparse class and implements \_\_mul\_\_ and \_\_rmul\_\_.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673:234,inherit,inherits,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/393#issuecomment-446377673,1,['inherit'],['inherits']
Modifiability,"As this issue is not closed I'll add a question here. . Is it possible, or if not could it be added, that the cells in the heatmaps are sorted within the groupby variables. Either by pseudotime if availible or just clustered simply by hierarchical clustering. This could add a more visually and intuitive pleasing ordering of cells. For example as in my figure above the groupby miss some property of the data with pattern over cells. If the cell ordering was random or default this pattern could not be seen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844:162,variab,variables,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/349#issuecomment-460428844,1,['variab'],['variables']
Modifiability,"As we don't have any extensions anymore we can close this for now. Also, many people start adapting numba. In the context of Scanpy this should usually be enough... let's talk again in case we need it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001:91,adapt,adapting,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/19#issuecomment-380415001,1,['adapt'],['adapting']
Modifiability,"Assuming you're on a debian based linux, please check the following:; - `echo $PATH` shows your PATH variable.; - `which git` shows you the location of your git installation. If nothing is shown, you need to install it.; - `apt install git` if you haven't installed it yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516:101,variab,variable,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1257#issuecomment-636457516,1,['variab'],['variable']
Modifiability,"Awesome, thanks for the suggestion to look in the .uns variable! Just to confirm, are the available palettes options that can be used with the palette keyword in the call to the sc.pl.tsne() function listed in the palettes.py file? For example things like vega_20_scanpy, zeileis_26, and godsnot_64?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390299178,1,['variab'],['variable']
Modifiability,"Back on the topic of getting dot plots a bit more flexible, I've been working on an approach that could work. You can check it out in [this binder environment](https://mybinder.org/v2/gh/ivirshup/scanpy-interactive/master?filepath=notebooks%2Fflexible_de.ipynb), but it's based on two main ideas:. 1. It'd be nice if there were an easy way to get aggregated values for groups, so I've added a crude `groupby` to `AnnData`; 2. Our differential expression results are like a 3d array, with axes `[""genes"", ""group"", ""values""]` where values are things like p-values and mean expression. Here's a quick example of the output:. ![image](https://user-images.githubusercontent.com/8238804/56495993-0dc4fc00-653b-11e9-8831-a830b2ead841.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-485384100:50,flexible,flexible,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-485384100,1,['flexible'],['flexible']
Modifiability,But the error comes from your variable names being tuples. The following fixes it.; ```; adata.var_names = [i[0] for i in adata.var_names]; ```. Let me think where it would be best to output a warning. I'm not quite sure where else it would lead to collisions. Do you need tuples in the index?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440431843,1,['variab'],['variable']
Modifiability,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211:202,plugin,plugins,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423514211,2,['plugin'],['plugins']
Modifiability,Can you extend scanpy functions so that I can show gene expression level on plot generated by sc.pl.diffmap? just like that monocle2 does.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-395336870:8,extend,extend,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-395336870,1,['extend'],['extend']
Modifiability,"Check out the PR. Iâ€™m removing the GitHub actions workflow, not pre-commitâ€™s config file. Iâ€™m talking about having two checks that do the same thing, so letâ€™s keep the faster one:. <img width=""830"" alt=""image"" src=""https://user-images.githubusercontent.com/291575/161737449-5d10a522-9c6d-41fc-bd55-5a88518f0aee.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2206#issuecomment-1088547896:77,config,config,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2206#issuecomment-1088547896,1,['config'],['config']
Modifiability,"Checking locally, this PR causes to docs to be fully rebuilt with no other changes. I think the issue is that `typehints_formatter` is being changed after config init, not `typehints_default`. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204#issuecomment-1087568170:155,config,config,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204#issuecomment-1087568170,2,['config'],['config']
Modifiability,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754:397,maintainab,maintainability,397,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629#issuecomment-489105754,1,['maintainab'],['maintainability']
Modifiability,"Cool, @Koncopd! I'll rewrite a recipe for this with the new functions, which should be a lot more memory effective.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-470859467:21,rewrite,rewrite,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-470859467,1,['rewrite'],['rewrite']
Modifiability,"Cool, very interesting! :smile: Greatest advantage at first sight for me: `scanpy.api.AnnData` is now `anndata.AnnData`. Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... The simpler docstrings are also nice... but it's going to be a lot of work to rewrite all the docstrings... also, there might be some danger of introducing bugs as one needs to rewrite the function headers. Hm, ... I'm a bit hesitant to just do this right away... Let's discuss! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109:295,rewrite,rewrite,295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379778109,2,['rewrite'],['rewrite']
Modifiability,"Deduplication always makes sense. I often use speaking variable names instead of comments to clarify what Iâ€™m doing. Here the 6 reasons why Iâ€™m convinced of the way I suggested:. 1\) If I look at your change as it is, I have no idea what parameters are missing compared to `doc_scatter_bulk`. If you used variables, we could see it at a glance. 2) You could add a comment explaining that this one is just temporary (Hard to do in-line in a docstring). If one makes changes to the parameters, 3) they only have to make them once and 4) canâ€™t forget to make them twice. 5) Also the diff becomes much smaller and 6) git will be able to track doc changes that are made in the mean time. So do it please.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476512533:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476512533,2,['variab'],"['variable', 'variables']"
Modifiability,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:184,variab,variable,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904,1,['variab'],['variable']
Modifiability,"Do you have values in `adata.raw`? Raw can have a different set of variables than `adata.var`, which could be causing the issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/817#issuecomment-528735020:67,variab,variables,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817#issuecomment-528735020,1,['variab'],['variables']
Modifiability,Does using `adata.var_names_make_unique()` also makes the variable names of `adata.X` unique?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763972254:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763972254,1,['variab'],['variable']
Modifiability,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6).; But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline.; I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. ; In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020:307,variab,variable,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764494020,1,['variab'],['variable']
Modifiability,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it?; <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935:293,config,configuration,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935,1,['config'],['configuration']
Modifiability,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:865,extend,extend,865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507,3,['extend'],"['extend', 'extendfrac', 'extendrect']"
Modifiability,"For me it's good, I would just merge so we have a prototype version for the vignette, then we can extend for other data type/modify it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419:98,extend,extend,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419,1,['extend'],['extend']
Modifiability,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:825,variab,variables,825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302,1,['variab'],['variables']
Modifiability,"Found it after I wrote it.ðŸ˜„ Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:333,layers,layers,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200,5,['layers'],['layers']
Modifiability,"From my side it's ready to be merged. I have left a note in the comments about extending this to enrichment scores, and that this would be difficult. I though it might be good to leave that in there in case anyone wants to extend it later. Thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/549#issuecomment-478145349:79,extend,extending,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549#issuecomment-478145349,2,['extend'],"['extend', 'extending']"
Modifiability,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:281,variab,variable,281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707,2,['variab'],['variable']
Modifiability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:44,refactor,refactor,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactor']
Modifiability,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966:157,variab,variable,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966,1,['variab'],['variable']
Modifiability,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:642,adapt,adapted,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,1,['adapt'],['adapted']
Modifiability,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713:451,flexible,flexible,451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713,1,['flexible'],['flexible']
Modifiability,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:; - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient.; - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced).; - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient); while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. ; Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802:556,layers,layers,556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802,1,['layers'],['layers']
Modifiability,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:237,flexible,flexible,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368,2,['flexible'],['flexible']
Modifiability,"Have you subsetted your AnnData object to highly variable genes, while keeping the full dataset in `.raw`? In that case it could be that genes that are found as markers via `rank_genes_groups`, are not in `adata.var_names`, but only in `adata.raw.var_names` and therefore cannot be found by the plotting function. I've previously encountered issues with this, but I thought it had been solved now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456769781:49,variab,variable,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456769781,1,['variab'],['variable']
Modifiability,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:471,flexible,flexible,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,2,['flexible'],['flexible']
Modifiability,"Hello,; I have the same doubt. I think with the use of sc.pp.scale, the distribution of genes(equal to different variables) is normal distribution which mean is 0 and variance is 1. And this is an ideal data moduel for PCA. So I wanna know whether sc.pp.scale is the import step before sc.tl.pca for the reason I guess above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103660346:113,variab,variables,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103660346,1,['variab'],['variables']
Modifiability,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:407,config,config,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318,1,['config'],['config']
Modifiability,Here is the way I extracted the most variable genes in scanpy: ; ![Screen Shot 2023-12-22 at 11 59 13 AM](https://github.com/scverse/scanpy/assets/65792233/97be237d-eff8-4910-b858-163d301c2bf6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897471:37,variab,variable,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897471,1,['variab'],['variable']
Modifiability,"Hey @buesra-oezmen! I don't think this is a UMAP issue, but instead an issue of the MultiVI parameterization or the data. But as I know the data quite well in this case, I'm pretty sure it's maybe just a MultiVI model that is not sufficiently trained. Maybe try for a few more epochs? Or otherwise maybe the network architecture needs to be adapted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471:92,parameteriz,parameterization,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471,2,"['adapt', 'parameteriz']","['adapted', 'parameterization']"
Modifiability,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467,1,['adapt'],['adapted']
Modifiability,"Hey @ivirshup, . `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False, even if it's True when run from within the notebook. That causes figures to end up very blurry due to the missing png2x config. Am I missing anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495:210,config,config,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495,1,['config'],['config']
Modifiability,"Hey Phil!. Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:193,variab,variable,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['variab'],['variable']
Modifiability,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:778,extend,extend,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490,1,['extend'],['extend']
Modifiability,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/347#issuecomment-436379004:113,variab,variable,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347#issuecomment-436379004,1,['variab'],['variable']
Modifiability,"Hey!. Logistic regression currently doesnâ€™t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but donâ€™t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so thereâ€™s no tool iâ€™m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:119,variab,variables,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,1,['variab'],['variables']
Modifiability,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:392,Config,Configure,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859,1,['Config'],['Configure']
Modifiability,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellrangerâ€™s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we canâ€™t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,adapt,adapt,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,1,['adapt'],['adapt']
Modifiability,"Hey, thanks a lot for spotting & the nice reproducible example! Big help. From my first look into it, it seems this is a bug indeed;. **Bug appearing when**; - `batch_key` is not `None` and; - `flavor` is `â€œseuratâ€` or `â€œcell_rangerâ€` and; - then using `subset=True`. Other cases are not suffering from this it seems (e.g. `flavor=""seurat_v3""`, or when `batch_key=None`). **Issue**; It appears in the cases describe above, `subset=True` will cause the first `n_top_genes` many genes of `adata.var` to be used as selection: not the actual `n_top_genes` highly variable genes. Fix is on the way: I'll follow up here. **Your Example**; Reveals that `sc.pp.highly_variable_genes(ad_sub, n_top_genes = 1000, batch_key = ""Age"", subset = True)` suffers from this. . **Circumvent bug**; For now, I recommend not using `subset=True` if the cases above hold for you:; Rather, use . `adata_subset = adata[:, adata.var[""highly_variable""]]`. when subsetting: which is basically the ""subsetting afterwards"" strategy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3027#issuecomment-2090618325:559,variab,variable,559,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3027#issuecomment-2090618325,1,['variab'],['variable']
Modifiability,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:113,polymorphi,polymorphic,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790,1,['polymorphi'],['polymorphic']
Modifiability,"Hi !; To answer about how it is useful, we are using ICA in our lab to a dataset of more than 100k cells, with a lot of complexity, and the main advantage of ICA against PCA is that it helps us detecting small populations of cells. As these small populations are not accounting for a lot of variance within the dataset, using a treshold on PCs, we discarded the PCs that would allow the separate them.; Another advantage is that we do not make use of a selection of ""highly variable genes"" anymore, and use all genes expressed in more than 100 cells for the whole analysis... Doing the same and applying PCA gave us quite poor results.. . We made use of Seurat implementation.. and I tried fastICA from sklearn once but I couldn't obtain similar results... I have not looked thoroughly into seurat's code tough... . Hope it helps ! ; Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834:474,variab,variable,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834,1,['variab'],['variable']
Modifiability,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:75,variab,variable,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:118,variab,variable,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD, thanks for the recommendations we (@SharkieJones) will try lowering the number of variable genes (we ranked based on variance) and will look through the tutorials.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458246040:95,variab,variable,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458246040,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD,. Many thanks for your comments. . The route via PCA followed by clustering & embedding (UMAP/tSNE) works perfectly fine for me. I have also got some interesting results from the analysis. Now, I want to try clustering cells with specific gene sets instead of the conventional dimensional reduction. Yes, I tried the following lines before:. ```; adata.obsm['X_geneset1`] = adata[:, ['gene1', 'gene2', 'gene3', 'gene4']].X; ```; It still says, KeyError: 'Indices ""[\'Ada\', \'Mustn1\', \'Mlc1\', \'Gfra\', \'Gm765\', \'Csrp2\', \'Socs2\', \'Dnajb9\']"" contain invalid observation/variables names/indices.'. All of these genes are present in my dataset. I am still trying to figure out why this is happening :/ ; Maybe, I will paste the short code snippet later. . P.S: Sorry for getting off the subject. Is there an alternative normalization step included apart from the log-normalization method? For example, TMM in edgeR & SCnorm- that uses quantile regression to calculate the dependence of read counts on sequencing depth for each gene (when count-depth relationship varies among genes).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552:592,variab,variables,592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552,1,['variab'],['variables']
Modifiability,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:815,extend,extended,815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['extend'],['extended']
Modifiability,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:338,rewrite,rewrite,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211,1,['rewrite'],['rewrite']
Modifiability,"Hi @aditisk,. You can always make a dummy `.obs` variable for cluster membership. Something like this:; `adata.obs['cluster_dummy'] = adata.obs['louvain'] == adata.obs['louvain'].cat.categories[0]`. By iterating over the last index (currently at 0), you can create dummy variables to visualize via `sc.pl.umap(adata, 'cluster_dummy')`. Hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/513#issuecomment-469174695:49,variab,variable,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513#issuecomment-469174695,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946:120,variab,variable,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946,3,['variab'],['variable']
Modifiability,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:648,layers,layers,648,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136,1,['layers'],['layers']
Modifiability,"Hi @geovp!. Yes, I mean the original image that was supplied to SpaceRanger pipeline.; It doesn't have to be a TIFF image - in my experience slide scanners save; JPEG images internally, so there is no value in converting that to TIFF.; Also, it would be cool to use sc.pl.spatial for other technologies - say to; overlay single cell spatial over the microscopy image image. Nice, I was using this hacky way before (if I remember correctly I also; changed spot size in the respective slot) - so it does work. I am wondering if you could add support for a fullres slot with size factor; 1 and explain which variables need to be set for it to work in the tutorial. On Thu, Oct 1, 2020 at 8:32 PM giovp <notifications@github.com> wrote:. > Hi @vitkl <https://github.com/vitkl> ,; > by fullres you mean the tiff image yes? This is not supported for now; > unfortunately, but we are working toward some extensions to make this; > possible (cc @hspitzer <https://github.com/hspitzer> ).; > One hacky way to go about this for now could be to:; >; > - assign the tiff to the hires slot in; > adata.uns['spatial]['library_id']['images']['hires']; > - change the hires scalefactor value to 1 in the respective slot; > This should work. also for plotting the spots in the right size. Of; > course, this is also possible if you replace the ""lowres"" instead.; > Let me know what you think about it and if it works.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1436#issuecomment-702351783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTV5FBT2DB4GKUIZUVNTSITKMBANCNFSM4R5XDYSQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732:605,variab,variables,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732,1,['variab'],['variables']
Modifiability,"Hi @gheimberg,. In your example you are not using a deepcopy to assign `adata.X` to `adata.layers['other']`. So when you log transform the data in the layer, it automatically log transforms the data in `adata.X` as well, as you just passed the reference. That being said, this is still a bug as even with a `adata.X.copy()` the warning is given.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535:91,layers,layers,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535,1,['layers'],['layers']
Modifiability,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:92,variab,variable,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938,2,['variab'],['variable']
Modifiability,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:364,layers,layers,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748,2,['layers'],['layers']
Modifiability,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:339,variab,variable,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656,1,['variab'],['variable']
Modifiability,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:377,variab,variable,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,3,['variab'],['variable']
Modifiability,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:620,variab,variable,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817,1,['variab'],['variable']
Modifiability,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:212,variab,variable,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221,1,['variab'],['variable']
Modifiability,"Hi Alma,. thanks for raising your thoughts here!. Iâ€™ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if youâ€™re interested in the distance matrix, `adata.obsp['distances']` would be what youâ€™re looking for! Coming back to your code example, here the test should be a pass:; ```py; # Import packages. import scanpy as sc; import anndata as ad; import numpy as np. # set random seed; np.random.seed(42). # create dummy data; adata = ad.AnnData(shape=(1000,1)); adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities; k = 10; sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell; gr = adata.obsp['distances']; nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1; np.testing.assert_equal(nn, k-1); ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point!. That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2587#issuecomment-1691673182:603,flexible,flexible,603,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587#issuecomment-1691673182,2,['flexible'],['flexible']
Modifiability,"Hi Dan, about 1.: Iâ€™m asking you what the semantic meaning is :smile: . about 2.: there are two functions called `dendrogram`, and they have compatible signatures. Each computed dendrogram can be plotted. So what Iâ€™m saying is that the plotting version hasnâ€™t been adapted. Also an important question: in `tl.dendrogram`, we call `_choose_representation`, which will compute a PCA for the .obs axis. When specifying `axis='var'`, should it compute a PCA for the `var` axis instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869:265,adapt,adapted,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869,1,['adapt'],['adapted']
Modifiability,"Hi Dustin!. Thank you for providing a template. You can easily read that template in with two additional lines of code:; ```; import scanpy.api as sc; import pandas as pd; adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T; adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'); adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'); ```; I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/103#issuecomment-373161702:732,extend,extend,732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103#issuecomment-373161702,1,['extend'],['extend']
Modifiability,Hi GÃ¶kcen: makes sense!. Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207:148,variab,variables,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207,1,['variab'],['variables']
Modifiability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs Ã— n_vars = 60498 Ã— 466 ; AnnData object with n_obs Ã— n_vars = 466 Ã— 60498 ; AnnData object with n_obs Ã— n_vars = 466 Ã— 60498 ; AnnData object with n_obs Ã— n_vars = 466 Ã— 60498 ; obs: 'n_genes'; View of AnnData object with n_obs Ã— n_vars = 311 Ã— 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:783,Variab,Variable,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,3,['Variab'],['Variable']
Modifiability,"Hi Quentin,. When you plot a categorical variable for the first time, scanpy stores the colors for each category in adata.uns, that's why it is modifying your adata. For continuous variables (like your adata.X), it does not do that, hence there is no warning there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi Samuele,. `covariates` argument refers to the additional covariates (biological or technical) that are used in the model fit. It's the `mod` parameter in the R function combat (https://www.rdocumentation.org/packages/sva/versions/3.20.0/topics/ComBat) and `X` in equation 2.1 in Johnson et al. 2007, https://academic.oup.com/biostatistics/article/8/1/118/252073. Since only the batch variable is ""regressed out"" from the gene expression, adding extra covariates changes the way batch effect coefficient is estimated. By the way, https://scanpy.discourse.group is a better place to ask questions and start such discussions :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/780#issuecomment-521669600:387,variab,variable,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521669600,1,['variab'],['variable']
Modifiability,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,adapt,adapt,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798,1,['adapt'],['adapt']
Modifiability,"Hi Shamini,. Have you tried running the highly variable genes function on the non-log-transformed, non-normalised counts? You want to use raw counts, see the documentation:; `Expects logarithmized data, except when flavor='seurat_v3', in which count data is expected.`; The numbers in your count matrix are too large at some point in the hvg calculation, might be solved by passing it the data in the correct format!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2242#issuecomment-1256969218:47,variab,variable,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2242#issuecomment-1256969218,1,['variab'],['variable']
Modifiability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. ðŸ˜„",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:481,variab,variable,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,1,['variab'],['variable']
Modifiability,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:597,config,config,597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,7,"['Config', 'config']","['Configuration', 'config']"
Modifiability,"Hi everyone, here is the way I extracted the top 500 most variable gees in seurat: ; ![Screen Shot 2023-12-22 at 11 58 39 AM](https://github.com/scverse/scanpy/assets/65792233/5b59b6d1-696e-4695-a7df-ad8f74d6412f)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897023:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897023,1,['variab'],['variable']
Modifiability,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:523,flexible,flexible,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859,2,['flexible'],['flexible']
Modifiability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1638,variab,variable,1638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['variab'],['variable']
Modifiability,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file; ; Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:480,variab,variable,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,4,['variab'],"['variable', 'variables-axis']"
Modifiability,"Hi! That function is for reading the files output by [cellrangerâ€™s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we canâ€™t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,adapt,adapt,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,1,['adapt'],['adapt']
Modifiability,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:366,layers,layers,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337,1,['layers'],['layers']
Modifiability,"Hi!. It looks like you have too many 0 count genes in your dataset. I would filter genes and cells before calculating highly variable genes. In case you're interested, I've been working on a tutorial for single-cell RNA-seq analysis. It's available [here](www.github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/509#issuecomment-468852316:125,variab,variable,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509#issuecomment-468852316,1,['variab'],['variable']
Modifiability,"Hi!. The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-389749721:33,variab,variable,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-389749721,1,['variab'],['variable']
Modifiability,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place.; 	; ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X; sc.pp.downsample_counts(pbmc, counts_per_cell=500); sc.pp.normalize_total(pbmc, target_sum=1e4); ```. Here's the traceback:. ```pytb; Normalizing counts per cell. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-136-3305b6c650f4> in <module>; 2 pbmc.X = pbmc.raw.X; 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500); ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace); 166 adata.obs[key_added] = counts_per_cell; 167 if hasattr(adata.X, '__itruediv__'):; --> 168 _normalize_data(adata.X, counts_per_cell, target_sum); 169 else:; 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy); 14 after = np.median(counts[counts>0]) if after is None else after; 15 counts += (counts == 0); ---> 16 counts /= after; 17 if issparse(X):; 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```. ```; >>> pbmc.X; <700x765 sparse matrix of type '<class 'numpy.int64'>'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-538776417:1013,layers,layers,1013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-538776417,1,['layers'],['layers']
Modifiability,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:264,variab,variable,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695,2,['variab'],['variable']
Modifiability,"Hi, @andrea-tango ; About the second issue - you dont need pca; you can do something like. ```; # project reference adata to latent dimensions with your autoencoder; adata_ref.obsm['X_latent'] = autoencoder.to_latent(adata_ref.X); # use your latent variables to calculate neighbors; sc.pp.neighbors(adata_ref, use_rep='X_latent'); sc.tl.umap(adata_ref); # project your new adata to latent dimensions with your autoencoder; adata_new.obsm['X_latent'] = autoencoder.to_latent(adata_new.X); sc.tl.ingest(adata_new, adata_ref, embedding_method='umap'); ```. About the first, yes, ingest needs vars in the same order. The ordering thing you describe is definitely not the issue with ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540:249,variab,variables,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540,1,['variab'],['variables']
Modifiability,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:203,variab,variable,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646,2,['variab'],['variable']
Modifiability,"Hi, everything is OK with the benchmarks, `regress_out` would fail if called with variables that doesnâ€™t exist. The reason these are named differently is here: https://github.com/scverse/scanpy/blob/ad657edfb52e9957b9a93b3a16fc8a87852f3f09/benchmarks/benchmarks/_utils.py#L27-L31. I did that to be able to run benchmarks benchmarks on multiple data sets with the same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2185859889:82,variab,variables,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2185859889,1,['variab'],['variables']
Modifiability,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:272,variab,variables,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889,1,['variab'],['variables']
Modifiability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1110,variab,variable,1110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['variab'],['variable']
Modifiability,"Hi, the expression matrix I exported from adata.write only have the top variable genes. Is there a way to output the raw matrix including all genes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-497746073:72,variab,variable,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-497746073,1,['variab'],['variable']
Modifiability,"Hi, this issue tracker is better suited for bugs or enhancement request. Please ask e.g. here: https://discourse.scverse.org/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3080#issuecomment-2154471120:52,enhance,enhancement,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3080#issuecomment-2154471120,1,['enhance'],['enhancement']
Modifiability,"Hi, you can also do this directly; `adata.obsm[""mylayer_pca""] = sc.tl.pca(adata.layers[""mylayer""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567:80,layers,layers,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567,1,['layers'],['layers']
Modifiability,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667#issuecomment-497027543:185,variab,variable,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667#issuecomment-497027543,2,['variab'],['variable']
Modifiability,"Hi,. The issues I was mostly running into were that when saving the anndata; variable as a h5ad file, 'pheno_jaccard_ig' was not compatible with this; action. So, I had to either remove pheno_jaccard_ig from the anndata object; and then save it as h5ad or convert it to a sparse matrix. This also; happened with a few other functions I tried on the anndata object, and I; kept getting the error ""this function is not compatible with COO matrix; format"", always talking about pheno_jaccard_ig. Therefore, since a sparse; matrix object does not have any problems with the functions I was running; on adata, changing pheno_jaccard_ig to a sparse matrix from the start makes; sense to circumvent any of those issues I was getting before. I hope this makes sense.; Thank you,; Deena Shefter. On Wed, Jul 27, 2022 at 6:10 PM Lukas Heumos ***@***.***>; wrote:. > Hi,; >; > could you please provide more details? What issues did you run into?; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/pull/2295#issuecomment-1197424392>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMILUOEK7J64GU3YOR7DB53VWGXULANCNFSM534YT5ZA>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180,1,['variab'],['variable']
Modifiability,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:; ```; adata_needed.obs['cell_type'] = my_list_with_cd_labels; scn.tl.pca(adata_needed); scn.pl.pac(adata_needed, color='cell_type'); ```. Here the variable `my_list_with_cd_labels` should look someting like this:; `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`; Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112:196,variab,variable,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112,2,['variab'],['variable']
Modifiability,"Hi,. We get this error without swapping axes:. The code is ; ```; genes = [""DES"", ""CD34"", ""COL1A1""]; sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ); ```; The error is ; ```; IndexError Traceback (most recent call last); <ipython-input-35-8f09494e5255> in <module>; ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 929 axs_list.append(ax); 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,; --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds); 932 ; 933 if stripplot:. IndexError: list index out of range; ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/465#issuecomment-461450618:710,variab,variable,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465#issuecomment-461450618,1,['variab'],['variable']
Modifiability,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```; adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]; sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'); ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464:40,variab,variable,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464,2,['variab'],['variable']
Modifiability,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:68,variab,variable,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065,1,['variab'],['variable']
Modifiability,"Hi,. thanks for you interest in scanpy!. Does this issue still persist for you?; If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472#issuecomment-1718993973:107,extend,extend,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472#issuecomment-1718993973,1,['extend'],['extend']
Modifiability,"Hi,; Problem solved! Thanks a lot.; I am new to scanpy and now find it flexible and well designed. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/689#issuecomment-502412783:71,flexible,flexible,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689#issuecomment-502412783,1,['flexible'],['flexible']
Modifiability,"Hi. This is unlikely to be a scanpy issue. You probably donâ€™t have enough memory or thereâ€™s some problem with your Jupyter configuration. But in any case, we need more information to tell which one it is. Please share the logs that `jupyter lab` created, especially any stack traces around â€œkernel died, restartingâ€",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889:123,config,configuration,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889,1,['config'],['configuration']
Modifiability,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,1,['variab'],['variable']
Modifiability,"Hm, I adapted your reproducer to use scanpy 1.10.3â€™s code and it doesnâ€™t seem to be an issue: https://gist.github.com/flying-sheep/b2ae449ab70a9358e07a82f284de5dca#file-score_genes_diagnostics_tests2-ipynb. Iâ€™m going to assume that this is fixed in 1.10.3. If you can reproduce it with 1.10.3, we can reopen it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983:6,adapt,adapted,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983,1,['adapt'],['adapted']
Modifiability,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Modifiability,"Hmm ok, I mean that'd work too, I guess this is more like the difference between programmer's view and practitioner's view ðŸ˜„ . This is definitely more flexible and cleaner in terms of API design, but if we imagine a notebook with e.g. five sc.pl.dotplot calls, wouldn't it be painful to write this for each call (and memorize)? There are two reasons I suggested `set_figure_params` . 1. It already has all the tricks to make figures more publication-ready e.g. rasterization, Arial font etc. ; 2. Italicized gene names is likely going to be a session-wide (or project-wide) decision, where one would use it in all figures not just in one, which brings us to the importance of conciseness. . Last suggestion is like `theme(axis_text_x=element_text(angle=90, hjust=1))` kind of argument but `rotation=90` also works ðŸ˜ƒ Anyway, my two cents, happy with any option that achieves this, the community will appreciate it I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842:151,flexible,flexible,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842,1,['flexible'],['flexible']
Modifiability,"Hmm, doesnâ€™t seem to work with `functools.partial`. It works if I do almost the same, but with a lambda:. ```py; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(lambda *args, **kw: sc.pl.scatter(*args, basis=""pca"", **kw)); ```. But I think the custom solution above is better anyway! `wraps` is if you really want to pose as the wrapped function, while we only want to inherit its signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934:407,inherit,inherit,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934,1,['inherit'],['inherit']
Modifiability,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:498,variab,variables,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638,1,['variab'],['variables']
Modifiability,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:57,Extend,Extending,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,1,['Extend'],['Extending']
Modifiability,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:442,variab,variable,442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636,1,['variab'],['variable']
Modifiability,"How about [changing the encoding globally](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python#17628350)? Would that break anything? Also, there is the same bug with the package you rely on, `louvain`. As for the properly configured system, ubuntu:17.10 is the most generic and recent system I can think of, shouldn't it be properly configured out of the box? If not, is there a way to configure the system so that the encoding is globally set to utf-8?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172:250,config,configured,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172,3,['config'],"['configure', 'configured']"
Modifiability,"I agree that such an opportunity totally should be, but the example given is not relevant, because to ingest data you have to have all the genes, which were involved in the determination of neighbors, to be present in the query dataset.; The relevant example would be when you somehow managed to count common embeddings (e.g. Harmony-adjusted PCA on the reference's highly variable genes and corresponding Symphony-adjusted PCA on the query), you can then use these embeddings to ingest Umap without the necessity for all the gene_names to be the same.; So maybe the check of equality of gene vars is required only if inside tl.ingest initial embeddings for the query are calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993:373,variab,variable,373,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993,1,['variab'],['variable']
Modifiability,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980:147,evolve,evolve,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980,1,['evolve'],['evolve']
Modifiability,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:; `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-475639263:258,layers,layers,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-475639263,1,['layers'],['layers']
Modifiability,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527:243,flexible,flexible,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527,1,['flexible'],['flexible']
Modifiability,"I also recently encountered this issue. I've dug into the problem a little bit and for me the cause seems to be that the sc.pp.scale function introduces the NaN values. This occurs for columns which show very little variance and are almost constant. According to the current documentation this should not be the current expected behaviour though and should only (possibly) occur in future versions: . `Variables (genes) that do not display any variation (are constant across all observations) are retained and (for zero_center==True) set to 0 during this operation. In the future, they might be set to NaNs.`. So I'm not sure if this is a bug or if the documentation has not been updated yet. . I've currently circumvented the issue by scaling in sklearn (which retains 0s instead of NaNs) and manually loading the scaled results into my adata object as this is the behaviour I would like for my dataset. In case my example dataset would be helpful let me know then I can share it with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706:402,Variab,Variables,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706,1,['Variab'],['Variables']
Modifiability,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:384,flexible,flexible,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112,1,['flexible'],['flexible']
Modifiability,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,adapt,adapted,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688,1,['adapt'],['adapted']
Modifiability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:29,variab,variable,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['variab'],['variable']
Modifiability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:436,config,config,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,2,"['config', 'layers']","['config', 'layers']"
Modifiability,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:548,variab,variables,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541,5,['variab'],"['variable', 'variables']"
Modifiability,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,2,['variab'],['variable']
Modifiability,"I created a PR to this branch to add GPU support for :; *`tl.rank_gene_groups` with method='logreg'; *`tl.embedding_density`; *`correlation_matrix`; *`diffmap`; I added `.layers` support for `pp.pca`. This helps with the ""Pearson Residuals"" workflow.; The default pca solver for device GPU is now ""auto""; I also fixed a bug in `tl.rank_gene_groups` with `method='logreg'` with selecting groups (eg. groups = [""2"",""1"",""5""]) that is currently still in scanpy.; ![image](https://user-images.githubusercontent.com/37635888/179788802-6783f87d-19eb-497c-922e-59c18d6015d5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399:171,layers,layers,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399,1,['layers'],['layers']
Modifiability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs Ã— n_vars = 14775 Ã— 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs Ã— n_vars = 14775 Ã— 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs Ã— n_vars = 14775 Ã— 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:703,variab,variable,703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['variab'],['variable']
Modifiability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:166,variab,variable,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,1,['variab'],['variable']
Modifiability,"I don't think so, not unless you call `sc.set_figure_params()`. But this modifies the global config.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/897#issuecomment-556743231:93,config,config,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897#issuecomment-556743231,1,['config'],['config']
Modifiability,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:230,variab,variables,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460,1,['variab'],['variables']
Modifiability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:864,variab,variables,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['variab'],['variables']
Modifiability,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:180,config,config,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003,3,['config'],['config']
Modifiability,"I extended the documentation a bit now, see https://github.com/theislab/scanpy/commit/c7e58e3a8e32b7f395e25267cd3cba684d6d40c4.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/780#issuecomment-521671691:2,extend,extended,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521671691,1,['extend'],['extended']
Modifiability,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:102,config,config,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242,3,['config'],['config']
Modifiability,"I have been unable to get this to look good by default. It can be made to look good by playing around with the parameters, but then we're not really saving the user much effort. A strategy that seemed to work okay was to repel the labels from the points, followed by a second repulsion from other labels. But then I had to redraw the lines manually. Current thoughts are to punt this down the road. Maybe there will be a better solution in the future, or maybe there's a clever parameterization fix I hadn't thought of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935:478,parameteriz,parameterization,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935,1,['parameteriz'],['parameterization']
Modifiability,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,adapt,adapted,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069,1,['adapt'],['adapted']
Modifiability,"I have the same error on scanpy 1.9.5, seaborn 0.13.0, the error seems to be specific to 'multi_panel = True' and produces 3 empty graphs that all inherit the ""n_genes_by_counts"" x-label instead of the proper one.; The same graph is produced normally with 'multi_panel = False'. `sc.pl.violin(full_adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_MT'], multi_panel=True, stripplot=False)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761837779:147,inherit,inherit,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761837779,1,['inherit'],['inherit']
Modifiability,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:224,flexible,flexible,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,2,['flexible'],['flexible']
Modifiability,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. ; pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml; precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194:103,config,config,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194,2,['config'],['config']
Modifiability,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:307,variab,variable,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351,1,['variab'],['variable']
Modifiability,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664,2,['variab'],['variable']
Modifiability,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:660,refactor,refactoring,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['refactor'],['refactoring']
Modifiability,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:219,plugin,plugins,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,2,['plugin'],['plugins']
Modifiability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:506,variab,variable,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,2,['variab'],['variable']
Modifiability,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830:265,layers,layers,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830,1,['layers'],['layers']
Modifiability,"I only just now got the distinction between types and classes in python. So when they talk about â€œtypesâ€, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class thatâ€™s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). Whatâ€™s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. Thereâ€™s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesnâ€™t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:357,enhance,enhance,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,3,"['enhance', 'inherit']","['enhance', 'inherit']"
Modifiability,"I ran your code snippet multiple times on my end and I got the same results each time. Is this true for you as well? If both you and your partner can generate the same results consistently each time, then it is strange that your results disagree with each other... Some followup questions I have:; 1) Are ALL packages the same version? (Packages like Numba, scipy, sklearn, etc. should also be the same version to remove that as a potential source of variability); 2) Are you guys using the same operating system? ; 3) Can you run UMAP directly on the randomly generated matrix to see if your embeddings are the same? If they are, UMAP is likely not at fault.; 4) If you perturb your nearest neighbor matrix by adding noise to the edges such that the total edge weight differs by ~0.001 between perturbations, can you recreate the big differences in the UMAP projection? Small differences in the edge weights of the nearest neighbor graph CAN lead to huge differences in the UMAP projection if the graph has no inherent structure (which should be the case for randomly generated data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404:451,variab,variability,451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404,1,['variab'],['variability']
Modifiability,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487563174:390,extend,extend,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487563174,1,['extend'],['extend']
Modifiability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:643,variab,variable,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,1,['variab'],['variable']
Modifiability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Modifiability,"I see, thereâ€™s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py; def _plot_legend(self, legend_ax, return_ax_dict, normalize): ; self._plot_colorbar(legend_ax, normalize) ; return_ax_dict['color_legend_ax'] = color_legend_ax; ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829:456,flexible,flexible,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829,1,['flexible'],['flexible']
Modifiability,"I see. Yes then maybe we should change this annotation:; `groups : {â€˜allâ€™}, Iterable[str] (default: 'all').`; `Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups.`; to something like:; `Subset of groups, e.g. ['g1', 'g2', 'g3'], for which differentially expressed genes should be calculated, or 'all' (default) for all groups.`; ?; I could also take a look to see how we can make the reference argument more flexible, if you agree that would be a good feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985:476,flexible,flexible,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985,1,['flexible'],['flexible']
Modifiability,I think @falexwolf voiced my thoughts much more eloquently. A non-hidden directory in the root folder makes a sensible default to me. Would anyone be against also having some environmental variables/ a scanpy config (Iâ€™m thinking `.cfg` or `.json`) so this (and things like verbosity) donâ€™t have to be set manually each session?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476615093:189,variab,variables,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476615093,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473:408,flexible,flexible,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473,1,['flexible'],['flexible']
Modifiability,I think I messed something up when trying to rewrite the history @ivirshup :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499:45,rewrite,rewrite,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499,1,['rewrite'],['rewrite']
Modifiability,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via â€˜from xxx import fooâ€™ are upgraded to new versions when â€˜xxxâ€™ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529:104,config,config,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529,1,['config'],['config']
Modifiability,"I think it's reasonable that the runtime gets the final say. It's pretty standard for precedence to go: `runtime > environment > config file`, right? I don't think it's reasonable for a library to make the decision, as it should be done by the program.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220:129,config,config,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220,1,['config'],['config']
Modifiability,I think pynndescent setting the value at import time will take precedence over the environment variable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048:95,variab,variable,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048,1,['variab'],['variable']
Modifiability,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-509119409:190,variab,variable,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509119409,1,['variab'],['variable']
Modifiability,"I think that's a good idea. A general solution would be to move the part of rank_genes_groups where some statistics are calculated (e.g. log2fc, fractions, mean expression per group) to a different function with more flexible features. For example, users run regress_out or combat sometimes and then run rank_genes_groups on these corrected values, but they wanna calculate log2fc and other summary stats on the ""raw"" logTP10k values. Having another function with a layer argument would solve this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680:217,flexible,flexible,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680,1,['flexible'],['flexible']
Modifiability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Modifiability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:795,variab,variable,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,1,['variab'],['variable']
Modifiability,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:133,variab,variable,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431,3,['variab'],"['variable', 'variables']"
Modifiability,"I think the two best ways to go forward:. - Either the Seurat people agree their implementationâ€™s order is a bug and we switch to the paper order. Then we donâ€™t _necessarily_ need to add any way to configure it, just to follow suit. But we could add a way to configure it to support different possible orderings; - Or they decide that it isnâ€™t, in which case we should add that way to configure things. I think it makes more sense to encode orthogonal choices into orthogonal options. If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. E.g. if the following makes sense, then it should for sure be multiple options:. ```python; for flavor, order in product(; ('seurat_v3', 'seurat'),; ('rank', 'batches'),; ):; hvg(â€¦, flavor=flavor, order=order); ```. (`order`, `'rank'`, and `'batches'` are ad-hoc names, not necessarily good ones)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1893265335:198,config,configure,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1893265335,3,['config'],['configure']
Modifiability,"I think these plots are well overdue for a refactor. It would make sense to me if the kind of marginal plots were abstracted out into their own classes. Maybe like `MarginalBar`, `MarginalDendrogram`, `MarginalLabels`. But if you have any thoughts about how you think it could be done, or want to talk it over I'd be happy to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1088642888:43,refactor,refactor,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1088642888,1,['refactor'],['refactor']
Modifiability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:24,enhance,enhancement,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['enhance'],['enhancement']
Modifiability,"I think this would be good. I can think of cases where I wouldn't want every gene to have the same quantile normalization (e.g. when plotting markers for populations whose frequencies differ by orders of magnitude). In addition to including a quantile argument, would you also include a ""vectorized"" vmax, vmin argument?. ```python; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[2.0, 3.0]); ```. If you want to get even more flexible about how cutoffs can be chosen, we could also allow `vmin` and `vmax` to be callables. That way you could get the behavior of specifying a quantile from:. ```python; from functools import partial. sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=partial(np.quantile, q=.99)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-520202242:438,flexible,flexible,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-520202242,1,['flexible'],['flexible']
Modifiability,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:176,variab,variables,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760,1,['variab'],['variables']
Modifiability,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python; np.where((adata.X[[0], :] == adata.X).all(axis=0)); ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error?. --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830:22,variab,variables,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830,1,['variab'],['variables']
Modifiability,"I tried to set `var_names` from gene_symbols, and I get a warning message:; `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:; `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```; File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(); ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184:77,Variab,Variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184,1,['Variab'],['Variable']
Modifiability,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,adapt,adaptation,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214,1,['adapt'],['adaptation']
Modifiability,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192:499,variab,variables,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192,1,['variab'],['variables']
Modifiability,"I wonder if `dimensions` does too much or too little. Dimensions should always match, no? Flipping just one of the layers while plotting has no use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2520#issuecomment-1598796691:115,layers,layers,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520#issuecomment-1598796691,1,['layers'],['layers']
Modifiability,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:187,variab,variable,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,1,['variab'],['variable']
Modifiability,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:292,layers,layers,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823,4,['layers'],['layers']
Modifiability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:109,variab,variable,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,7,['variab'],"['variable', 'variable-and-a-categorical-variab', 'variables']"
Modifiability,"I'd like to merge this. I think we could just rename the function, and deprecate `subsample`. I'd like the name `sample` more for this function if we could add a `dim` argument so users can subsample on the variable as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/943#issuecomment-577981681:207,variab,variable,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943#issuecomment-577981681,1,['variab'],['variable']
Modifiability,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:186,variab,variable,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018,3,['variab'],"['variable', 'variables']"
Modifiability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:418,flexible,flexible,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['flexible'],['flexible']
Modifiability,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,2,"['adapt', 'variab']","['adapted', 'variable']"
Modifiability,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,adapt,adapting,1718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,1,['adapt'],['adapting']
Modifiability,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/145#issuecomment-386717637:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145#issuecomment-386717637,1,['variab'],['variable']
Modifiability,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Modifiability,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:130,layers,layers,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358,4,"['adapt', 'layers']","['adapt', 'layers']"
Modifiability,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, â€¦). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:125,config,config,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,4,"['Config', 'config']","['Config', 'config']"
Modifiability,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Modifiability,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core.; e.g.; ```; sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(); ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ...; Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014:249,variab,variable,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014,1,['variab'],['variable']
Modifiability,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:262,config,configuration,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825,1,['config'],['configuration']
Modifiability,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally.; * What's the appropriate way to set logging level? It seems to keep changing and breaking things; * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932:168,config,configuration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932,6,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"I've run into the same issue. In my case `adata.layers[""analytic_pearson_residuals""].sum(1)` gives an array of nans because there are nans in `analytic_pearson[""X""]`, as indicated by RuntimeWarning. . I am still only investigating this, but if treating nans as 0 is OK there is numpy.nansum function that could be used instead of sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496#issuecomment-1778940190:48,layers,layers,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496#issuecomment-1778940190,1,['layers'],['layers']
Modifiability,"IIRC, you can limit the number of CPUs used through blas. This works on my machine:. ```; export OMP_NUM_THREADS=1; ```. Different blas libraries use different environment variables for this, so I'd check to make sure it's actually restricting the number of threads used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499:172,variab,variables,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499,1,['variab'],['variables']
Modifiability,"If a linter flexible enough to enforce this existed, it would be great. The test should definitely exist, something in our code requires the docstrings to have that format, I just forgot which part. (But in any case it guarantees consistent formatting so thatâ€™s nice). #1492 should fix that test to ignore blank lines for the time being. Also isnâ€™t it cool that it points exactly to the problematic line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155:12,flexible,flexible,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155,1,['flexible'],['flexible']
Modifiability,"In #458 @fidelram suggested that this would be the way to go. If I put the text into another variable, this variable will only be used once. Does this still make sense? Anyways, I think this is just temporary until `pl.scatter` is in a better shape if I follow @falexwolf correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476509763:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476509763,2,['variab'],['variable']
Modifiability,"In https://github.com/scverse/scanpy/pull/2220, DocSearch was removed from the `latest` docs. Our current theme would probably support it, so we could re-introduce it (https://github.com/pydata/pydata-sphinx-theme/issues/795). @ivirshup how do I get access to our DocSearch account?. PS: Thereâ€™s more discussion about search plugins supported by our theme here: https://github.com/pydata/pydata-sphinx-theme/issues/202",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2763#issuecomment-1825412324:325,plugin,plugins,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2763#issuecomment-1825412324,1,['plugin'],['plugins']
Modifiability,"In lines 345 - 348, a list of dtypes was getting appended instead of extended. Fixed in PR #1070 ; ```; dtypes.append([; ('highly_variable_nbatches', int),; ('highly_variable_intersection', np.bool_),; ]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726:69,extend,extended,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726,1,['extend'],['extended']
Modifiability,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:98,variab,variables,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['variab'],['variables']
Modifiability,"In theory Combat knows how to take care of zero variance genes according to the; [code](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/preprocessing/_combat.py#L124).; Well, post-Combat apparently NaNs are everywhere:; ```; np.sum(np.isnan(adata_Combat.X)); Out[2]: 8089368. np.sum(~np.isnan(adata_Combat.X)); Out[3]: 0; ```; This is really weird if only 3 genes have zero variance, right? Could it have anything to do with this warnings?:; ```; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854:556,variab,variables,556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854,1,['variab'],['variables']
Modifiability,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python; adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression""; sc.pl.pca(adata, var_key=""gex""); sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); # This also has the nice feature that it could abstract out the current `use_highly_variable` argument; ```. View based:. ```python; gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]; sc.pp.pca(gex_view) # Calculate pca on gene expression; sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618:909,variab,variable,909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618,1,['variab'],['variable']
Modifiability,Interesting paper... the question is which is worse false signals from experiment or from the imputation.; Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494737289:511,variab,variability,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494737289,1,['variab'],['variability']
Modifiability,"Interestingly, I can't seem to reproduce this even with `pip` on-top of a conda install:. <details>; <summary> me trying </summary>. ```python; isaac@Mimir:~/tmp/genomic-features-docs; $ mamba create -n test-2978 ""anndata==0.9.0"" ipython scanpy; [ ... ]; isaac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:584,enhance,enhanced,584,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,1,['enhance'],['enhanced']
Modifiability,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:1080,flexible,flexible,1080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['flexible'],['flexible']
Modifiability,"Is HVG function designed to be done after regress out?; ```; # Regress out process of interest from expression data; sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') ; # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio); sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', ; batch_key=None, n_top_genes =2000); print('\n','Number of highly variable genes: {:d}'.format(; np.sum(adata_b_rn_sub2.var['highly_variable']))); rcParams['figure.figsize']=(10,5); sc.pl.highly_variable_genes(adata_b_rn_sub2); ```; ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png); If I do scale before HVG; ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-841394437:409,variab,variable,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-841394437,1,['variab'],['variable']
Modifiability,"Isaac says that lobpcg seems much less precise and is probably not worth it, so we should probably just adapt the warning to mention the imprecision and call it a day",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465:104,adapt,adapt,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465,1,['adapt'],['adapt']
Modifiability,"Isaac,. this is great, thank you so much!. Regarding the default for the dataset directory. I like this solution!. Very small edits in addition to what I commented in the code:; * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`?; * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272; * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`?. Notes:; * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(â€¦)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and GÃ¶kcen favored if I'm correctly summarizing the long thread?; * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose.; * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file...; * `pyplot.rc_context` sounds awesome.; * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822:967,config,config,967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822,4,['config'],['config']
Modifiability,"It is, too bad numpy has no good variable-length string array type. When would bytes make sense? Bytes just mean â€œdata, but I donâ€™t know its structure or am about to write it to diskâ€",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-545906897:33,variab,variable-length,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545906897,1,['variab'],['variable-length']
Modifiability,"It looks like the issue here is that you have a different number of variables. Here's a small example of what's happening:. ```python; import pandas as pd. a = pd.DataFrame({""bool"": [True, False]}, index=[0, 1]); b = pd.DataFrame(index=[0,1,2]). b[""bool""] = a[""bool""]; b; ```. ```; bool; 0 True; 1 False; 2 NaN; ```. So when you try to subset by `adata.var.highly_variable` you have a bunch of null values in that index, which `AnnData` does not allow (it's not super obvious what the right thing to do here is anyways). What you might want to do is:. ```python; adata = adata[:, adata.var.highly_variable & adata.var.highly_variable.notna()].copy(); ```. or. ```python; adata = adata[:, ACT_sub2.var_names[ACT_sub2.var['highly_variable']]].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095#issuecomment-1015376680:68,variab,variables,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095#issuecomment-1015376680,1,['variab'],['variables']
Modifiability,It seems like `_read_legacy_10x_h5()` invokes ` _collect_datasets()` without taking `genome` into account? Perhaps this was lost during a refactor?; https://github.com/scverse/scanpy/blob/bd06cc3d1e0bd990f6994e54414512fa0b25fea0/scanpy/readwrite.py#L222,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1112677198:138,refactor,refactor,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1112677198,1,['refactor'],['refactor']
Modifiability,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look?. ```; color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None); Color map to use for continous variables. Anything that works for `cmap`; argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,; `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used.; palette : `str`, list of `str`, or `Cycler` optional (default: `None`); Colors to use for plotting categorical annotation groups. The palette can be; a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list; of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object.; If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical; variable already has colors stored in `adata.uns[""{var}_colors""]`.; ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/477#issuecomment-462730256:206,variab,variables,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477#issuecomment-462730256,2,['variab'],"['variable', 'variables']"
Modifiability,"It would help to tell *where* in the code the error is thrown. Please provide a traceback. > Does using adata.var_names_make_unique() also makes the variable names of adata.X unique?. If X is a DataFrame, yes. Otherwise X doesnâ€™t have any names stored inside (`var_names` are stored as `.var.index`.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763974778:149,variab,variable,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763974778,1,['variab'],['variable']
Modifiability,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:140,config,config,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453,1,['config'],['config']
Modifiability,"It's definitely a problem that you are seeing all of these version restrictions at once. This may be related to having too many entries in your PYTHONPATH environment variable. `PYTHONPATH` should probably just be empty, since python already knows to look where pip installs packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532:167,variab,variable,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532,1,['variab'],['variable']
Modifiability,"It's not just that the length is different, is that `sc.get.obs_df(adata, [""col""], use_raw=x)[""col""]` is the same regardless of the value of `x`, but it's different for `var_df`. I think it's easier to build code around functions with more orthogonal arguments. > However, I consider that since this option is everywhere it should be here as well. . Could we add an example of `sc.get.var_df(adata.raw, ...)`, leave out `use_raw` for now, and see if anyone complains?. I've been trying to leave out `use_raw` on functions where variable length matters anyways. For example: `adata.var_vector`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337:528,variab,variable,528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337,1,['variab'],['variable']
Modifiability,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108:304,variab,variable,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108,1,['variab'],['variable']
Modifiability,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:904,variab,variables,904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['variab'],['variables']
Modifiability,"Iâ€™m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot!. @davidsebfischer itâ€™s maintained, right?. Iâ€™m going to close this unless Iâ€™m wrong and we want to enhance `rank_genes_groups` after all",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640143156:284,enhance,enhance,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640143156,1,['enhance'],['enhance']
Modifiability,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```; In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-385-66af52bcd3f3> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace); 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(; 71 obs_metrics[""total_{expr_type}""]); ---> 72 proportions = top_segment_proportions(X, percent_top); 73 # Since there are local loop variables, formatting must occur in their scope; 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns); 182 if not isspmatrix_csr(mtx):; 183 mtx = csr_matrix(mtx); --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns); 185 else:; 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0; ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450:819,variab,variables,819,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450,1,['variab'],['variables']
Modifiability,Just made a pull request that fixes this issue. @brianpenghe . I did some debugging and the variable `num_rows` was incorrectly calculated only when `swap_axes==False` on line 880 of `_anndata.py`. Instead of `num_rows = len(categories)` it should be `num_rows = len(var_names)` . If you make that small change in your _anndata.py in `~/anaconda3/lib/site-packages/scanpy/plotting/_anndata.py` then recompile the packages using `python -m compileall .` and restart python it should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/405#issuecomment-471238684:92,variab,variable,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405#issuecomment-471238684,1,['variab'],['variable']
Modifiability,"Just push things like this on the master branch. :wink: And, consider using `'a string with a variable: {}'.format(variable)` for formatting strings. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493,2,['variab'],['variable']
Modifiability,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>; <summary>Script </summary>. ```python; import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:548,layers,layers,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['layers'],['layers']
Modifiability,"Just to follow-up because I also had a similar question to the OP. Here's one way to plot several marker genes in different colors on the same UMAP plot. The trick is to make different colormaps that have an alpha gradient so that cells with NA expression appear transparent. Then just use matplot axes to merge the images. The only issue is that Scanpy doesn't yet allow you to remove colorbars for continuous variables, so the multiple colorbars can throw off the scaling, which you can work around by changing the figure parameters. ```; #make red colormap; colors2 = plt.cm.Reds(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:411,variab,variables,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['variab'],['variables']
Modifiability,"Letâ€™s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424995243:46,plugin,plugin,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424995243,1,['plugin'],['plugin']
Modifiability,"Look at the documentation before you ask questions. The object returned from the function you called doesnâ€™t return a matplotlib object, it returns a dictionary, assuming that the â€˜showâ€™ parameter is off. You canâ€™t loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the â€˜ylimâ€™ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. â€”; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWF",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:656,variab,variable,656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209,1,['variab'],['variable']
Modifiability,"Looking at this again, now that I have gone through everything, I think we actually need to check types directly and shouldn't rely on `isbacked` because it is possible to do something like `adata.layers['foo'] = sparse_dataset(g_layer)` and this should also error our with a helpful message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455:197,layers,layers,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455,1,['layers'],['layers']
Modifiability,"Looks good! Remaining questions:. - The plan is to add the Visium reading function to `anndata`, right?; - Youâ€™re repeating yourself with the docs: `doc_scatter_basic` (and therefore `doc_scatter_embedding`) and the docstring of `pl.spatial` both contain similar text for the same parameters. If you want to reorder them, you could do something fancy (like slicing doc_scatter_embedding) or just mention the parameter names in the free text, something like:. ```restructuredtext; Scatter plot in spatial coordinates. Use the parameter `img_key` to see the microscopy image in the background.; Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed,; and `scale_spot` to control the size of the Visium spots plotted on top.; ```. - Is it possible to derive the amount of cropping? Then we could extend the `crop_coord` parameter to this:. ```py; Union[; Iterable[Literal['left', 'l', 'right', 'r', 'top', 't', 'bottom', 'b']],; Tuple[int, int, int, int], # l, r, t, b; ]; ```. - Maybe it makes sense to add some test data and a test plot? (very low res of course)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703:810,extend,extend,810,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703,1,['extend'],['extend']
Modifiability,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:366,extend,extending,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085,2,['extend'],"['extended', 'extending']"
Modifiability,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:221,extend,extend,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105,1,['extend'],['extend']
Modifiability,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:123,config,configure,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004,1,['config'],['configure']
Modifiability,"Matplotlib takes a while but less time. Can you please point me to what you mean with the environment variables?. No idea about tables, @falexwolf wrote the sim module I think and itâ€™s not commonly used â€¦. I donâ€™t think import times change noticably, but I didnâ€™t measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-522595568:102,variab,variables,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522595568,1,['variab'],['variables']
Modifiability,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:308,variab,variable,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577,1,['variab'],['variable']
Modifiability,"My opinion would be that you need to write `adata.raw = adata.copy()` if you want a copy to be made, since almost all assignments do not create a copy of the assigned object in anndata. But we should look into whether this is a change that was made deliberately or not. If we don't change it, we could maybe warn if we're mutating `adata.X` and `adata.raw.X` also refers to the same thing?. Overall, I would recommend that you use `adata.layers[""counts""] = adata.X.copy()` instead of using `.raw` at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2150583605:438,layers,layers,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2150583605,1,['layers'],['layers']
Modifiability,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:1311,refactor,refactoring,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738,1,['refactor'],['refactoring']
Modifiability,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:58,variab,variables,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,4,['variab'],['variables']
Modifiability,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:254,config,configuration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466,1,['config'],['configuration']
Modifiability,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:350,variab,variables,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483,2,['variab'],['variables']
Modifiability,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402:129,extend,extend,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402,1,['extend'],['extend']
Modifiability,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:52,variab,variable,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621,1,['variab'],['variable']
Modifiability,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:524,variab,variable,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879,1,['variab'],['variable']
Modifiability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,2,"['adapt', 'variab']","['adapt', 'variabale']"
Modifiability,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:80,Extend,Extended,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008,1,['Extend'],['Extended']
Modifiability,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706; * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`; * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`; * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658; * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852:487,flexible,flexible,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852,1,['flexible'],['flexible']
Modifiability,"OK, got the formatting issues. Let's discuss at the office. Let's stick with `None`, this just requires to rewrite a very small number of strings... will not be a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404268015:107,rewrite,rewrite,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404268015,1,['rewrite'],['rewrite']
Modifiability,"OK, seems like I misunderstood the point about zero inflation here. You just meant â€œlarge number of zeroesâ€ as in â€œpretty sparseâ€ then?. A factor of 10 isnâ€™t that bad for something thatâ€™s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our â€“ currently slow but weâ€™ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:290,enhance,enhancement,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['enhance'],['enhancement']
Modifiability,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str; > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129:307,variab,variables,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129,1,['variab'],['variables']
Modifiability,Oh! I keep forgetting to ask: Why did the original scanpy config file get removed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479745339:58,config,config,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479745339,1,['config'],['config']
Modifiability,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:; ```; For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521:452,variab,variable,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521,1,['variab'],['variable']
Modifiability,"Ok, makes sense.; What if I implement that `cmap `in `embedding` also accepts a `dict ` of `{variable: colormap}`?. `{'n_counts_all': 'copper', 'n_genes_cmap': matplotlib.colors.Colormap}`. It maps variable names to `str ` or `Colormap`. Therefore, the colormap can be processed before and is not stored in AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081,2,['variab'],['variable']
Modifiability,Okay... scvelo uses `adata.uns['velocity_settings']['embeddings'].extend()` which only works on lists. @VolkerBergen shall i report this again in the `scvelo` repo or is this sufficient for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/887#issuecomment-545948736:66,extend,extend,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545948736,1,['extend'],['extend']
Modifiability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:325,maintainab,maintainability,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,1,['maintainab'],['maintainability']
Modifiability,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesnâ€™t appear in the list of checks, and the PR thinks all checks have run. This doesnâ€™t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:297,config,configure,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136,1,['config'],['configure']
Modifiability,"One thing to note:; if the categories of the groupby variable (clusters in this case) don't match up with the categories in the marker_genes_dict, there will be no matching if colors between rows and columns. This is what also happens in the tutorial, where; marker_genes_dict = {'NK': ['GNLY', 'NKG7'],; 'T-cell': ['CD3D'],; 'B-cell': ['CD79A', 'MS4A1'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; while the clusters are numbers (""1"", ""2"" etc.).; We could consider throwing a warning in that case, but I don't think it's necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826:53,variab,variable,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826,1,['variab'],['variable']
Modifiability,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:7,adapt,adapt,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['adapt'],['adapt']
Modifiability,"Please add the relevant part of `jupyter lab`â€™s log. If itâ€™s a SEGFAULT, please reproduce with the [`PYTHONFAULTHANDLER`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONFAULTHANDLER) env variable set to a non-empty string to get a traceback",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840#issuecomment-1929143068:198,variab,variable,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840#issuecomment-1929143068,1,['variab'],['variable']
Modifiability,"Please ask questions on https://discourse.scverse.org/. See the Note in the 2017 tutorial:. > [!NOTE]; > ; > If you donâ€™t proceed below with correcting the data with `sc.pp.regress_out` and scaling it via `sc.pp.scale`, you can also get away without using `.raw` at all.; > ; > The result of the previous highly-variable-genes detection is stored as an annotation in `.var.highly_variable` and auto-detected by PCA and hence, `sc.pp.neighbors` and subsequent manifold/graph tools. In that case, the step actually do the filtering below is unnecessary, too. Since data sizes these days are big enough that a sparse .X is all but necessary (`pp.scale` densifies data), and methods exist that work with unscaled expression values and therefore donâ€˜t need scaling, people tend to not do it these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3095#issuecomment-2154540758:312,variab,variable-genes,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3095#issuecomment-2154540758,1,['variab'],['variable-genes']
Modifiability,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesnâ€™t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things Iâ€™m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:141,plugin,plugins,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746,1,['plugin'],['plugins']
Modifiability,"Recently I need to run PCA on multiple layers of AnnData. If no one is working on this, I can work on a pull request for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960:39,layers,layers,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960,1,['layers'],['layers']
Modifiability,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398682339:195,evolve,evolved,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398682339,1,['evolve'],['evolved']
Modifiability,Related question - is it necessary to do. `adata.layers['counts']=adata.X.copy()`. or is:. `adata.layers['counts']=adata.X` sufficient?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413237366:49,layers,layers,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413237366,2,['layers'],['layers']
Modifiability,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:253,variab,variable,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637,1,['variab'],['variable']
Modifiability,"Reviewed harmonization paying attention to some more details. What do you think of using the `|` separator to describe `adata.X | adata.layers[layer]` e.g. [here](https://icb-scanpy--2742.com.readthedocs.build/en/2742/generated/scanpy.pp.regress_out.html)?. Some things causing some sort of heterogeneity and are NOT taken care of here:; - the inconsistent and mixed use of `inplace` and `copy` (effort: lot of work); - some inconsistent use of `key_added` & flavours thereof, which affect the return section (effort: medium amount of work). What is also not taken care of here:; - Other small things, such as [ingest](https://scanpy.readthedocs.io/en/latest/generated/scanpy.tl.ingest.html) not having a `return_joint` argument although this is mentioned in its doc. Might raise smaller issues in the future for these specific things rather than bloating this purpose-driven PR up?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1812943870:136,layers,layers,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1812943870,1,['layers'],['layers']
Modifiability,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:184,variab,variable,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,1,['variab'],['variable']
Modifiability,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:11,enhance,enhanced,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721,2,"['enhance', 'variab']","['enhanced', 'variables']"
Modifiability,Should be fixed by: https://github.com/algolia/docsearch-configs/pull/4840,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336:57,config,configs,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336,1,['config'],['configs']
Modifiability,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons); 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for ðŸ˜.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479:598,variab,variable,598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479,1,['variab'],['variable']
Modifiability,"So far as I can tell, any further downstream operations also acts on layers... so it is not useful to store raw counts there since they will just be modified with counts normalization, log normalization, etc. Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed rather than preserving the raw-er aspect of the counts matrix. Not sure if this is new behavior but it is super frustrating",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2070663668:69,layers,layers,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2070663668,3,['layers'],['layers']
Modifiability,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308:104,variab,variable,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308,1,['variab'],['variable']
Modifiability,So one should always use .copy() when creating new layers from .X?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389#issuecomment-1413251794:51,layers,layers,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389#issuecomment-1413251794,1,['layers'],['layers']
Modifiability,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:66,variab,variables,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768,2,['variab'],['variables']
Modifiability,"So quick! thanks. I have been using the dendrograms for a while so; hopefully not so many bugs appear. Something that I wanted to have for; discussion is on some parameters relevant for the dendrogram, like the; genes used, the correlation method and the linkage method. All this can be; modified but currently is hard coded as I didn't want to add 3 more; parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I; thought about setting some variables like the rcParams for matplotlib but I; think is not justified for just 3 parameters and can be very confusing. Or; we can have a function to compute a dendrogram with all parameters; required, and save this in .uns like rank_genes_groups. Then if other; functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430667617:494,variab,variables,494,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430667617,1,['variab'],['variables']
Modifiability,"So the user experience will be:. 1. Theyâ€™ll go through the example notebooks where theyâ€™ll learn how to download data. â†’ The notebooks should mention where to configure the cache directory. 2. Theyâ€™ll download data, probably not paying attention to the output immediately. â†’ We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe theyâ€™ll eventually look at the settings module in the online documentation. â†’ We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). â†’ We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:159,config,configure,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,3,['config'],"['configure', 'configured']"
Modifiability,So what is now the recommended way of calculating and plotting PCA on layers? Is it still the same as suggested in https://github.com/theislab/scanpy/issues/1301? With this I am still unable to use sc.pl.pca_variance_ratio.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999:70,layers,layers,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999,1,['layers'],['layers']
Modifiability,"So, does that mean that every time we apply some kind of filtration (adata = adata[ condition]) we should use .copy()? ; For instance, when filtering the highly variable genes (see the image extracted from the scanpy legacy workflow)? ; ![image](https://github.com/scverse/scanpy/assets/64482157/e929e440-c093-4571-b0c3-43febd052128)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2192467124:161,variab,variable,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2192467124,1,['variab'],['variable']
Modifiability,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:71,variab,variable,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['variab'],['variable']
Modifiability,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,adapt,adapted,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289,1,['adapt'],['adapted']
Modifiability,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:72,refactor,refactoring,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813,1,['refactor'],['refactoring']
Modifiability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! Itâ€™s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - Iâ€™ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:133,extend,extend,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"Sorry for the late response! This seems to have come just after I went through the issues last weekend...; ; It looks great! :smile:. Some small notes:; * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...); * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916:469,variab,variables,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916,3,"['extend', 'rewrite', 'variab']","['extend', 'rewrite', 'variables']"
Modifiability,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this?. I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/331#issuecomment-435733122:93,variab,variables,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331#issuecomment-435733122,1,['variab'],['variables']
Modifiability,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Modifiability,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,config,configuration,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773,1,['config'],['configuration']
Modifiability,Sounds good - added an entry to the release note and updated the tests to not use the parameterization.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767:86,parameteriz,parameterization,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767,1,['parameteriz'],['parameterization']
Modifiability,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:534,layers,layers,534,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478,1,['layers'],['layers']
Modifiability,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool; > tl.dendogram in the clustering section that has a parameter to select the; > clustering for which one wants a dendogram, typically defaulting to; > louvain (unfortunately, we still don't have a good consistent naming; > convention across all tools; groupby predominates but is not ideal in; > this setting. grouping or cluster_key would also be possible).; >; > You can still have the plotting functions call that function with default; > parameters. But the user wants more control, he or she can run the; > dendogram tool.; >; > Of course, we also want dendograms for genes. I think the most elegant; > (but maybe confusing solution) is to do it as in pl.scatter, where; > annotation of observations is selected if the key is in .obs and; > variables annotations are selected if the key is in .var. What do you; > think?; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430880800:956,variab,variables,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430880800,1,['variab'],['variables']
Modifiability,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file); 2. I pulled his changes, which covered the writing portion of the needed fixes; 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2; 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460184736:525,variab,variable,525,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460184736,1,['variab'],['variable']
Modifiability,"Sure, I can have a look at the docs. I've been using it because of the `layers` argument since `pl.umap` does not seem to have it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476005968:72,layers,layers,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476005968,1,['layers'],['layers']
Modifiability,"Sure, and I'm not against supporting special cases! Could you please explain the setup?. Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. Some systems are strange. We should be nice and support those systems while still doing the correct thing by default. We shouldn't do the wrong thing by default to accommodate strange cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606:248,variab,variable,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606,1,['variab'],['variable']
Modifiability,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:275,variab,variables,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:803,variab,variable,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276,3,['variab'],['variable']
Modifiability,"Thank you for your thoughts!. 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible...; 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. ; 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976:502,variab,variables,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976,1,['variab'],['variables']
Modifiability,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:; ```; preprocessing/_combat.py:150: in combat; s_data, design, var_pooled, stand_mean = stand_data(model, data); preprocessing/_combat.py:78: in stand_data; design = design_mat(model, batch_levels); preprocessing/_combat.py:32: in design_mat; model, return_type=""dataframe""); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix; NA_action, return_type); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design; NA_action); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders; formula_like = ModelDesc.from_formula(formula_like); ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula; tree = parse_formula(tree_or_string); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula; _atomic_token_types); ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse; for token in token_source:; ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula; yield _read_python_expr(it, end_tokens); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr; for pytype, token_string, origin in it:; ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next; return six.advance_iterator(self._it); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):; # Since formulas can only contain Python expressions, and Python; # expressions cannot meaningfully c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530:62,adapt,adapting,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530,1,['adapt'],['adapting']
Modifiability,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:430,extend,extend,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051,1,['extend'],['extend']
Modifiability,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-2-aae861244dfa> in <module>; ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] for k in keys]; 211",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885:493,layers,layers,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885,5,['layers'],['layers']
Modifiability,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,adapt,adapt,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444,1,['adapt'],['adapt']
Modifiability,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:131,plugin,plugins,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:146,config,config,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244,3,['config'],['config']
Modifiability,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs Ã— n_vars = 2773 Ã— 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:1545,layers,layers,1545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097,1,['layers'],['layers']
Modifiability,"Thanks a lot for the useful comments & I am a great admirer of scanpy, @falexwolf . I am trying to cluster cells based on specific gene sets (same as @biskra). I am working with loom files & after the data preprocessing and finding out highly variable genes, I have subgrouped the HVG(s) into five different categories by functional annotation/pathway enrichment analysis. Then I tried to subset 'adata' to the gene group I am interested in to carry out the embedding & clustering:. adata = adata[:, adata.var['highly_variable']]. #From the highly variable genes, let's say I want to use Gene1, Gene2,... Gene500 for the Louvain clustering instead of the PCA. Then, I tried to do what you suggested before:. #I have nothing stored under adata.obsm; adata.var['highly_variable'] = adata[['gene1', 'gene2', 'gene3', 'gene4']].X. I am getting this error despite the fact that these genes are in the HVG list: ; #KeyError: ""None of [Index(['Map7d1', 'Ndufa2', 'Klc2', 'Slc35b2'], dtype='object')] are in the [index]"". If this works, then the community graph can be computed:; sc.pp.neighbors(adata, use_rep='highly_variable'). I shall be grateful if you can help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487964976:243,variab,variable,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487964976,2,['variab'],['variable']
Modifiability,"Thanks for a quick response and the comments. > The main change here is passing None instead of 0 to total, right?. It was actually setting it in the contructor, rather than assigning it to the tqdm object (the latter doesn't work). Here's before:; ![old](https://user-images.githubusercontent.com/46717574/100207740-3b88d880-2f08-11eb-882f-cae14be0837e.png); and after:; ![new](https://user-images.githubusercontent.com/46717574/100207756-3fb4f600-2f08-11eb-85f8-5938ff04572d.png). > Also: this makes some errors with files still existing make much more sense. I had no idea KeyboardInterrupt doesn't inherit from Exception. I didn't know that either, so I looked it up (it actually inherits from `BaseException` among other things:; https://docs.python.org/3/library/exceptions.html#exception-hierarchy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404:602,inherit,inherit,602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404,2,['inherit'],"['inherit', 'inherits']"
Modifiability,"Thanks for implementing this! I used it to regress out total counts and cell cycle scores before highly variable gene selection, and it worked well. The clusters are better separated without artifacts, unlike running regressing function after HVG.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2731#issuecomment-1800547868:104,variab,variable,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2731#issuecomment-1800547868,1,['variab'],['variable']
Modifiability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:198,variab,variables,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,1,['variab'],['variables']
Modifiability,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes?. I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929:203,variab,variables,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929,2,['variab'],['variables']
Modifiability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,1,['variab'],['variable']
Modifiability,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208:45,refactor,refactoring,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208,1,['refactor'],['refactoring']
Modifiability,"Thanks for the PR!. While I like the idea of being able to be more flexible around how the dot plots are made, I'm not sure I like the idea of adding more argument specific behavior. That is, I don't like that passing a boolean flag changes the meaning of the values for the `var_names` and `groupby` arguments. E.g. the `group_by` is the selection of `obs` corresponding to the plots rows, while `var_names` was the variable for the columns. What could maybe be done instead is to have an argument for column grouping. Instead of. ```python; sc.pl.dotplot(adata, var_names='C1QA', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. It could be more like. ```python; sc.pl.dotplot(adata, var_names='C1QA', group_by='louvain', group_cols='sampleid'); ```. I would ideally like a solution here to work for all the other ""grouped plots"" as well. I'll write a bit more about this in the parent issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001:67,flexible,flexible,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001,2,"['flexible', 'variab']","['flexible', 'variable']"
Modifiability,"Thanks for the PR, thatâ€™s it. We know about the two broken tests (e.g. see #3068), they can be ignored for this PR. Not all changes I made in #3097 were necessary, just re-adding the `if len(gene_pool) < len(var_names)` branch. I made the refactoring PR since your changes already refactored the function in a good way, I just went a bit further so the repeated code for â€œget row/col means of a gene subset of `adata`â€ code could be reused, and your `get_indexer` change would only need to be added in a single location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2921#issuecomment-2149299253:239,refactor,refactoring,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2921#issuecomment-2149299253,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Modifiability,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:267,layers,layers,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"Thanks for the response. I tried that several times, but couldnâ€™t get the install to work:. â€œCannot find the C core of igraph on this system using pkg-configâ€ etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. â€”; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:151,config,config,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986,1,['config'],['config']
Modifiability,"Thanks for the suggestion. Actually, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > â€”; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:676,variab,variable,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368,1,['variab'],['variable']
Modifiability,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:277,variab,variables,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398,2,['variab'],['variables']
Modifiability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:296,variab,variable,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['variab'],['variable']
Modifiability,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:267,evolve,evolves,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745,1,['evolve'],['evolves']
Modifiability,"Thanks for your comment Jason!; Don't you think the sentence before the one you quoted:; ""If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""].""; in combination with the default being ""None"" would make this clear?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845:162,variab,variable,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845,1,['variab'],['variable']
Modifiability,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884#issuecomment-554084273:409,variab,variables,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-554084273,1,['variab'],['variables']
Modifiability,"Thanks to everyone who commented here. Hey @flying-sheep, it passes the tests in https://github.com/scverse/scanpy/blob/e285c0f6ec77631d14d748d0927d38aae4391886/tests/test_aggregated.py; please let me know if I should fix or refactor anything; Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2325176926:225,refactor,refactor,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2325176926,1,['refactor'],['refactor']
Modifiability,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:135,config,config,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023,3,['config'],['config']
Modifiability,That could also work. But this would require a bit of a rewrite. I think the current solution is simpler and also really fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797:56,rewrite,rewrite,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797,1,['rewrite'],['rewrite']
Modifiability,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:1080,variab,variable,1080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410,1,['variab'],['variable']
Modifiability,"That is interesting... do you know where the randomness is coming in? I think `sc.pp.highly_variable_genes()` can have some variability. These two VMs have the same operating system and hardware otherwise, right? I've had reproducibility issues moving between Fedora 25 and 28. In the end the libraries we use rely on underlying kernel numerics. There's a limit to how reproducible one can be. This only really becomes an issue if the biological interpretation is no longer consistent. Of course we'd like to be reproducible before then as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704:124,variab,variability,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704,1,['variab'],['variability']
Modifiability,"That issue report mentions setting the `PYTHONHASHSEED` environment variable to `0` (next to all the seed setting) worked to create a fully reproducible workflow. If that doesn't work for you, it might be good to continue the discussion there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306:68,variab,variable,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306,1,['variab'],['variable']
Modifiability,"That's a great idea. It might require some reorganization, though, because currently `use_raw` is checked two places: once in `sc.pl.scatter()`, because it needs to know whether to look for variables in raw or not when deciding how to call `_scatter_obs()`, and again in `_scatter_obs()` itself. And it would probably be bad to do `adata = adata.raw.to_adata()` twice?. On another note, some pytests that are in files I did not edit are now failing because they can't find `anndata.tests` to import. I'm not sure if I messed something up by adding tests to `test_plotting.py` or whether this is a different issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046:190,variab,variables,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046,1,['variab'],['variables']
Modifiability,"That's an interesting idea and I see how it would be useful. I don't think it's going to be easy to implement, since I believe our code is heavily based around having groups of observations on one axis, groups of variables on the other. . Definitely something to keep in mind for a refactor though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914:213,variab,variables,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914,2,"['refactor', 'variab']","['refactor', 'variables']"
Modifiability,"Thatâ€™s exactly backwards: I find it annoying if packages modify state on import. We already jump through hoops in our testing framework to work around our misbehavior:. https://github.com/theislab/scanpy/blob/681ce93e7e58956cb78ef81bc165558b84d6ebb0/scanpy/tests/conftest.py#L4-L6. `import matplotlib.pyplot [as plt]` means â€œIâ€™m an end user who just opened a notebook and I want the kitchen sink, give me everything and configure everythingâ€. Libraries shouldnâ€™t do it and scanpy is one. When we still had `scanpy.api` there would have been a case for importing pyplot there, as `scanpy.api` was for interactive use. Now we donâ€™t have any excuses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212:420,config,configure,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212,1,['config'],['configure']
Modifiability,Thatâ€™s super redundant now. Please extract all that text from `doc_scatter_bulk` into another variable and import and use that one instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476508242:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476508242,1,['variab'],['variable']
Modifiability,"Thatâ€™s the way anyway. I think first step would be to rewrite our `test` extra in terms of a) whatâ€™s needed for testing and b) what really are scanpy features being tested:. ```toml; test = [; 'pytest',; 'scanpy[dask]',; 'scanpy[zarr]',; ]; ```. then we can extra-by-extra make parts of our test suite optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088726702:54,rewrite,rewrite,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088726702,1,['rewrite'],['rewrite']
Modifiability,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python; adata.layers[""counts""] = adata.X.copy(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806:5,layers,layers,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806,2,['layers'],['layers']
Modifiability,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:81,plugin,plugin,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975,1,['plugin'],['plugin']
Modifiability,"The data also allows to detect a next issue: When multiple genes have the same value of `disp_cut_off`. Can be found if here e.g. dont do `sc.pp.normalize_total`:. ```py; import scanpy as as; adata = sc.datasets.pbmc3k(); # sc.pp.normalize_total(adata, target_sum=10000); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); adata.var[""highly_variable""].sum(); ```; ```; 10367; ```; Which is due to many genes having the value selected for the `disp_cut_off` here, having . ...`x[n-2]` = `x[n-1 ]` = `x[n]` = `x[n+1] `= `x[n+2]`... https://github.com/scverse/scanpy/blob/b918a23eb77462837df90d7b3a30a573989d4d48/src/scanpy/preprocessing/_highly_variable_genes.py#L408-L418. I tried to check how Seurat is proceeding in such a case, expecting to see how it breaks the ties. (data downloaded from [here](https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz)); Here I'm actually not sure how to turn off the `scale.factor` argument? Its set to 10'000 by default. ```R; library(dplyr); library(Seurat); library(patchwork). pbmc.data <- Read10X(data.dir = ""filtered_gene_bc_matrices/hg19/""). pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k""). pbmc <- NormalizeData(pbmc, normalization.method = ""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot"", nfeatures = 10000). length(VariableFeatures(pbmc)); ```; ```; 2292; ```; However, it turns out Seurat seems to restrict to the genes which are variable in the sense of passing the set mean threshold and normalized dispersion thresholds. These thresholds are ignored in scanpy if the number of genes is given. So not really an insight of how to break ties in this case. Would suggest to make a new issue, which the potential project on comparing the frameworks could address.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888:1412,Variab,VariableFeatures,1412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888,2,"['Variab', 'variab']","['VariableFeatures', 'variable']"
Modifiability,"The docs donâ€™t say it does, so this would be an enhancement, not a bug fix. itâ€™s not high priority, since itâ€™s easy to just do `ax = plt.subplot(); sc.pl.rank_genes_groups(adata, ax=ax)`. contributions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205#issuecomment-2437741950:48,enhance,enhancement,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205#issuecomment-2437741950,1,['enhance'],['enhancement']
Modifiability,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,adapt,adapting,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891,1,['adapt'],['adapting']
Modifiability,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430674069:785,variab,variables,785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430674069,1,['variab'],['variables']
Modifiability,"The latest version of scanpy added the possibility to enhance the plots in multiple ways. For this you need to use the new classes documented here https://scanpy.readthedocs.io/en/stable/api/scanpy.plotting.html#classes. In particular you want to check the function `add_totals()`: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.StackedViolin.add_totals.html#scanpy.pl.StackedViolin.add_totals. For your case you can do:; ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.StackedViolin(adata, markers, groupby='bulk_labels').add_totals().show(); ```. Other options using the function that you are familiar with is:. ```PYTHON; # here return_fig=True is used; plot = sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', return_fig=True); plot.add_totals().show(); ```. You can find further info here: https://github.com/theislab/scanpy/pull/1210",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193:54,enhance,enhance,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193,1,['enhance'],['enhance']
Modifiability,"The main change here is passing `None` instead of `0` to `total`, right?. Also: this makes some errors with files still existing make much more sense. I had no idea `KeyboardInterrupt` doesn't inherit from `Exception`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444:193,inherit,inherit,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444,1,['inherit'],['inherit']
Modifiability,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÃ„t show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:426,variab,variable,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441,1,['variab'],['variable']
Modifiability,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,config,configuration,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['config'],['configuration']
Modifiability,"The scanpyToCellbrowser function has an option useRaw that will use the; .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; the matrix and all annotations, or anndataToTsv to write just the matrix.; Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top; > variable genes. Is there a way to output the raw matrix including all genes?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560:189,variab,variable,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560,2,['variab'],['variable']
Modifiability,"The size of my dataset is:. ```AnnData object with n_obs Ã— n_vars = 19091 Ã— 23315```. Here is the full code:. ```; import scanpy as sc; import pandas as pd; import numpy as np. from anndata import AnnData. def harmony_integrate(; adata: AnnData,; key: str,; basis: str = ""X_pca"",; adjusted_basis: str = ""X_pca_harmony"",; **kwargs,; ):; try:; import harmonypy; except ImportError:; raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(); adata_merge.X = adata_merge.layers['counts']; sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(); sc.experimental.pp.normalize_pearson_residuals(adata_merge); adata_merge.layers['apr'] = adata_merge.X.copy(); sc.tl.pca(adata_merge, svd_solver=""arpack""); adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(); adata2 = adata_merge.copy(); ```. The frist test:. ```; # scanpy 1.9.6 that changes of this PR won't have taken effect yet.; # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py; harmony_integrate(adata1, key='batch', basis='X_pca_30'); harmony_integrate(adata2, key='batch', basis='X_pca_30'); np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%); Max absolute difference: 1.20792265e-12; Max relative difference: 4.37537551e-09; x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,; 0.564897],; [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:709,layers,layers,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,2,['layers'],['layers']
Modifiability,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:4,variab,variable,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943,1,['variab'],['variable']
Modifiability,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:4,variab,variable,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759,3,['variab'],['variable']
Modifiability,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:423,variab,variability,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639,1,['variab'],['variability']
Modifiability,"Thereâ€™s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. Theyâ€™ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (â€œFunction blah excepted a parameter foo of type Bar, but you passed a foo of type Bazâ€). iâ€™m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:298,variab,variables,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['variab'],['variables']
Modifiability,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915:189,config,configuration,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915,1,['config'],['configuration']
Modifiability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,config,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['config'],['configuration']
Modifiability,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python; import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs); sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]); ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272:96,variab,variables,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272,1,['variab'],['variables']
Modifiability,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407:42,layers,layers,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407,2,['layers'],['layers']
Modifiability,This is solved in PR #425 (which also includes other enhancements),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/465#issuecomment-461456817:53,enhance,enhancements,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465#issuecomment-461456817,1,['enhance'],['enhancements']
Modifiability,"This is the anndata working properly:; ```; AnnData object with n_obs Ã— n_vars = 24759 Ã— 29612; obs: 'sample', 'batch', 'n_counts'; var: 'ensembl_id', 'n_cells'; layers: 'counts'; ```; After normalization and logarithmize it with:; ```; adata.X = sc.pp.normalize_total(adata, inplace=False)['X']; adata.X = sc.pp.log1p(adata.X); ```; And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:; ```; AnnData object with n_obs Ã— n_vars = 17217 Ã— 33704; obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'; var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'; obsm: 'X_umap'; layers: 'counts'; ```; Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: ; ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):; ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:; ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2556#issuecomment-1643597794:162,layers,layers,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556#issuecomment-1643597794,3,['layers'],['layers']
Modifiability,"This is the origin of the error:. Some of the cells don't have neighbours at the variable adata_ref.obsp['distances'].tolil().rows:. ```; array([list([223, 280, 316, 5791]), list([3877, 5899, 7766, 7807]),; list([165, 304, 423, 713]), ..., list([]),; list([94, 865, 7077, 7666]), list([])], dtype=object); ## (the maximum 4 elements of each list comes from having run sc.pp.neighbors(adata_ref, n_neighbors = 5)) ##; ```. The above array is impossible to stack with np.stack due to the sublists having different lengths. A potential solution might be filtering out those cells without neighbours, though this is suboptimal. I have tried it and new rows remain empty. Only after repeating it a second time, it works:. ```; DEFINED_NEIGHB_NUM =5; sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref, n_neighbors = DEFINED_NEIGHB_NUM ); sc.tl.umap(adata_ref). b = np.array(list(map(len,adata_ref.obsp['distances'].tolil().rows))) == DEFINED_NEIGHB_NUM -1; adata_ref = adata_ref[b]; ```. A better solution would be correcting the Nearest Neighbour assignment so that it doesn't create empty distance lists. Did I understand this correctly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104437780:81,variab,variable,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104437780,1,['variab'],['variable']
Modifiability,"This line:. https://github.com/scverse/scanpy/blob/383a61b2db0c45ba622f231f01d0e7546d99566b/pyproject.toml#L162. means that pytest imports that module together with the other plugins, and *then* runs tests. That means that it will `import scanpy.testing._pytest` (i.e. `scanpy` and everthing imported in there) before pytest-cov is loaded and can do anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122:175,plugin,plugins,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122,1,['plugin'],['plugins']
Modifiability,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:728,extend,extend,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596,1,['extend'],['extend']
Modifiability,"This might be true the expected behaviour is not mentioned here.; Then as you pointed out it is more of an enhancement, which would be to make it match other plotting functions behaviour ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205#issuecomment-2446164660:107,enhance,enhancement,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205#issuecomment-2446164660,1,['enhance'],['enhancement']
Modifiability,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:166,plugin,plugins,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141,2,['plugin'],"['plugin', 'plugins']"
Modifiability,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:158,config,config,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267,3,['config'],['config']
Modifiability,"Totally forgot about this one sorry :(. **The problem** ; In `scatter_base`: https://github.com/theislab/scanpy/blob/040e61ff50836d4a6cdd7da7482dcb4ee50d05ae/scanpy/plotting/_utils.py#L736-L740. For non categorical variables, this code gets the current figure and adds a separate axis on which the colorbar is plotted.; Therefore, the axes objects on which the data is plotted do not contain a legend object.; Instead, `fig` should contain the colorbar axis and we could maybe manage to manipulate it as a workaround. There is also this DeprecationWarning popping up.; ```pytb; MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.; ax_cb = fig.add_axes(rectangle); ```. **Current workaround (also for all other sort of plots)**; The problem here is really that we don't have two separate figures / axes aren't handled correctly.; Good news is, that there is a way around using `plt.subplots` and using given `Axes` objects. even if we want to plot 2 plots side by side in a jupyter notebook (original post here: https://stackoverflow.com/questions/21754976/ipython-notebook-arrange-plots-horizontally).; However, `sc.pl.scatter` isn't exposing the figure object but only the axis. But if we specify `show=False`, it returns the axis and we can obtain the figure object using `matplotlib.pyplot.gcf()`.; Store these figures in a list and pass them to the `plot_nice()` function which will plot all your figures side by side until it runs out of space, after which it will create a linebreak and continue. Therefore, you can specify how many figures you want to plot per line, using the individual `figsize` argument. For my example it would look like this:; ```python; from flow_layout import plot_nice # import the required plot",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283:215,variab,variables,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283,1,['variab'],['variables']
Modifiability,Using RMM works but only to a certain extend. As far as I understand it you can oversubscribe VRAM to a maximum of 2X. If you go above that youâ€™ll get a memory alloc error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372:38,extend,extend,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372,1,['extend'],['extend']
Modifiability,"Using `coverage run` is intentional. Please just configure the shell so it exits when a line fails instead of changing this. The reason is at we have `addopts = ['-p', 'scanpy.testing.pytest']` (or so). If we use `pytest --cov`, coverage for things run during import breaks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956852933:49,config,configure,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956852933,1,['config'],['configure']
Modifiability,"Very strange. `variable` is some assigned name after an internal `pandas.melt`. . First, I would not recommend to plot all `adata.var_names` unless they are fewer (<30). But that seems not to be the problem. To discard a problem with seaborn violin plot, can you try `sc.pl.matrixplot` instead?. Also, do you get the same output in both cases after. ```; adata.obs.head(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/318#issuecomment-431813153:15,variab,variable,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318#issuecomment-431813153,1,['variab'],['variable']
Modifiability,We could then also consider extending it with rapids single cell @Intron7 . More of a `scverse reproducibility` page and maybe also bring in scvi tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696:28,extend,extending,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696,1,['extend'],['extending']
Modifiability,"We probably have two problems:. 1. CI doesnâ€™t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldnâ€™t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldnâ€™t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI â€¦ also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:214,config,config,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964,3,['config'],"['config', 'configurable']"
Modifiability,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634:42,layers,layers,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634,1,['layers'],['layers']
Modifiability,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/437#issuecomment-456209252:23,config,configurable,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437#issuecomment-456209252,1,['config'],['configurable']
Modifiability,"Well, the documentation of `highest_expr_genes` doesnâ€™t say that it supports layers, but this is a very sensible feature request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437659493:77,layers,layers,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437659493,1,['layers'],['layers']
Modifiability,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. ; * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479505756:51,variab,variables,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479505756,2,['variab'],"['variable', 'variables']"
Modifiability,"Weâ€™re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesnâ€™t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isnâ€™t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:40,config,configurable,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['config'],['configurable']
Modifiability,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:27,variab,variable,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273,1,['variab'],['variable']
Modifiability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:290,Refactor,Refactoring,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,2,"['Refactor', 'enhance']","['Refactoring', 'enhance']"
Modifiability,"When using **scanpy.pl.paga_path**, I experience the same error as @plrlhb12 (TypeError: **float() argument must be a string or a number, not 'csr_matrix'**) and I can also only generate a plot after deleting adata.raw. As a consequence, I can only plot genes that are filtered for high variability during preprocessing and still present in adata.var.gene_ids. ; I would be glad if there was a way to make it work without deleting adata.raw and therefore being able to plot also non-highly variable genes! Thank you!. **Versions:**; > anndata==0.7.4 matplotlib==3.3.0 numpy==1.19.1 pandas==1.1.0 scanpy==1.6.0 scipy==1.5.2 sklearn==0.23.1 igraph==0.8.2 leidenalg==0.8.1 umap==0.4.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766:287,variab,variability,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766,2,['variab'],"['variability', 'variable']"
Modifiability,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095:207,adapt,adapted,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095,1,['adapt'],['adapted']
Modifiability,Works for me! Iâ€™d say we refactor the helper function in a separate PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-530443204:25,refactor,refactor,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530443204,1,['refactor'],['refactor']
Modifiability,"Yeah the marker assignments here are random, that might be a bit confusing indeed. The only point is that the colors for the clusters (columns) match with the *names* of the marker gene lists (rows). So if, in an actual dataset, you now name the correct clusters ""T cells"", ""B cells"" etc. in you obs.clusters variable, and match those names with your marker gene dict keys, everything will match up. ; Does that make sense?; I just didn't go through annotations of the clusters here, that would be a bit tedious. But try it out on your own (correctly annotated) anndata object if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875:309,variab,variable,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875,1,['variab'],['variable']
Modifiability,"Yeah, AnnData doesnâ€™t serialize arbitrary attributes to disk. I assume the output of `fit_transform` is cellÃ—gene? Then you could do `all_data.layers['magic'] = ...`. If the output is `cellÃ—y` with `y != n_genes` then you should do `all_data.obsm['magic'] = ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/426#issuecomment-454072095:143,layers,layers,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426#issuecomment-454072095,1,['layers'],['layers']
Modifiability,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559186183:67,variab,variable,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559186183,1,['variab'],['variable']
Modifiability,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs Ã— n_vars = 68865 Ã— 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:710,layers,layers,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223,1,['layers'],['layers']
Modifiability,"Yeah, that looks pretty weird. In your example, where did the variable `integrated_anterior` come from?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049165791:62,variab,variable,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049165791,1,['variab'],['variable']
Modifiability,"Yeah, that makes absolute sense. . Even better would be to add layers similar to `adata.X` and pass that into any function. Something like `adata.obs_custom`. In which case `adata.obs_custom` will inherit the same properties as that of `adata.obs` and users can make as many as they need in an organized manner. It will also allow users to store different values with the same column name (of course in different layers). e.g. `adata.obs_custom['same_column_name']` and `adata.obs_custom2['same_column_name']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839:63,layers,layers,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839,3,"['inherit', 'layers']","['inherit', 'layers']"
Modifiability,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic.; However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456776304:78,variab,variable,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456776304,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,config,configuration,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328,1,['config'],['configuration']
Modifiability,"Yes I would like to separate both `sample` and `leiden_r1`, I should create a new variable `adata.obs['leiden+sample']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609699372:82,variab,variable,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609699372,1,['variab'],['variable']
Modifiability,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,adapt,adapting,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139,1,['adapt'],['adapting']
Modifiability,"Yes, I'll send an example in a bit, recovered variable genes seem wildly discrepant. I can get to this tomorrow! Thanks for your quick response",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1865046956:46,variab,variable,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1865046956,1,['variab'],['variable']
Modifiability,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". ðŸ™‚ And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-399892886:328,variab,variables,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-399892886,1,['variab'],['variables']
Modifiability,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you ðŸ˜„). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,adapt,adapt,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468,1,['adapt'],['adapt']
Modifiability,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:831,variab,variables,831,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242,2,['variab'],['variables']
Modifiability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:141,variab,variable,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,2,['variab'],['variable']
Modifiability,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:441,extend,extended,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467,1,['extend'],['extended']
Modifiability,"Yes, that is what I currently do. It is just a matter of aesthetics. Since I have a large number of variables that I generate with custom functions, I wanted to store them separately based on what they represent in separate modules under (`adata.uns`). . Adding everything to `adata.obs` quickly gets cluttered. No worries just wanted to see if it was an option. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538:100,variab,variables,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538,1,['variab'],['variables']
Modifiability,"Yes, then this could be extended in scanpy. I imagine this would be very useful for reference mapping visualisations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133898960:24,extend,extended,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133898960,1,['extend'],['extended']
Modifiability,"Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-390478463:184,extend,extend,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390478463,1,['extend'],['extend']
Modifiability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:319,variab,variable,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['variab'],['variable']
Modifiability,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:48,layers,layers,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801,2,['layers'],['layers']
Modifiability,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):; 1. Using fewer genes is computationally less expensive for downstream analysis.; 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430:238,variab,variable,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430,1,['variab'],['variable']
Modifiability,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1791,Rewrite,Rewrite,1791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['Rewrite'],['Rewrite']
Modifiability,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py); `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073:253,variab,variable,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073,1,['variab'],['variable']
Modifiability,"```; Traceback (most recent call last):; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx/events.py"", line 101, in emit; results.append(listener.handler(self.app, *args)); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx_autodoc_typehints/__init__.py"", line 446, in process_docstring; formatted_annotation = format_annotation(; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/scanpydoc/elegant_typehints/formatting.py"", line 126, in format_annotation; arg_name = variables[""argname""].replace(r""\_"", ""_""); KeyError: 'argname'; ```. Not sure why RTD fails with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735:643,variab,variables,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735,1,['variab'],['variables']
Modifiability,"```restructuredtext; Returns ; ------- ; adata : :class:`~scanpy.api.AnnData` ; Annotated data matrix, where obsevations/cells are named by their ; barcode and variables/genes by gene name. The data matrix is stored in ; `adata.X`, cell names in `adata.obs_names` and gene names in ; `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/179#issuecomment-398095606:160,variab,variables,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179#issuecomment-398095606,1,['variab'],['variables']
Modifiability,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:51,variab,variables,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296,1,['variab'],['variables']
Modifiability,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754:219,adapt,adapted,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754,1,['adapt'],['adapted']
Modifiability,"`n_genes_user` is supposed to limit the length of the returned tables. however, one still needs to search all genes `n_genes` (`== X.shape[1]`) in order to get the top-scoring ones. this is the rationale behind the two variables and the naming",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/71#issuecomment-359410549:219,variab,variables,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/71#issuecomment-359410549,1,['variab'],['variables']
Modifiability,"a import stacked_violin; 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); 308 ; 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 819 if isinstance(var_names, str):; 820 var_names = [var_names]; --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 822 ; 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 510 ; 511 def __getitem__(self, index):; --> 512 oidx, vidx = self._normalize_indices(index); 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); 538 obs, var = super(Raw, self)._unpack_index(packed_index); 539 obs = _normalize_index(obs, self._adata.obs_names); --> 540 var = _normalize_index(var, self.var_names); 541 return obs, var; 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); 270 raise KeyError(; 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 272 .format(index)); 273 return positions.values; 274 else:; ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222:1957,layers,layers,1957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222,1,['layers'],['layers']
Modifiability,"a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1596,variab,variability,1596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638,2,['variab'],['variability']
Modifiability,"aac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Version('0.10.6')>; ```. </details>. Interesting to see that this seems to work now!. I do think the recommended solution here is ""don't do this"", but I'm considering just using `anndata.__version__` here since that will work when the package metadata is broken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:1858,enhance,enhanced,1858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,1,['enhance'],['enhanced']
Modifiability,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:552,variab,variable,552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,2,['variab'],['variable']
Modifiability,"ally, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > â€”; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:1038,variab,variable,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368,1,['variab'],['variable']
Modifiability,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3555,flexible,flexible,3555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['flexible'],['flexible']
Modifiability,"before normolization, you can do adata.layers['counts']=adata.X.copy() to add the counts of all genes to the layers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1153040097:39,layers,layers,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1153040097,2,['layers'],['layers']
Modifiability,"bols; > under the column adata.var[â€œgene_nameâ€]. When I call:; > sc.pl.umap(adata, color=['ENSMUSG00000074637']); > It plots no problem. However, when I call:; > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'); > I get the following error:; >; > Traceback (most recent call last):; >; >; >; > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>; >; > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; >; > return plot_scatter(adata, basis='umap', **kwargs); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter; >; > use_raw=use_raw, gene_symbols=gene_symbols); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values; >; > .format(value_to_plot, adata.obs.columns)); >; >; >; > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',; >; > 'louvain'],; >; > dtype='object'); >; >; > Inspecting adata.var[""gene_name""] give:; >; > index; >; > ENSMUSG00000002459 Rgs20; >; > ENSMUSG00000033740 St18; >; > ENSMUSG00000067879 3110035E14Rik; >; > ENSMUSG00000025912 Mybl1; >; > ENSMUSG00000016918 Sulf1; >; > ENSMUSG00000025938 Slco5a1; >; > ENSMUSG00000025930 Msc; >; > ENSMUSG00000025921 Rdh10; >; > ENSMUSG00000025777 Gdap1; >; > ENSMUSG00000025776 Crispld1; >; > ENSMUSG00000025927 Tfap2b; >; > ENSMUSG00000025931 Paqr8; >; > ENSMUSG00000026158 Ogfrl1; >; > ...; >; >; > I'm not sure what I'm doing wrong here. I can do just about anything using; > the ensembl ids, but I am having a lot of trouble using the gene symbols. I; > would like to be able to use the gene symbols in the plots for umap,; > violin, pca, etc. Any help would be much appreciated. Thanks!; >;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-472840788:1447,variab,variable,1447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-472840788,1,['variab'],['variable']
Modifiability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:305,extend,extending,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['extend'],['extending']
Modifiability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:2150,refactor,refactored,2150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactored']
Modifiability,"continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[â€œgene_nameâ€]`. When I call:; `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`; It plots no problem. However, when I call:; `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`; I get the following error:; ```; Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>; sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter; use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values; .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',; 'louvain'],; dtype='object'); ```; Inspecting adata.var[""gene_name""] give:; ```; index; ENSMUSG00000002459 Rgs20; ENSMUSG00000033740 St18; ENSMUSG00000067879 3110035E14Rik; ENSMUSG00000025912 Mybl1; ENSMUSG00000016918 Sulf1; ENSMUSG00000025938 Slco5a1; ENSMUSG00000025930 Msc; ENSMUSG00000025921 Rdh10; ENSMUSG00000025777 Gdap1; ENSMUSG00000025776 Crispld1; ENSMUSG00000025927 Tfap2b; ENSMUSG00000025931 Paqr8; ENSMUSG00000026158 Ogfrl1; ...; ```; I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-472756442:1252,variab,variable,1252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-472756442,1,['variab'],['variable']
Modifiability,"d remove the graph slot and this error is gone, but now I have a new error: . ```; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-2-aae861244dfa> in <module>; ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] for k in keys]; 211 ; --> 212 return arrays_t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885:1023,layers,layers,1023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885,1,['layers'],['layers']
Modifiability,"dFragment-->; </body>; </html>. Both tutorial adatas after a successful ingest:; ```. (AnnData object with n_obs Ã— n_vars = 700 Ã— 208; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap', 'rep'; varm: 'PCs'; obsp: 'distances', 'connectivities',; AnnData object with n_obs Ã— n_vars = 2638 Ã— 208; obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'; var: 'n_cells'; uns: 'draw_graph', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'; varm: 'PCs'; obsp: 'distances', 'connectivities'); ```. Now my data, adata_ref:. <html><body>; <!--StartFragment--><div class=""lm-Widget p-Widget lm-Panel p-Panel jp-Cell-inputWrapper""><div class=""lm-Widget p-Widget jp-InputArea jp-Cell-inputArea"">. Â  | celltype | louvain; -- | -- | --; cell1 | hepatic stellate cells | 1; cell2 | cholangiocytes | 1; ... | ... | ... <p>8439 rows Ã— 2 columns</p>; </div></div><!--EndFragment-->; </body>; </html>. and my adata that I wish to ingest:. <html><body>; <!--StartFragment-->. Â  | louvain; -- | --; cell1 | 0; cell2 | 0; ... <!--EndFragment-->; </body>; </html>. Both my adata files have the same 40 variables and pca/umaps, they look like this:. ```; (AnnData object with n_obs Ã— n_vars = 8989 Ã— 40; obs: 'louvain'; uns: 'pca', 'neighbors', 'umap', 'louvain', 'louvain_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities',; AnnData object with n_obs Ã— n_vars = 8439 Ã— 40; obs: 'celltype', 'louvain'; uns: 'pca', 'neighbors', 'umap', 'louvain', 'louvain_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'); ```. I suspect the error stems from the Nearest Neighbours. Or maybe my number of variables (40) is too small?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104240383:2742,variab,variables,2742,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104240383,2,['variab'],['variables']
Modifiability,"donâ€™t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,adapt,adapting,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361,1,['adapt'],['adapting']
Modifiability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2875,inherit,inherits,2875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['inherit'],['inherits']
Modifiability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1959,variab,variable,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['variab'],['variable']
Modifiability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1951,parameteriz,parameterized,1951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['parameteriz'],['parameterized']
Modifiability,good catch! Thank you for reporting this. ; Pinging @Koncopd since I believe you were involved in major refactoring of this. ; Thank you @rpeys,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951:104,refactor,refactoring,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951,1,['refactor'],['refactoring']
Modifiability,"group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); > 819 if isinstance(var_names, str):; > 820 var_names = [var_names]; > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); > 822; > 823 if 'color' in kwds:; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); > 1983 matrix = adata[:, var_names].layers[layer]; > 1984 elif use_raw:; > -> 1985 matrix = adata.raw[:, var_names].X; > 1986 else:; > 1987 matrix = adata[:, var_names].X; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); > 510; > 511 def __getitem__(self, index):; > --> 512 oidx, vidx = self._normalize_indices(index); > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; > 514 else: X = self._adata.file['raw.X'][oidx, vidx]; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); > 538 obs, var = super(Raw, self)._unpack_index(packed_index); > 539 obs = _normalize_index(obs, self._adata.obs_names); > --> 540 var = _normalize_index(var, self.var_names); > 541 return obs, var; > 542; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); > 270 raise KeyError(; > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; > --> 272 .format(index)); > 273 return positions.values; > 274 else:; >; > Cheers,; > Samuele; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910:3239,variab,variables,3239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910,1,['variab'],['variables']
Modifiability,"he 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress!. Minimal reproducer:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[:411].copy(); sc.pp.neighbors(pbmc); res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]); ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:1557,variab,variables,1557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541,1,['variab'],['variables']
Modifiability,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```; cat_columns = adata.obs.select_dtypes(['category']).columns; adata.obs[cat_columns] = adata.obs[cat_columns].astype(str); del cat_columns; ```; but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979:71,variab,variables,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979,2,['variab'],"['variable', 'variables']"
Modifiability,"hi Alex, I'm not sure how to test this either. I tried to run in a new clean conda environment but cant use more than 50% of the cells in pre-processing (otherwise the process is killed at even the normalization stage, ""pp.normalize_ per_cell"" and uses over 120GB RAM); The file from 10X i'm using is the link to ""1M_neurons aggr - Gene / cell matrix HDF5 (filtered)"" from https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/ ; This is what i type into the terminal; $ python cluster_mouse_brain.py 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5 (0:01:19.78); running recipe zheng17; filtered out 3983 genes that are detected in less than 1 counts; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Killed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-469664995:566,Variab,Variable,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-469664995,3,['Variab'],['Variable']
Modifiability,"hi,. finally managed to add some tests. Had to refactor the original `test_score_genes.py` a little, I hope that's ok: The one test that was already there still exists, I just pulled out the creation of the adata into a separate function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235:47,refactor,refactor,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235,1,['refactor'],['refactor']
Modifiability,"how to add the raw counts to my h5ad object?; Any idea how to convert the three data slot from Seurat [raw, data, scaled.data] to h5ad layers?. ```; SeuratObject@assays$RNA@counts; SeuratObject@assays$RNA@data; SeuratObject@assays$RNA@scale.data; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210:135,layers,layers,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210,1,['layers'],['layers']
Modifiability,"htly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1953,Variab,Variables,1953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['Variab'],['Variables']
Modifiability,"images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3170,variab,variables,3170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variables']
Modifiability,"ing, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1228,variab,variables,1228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variables']
Modifiability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; â€‹; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9724,config,config,9724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['config'],['config']
Modifiability,"it should definitely work. on a properly configured system (including docker images), the encoding should be UTF-8. youâ€™re right, we should probably do it. the only reason we didnâ€™t yet is that we open quite a few files in the codebase, and if one of those open calls expects UTF-8, itâ€™ll break again, but this time deeper down and harder to reproduce.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072:41,config,configured,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072,1,['config'],['configured']
Modifiability,"itch the t-SNE implementation to openTSNE at the very least. For the recipes, there' already something similar in the preprocessing module. So I'd imagine calling standard t-SNE with `sc.tl.tsne` and the recipes like `sc.tl.tsne.recipe_multiscale`. > And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is very similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. I very much prefer option 1. If I understand option 2 correctly, we would normalize the 15 neighbors to essentially `perplexity=5`. I've never once found a case where that is useful, so having this as the default behaviour in scanpy seems like a really bad idea (I foresee a lot of issues in the style ""why is t-SNE not working?""). Using a uniform kernel produces results that are virtually indistinguishable from vanilla t-SNE, so that's fine IMO, and it's faster as well. It's still less than the default `perplexity=30`, but this seems like the best option. Whatever we agree on, the same can be applied to the ingest functionality, so adding that would also be straightforward. > Moreover, we could make the standard t-SNE available by extending sc.pp.neighors with method=""tsne"" (there are several methods there already). I don't understand this, why would this belong on `sc.pp.neighbors`? The graph weighing should go into the `sc.tl.tsne` call. Are the UMAP weights assigned to the graph in `sc.pp.neighbors`? That seems questionable. I would expect the output to be a directed, unweighted graph, and let each method take care of the graph. If anything, I'd expect it to weight it using the Jaccard index of shared nearest neighbors, which seems to me like pretty much the standard thing to do in single-cell analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360:1465,extend,extending,1465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360,1,['extend'],['extending']
Modifiability,"iterally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` Ã— `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:951,Variab,Variables,951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['Variab'],['Variables']
Modifiability,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 658 # check if value to plot is in var; 659 elif use_raw is False and value_to_plot in adata.var_names:; --> 660 color_vector = adata[:, value_to_plot].X; 661 ; 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1307 def __getitem__(self, index):; 1308 """"""Returns a sliced view of the object.""""""; -> 1309 return self._getitem_view(index); 1310 ; 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1311 def _getitem_view(self, index):; 1312 oidx, vidx = self._normalize_indices(index); -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1314 ; 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 723 self._X = None; 724 else:; --> 725 self._init_X_as_view(); 726 ; 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 750 shape = (; 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 753 ); 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 148 return 1; 149 else:; --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-456954317:2233,layers,layers,2233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-456954317,1,['layers'],['layers']
Modifiability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of [â€¦]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:2167,config,configure,2167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['config'],['configure']
Modifiability,"manually edit such minor things that can be easily addressed in the plotting code ðŸ˜„ . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2206,config,config,2206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,1,['config'],['config']
Modifiability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1271,refactor,refactor,1271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,1,['refactor'],['refactor']
Modifiability,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4212,config,config,4212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['config'],['config']
Modifiability,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` Ã— `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1317,variab,variables,1317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['variab'],['variables']
Modifiability,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1890,layers,layers,1890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['layers'],['layers']
Modifiability,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1756,variab,variable,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variable']
Modifiability,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,adapt,adapt,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['adapt'],['adapt']
Modifiability,"normalize total operates inplace. in the code example above `adata.layers[""counts""]` and `adata.X` are referencing the same object. You should add `.copy()` if you want independent behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389#issuecomment-1371002904:67,layers,layers,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389#issuecomment-1371002904,1,['layers'],['layers']
Modifiability,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:1653,variab,variable,1653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,1,['variab'],['variable']
Modifiability,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034:87,refactor,refactoring,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034,1,['refactor'],['refactoring']
Modifiability,"olin; 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); 308 ; 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 819 if isinstance(var_names, str):; 820 var_names = [var_names]; --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 822 ; 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 510 ; 511 def __getitem__(self, index):; --> 512 oidx, vidx = self._normalize_indices(index); 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); 538 obs, var = super(Raw, self)._unpack_index(packed_index); 539 obs = _normalize_index(obs, self._adata.obs_names); --> 540 var = _normalize_index(var, self.var_names); 541 return obs, var; 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); 270 raise KeyError(; 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 272 .format(index)); 273 return positions.values; 274 else:; ```. Cheers,; Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222:2892,variab,variables,2892,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222,1,['variab'],['variables']
Modifiability,"or highly expressed genes is probably more meaningful difference than for a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1218,variab,variable,1218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638,1,['variab'],['variable']
Modifiability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2115,layers,layers,2115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,2,['layers'],['layers']
Modifiability,"ose docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1019,variab,variables,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['variab'],['variables']
Modifiability,"ould I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious to know. Ultimately, the decision is up to you guys, since this is more of a desi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2888,refactor,refactoring,2888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['refactor'],['refactoring']
Modifiability,"owngrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1186,layers,layers,1186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,1,['layers'],['layers']
Modifiability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1159,enhance,enhanced,1159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['enhance'],['enhanced']
Modifiability,"py log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [â€¦] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1457,config,configured,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['config'],['configured']
Modifiability,"pytables is starting to throw warnings, so it may be time for a rewrite and moving the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043:64,rewrite,rewrite,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043,1,['rewrite'],['rewrite']
Modifiability,"resolve the dependencies, and for that purpose gets all the installed packagesâ€™ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees â€œ1.7.0rc2â€ and decides â€œIâ€™ll update this even when not asked to updateâ€. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, thatâ€™s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since weâ€™re past 1.7 now, unless you havenâ€™t git-pulled yet, I assume your dev installâ€™s metadata has gone stale. I guess pip wouldnâ€™t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess thatâ€™s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,adapt,adapted,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['adapt'],['adapted']
Modifiability,"s this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1075,variab,variable,1075,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variable']
Modifiability,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037:64,extend,extended,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037,1,['extend'],['extended']
Modifiability,"sorry, copilot hallucinated me a layer variable!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437815137:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437815137,1,['variab'],['variable']
Modifiability,"sounds good!. but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-432728241:64,extend,extend,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-432728241,1,['extend'],['extend']
Modifiability,"sounds to me like you know what you're doing. Where do you get your PCA from? Seurat? Otherwise I would consider running `sc.pp.pca(adata, svd_solver='arpack')` with the highly variable gene set and your pre-processed data from Seurat. Also, for the future, an easier way to go between Seurat and Scanpy might be [anndata2ri](https://www.github.com/flying-sheep/anndata2ri). I have an example notebook for that [here](https://www.github.com/LuckyMD/Code_snippets).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498843181:177,variab,variable,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498843181,1,['variab'],['variable']
Modifiability,"sure! in short: alex said he didnâ€™t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,adapt,adapt,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977,1,['adapt'],['adapt']
Modifiability,"t scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1082,config,config,1082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,1,['config'],['config']
Modifiability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [â€¦] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1772,config,configs,1772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['config'],"['configs', 'configuration']"
Modifiability,"th issues around categorical values. To me this suggests a need to have separate arguments for the two cases (`order_categorical`, `order_continuous`), though this raises issues with ""vectorizing"" the argument. Docstrings for these arguments would look something like:. ```rst; order_continuous: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""ascending""; How to order points in plots colored by continuous values. Options include:; * ""current"": use current ordering of AnnData object; * ""random"": randomize the order; * ""ascending"": points with the highest value are plotted on top; * ""descending"": points with lowest value are plotted on top; order_categorical: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""random""; How to order non-null categorical points in the plot. Uses same options as order_continuous.; ```. In this case, `sort_order` would be deprecated, and tell the user to use `order_continuous` instead. ## Potential extensions. * We could also allow users to pass `Callable[Vector, Vector[int]]`s (e.g. function which takes color vector, returns vector of integers) as arguments. ## Possible issues. ### Vectorization could be complicated. Vectorization of argument unclear/ maybe not possible. That is, what if I want the same variable twice, but ordered differently? This would look like: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8""], order_continuous=[""ascending"", ""descending""]); ```. Now what if I wanted to also plot a categorical value? Is this: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8"", ""leiden""], order_continuous=[""ascending"", ""descending"", None]); ```. ### Null values. This solution assumes we still want null values plotted on bottom. Should there be control over that?. ## Some references for other libraries:. * [`altair.Sort`](https://altair-viz.github.io/user_guide/generated/core/altair.Sort.html#altair.Sort); * (I'm actually not sure if other libraries do this, datashader does `max`/ `min`/ `mean` which is sorta similar?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554:1598,variab,variable,1598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554,1,['variab'],['variable']
Modifiability,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1230,variab,variables,1230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119,2,['variab'],['variables']
Modifiability,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:989,variab,variable,989,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['variab'],['variable']
Modifiability,"that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```; â¡ â „â €â ¨â¡¯â¢€â €â â¢¡â €â  â¢€â  â¢‚â €â â €â â â£¢â €â¢‚â €â ˆâ ’â£‚â ‚â €; â †â¢Œâ â¡â ˆâ €â¡€â –â ‚â €â â ‚â €â ‰â â¡€â €â €â ˆâ  â „â ‰â¢€â¡€â €â €â €â ‚; â ‘â €â  â €â ƒâ €â €â …â¢€â  â „â¡€â …â ‚â¢€â ªâ €â ¦â¢€â €â¢ƒâ ˆâ¢€â Œâ šâ €â €â ƒ; â â ‚â¡ƒâ ˆâ €â¢€â €â ™â¢€â ¥â €â €â „â¡â €â  â ˆâ €â ˆâ ƒâ ‚â  â£€â €â ˆâ£â â †; â¡€â â â  â €â â¢â¡„â£‚â €â €â ˜â €â €â €â  â ‚â €â¡€â ¨â â €â €â €â â â £â ¤; â €â¡â¢€â¢¢â €â â ”â €â â €â ƒâ €â¢€â¢€â â ƒâ „â €â¡‡â Šâ „â €â¡ˆâ¢€â €â €â£€â †; â €â¢â£¤â¡„â  â ‚â ƒâ¡ˆâ ˜â €â €â €â¡‚â °â¢„â Šâ¡‚â €â â ‚â €â „â €â €â¢±â ©â ˆâ¢€; â¢â €â ‘â šâ â ‚â ‚â â â €â €â¢€â  â €â â ˆâ ˆâ¡¨â €â ‚â €â¡ˆâ ˆâ â¡â£€â¢â ‚; â €â €â €â â €â  â …â â¡ â ‡â¢â €â €â –â¢‰â£€â €â¢€â €â  â¡€â €â¡€â¢°â â ‚â¢‰â ‚; â €â €â €â ‚â  â¢ â¡â¡„â¡Œâ €â €â  â¢…â €â „â €â¢•â¢â €â „â¡‚â¢€â ‚â €â ‚â ˆâ¡¸â ‚; â €â €â €â¢â¡‚â €â¢€â â €â °â¡€â ‘â¡€â €â  â €â â¢€â ˆâ †â ¤â „â¢€â €â£€â ¢â¡€â €; â ‚â¢€â¢ªâ¢˜â €â¢€â ©â …â¢„â „â  â  â â €â €â¢€â  â ‚â €â â¡˜â €â €â â ¢â¡â €â €; â¢€â Œâ¡˜â ˜â ‚â „â¢€â €â¢ â ”â ˆâ¢€â ˆâ €â €â  â¡€â¡‚â „â¢€â €â €â €â â ”â¢ˆâ¢°â €; â â â¡€â¡ â €â â  â ˆâ €â¢€â €â ˜â ‚â €â €â €â â °â „â¡¡â  â¡€â €â €â ‚â  â â ; ```. While this is one with blocks along the diagonal:. ```; â ¿â£§â£¤â£¤â£¤â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â ¿â ¿â ¿â ¿â£§â£¤â£¤â£¤â£¤â£¤â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â ›â ›â ›â ›â ›â ›â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â¡„â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:2007,variab,variables,2007,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variables']
Modifiability,"the idea is you want to take advantage of the very nice macro scanpy provides to make multiple umap subplots, when you specify multiple variables to color by (`sc.pl.umap(colors=['cell_type', 'other'])`). However, for each of those subplots, you might like to place the legend in different places, e.g. on the data for the cell types, but off to the side for the other variable. Unfortunately, scanpy only allows you to specify `legend_loc` once for all the subplots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2309#issuecomment-1257066552:136,variab,variables,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2309#issuecomment-1257066552,2,['variab'],"['variable', 'variables']"
Modifiability,"tion with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3864,rewrite,rewrite,3864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['rewrite'],['rewrite']
Modifiability,"took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any good way for a conver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:1019,variab,variable,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499,1,['variab'],['variable']
Modifiability,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:1968,inherit,inherits,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['inherit'],['inherits']
Modifiability,"use `cmap`. In general you can use any option available for; `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram; > <https://github.com/fidelram> how to we change the color palette for; > numerical variables? currently setting palette = 'Oranges' only works for; > the categorical ones; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074:331,variab,variables,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074,1,['variab'],['variables']
Modifiability,"var_names[0:4], groupby='celltype', color_map = 'Reds'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-72-4fc81df5ca3f> in <module>; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_names); 157 return obs, var; 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 91 not_found = indexer[positions < 0]; 92 raise KeyError(; ---> 93 f""Values {list(not_found)}, from {list(indexer)}, ""; 94 ""are not valid obs/ var names or indices.""; 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:1626,layers,layers,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652,1,['layers'],['layers']
Modifiability,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:1452,variab,variable,1452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,2,['variab'],['variable']
Modifiability,"ython3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10953,flexible,flexible,10953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['flexible'],['flexible']
Modifiability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1766,variab,variables,1766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,['variab'],['variables']
Modifiability,"â €â ‚â  â¢ â¡â¡„â¡Œâ €â €â  â¢…â €â „â €â¢•â¢â €â „â¡‚â¢€â ‚â €â ‚â ˆâ¡¸â ‚; â €â €â €â¢â¡‚â €â¢€â â €â °â¡€â ‘â¡€â €â  â €â â¢€â ˆâ †â ¤â „â¢€â €â£€â ¢â¡€â €; â ‚â¢€â¢ªâ¢˜â €â¢€â ©â …â¢„â „â  â  â â €â €â¢€â  â ‚â €â â¡˜â €â €â â ¢â¡â €â €; â¢€â Œâ¡˜â ˜â ‚â „â¢€â €â¢ â ”â ˆâ¢€â ˆâ €â €â  â¡€â¡‚â „â¢€â €â €â €â â ”â¢ˆâ¢°â €; â â â¡€â¡ â €â â  â ˆâ €â¢€â €â ˜â ‚â €â €â €â â °â „â¡¡â  â¡€â €â €â ‚â  â â ; ```. While this is one with blocks along the diagonal:. ```; â ¿â£§â£¤â£¤â£¤â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â ¿â ¿â ¿â ¿â£§â£¤â£¤â£¤â£¤â£¤â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €; â €â €â €â €â €â ›â ›â ›â ›â ›â ›â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â¡„â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â ¿â ¿â ¿â ¿â ¿â ¿â ¿â ¿â ¿â ¿â£§â£¤â €â €â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ›â¢»â£¶â£¶â €â €â €; â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˜â ›â¢»â£¶â¡†â €; â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ ‰â£¿â£¿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4201,layers,layers,4201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,3,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"æˆ‘åŒæ ·é¢ä¸´ç€è¿™ä¸ª bug; æˆ‘çš„ä»£ç æ˜¯; ```python; #genes_to_plot = ['Blvrb','Klf1','Serpina3f','Coro1a','Napsa','Ly6c2']; genes_to_plot = ['Blvrb',#MEP marker; 'Klf1',#MEP marker; 'Serpina3f']#CMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); genes_to_plot = ['Coro1a',#CMP and GMP marker; 'Napsa',#GMP marker; 'Ly6c2']#GMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); ```; é”™è¯¯ä¿¡æ¯ï¼š; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-61edd8063c25> in <module>(); 3 'Klf1',#MEP marker; 4 'Serpina3f']#CMP marker; ----> 5 sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); 6 genes_to_plot = ['Coro1a',#CMP and GMP marker; 7 'Napsa',#GMP marker. ~/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 124 (x in adata.obs.keys() or x in adata.var.index); 125 and (y in adata.obs.keys() or y in adata.var.index); --> 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; 128 return _scatter_obs(**args). ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key); 4069 False; 4070 """"""; -> 4071 hash(key); 4072 try:; 4073 return key in self._engine. TypeError: unhashable type: 'list'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041:954,layers,layers,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041,1,['layers'],['layers']
Performance," (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2084,cache,cached,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/python3.12/site-packages/dask/threaded.py"", line 90 in get; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 662 in compute; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 376 in compute; File ""~/Dev/scanpy/tests/test_utils.py"", line 243 in test_is_constant_dask; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 159 in pytest_pyfunc_call; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 1627 in runtest; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 17",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:4389,queue,queue,4389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['queue'],['queue']
Performance," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2003,cache,cached,2003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1282,optimiz,optimized,1282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['optimiz'],['optimized']
Performance," parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneTy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3198,optimiz,optimization,3198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance, py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5795,cache,cached-property,5795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cached-property']
Performance," the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1323,perform,perform,1323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['perform'],['perform']
Performance," we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1626,perform,performing,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643,1,['perform'],['performing']
Performance,![Screenshot from 2024-04-04 12-00-02](https://github.com/scverse/scanpy/assets/37635888/adfce8bd-34ac-44c7-9530-43b3a73577e8). I have some concerns about the performance of the no numba version for larger datasets. So it might be better to either switch to the numba kernel for larger datasets or take the compile hit for small datasets,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2036739880:159,perform,performance,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2036739880,1,['perform'],['performance']
Performance,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2863,cache,cache,2863,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['cache'],['cache']
Performance,"(most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1345,cache,cached-property,1345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['cache'],['cached-property']
Performance,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:164,cache,cached,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,10,['cache'],['cached']
Performance,*; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.w,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1122,cache,cached,1122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,", tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer); 3475 # trying to reindex on an axis with duplicates; 3476 if not self._index_as_unique and len(indexer):; -> 3477 raise ValueError(""cannot reindex from a duplicate axis""); 3478 ; 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis; ```; Loading a single h5 file works and produces expected output:; ```; a = sc.read_10x_h5('./a.h5', gex_only = True); a; AnnData object with n_obs Ã— n_vars = 7474 Ã— 31053; var: 'gene_ids', 'feature_types', 'genome'; ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:4485,Load,Loading,4485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,1,['Load'],['Loading']
Performance,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar!. ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. Iâ€™ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357:350,tune,tuned,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357,1,['tune'],['tuned']
Performance,"-------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1626,cache,cache,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,2,['cache'],['cache']
Performance,"--------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWeb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1311,Bottleneck,Bottleneck,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['Bottleneck'],['Bottleneck']
Performance,"-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4197,cache,cached,4197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1749,cache,cached,1749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (7.1.1); Requirement already satisfied: networkx>=2.3 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.6.3); Requirement already satisfied: importlib-metadata>=0.7 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.8.2); Requirement already satisfied: joblib in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.1.0); Requirement already satisfied: sinfo in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.3.4); Requirement already satisfied: patsy in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.2); Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Requirement already satisfied: six in c:\users\charles\anaconda3\lib\site-packages (from h5py>=2.10.0->scanpy) (1.16.0); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.7.0); Requirement already satisfied: cycler>=0.10 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (0.11.0); Requirement already satisfied: pyparsing>=2.2.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (3.0.4); Requirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:2644,cache,cached,2644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,".1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2306,cache,cached-property,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached', 'cached-property']"
Performance,"/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.py; # sphinx.ext.autodoc.preserve_defaults (1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/autodoc/preserve_defaults.py; # sphinx.ext.autodoc.type_comment (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/aut",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1508,Load,Loaded,1508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['Load'],['Loaded']
Performance,"0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 3: As you recommend, I do `!pip uninstall tables` and `conda install -c conda-forge pytables`, then; ```python; import tables # pass. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_15024/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 19 from numpy import random; 20 from scipy import sparse; ---> 21 from anndata import AnnData, __version__ as anndata_version; 22 from textwrap import dedent; 23 f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:3476,load,load,3476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"1.4.1 is out, are we sure this is as scalable as it was before and not a backwards breaking change. If yes, we can merge immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487026612:37,scalab,scalable,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487026612,1,['scalab'],['scalable']
Performance,2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1379,cache,cached,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1846,cache,cached,1846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2939,cache,cached,2939,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from import,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1528,cache,cached,1528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"30 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242922 5.806298 7.895355; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598463; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; means dispersions dispersions_norm; index ; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242923 5.806298 7.895356; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598464; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; ```. To generate seurat_hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:3558,Load,Load,3558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,2,"['Load', 'load']","['Load', 'load']"
Performance,"9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9295,cache,cached-property,9295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,":; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m); ); dblX = pos * adata.X; # TODO: Downsample total counts; srcs = np.sort(combos.reshape(n_doublets, 2), axis=1); obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]); var = pd.DataFrame(index=adata.var_names); return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5; pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""); pbmc.var[""gene_symbols""] = pbmc.var.index; pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc); dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc; dblt.raw = dblt. pbmc = preprocess(pbmc); dblt = preprocess(dblt). sc.pp.pca(pbmc); pca_update(dblt, pbmc). umap = UMAP(); pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]); dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""); sc.tl.embedding_density(dblt, ""umap""); ```; </details>. <details> ; <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python; pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data; dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:2255,Load,Load,2255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['Load'],['Load']
Performance,"; 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)); 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]; 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)); 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]; 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)); 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]; 7256567839680908872 --> 687812693798660380; 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]; 5898621639535744825 --> 3901936098833081796; 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]; 1659302467096852217 --> 8795010127805778162; 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]; 7976077601232067203 --> 1203378416021505679; 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500; 5169565091578776769[""()""]; 9179805111332178500 --> 5169565091578776769; 814146044537405006((and)); 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]; 814146044537405006 --> 1050532709569538834; ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid; flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]; ```. with individual operations, but Iâ€™m not sure if thatâ€™s worth the code readability problems. Smells of premature optimization. <details>; <summary>mean_var graph</summary>. ```mermaid; flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]; step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]; step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2621#issuecomment-1753182156:2458,optimiz,optimization,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621#issuecomment-1753182156,1,['optimiz'],['optimization']
Performance,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:14523,load,load,14523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['load'],['load']
Performance,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:91,Bottleneck,Bottleneck,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,2,"['Bottleneck', 'cache']","['Bottleneck', 'cached-property']"
Performance,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,; Changfeng",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391#issuecomment-1412756118:330,load,load,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391#issuecomment-1412756118,1,['load'],['load']
Performance,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:229,queue,queues,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531,1,['queue'],['queues']
Performance,"> @flyingsheep I can assure you, that's the normal case in academic HPC systems. I agree that this is a huge and common problem in many HPC systems. I usually install conda and R packages to non-home directories with bigger space to avoid issues on servers. One can fill up hundreds of MB by just installing a single package e.g. human genome from Bioconductor ðŸ˜„ . > Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. There is a user home and the cache is `~/.cache` and $XDG_CACHE_HOME is undefined (at least in my case). Some pip wheel files are there for example. . Although it's painful to work in such systems, I believe it's user's responsibility to fix this. One idea might be to print a warning when the cache directory is created for the first time along with the path itself to inform the user about where files are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878:413,cache,cache,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878,5,['cache'],['cache']
Performance,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:109,cache,cached,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['cache'],"['cache', 'cached']"
Performance,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, Â¯\\\_(ãƒ„)_/Â¯. > Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:68,cache,cache,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['cache'],"['cache', 'cached']"
Performance,"> By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:824,optimiz,optimization,824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088,1,['optimiz'],['optimization']
Performance,"> Can we keep the docs on what exactly is happening + how to troubleshoot somewhere in this doc? This means things like: How to tag + build locally, twine check, list contents of distributed file etc. Sure, as we agreed on in person, Iâ€™ll just add a section to the end of the document.; If the build process or package structure arenâ€™t touched, doing things manually isnâ€™t necessary. > We should also automate some checks to avoid broken releases. As we agreed in person: Letâ€™s postpone this. E.g. don't allow this except on specific branches + probably turn on merge queue so we know only commits that pass tests + doc builds get to those branches. This PR automatically does `twine check`, which is enough improvement over â€œtrust the person doing the release to do thatâ€ to be worth the change, even if it wasnâ€™t for the added convenience!. /edit: all addressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678:568,queue,queue,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678,1,['queue'],['queue']
Performance,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:288,load,loadings,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,2,['load'],['loadings']
Performance,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:485,multi-thread,multi-threaded,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273,2,['multi-thread'],['multi-threaded']
Performance,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, itâ€™ll break docstrings with indentation (code blocks, lists, â€¦). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (itâ€™s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We donâ€™t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i donâ€™t think itâ€™s possible to get bugs this way: weâ€™re just adding type annotations, we donâ€™t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:719,load,loaded,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,1,['load'],['loaded']
Performance,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336:272,perform,perform,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336,1,['perform'],['perform']
Performance,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:682,perform,performed,682,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257,1,['perform'],['performed']
Performance,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:671,optimiz,optimize,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:561,load,load,561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['load'],['load']
Performance,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:106,multi-thread,multi-threaded,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438,1,['multi-thread'],['multi-threaded']
Performance,> I had the same problem when I loaded sample data from a csv as a data frame and assigned it to adata.obs = df; > [â€¦](#). I meet the same problem when I try replace the adata.obs with annother pandas dataframe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157:32,load,loaded,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157,1,['load'],['loaded']
Performance,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust â€“ I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:588,load,loadings,588,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033,1,['load'],['loadings']
Performance,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:240,cache,cache,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610,1,['cache'],['cache']
Performance,"> I think it's really good to record the key added. I have a couple questions about the quality score.; > ; > * Should it still be called a modularity score if we aren't using the modularity quality function?. If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term `scaled modularity` anymore. > * Do you have some use cases for recording the modularity score? My impression was that it may not have much interpretable meaning, especially between different graphs.; > . To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). > Also, should this stuff be mirrored to `leiden`?. It's done.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529235054:406,optimiz,optimization,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529235054,1,['optimiz'],['optimization']
Performance,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:548,optimiz,optimized,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037,3,"['bottleneck', 'optimiz', 'perform']","['bottlenecks', 'optimized', 'performance']"
Performance,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:571,load,load,571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523,3,"['cache', 'load']","['cache', 'cached', 'load']"
Performance,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:970,perform,performance,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['perform'],['performance']
Performance,"> IIRC, it's discussed in more detail in Malte's paper:; > ; > > ; > ; > In the same way that cellular count data can be normalized to make them comparable between cells, gene counts can be scaled to improve comparisons between genes. Gene normalization constitutes scaling gene counts to have zero mean and unit variance (z scores). This scaling has the effect that all genes are weighted equally for downstream analysis. There is currently no consensus on whether or not to perform normalization over genes. While the popular Seurat tutorials (Butler et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0020)) generally apply gene scaling, the authors of the Slingshot method opt against scaling over genes in their tutorial (Street et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0125)). The preference between the two choices revolves around whether all genes should be weighted equally for downstream analysis, or whether the magnitude of expression of a gene is an informative proxy for the importance of the gene. In order to retain as much biological information as possible from the data, we opt to refrain from scaling over genes in this tutorial.; > ; > https://www.embopress.org/doi/full/10.15252/msb.20188746; > ; > Since there has been no new development on this topic, we cited Malte and also opted not to scale. This is also discussed by Malte himself in the issue that was cited above.; > ; > I cannot comment on spatial data itself and make confident statements here. Thanks a lot; so is there a conclusion or recommendation whether scale or not on spatial data? @ivirshup @Zethson",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034435734:476,perform,perform,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034435734,1,['perform'],['perform']
Performance,"> If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term scaled modularity anymore. By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?. > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). A couple follow up points on this and @LuckyMD's points. * I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; * I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529329056:198,optimiz,optimization,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529329056,1,['optimiz'],['optimization']
Performance,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward?. Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this.; * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way.; * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678:1327,Perform,Performance,1327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678,1,['Perform'],['Performance']
Performance,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:305,perform,perform,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['perform'],['perform']
Performance,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?. I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({; library(reticulate); library(SingleCellExperiment); library(glue); library(clustree); }); sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE); H5AD_PATH = args[1]; OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")); print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {; adata <- sc$read_h5ad(h5ad_path). return(adata); }. count_clusterings = function(adata){; # Ryan suggests:; # length(grep(""leiden"",names(adata$obs))). clusterings = c(); for (x in adata$obs_keys()){; if (startsWith(x, ""leiden"")){; clusterings = append(clusterings, x); }; }; ; return(length(clusterings)); }. set_fig_dimensions = function(num_clusterings){; width = 10; height = (0.6 * num_clusterings); ; if (height < 8){; height = 8; }; ; png(width = width, height = height); options(repr.plot.width = width, repr.plot.height = height); ; return(list(width=width,height=height)); }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)); # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(; x=adata$obs,; prefix=""leiden_"",; # suffix = NULL,; # metadata = NULL,; # count_filter = 0,; # prop_filter = 0.1,; # layout = ""sugiyama"",; # layout = ""tree"",; # use_core_edges = FALSE,; # highlight_core = FALSE,; # node_colour = prefix,; # node_colour_aggr = NULL,; # node_size = ""size"",; # node_size_aggr = NULL,; # node_size_range = c(4, 15),; # node_alpha = 1,; # node_alpha_aggr = NULL,; # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409:208,load,loads,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409,2,['load'],"['load', 'loads']"
Performance,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:1108,load,load,1108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980,1,['load'],['load']
Performance,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:370,perform,performs,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924,1,['perform'],['performs']
Performance,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1456,perform,performance,1456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['perform'],['performance']
Performance,> Let's update the notebook as well. Would be great to understand performance difference before merging + get rid of the horrible densifying operation in `dask.ipynb`. On it: https://github.com/scverse/scanpy-tutorials/pull/137,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999:66,perform,performance,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999,1,['perform'],['performance']
Performance,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:332,load,loading,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['load'],['loading']
Performance,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:87,perform,performance,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,4,['perform'],"['performance', 'performing']"
Performance,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented?. I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246:39,perform,perform,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246,1,['perform'],['perform']
Performance,"> Scipy is actually under ~/.cache on my mac, Â¯\\_(ãƒ„)_/Â¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:29,cache,cache,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,6,['cache'],"['cache', 'cached']"
Performance,"> So it might be better to either switch to the numba kernel for larger datasets or take the compile hit for small datasets. The compiled versions should get cached, so it's a one time cost per install. No?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346:158,cache,cached,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346,1,['cache'],['cached']
Performance,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of [â€¦]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:431,cache,cached,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],"['cache', 'cached']"
Performance,"> The internals will very likely change over the coming months/year. Good to know, thanks! Any hints about what will change here? In particular, I'm wondering if there might be a `jax` implementation as I'm a bit more keen on that as a dependency. > But most embedding problems (including all problems specified using the preserve_neighbors function, which is the most commonly used recipe) have associated weighted graphs. I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. Would this be the right way to retrieve the graphs for the object, or is `distortions` not the right field?. ```python; from scipy import sparse. weights = mde.distortions().cpu().numpy(); edges = mde.edges.cpu().numpy(). graph = sparse.coo_matrix((weights, (edges[:, 0], edges[:, 1])), shape=(mde.n_items, mde.n_items)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274:469,perform,perform,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274,1,['perform'],['perform']
Performance,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:457,perform,perform,457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,1,['perform'],['perform']
Performance,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:236,cache,cacheing,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715,2,['cache'],"['cache', 'cacheing']"
Performance,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we donâ€™t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:200,cache,cached,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200,1,['cache'],['cached']
Performance,"> You can actually already do this by changing the order of your categorical, e.g.:; > ; > ```python; > adata = sc.datasets.pbmc3k_processed(); > # plot default:; > sc.pl.umap(adata, color=""louvain""); > # reorder categories alphabetically; > adata.obs.louvain = adata.obs.louvain.cat.reorder_categories(; > sorted(adata.obs.louvain.cat.categories); > ); > # plot with new category order:; > sc.pl.umap(adata, color=""louvain""); > ```; > ; > Which gives: <img alt=""Screenshot 2022-09-24 at 19 07 31"" width=""390"" src=""https://user-images.githubusercontent.com/32548783/192110283-af0d14c5-0d79-4ecd-96ff-c079f5743887.png"">. Thanks for your replay. Here, I changed the order of categorical as below:. # 0. loading data; adata = sc.datasets.pbmc3k_processed(); # 1. plot default:; sc.pl.umap(adata, color=""louvain""). # 2. show the default order of categories:; adata.obs['louvain'].cat.categories; # **Index(['CD4 T cells', 'CD14+ Monocytes', 'B cells', 'CD8 T cells', 'NK cells', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], dtype='object')**. # 3. reorder categories as customize; adata.obs['batch_3rd'].cat.reorder_categories(['CD8 T cells', 'CD4 T cells', 'B cells', 'NK cells', 'CD14+ Monocytes', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], inplace=True, ordered=True); adata.obs['batch_3rd'].cat.categories; # **Index(['CD8 T cells', 'CD4 T cells', 'B cells', 'NK cells', 'CD14+ Monocytes', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], dtype='object')**. # 4. plot with new category order:; sc.pl.umap(adata, color=""louvain"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257082217:701,load,loading,701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257082217,1,['load'],['loading']
Performance,"> absolute numbers of cells expressing a gene is similar between clusters as a use case. I am aware that this is a bit of a niche problem, and I am not particularly happy with domino plots as a solution either. I have no better vehicle to discuss this than opening an issue :( Hopefully this inspires the next person who deals with this problem. As to the question, maybe sticking to this example will help me explain:. I am looking at the expression of Hb9/Mnx in my whole-body dataset. I notice from the feature scatter that it seems to be somewhat expressed in clusters 0, 2, and 18. Wanting to be sure, I look at the dotplot. The dotplot tells me that there is a greater proportion of cells in cluster 18 that express it, compared to 0 and 2. The dotplot might make me believe that Hb9 is a marker for cluster 18, and if I do an in-situ hybridisation, these are the cells I would be staining. However, the truth is that the vast majority of cells that express Hb9 are actually in clusters 0 and 2, different cell types than 18. The number of cells in each cluster correlates with the number of cells in the organism, so if I performed the in-situ I would get lots of cells that I could mistakenly all identify as cluster 18. Does this make more sense?. EDIT: I am not advocating for domino plots to be part of ScanPy. I am simply trying to start a discussion, and trying to see if there was an easy fix that I missed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889:1129,perform,performed,1129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889,1,['perform'],['performed']
Performance,"> also as I asked before: why go away from dataclasses?. I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now; * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1887297037:657,cache,cached,657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1887297037,1,['cache'],['cached']
Performance,"> only working on genes. technically it could work on continuos covariates as well, should I add that option?. Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in?. > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288:882,perform,performance,882,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288,1,['perform'],['performance']
Performance,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:1216,perform,performance,1216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984,2,"['multi-thread', 'perform']","['multi-threaded', 'performance']"
Performance,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:242,cache,cache,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,3,['cache'],['cache']
Performance,"> then I'd say NearMiss and related are straightforward and scalable (just need to compute a kmeans whcih is really fast). For sampling from datasets, I would want to go with either extremely straightforward or something that has been shown to work. Maybe we could start with use provided labels to downsample by?. > reshuflling is performed. Reshuffling meaning that the order is changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364:60,scalab,scalable,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364,2,"['perform', 'scalab']","['performed', 'scalable']"
Performance,"> though I definitely see a lot of toolkits using something like ~/.{toolkitname} on my mac. . Thereâ€™s two possible reasons: 1.: Weâ€™re talking about something from the 80â€™s like SSH or BASH. They earned their right to things their way because theyâ€™re older than the standards theyâ€™re not following. Or 2.: Whoever designed this didnâ€™t do their research and just hacked in first thing that came to mind. This is not an excuse. MacOS knows about `~/Library/Caches` and to clean it out when disk space gets scarce. The same applies to Linuxâ€™ and Windowsâ€™ respective canonical cache directories. Each OS has that place specifically to place things like those datasets there. > For example, the main hpc I'm on only gives me about 1gb of space where appdirs would put these files. Ouch. Seems like itâ€™s time to file a bug report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476558610:455,Cache,Caches,455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476558610,2,"['Cache', 'cache']","['Caches', 'cache']"
Performance,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it canâ€™t be used for Zheng17 yet. It would require significant work in CuPy to get it working:; >; > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557644893:857,queue,queue,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557644893,1,['queue'],['queue']
Performance,">A couple follow up points on this and @LuckyMD's points:; > I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. I'm not sure I entirely understand your point. The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""? . I have looked at some stats for communities in protein-protein interaction networks, and the quality of the communities can change dramatically with louvain output there (could find the link if you think it's relevant). However, modularity as a score is fairly degenerate toward the optimal score and therefore the value often doesn't change that much between the optimized partitions. I'm not sure how this is on the knn graphs though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195:579,optimiz,optimized,579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195,2,['optimiz'],['optimized']
Performance,"@Celine-075, I'm about this statement:. > When i previously performed leiden clustering on my data, the shape of the UMAP changed, as expected. This is not expected unless you recompute UMAP. What do you mean by clustered UMAP?. > So here you see that part of cluster 1 is actually added to cluster 2 (which also make sense when looking at the expression profiles of those groups). I'm not sure I can see that, since it's not obvious which point in the first plot corresponds to a point in the other plot. I think a [confusion matrix](https://scanpy.readthedocs.io/en/stable/generated/scanpy.metrics.confusion_matrix.html) (or using the same UMAP layout) would be a more appropriate way to compare the clusterings here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034421743:60,perform,performed,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034421743,1,['perform'],['performed']
Performance,"@Intron7 I think the aim here is indeed to not keep anything in VRAM anyway. In the code/functions I propose here, the data is only transiently stored in device memory for calculation and the resulting output is always transfered back to host once finished. Moreover, I also think that loading a huge mtx file with a 4Go GPU is not impossible. From what I understood rmm should allow oversubscription on host RAM using the following command:; ```python; rmm.reinitialize(managed_memory=True); cp.cuda.set_allocator(rmm.rmm_cupy_allocator); ```. I had a look at your code and GPU accelerated preprocessing functions would be also welcome in scanpy in my opinion! I feel that `scale` and `regress_out` could benefit from such speedup for example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794:286,load,loading,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794,1,['load'],['loading']
Performance,"@Intron7 this was surprisingly hard to get right. Unfortunately, there are now a few more checks and some `hstack`ing. Do those tank the performance?. /edit: I benchmarked some, this is better than what we had before",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2756#issuecomment-1816509533:137,perform,performance,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2756#issuecomment-1816509533,1,['perform'],['performance']
Performance,"@Koncopd ; hi, sc.pp.neighbors doesn't have hsnw which has superior performance from what I've seen in my data and literature https://arxiv.org/abs/1603.09320",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394:68,perform,performance,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394,1,['perform'],['performance']
Performance,"@Koncopd Hi, Thank you for the pointer. It seems to be a problem caused by pytables package. But I still couldn't import tables after installing and uninstalling pytables packages for many times. And I'm in Windows system.; (base) C:\Users\yuhong>python; ```; (base) C:\Users\yuhong>conda list | grep pytables; pytables 3.6.1 py37h14417ae_3 conda-forge ; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tables; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232:790,load,load,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232,1,['load'],['load']
Performance,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that?. Thanks again!!. Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-565168151:620,perform,perform,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-565168151,1,['perform'],['perform']
Performance,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:447,cache,caches,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['cache'],['caches']
Performance,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:182,Load,Loading,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828,1,['Load'],['Loading']
Performance,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352465866:229,perform,performance,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352465866,1,['perform'],['performance']
Performance,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:602,perform,perform,602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161,2,['perform'],['perform']
Performance,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:87,load,loadings,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157,1,['load'],['loadings']
Performance,"@andrea-tango ; Really awesome!; I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471241654:80,tune,tune,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471241654,1,['tune'],['tune']
Performance,"@ashish615 after doing some benchmarking myself I found out that your solution for `axis=1` is under performing compared to `axis=0` for larger arrays. I think that is because of the memory access pattern you choose. I rewrote the function with that in mind. I'll again make a PR to you, because for some reason you disallow us from making changes to your PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099#issuecomment-2191349887:101,perform,performing,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099#issuecomment-2191349887,1,['perform'],['performing']
Performance,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want.; * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file?; * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf?. -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817:421,perform,performance,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817,1,['perform'],['performance']
Performance,"@atarashansky, the performance is looking very very good:. ```python; import scanpy as sc; import numpy as np; from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data; # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64); %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True); # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s; # Wall time: 2.93 s; # Peak memory (including dataset) is about 770 MB; %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True); # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s; # Wall time: 7min 43s; # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]); assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]); ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does?. Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303:19,perform,performance,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303,1,['perform'],['performance']
Performance,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:342,perform,performing,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073,1,['perform'],['performing']
Performance,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:83,load,load,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114,1,['load'],['load']
Performance,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:731,perform,performant,731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,1,['perform'],['performant']
Performance,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:776,perform,performs,776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['perform'],['performs']
Performance,"@falexwolf ; In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-396115524:263,perform,perform,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-396115524,1,['perform'],['perform']
Performance,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesnâ€™t model zero inflation, itâ€™s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:477,perform,performance,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723,1,['perform'],['performance']
Performance,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:216,perform,perform,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,3,['perform'],"['perform', 'performs']"
Performance,"@falexwolf, @flying-sheep . Just to recap what's left to be resolved here. * I'll reset the default value of `datasetdir` to the current value ""./data""; * Related, how about `datasetdir` instead of `datasetsdir`? It matches more to `cachedir` and `figdir`. Also, by analogy, it's ""potato sack"" not ""potatoes sack"" so ""dataset directory"" sounds more natural that ""datasets directory"" to me.; * `ebi_expression_atlas` vs `ebi_sc_expression_atlas`; * Potentially adding a class for settings right now?; * I think this becomes more important if `datasetdir` is documented. I bet people will set it with a `str` instead of a `Path` and that'll break things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478843888:233,cache,cachedir,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478843888,1,['cache'],['cachedir']
Performance,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for ðŸ˜Š; * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots â€“ especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991:610,optimiz,optimization,610,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991,1,['optimiz'],['optimization']
Performance,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415853701:355,perform,perform,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415853701,1,['perform'],['perform']
Performance,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition?. I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416207545:70,perform,performs,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416207545,1,['perform'],['performs']
Performance,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:338,perform,performs,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['perform'],['performs']
Performance,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:516,load,load,516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062,1,['load'],['load']
Performance,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:89,perform,performance,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111,1,['perform'],['performance']
Performance,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` â†’ `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:1812,perform,performance,1812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['perform'],['performance']
Performance,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:130,optimiz,optimization,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:388,cache,cache,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,2,['cache'],['cache']
Performance,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:628,load,load,628,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344,1,['load'],['load']
Performance,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:275,load,load,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900,2,['load'],['load']
Performance,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` â€“ `~/scikit_learn_data`; * `seaborn` â€“ `~/seaborn-data`; * `NLTK` â€“ `~/nltk_data`; * `keras` and `tensorflow` â€“ `~/.keras/datasets`; * `conda` â€“ `~/miniconda3/`; * `intake` â€“ `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages â€“ same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:934,Cache,Caches,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,2,"['Cache', 'cache']","['Caches', 'cache']"
Performance,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both?. Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/570#issuecomment-478211254:71,optimiz,optimization,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570#issuecomment-478211254,1,['optimiz'],['optimization']
Performance,"@grst I don't think `leiden` is the issue here, but `pynndescent`. My guess is this is going to have to do with the CPU that gives different results being much older using a different instruction set that the other intel processors. This could be triggered by either use of any parallelism at all or `pynndescent` being pretty liberal with the use of `numba`'s `fastmath`, and different CPUs having different features. Do you get the same graph out of `pynndescent` if you are make the computation single threaded? If not, we may be able to look at the assembly to see which feature sets give different results. It's possible these can be turned off with a flag. But we may then have to consider what kind of a performance hit we'd be willing to take for exact reproducibility on discontinued chip sets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078:711,perform,performance,711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078,1,['perform'],['performance']
Performance,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. ; In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498158306:253,optimiz,optimize,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498158306,3,['optimiz'],"['optimize', 'optimized', 'optimizing']"
Performance,"@ivirshup . > Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. The quality score returned by RBConfigurationVertexPartition is unscaled modularity, so it's something like `41726.23`. So how would one interpret that? I don't think there is any point in reporting raw RBConfigurationVertexPartition quality value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977:171,optimiz,optimization,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977,2,['optimiz'],"['optimization', 'optimized']"
Performance,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. â€œA Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.â€* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-427291301:77,optimiz,optimized,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427291301,1,['optimiz'],['optimized']
Performance,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662:57,perform,performance,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662,1,['perform'],['performance']
Performance,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. Weâ€™ll have to think about naming though. Maybe we want to namespace our settings like matplotlibâ€™s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:71,cache,caches,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481,1,['cache'],['caches']
Performance,"@ivirshup any way to force Azure to clear its cache or use a different runner? The â€œinvalid instructionâ€ error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:46,cache,cache,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,1,['cache'],['cache']
Performance,"@ivirshup is it possible that Travis has cached pbmc3k and that's what's causing the error? I really don't have it running pytest locally either. . Also as far as the code review -- I understand code is duplicated, but this code does not really fit in the existing implementation because it works a bit differently and requires raw data. Let me know how you'd like to address this. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412:41,cache,cached,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412,1,['cache'],['cached']
Performance,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right?. And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it?. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-44-72c504b15b2e> in <module>; 17 title='{} path'.format(descr),; 18 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:369,perform,perform,369,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967,1,['perform'],['perform']
Performance,"@ivirshup:; * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative.; * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426894394:568,optimiz,optimized,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426894394,1,['optimiz'],['optimized']
Performance,"@karenlawwc ; For the test.h5ad that youâ€™ve saved using adata.write, I think you want to load it with sc.read_h5ad() rather than sc.read_10x_h5(), since the saved test file will be in AnnData h5ad format",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1255636170:89,load,load,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1255636170,1,['load'],['load']
Performance,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:312,load,load,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['load'],['load']
Performance,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:1517,perform,perform,1517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692,1,['perform'],['perform']
Performance,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. Iâ€™m not quite sure why those slicing strategies arenâ€™t supported yet. I just figured maybe they were less trivial than the others and werenâ€™t immediately needed so they were pushed off to future feature requests. . The issue #2360 I canâ€™t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. Iâ€™m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:632,perform,performance,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727,1,['perform'],['performance']
Performance,"@sjfleming I am currently facing the same issue. I was able to load the h5 output with this input function you stated here https://lightrun.com/answers/broadinstitute-cellbender-read_10x_h5-error-in-scanpy-191. But after further analysis and I wanted to save the adata object with the write function to h5ad format, I am not able to read that saved h5ad object with scanpy again with error ; **test.h5ad contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp']**. For some reason, the genome ""GRCh38"" is not showing up in the available genomes options. And when I tried to use command test = sc.read_10x_h5 ('test.h5ad', genome = ""GRCh38), this error shows up again ; **Could not find genome 'GRCh38' in 'test.h5ad'. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp'].** . Do you know if this error is related to this pull request? and is there any fix to it so that I can save processed h5ad after cellbender and able to read it again? Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051:63,load,load,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051,1,['load'],['load']
Performance,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:215,perform,performance,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889,1,['perform'],['performance']
Performance,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some â€œtrueâ€ communities. Communities in Louvain method are not intended in hierarchical way.; I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/279#issuecomment-426898375:43,optimiz,optimizes,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426898375,1,['optimiz'],['optimizes']
Performance,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:99,load,loading,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600,1,['load'],['loading']
Performance,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-653220911:30,load,loading,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-653220911,1,['load'],['loading']
Performance,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553420798:208,optimiz,optimized,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553420798,1,['optimiz'],['optimized']
Performance,"Ah, I think I see what you're asking now. At the moment, I don't think we have a function for that. But this should be fairly straightforward to work around. Something like this should work:. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from sklearn.metrics import pairwise_distances; import seaborn as sns. def groupby_mean(adata, groupby):; grouped = adata.obs.groupby(groupby); results = np.zeros((grouped.ngroups, adata.n_vars), dtype=np.float64). for idx, indices in enumerate(grouped.indices.values()):; results[idx] = np.ravel(adata.X[indices].mean(axis=0)). return pd.DataFrame(results, columns=adata.var_names, index=grouped.groups.keys()). # Loading data; pbmc_full = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc_small = sc.datasets.pbmc68k_reduced().raw.to_adata(); var_intersect = pbmc_full.var_names.intersection(pbmc_small.var_names). # Calculate mean expression per cell type; full_means = groupby_mean(pbmc_full[:, var_intersect], ""louvain""); small_means = groupby_mean(pbmc_small[:, var_intersect], ""louvain""). # Correlation distance between celltypes; corr_mtx = pd.DataFrame(; pairwise_distances(full_means, small_means, metric=""correlation""),; index= full_means.index,; columns=small_means.index,; ); ```. Is this more of what you were thinking?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537:680,Load,Loading,680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537,1,['Load'],['Loading']
Performance,"Ah, okay... so you sample based on how representative a cell is of its neighbours, and then you use that weight to calculated PCA, marker genes, and perform visualizations. Is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494336456:149,perform,perform,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494336456,1,['perform'],['perform']
Performance,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:210,perform,performing,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208,1,['perform'],['performing']
Performance,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:157,Load,Load,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269,1,['Load'],['Load']
Performance,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546:65,load,loading,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546,1,['load'],['loading']
Performance,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197:148,perform,performance,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197,1,['perform'],['performance']
Performance,"As can be seen with the KL divergence values in the above table, while the output of Intel optimized t-SNE is different, it is equivalent in quality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122440284:91,optimiz,optimized,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122440284,1,['optimiz'],['optimized']
Performance,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:24,optimiz,optimization,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942,4,['optimiz'],"['optimization', 'optimize']"
Performance,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801:66,load,loading,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801,1,['load'],['loading']
Performance,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:608,perform,performance,608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:883,perform,performance,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['perform'],['performance']
Performance,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ); 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%); 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%); 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%); 	 257.750.810.841 instructions:u # 2,44 insn per cycle; 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%); 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%); 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%); 	; 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ); ```. - this PR:. ```console; $ git switch hvg_PR_numba; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ); 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%); 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%); 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%); 	 373.047.679.552 instructions:u # 2,19 insn per cycle; 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%); 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%); 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%); 	; 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266:1375,Perform,Performance,1375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266,1,['Perform'],['Performance']
Performance,"By default, scanpy took the expression data saved at adata.raw if that is not available it took the data from adata.X. If you are loading the expression data from csv or txt file, try to save adata.raw = data, before slicing for HVGs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-1770949492:130,load,loading,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-1770949492,1,['load'],['loading']
Performance,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:315,perform,performance,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,1,['perform'],['performance']
Performance,Can I load it if it's not in `zarr` format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2375620692:6,load,load,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2375620692,1,['load'],['load']
Performance,"Cool ! ; in order to load legacy h5, I had to freeze scanpy==1.8.2; now I included this fix in a scanpy fork. I hope it gets merged soon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2248#issuecomment-1127793416:21,load,load,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2248#issuecomment-1127793416,1,['load'],['load']
Performance,"Copying using the `copy` module is a bit ill defined for `AnnData` objects currently. This has to do with some internals of how we do views of arrays. In general I'd recommend doing copies via `adata.copy()`, which performs a deep copy. But it looks like there might be another problem with the PCA not being exactly reproducible. After a fair amount of checking that it was exactly reproducible, it looks like we forgot to actually pass the random seed... There has been fixed, and there will be a bug-fix release soon (#1240). This still does not fix the issue of reproducibility if you've made a shallow copy of a AnnData view with `copy`. I'll have to look into this a bit more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443:215,perform,performs,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443,1,['perform'],['performs']
Performance,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells?; Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672:233,perform,performing,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672,1,['perform'],['performing']
Performance,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1060,optimiz,optimized,1060,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:193,load,loadings,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904,1,['load'],['loadings']
Performance,"Don't know what this ""review"" process is , but basically it is ready. . De: ""Lukas Heumos"" ***@***.***> ; Ã€: ""theislab/scanpy"" ***@***.***> ; Cc: ""Yves Le Feuvre"" ***@***.***>, ""Mention"" ***@***.***> ; EnvoyÃ©: Jeudi 6 Janvier 2022 20:11:35 ; Objet: Re: [theislab/scanpy] Pca loadings n points patch (PR #2075) . [ https://github.com/Yves33 | @Yves33 ] is this ready for review? . â€” ; Reply to this email directly, [ https://github.com/theislab/scanpy/pull/2075#issuecomment-1006847333 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/ACEYIQUT75OTZC3MUGVAT3DUUXSOPANCNFSM5JWG2IZQ | unsubscribe ] . ; Triage notifications on the go with GitHub Mobile for [ https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675 | iOS ] or [ https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub | Android ] . ; You are receiving this because you were mentioned. Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047:275,load,loadings,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047,1,['load'],['loadings']
Performance,"E.g. if your original input matrix has 1,000,000 number of cells and 100; genes. You don't want to process all rows, so you can perform either; uniform sampling or weighted sampling on the data. I have performed; weighted sampling and sampled e.g. only 1,000 rows then each rows will have; a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>; wrote:. > I don't quite understand what sampled data with weights on the rows are.; > How do you weight individual cells in a dataset? What do weights like this; > mean?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098188:128,perform,perform,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098188,2,['perform'],"['perform', 'performed']"
Performance,"Finally, we could solve this elegantly without sacrificing a scalable design, as shown in the [tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Also, Scanpy is accepted in Genome Biology and will soon be published. Merry Christmas! :); Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-353766971:61,scalab,scalable,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-353766971,1,['scalab'],['scalable']
Performance,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks!. ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/399#issuecomment-448102220:95,load,load,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399#issuecomment-448102220,1,['load'],['load']
Performance,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:290,load,loadings,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707,1,['load'],['loadings']
Performance,"Goal:. Add `dask` use-cases to the scanpy benchmarks so we can understand performance changes. . Nice links:. 1. Example benchmark: https://github.com/scverse/scanpy/blob/main/benchmarks/benchmarks/preprocessing_counts.py; 2. Project we use for benchmarking: https://asv.readthedocs.io/projects/asv-runner/en/latest/index.html; 3. Dask local cluster: https://distributed.dask.org/en/stable/api.html#cluster; 4. Using scanpy and dask: https://scanpy.readthedocs.io/en/stable/tutorials/experimental/dask.html. NOTE: this `read_elem_as_dask` function in the notebook is with anndata 0.11 i.e., `pip install --pre anndata`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519:74,perform,performance,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519,1,['perform'],['performance']
Performance,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:67,perform,performing,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183,1,['perform'],['performing']
Performance,"Have you rebooted the python after updating anndata? Can you paste the exact error you're seeing? Are you allowed to share the object you're having trouble loading?. OP's error is bizarre, e.g. the last part seems to be pointing to an empty line. Like the package was updated but the python was not restarted. I can't recreate the exact one he's seeing, but I've managed to get other disjoint errors along those lines by updating anndata to 0.8.0 in a second terminal while the python in question is still running.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185:156,load,loading,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185,1,['load'],['loading']
Performance,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,; Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440#issuecomment-456429967:79,cache,cache,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440#issuecomment-456429967,1,['cache'],['cache']
Performance,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:702,cache,cached,702,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['cache'],['cached']
Performance,"Hello @ivirshup , Thanks for your reply. I figured that this might be an issue due to the anndata being read in backed mode. Although the file is large (7 gb in .h5ad format, and as soon as it gets read in memory, it blows up to 28 gb), but for now I have utilized a larger machine for performing my eda, and converted the sparse matrix to a dense one . Thanks for your clarification !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147#issuecomment-1053044750:286,perform,performing,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147#issuecomment-1053044750,1,['perform'],['performing']
Performance,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:326,load,loading,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562,1,['load'],['loading']
Performance,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:673,perform,performed,673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381,1,['perform'],['performed']
Performance,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:393,perform,performed,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Here's an example with the newest numba and llvmlite. . I noticed with fewer cells it works. . ```python; import scanpy as sc; import anndata; import numpy as np. a = anndata.AnnData(np.random.poisson(size=(4000, 5000))); b = anndata.AnnData(np.random.poisson(size=(10000, 5000))). sc.external.pp.mnn_correct(a, b); ```. ```; Performing cosine normalization...; Starting MNN correct iteration. Reference batch: 0; Step 1 of 1: processing batch 1; Looking for MNNs...; Computing correction vectors...; /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""compute_correction"" failed type inference due to: non-precise type pyobject; [1] During: typing of argument at /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py (107). File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-pac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200:326,Perform,Performing,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200,1,['Perform'],['Performing']
Performance,"Hey @giovp,; thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567:388,perform,performed,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567,1,['perform'],['performed']
Performance,"Hey @kchl5 and @vitkl,. Muon (`mu.read_10x_h5()`) should load it correctly if the `feature_types` value for the gRNAs is different from the one for the genes. As they are missing, I assume it is. Moreover, just in case you're interested, splitting by `feature_types` is even [a feature](https://github.com/scverse/mudata/blob/4d3b5f4e6039b4a31519584db5461a5809741dce/mudata/_core/mudata.py#L88) of the `MuData` initialiser, so running . ```py; adata = sc.read_10x_h5(h5file, gex_only=False); mdata = MuData(adata); ```. should also work, and this is roughly what muon does. (Thanks for tagging me, @adamgayoso!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2398#issuecomment-1386019022:57,load,load,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398#issuecomment-1386019022,1,['load'],['load']
Performance,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:656,optimiz,optimize,656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:419,cache,cache,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,1,['cache'],['cache']
Performance,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:408,tune,tuned,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504,1,['tune'],['tuned']
Performance,"Hey! ðŸ˜„ . I'd in principle happy if we move the default `scanpy.settings.cachedir` from `./cache/` to `appdirs.user_cache_dir()`. . However, if then any Scanpy installation breaks, as _the main hpc I'm on 1gb of space where appdirs would put these files_, I would probably not make this the default, but choose something like `~/cache-scanpy/`, that is, a visible directory in home (if we really want, `~/.scanpy/` is also fine). Under https://scanpy.readthedocs.io/en/latest/api/index.html#settings, we could also talk about other alternatives. I second Isaac's concern. Like many others, I'm computing on AWS these days and there, the canonical way of making data locally accessible is via EBS volumes. Hence, I'm used to setting the cachedir to that mount point with a visible name, knowing that this can hold a lot of data. I'd manually clean it if something that I don't use often takes too much space. So, I fear that `appdirs.user_cache_dir()` is not smart enough to figure out locations on a Linux system that hold the size of data that we're typically talking about. It would for sure be the right solution for laptops and work stations etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843:72,cache,cachedir,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843,4,['cache'],"['cache', 'cache-scanpy', 'cachedir']"
Performance,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellrangerâ€™s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we canâ€™t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:1106,cache,cache,1106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,2,['cache'],['cache']
Performance,"Hey, thanks for the description - yes your example dataset would be very helpful - if you could post a small code snippet here which generates this dataset and shows the specific steps you perform that would be great. If you cannot produce the dataset in a script, you could also send a link to the dataset (if its public or synthetic, making sure you're allowed to share) :). In both cases, sending a script here we can run too is a great help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191785768:189,perform,perform,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191785768,1,['perform'],['perform']
Performance,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:51,perform,performed,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662,1,['perform'],['performed']
Performance,"Hi @LuckyMD ,. Thanks so much for getting back to me this quickly. I just want to clarify that I am not running this analysis with the built-in 10x data set, I have followed the tutorial as seen on the link in the report, which says: ""The data consist in 3k PBMCs from a Healthy Donor and are freely available from 10x Genomics"". I have downloaded the file from the following URL, as seen in the tutorial:. http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz. This is also the same URL found on this link, directly from 10x:. https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k. The 10x summary [here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_web_summary.html) mentions LYZ as one of the most differentially expressed genes, yet it is missed by the sample analysis as performed in the Scanpy tutorial. As both use the exact same count matrix as a source, there are two possibilities here as far as I can see: either the thresholds and filtering parameters in the tutorial are inaccurate and miss important marker genes, or there is a bug that drops these genes. My question is which of the following is true. From your answer I would assume it's the former, in which case maybe a disclaimer pointing this out would be helpful in the tutorial page? I think, as it stands, the average user would assume important marker genes such as LYZ would not be missed by even a rough analysis of a PBMC data set. For reference, the [tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html) which the Scanpy one is apparently based on finds LYZ as a very important contributor to the first principal component.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053:865,perform,performed,865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053,1,['perform'],['performed']
Performance,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:64,perform,performing,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,4,['perform'],"['perform', 'performing']"
Performance,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds â€“ when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation â€“ I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then â€“ my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:795,perform,performing,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916,1,['perform'],['performing']
Performance,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:164,perform,performance,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216,1,['perform'],['performance']
Performance,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:1128,perform,performs,1128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['perform'],['performs']
Performance,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:577,scalab,scalability,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['scalab'],['scalability']
Performance,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:240,perform,performed,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723,1,['perform'],['performed']
Performance,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:547,load,load,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764,1,['load'],['load']
Performance,"Hi @flying-sheep , Iâ€™m using Scanpy on an HPC system, and even though the administrator updated it to the latest version, I'm still encountering the same error. -----; anndata 0.9.2; scanpy 1.10.2; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 2.1.0; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344:278,bottleneck,bottleneck,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344,1,['bottleneck'],['bottleneck']
Performance,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:542,optimiz,optimized,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,1,['optimiz'],['optimized']
Performance,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:620,load,loaded,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938,1,['load'],['loaded']
Performance,"Hi @ilan-gold,. Regarding your thought in this [comment](https://github.com/scverse/scanpy/pull/3061#issuecomment-2134651481), we can enable or disable Intel optimization from outside the code. However, users might not be aware of how to use this feature. Instead, if we add it to scanpy directly, all scanpy users will know the same option available. If we agree with the option discussed in this [comment](https://github.com/scverse/scanpy/pull/3061#issuecomment-2114783668 ), I can proceed with updating the t-SNE file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2136745993:158,optimiz,optimization,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2136745993,1,['optimiz'],['optimization']
Performance,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-537518323:87,load,loaded,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-537518323,1,['load'],['loaded']
Performance,"Hi @o0stsou0o ,; Could it be that you have `igraph` loaded somewhere while you were uninstalling? Not sure why you can't remove `igraph` otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-640070940:52,load,loaded,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-640070940,1,['load'],['loaded']
Performance,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:811,perform,perform,811,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245,1,['perform'],['perform']
Performance,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:437,perform,performed,437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157,2,['perform'],"['perform', 'performed']"
Performance,"Hi Alex,; I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error.; TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following; ### Load Data; x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0); ### Drop DAPI; x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1); ### Convert to AnnData; adata = sc.AnnData(x); ### Filter cells; sc.pp.filter_cells(adata, min_genes=1); sc.pp.filter_genes(adata, min_cells=1); adata.obs['n_counts'] = adata.X.sum(axis=1); ### Normalize data; sc.pp.log1p(adata); ### PCA; sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata); sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-456461004:312,Load,Load,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-456461004,1,['Load'],['Load']
Performance,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721:19,perform,perform,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721,1,['perform'],['perform']
Performance,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:107,load,loader,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952,4,['load'],['loader']
Performance,"Hi James!. Thank you for the remark! And you're right... several repetitions of the following are consistent:; ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. Thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:857,bottleneck,bottleneck,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['bottleneck'],['bottleneck']
Performance,"Hi Jorvis! This should be very easy. Use the text file reader:; ```; adata = sc.read_text(filename).transpose(); ```; or use the general purpose reader that writes cache files automatically; ```; adata = sc.read(filename, ext='txt').transpose() # 'tab', 'data', 'tsv' mean the same; ```; see the [API docs](https://scanpy.readthedocs.io/en/latest/api/index.html). The 'tsv' file ending is not yet in the latest release, I just commited that: https://github.com/theislab/scanpy/commit/884c5f8a6a39c43aef27c7398ec9c195b977a3d3. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056:164,cache,cache,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056,1,['cache'],['cache']
Performance,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:408,cache,cached,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,3,['cache'],"['cache', 'cached']"
Performance,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together.; But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice?; Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-486916420:170,cache,cache,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-486916420,1,['cache'],['cache']
Performance,Hi negative loadings are also important genes as you can see on the example of `LTB` and `HLA-DRA`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-527409146:12,load,loadings,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527409146,1,['load'],['loadings']
Performance,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-48-abf5bf78cb77> in <module>; ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap); 159 if as_heatmap:; 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 161 timeseries_as_heatmap(; 162 adata.X[adata.obs['dpt_order_indices'].values],; 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)); 198 ax.imshow(; --> 199 np.array(X, dtype=np.float_),; 200 aspect='auto',; 201 interpolation='nearest',. ValueError: setting an array element with a sequence.; ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance!. Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140:1717,perform,performing,1717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140,1,['perform'],['performing']
Performance,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:969,load,load,969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['load'],['load']
Performance,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528512005:336,perform,performance,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528512005,1,['perform'],['performance']
Performance,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file; ; Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:519,cache,cache,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,4,['cache'],['cache']
Performance,"Hi! That function is for reading the files output by [cellrangerâ€™s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we canâ€™t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:548,cache,cache,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,2,['cache'],['cache']
Performance,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:779,load,loaded,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695,1,['load'],['loaded']
Performance,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon.; No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197:422,perform,performance,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197,1,['perform'],['performance']
Performance,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:130,perform,perform,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646,1,['perform'],['perform']
Performance,"Hi, I have fixed the issue.; It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis.; I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor.; So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456#issuecomment-459623293:526,perform,performing,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456#issuecomment-459623293,1,['perform'],['performing']
Performance,"Hi, I tried the snippet, with fastICA and picard, and with a number of cells higher than 30,000, the whitening step cannot be completed. This seems be due to some Lapack limitations. ; `ValueError: Too large work array required -- computation cannot be performed with standard 32-bit LAPACK.`; I don't know how to get around this.... ; Best, ; ChloÃ©",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-540475625:253,perform,performed,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540475625,1,['perform'],['performed']
Performance,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> äºŽ2020å¹´6æœˆ5æ—¥å‘¨äº” ä¸Šåˆ2:31å†™é“ï¼š. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:56,load,load,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437,2,"['cache', 'load']","['cache', 'load']"
Performance,"Hi, just to confirm that I tried the new PAGA functions a while ago and the results look very good. (sorry for the delay of the response. I meant to respond to the thread much earlier but got busy doing other stuff.). Now I'm wondering about how to interpret the graph connectivities. An undirected graph does not imply whether two connected clusters are sequential (e.g. progenitors -> newborn neurons -> mature neurons) or on different branches but highly correlated (e.g. neuron subtype 1 vs. neuron subtype 2). . Do you think it's possible to use RNA velocity (http://velocyto.org/) to perform quantitative interference on the directionality of the edges? I have the velocity data but not sure how to mathematically infer edge directions. Maybe I should open a new issue on this or approach you via email? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393690042:590,perform,perform,590,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393690042,1,['perform'],['perform']
Performance,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway.; There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134:662,perform,perform,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134,1,['perform'],['perform']
Performance,"Hi, thanks for your ideas and discussion. For me, I think doing scaling is necessary because if the data is not centred to 0, the plane we find based on the covariance matrix may not be the optimized one. The PCA optimization process only works for data with 0 centered I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103829861:190,optimiz,optimized,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103829861,2,['optimiz'],"['optimization', 'optimized']"
Performance,"Hi, thanks for your interest in scanpy!. Iâ€™ll try to comment on your observations here with your code example:. ```; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet.; â†’ We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). â†’ A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:162,Load,Loading,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,1,['Load'],['Loading']
Performance,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502549720:422,perform,performing,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502549720,1,['perform'],['performing']
Performance,"Hi,. I don't think the data is actually different. Only the jitter in the violin plot places the dots in different places (this is an inherent stochastic effect). The underlying value of `n_genes` (and others) is still the same. You can check if `adata.obs['n_counts']` is the same in the 4 objects you load.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/977#issuecomment-572685233:303,load,load,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977#issuecomment-572685233,1,['load'],['load']
Performance,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928:47,perform,performed,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928,1,['perform'],['performed']
Performance,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```; adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]; sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'); ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464:77,perform,perform,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464,1,['perform'],['perform']
Performance,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:911,perform,perform,911,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065,1,['perform'],['perform']
Performance,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:184,perform,performed,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806,1,['perform'],['performed']
Performance,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:307,perform,performed,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,2,['perform'],['performed']
Performance,"Hm, youâ€™re right! looks like a Sphinx bug, as even setting it manually doesnâ€™t change things. Maybe pickling the function leads to it having a different object idâ€¦ . The only ways to fix this:. 1. set the value to a string like `'scanpydoc.elegant_typehints:typehints_formatter'` and implement importing that object in sphinx-autodoc-typehint; 2. make it so sphinx compares function-valued settings in a way that doesnâ€™t bust the cache.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204#issuecomment-1088546625:430,cache,cache,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204#issuecomment-1088546625,1,['cache'],['cache']
Performance,"Hmm okay, I thought leiden clustering pulls cells with similar expression closer to each other on the UMAP space? By clustered UMAP, i mean the UMAP produced after i performed leiden clustering on it. By unclustered i mean that I just plotted the UMAP without calculating the leiden clusters. . Then I dont know what happened, but when I plotted the UMAP without leiden clustering performed, it had a different shape in the UMAP then after I calculated the leiden clusters. I will check the confusion matrix and come back to it when I have the results. In the meantime I can only post this image where I put both UMAPs next to each other and drew what I meant about part of cluster1 being added to cluster2 after performing the leiden clustering: . ![change-umap](https://github.com/scverse/scanpy/assets/127406679/38399718-5296-4aa0-87a9-4cd5057b4b5d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034530366:166,perform,performed,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034530366,3,['perform'],"['performed', 'performing']"
Performance,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499770209:57,load,load,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499770209,1,['load'],['load']
Performance,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:475,load,load,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,2,"['load', 'perform']","['load', 'perform']"
Performance,"How? As said, theyâ€™re just for people and IDEs. Scanpy doesnâ€™t use them. It doesnâ€™t throw errors in case something doesnâ€™t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesnâ€™t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). Iâ€™m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:286,perform,performance,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,1,['perform'],['performance']
Performance,"Huh weird, it gets detected, but it doesnâ€™t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, itâ€™s clear that the correct (non-parallel) function is called from Daskâ€™s thread. Seems like calling numba from a `ThreadPoolExecutor` isnâ€™t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:617,Concurren,Concurrent,617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,3,"['Concurren', 'concurren']","['Concurrent', 'concurrently']"
Performance,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682:120,perform,performance,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358:1040,perform,performing,1040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358,1,['perform'],['performing']
Performance,I also [put this on StackOverflow](https://stackoverflow.com/questions/48326579/unable-to-iterate-over-pandas-dataframe-loaded-from-tabular-data),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/70#issuecomment-358711733:120,load,loaded-from-tabular-data,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70#issuecomment-358711733,1,['load'],['loaded-from-tabular-data']
Performance,"I also recently encountered this issue. I've dug into the problem a little bit and for me the cause seems to be that the sc.pp.scale function introduces the NaN values. This occurs for columns which show very little variance and are almost constant. According to the current documentation this should not be the current expected behaviour though and should only (possibly) occur in future versions: . `Variables (genes) that do not display any variation (are constant across all observations) are retained and (for zero_center==True) set to 0 during this operation. In the future, they might be set to NaNs.`. So I'm not sure if this is a bug or if the documentation has not been updated yet. . I've currently circumvented the issue by scaling in sklearn (which retains 0s instead of NaNs) and manually loading the scaled results into my adata object as this is the behaviour I would like for my dataset. In case my example dataset would be helpful let me know then I can share it with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706:803,load,loading,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706,1,['load'],['loading']
Performance,I am actually booting using the exact same disks so identical OS (Ubuntu 16.04) and BLAS libraries. I am just loading them up with different virtual machines with different numbers of CPUs. In both cases the CPUs are Intel Xeon E5 v3 (Haswell). Have not tried limiting the number of CPUs used by arpack. I didn't know that was something I could do! Do you have a tip on how to do so? I'll look this up and give it a shot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718:110,load,loading,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718,1,['load'],['loading']
Performance,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:800,cache,cache,800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,3,['cache'],['cache']
Performance,"I am using a sampling technique, which samples few rows without descreasing; performance. So speed is more than 10X time faster for larger dataset with; similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>; wrote:. > I'm not sure I entirely understand what the weights are based on. I'm; > trying to understand when you would suggest someone use your approach. Why; > do you give one cell a weight of 125? With this type of weight distribution; > you are basically manually changing the marker gene calculation focusing; > nearly only on a single cell. That seems strange to me.; >; > I'm trying to understand the need for scanpy to support weighted; > observations. At the moment I don't see when you would want to differently; > weight the observations... I'm familiar with using weights if I have some; > form of measurement error or uncertainty between samples. I don't really; > see how that holds here. Do you weight the cells based on some kind of; > quality score?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494124913:77,perform,performance,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494124913,1,['perform'],['performance']
Performance,"I can do that. I have a pickled object that I can share with you (how?).; Here is how you reproduce the error:; ```; import scanpy.api as sc; import pickle. # Load the object; with open(""example.pkl"",""rb"") as handle:; adata = pickle.load(handle). # Run Scanpy; sc.tl.rank_genes_groups(adata,groupby=""celltype""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960:159,Load,Load,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960,2,"['Load', 'load']","['Load', 'load']"
Performance,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:493,cache,cache,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,1,['cache'],['cache']
Performance,"I created a new environment (see below for package details) and there everything works as it should. . <Details>; <summary>Versions in the new working environment</summary>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.6; anyio NA; argon2 20.1.0; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.05.0; dateutil 2.8.1; decorator 5.0.9; fsspec 2021.05.0; get_version 2.1; h5py 3.2.1; idna 2.10; igraph 0.9.1; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.5.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:323,bottleneck,bottleneck,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,"I created an adata without using the functions provided by scanpy that; allow you to load single cell data. This kind of conversion is done is done; in that functions, right?. On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it ðŸ˜„; >; > Do you know how you ended up with non-string indices? Ideally, we would be; > able to prevent that from happening or at least warn the user about it.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-516271609:85,load,load,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516271609,1,['load'],['load']
Performance,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:144,load,loading,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,2,['load'],"['load', 'loading']"
Performance,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287:510,cache,cache,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287,1,['cache'],['cache']
Performance,"I encountered the same error (KeyError: 1) when trying to load the .mtx file with scanpy.read_10x_mtx(). After several unsuccessful attempts at renaming the columns and indices in the 'genes.tsv' file in different ways, I found a workaround that worked for me:. 1. Import the .mtx file separately using scanpy.read_mtx().; 2. Convert the imported data to a pandas DataFrame using .to_df().; 3. Manually name the columns and indices using the 'barcodes.tsv' and 'features.tsv' files, respectively. This approach allowed me to bypass the KeyError and successfully load the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888:58,load,load,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888,2,['load'],['load']
Performance,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/279#issuecomment-426950213:279,optimiz,optimization,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426950213,1,['optimiz'],['optimization']
Performance,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:268,optimiz,optimization,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342,1,['optimiz'],['optimization']
Performance,I had the same problem when I loaded sample data from a csv as a data frame; and assigned it to adata.obs = df. >,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-516289839:30,load,loaded,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516289839,1,['load'],['loaded']
Performance,"I have a similar problem. And this does not seam to be related to any python package version as I have 4 data sets loaded from CellRanger h5 files.; With two of these files the neighbors function works and with two it fails with likely a seg fault as a cpp_abort_hook process takes over. This is REALLY annoying as I also get this problem with a random number of different single cell data sets. I assume there is some issue with a dataset that results in a cpp error.; I do not want to debug that as cpp errors are a pain. Can you guess what the problem might be?; The cpp breaks after the multiprocessor step. The Python process has used my 10 processors to the max for some time, but then fallen back to 100%. So it seams it might be after collecting whatever has been produced in the first multiprocessor step.; Can you tell me what that could be so I can implement a test into my scripts? It also probably would be a good idea if you could implement that test into your package. I'll check the two other links, too. If I do not come back here assume both links were not helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313450128:115,load,loaded,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313450128,1,['load'],['loaded']
Performance,"I have been moving between interactive servers not on the queue. `icb-lisa`, `icb-sarah`, and `icb-mona`, and if none of those work, then the older servers `hias`, `sepp`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564:58,queue,queue,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564,1,['queue'],['queue']
Performance,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:10,perform,performed,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791,1,['perform'],['performed']
Performance,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:203,queue,queue,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214,2,['queue'],['queue']
Performance,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:823,scalab,scalable,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,1,['scalab'],['scalable']
Performance,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where.; * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python; from anndata import AnnData; from datetime import datetime; from functools import wraps; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(func):; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; call_start_record = dict(call_id=call_id, called_func=func.__name__); if type(args[0]) is AnnData:; call_start_record[""adata_id""] = id(args[0]); logger.msg(""call"", **call_start_record). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:420,perform,performance,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['perform'],['performance']
Performance,I like your suggestions. Especially the `filter_rank_genes_groups` use makes a lot of sense to me. The one thing I would suggest to take into account is that some of these filtering steps can be done before significance testing and therefore you would not have to perform multiple testing correction on the filtered out genes. This may be quite useful to some. That precludes filtering on p-value though. It also makes a case for filtering already in `rank_genes_groups` rather than in `sc.get`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770:264,perform,perform,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770,1,['perform'],['perform']
Performance,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:512,load,load,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276,2,['load'],['load']
Performance,"I never get adjustText to work without numerous rounds of parameter optimization, so yeah, I agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675:68,optimiz,optimization,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675,1,['optimiz'],['optimization']
Performance,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409829464:463,optimiz,optimizes,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409829464,1,['optimiz'],['optimizes']
Performance,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:585,optimiz,optimizer,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. ; @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271:677,optimiz,optimize,677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271,1,['optimiz'],['optimize']
Performance,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457:217,queue,queued,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457,1,['queue'],['queued']
Performance,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676:234,perform,perform,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676,1,['perform'],['perform']
Performance,"I tentatively added a benchmark that runs just on `_get_mean_var`. Locally I donâ€™t see any difference though, whatâ€™s wrong? Too small data? Numba not set up with correct number of threads?. /edit: also I think the machine is not sufficiently tuned. The original run (before I added the `mean_var` benchmarks) said â€œNo changes in benchmarks.â€",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3015#issuecomment-2066327499:242,tune,tuned,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3015#issuecomment-2066327499,1,['tune'],['tuned']
Performance,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>; <summary> Setup (loading, simulating, and projecting) </summary>. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from scipy import sparse; from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):; adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""); sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True); sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000); sc.pp.log1p(adata); return adata. def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:670,load,loading,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['load'],['loading']
Performance,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:33,bottleneck,bottleneck,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,5,"['bottleneck', 'perform']","['bottleneck', 'bottlenecks', 'perform', 'performance']"
Performance,"I think that correlation matrix is only in the latest master version. You can install it using:; ```; pip install git+https://github.com/theislab/scanpy.git; ```. Also, be sure to load scanpy as 'import scanpy as sc'. If you use the old method (`import scanpy.api as sc`) it will not work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206:180,load,load,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206,1,['load'],['load']
Performance,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````; def read(folder,mtx_file=None,features_file=None,...):; if mtx_file is not None:; # Load mtx file; else: ; # Fall back to load from folder; ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602:284,Load,Load,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602,2,"['Load', 'load']","['Load', 'load']"
Performance,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699:150,perform,perform,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699,1,['perform'],['perform']
Performance,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026:127,perform,performance,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026,1,['perform'],['performance']
Performance,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:167,load,loadings,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,2,['load'],['loadings']
Performance,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562:279,load,loading,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562,1,['load'],['loading']
Performance,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue?. This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914:244,load,loading,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914,1,['load'],['loading']
Performance,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:970,load,loadings,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018,1,['load'],['loadings']
Performance,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:21,load,loading,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,2,['load'],['loading']
Performance,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:172,Optimiz,Optimizers,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,2,"['Optimiz', 'load']","['Optimizers', 'loads']"
Performance,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:173,load,load,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,3,['load'],"['load', 'loading']"
Performance,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487260802:435,optimiz,optimize,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487260802,1,['optimiz'],['optimize']
Performance,"I'm still dubious of the value, especially when we provide different ways of ways of optimizing the score. What would you think of instead having `sc.metrics.modularity` where you match a clustering and a graph returning a modularity score?. It would basically wrap:. ```python; (; igraph.Graph.Weighted_Adjacency(adata.obsp[""connectivities""]); .modularity(adata.obs[""louvain""].cat.codes); ); ```. But you could also generate modularity scores for other labelings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869:85,optimiz,optimizing,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869,1,['optimiz'],['optimizing']
Performance,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, â€¦). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:342,cache,cachedir,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,5,['cache'],"['cache', 'cachedir']"
Performance,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:274,Perform,Performing,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,2,"['Perform', 'perform']","['Performing', 'performs']"
Performance,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results.; * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`.; * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555:603,perform,performance,603,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555,1,['perform'],['performance']
Performance,"IIRC, it's discussed in more detail in Malte's paper:. > ; In the same way that cellular count data can be normalized to make them comparable between cells, gene counts can be scaled to improve comparisons between genes. Gene normalization constitutes scaling gene counts to have zero mean and unit variance (z scores). This scaling has the effect that all genes are weighted equally for downstream analysis. There is currently no consensus on whether or not to perform normalization over genes. While the popular Seurat tutorials (Butler et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0020)) generally apply gene scaling, the authors of the Slingshot method opt against scaling over genes in their tutorial (Street et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0125)). The preference between the two choices revolves around whether all genes should be weighted equally for downstream analysis, or whether the magnitude of expression of a gene is an informative proxy for the importance of the gene. In order to retain as much biological information as possible from the data, we opt to refrain from scaling over genes in this tutorial. https://www.embopress.org/doi/full/10.15252/msb.20188746. Since there has been no new development on this topic, we cited Malte and also opted not to scale. This is also discussed by Malte himself in the issue that was cited above. I cannot comment on spatial data itself and make confident statements here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034415456:462,perform,perform,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034415456,1,['perform'],['perform']
Performance,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:338,load,load,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096,1,['load'],['load']
Performance,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126:183,perform,performance,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126,1,['perform'],['performance']
Performance,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246:45,load,load,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246,1,['load'],['load']
Performance,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:324,perform,performance,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008,1,['perform'],['performance']
Performance,"Indeed, but then I believe UMAP should be derived from gene space and not from PCA. Even if the variance could be decomposed on the same components, the loadings could have opposite sign and UMAP would interpret them as totally different samples",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133906744:153,load,loadings,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133906744,1,['load'],['loadings']
Performance,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028:193,perform,performance,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028,1,['perform'],['performance']
Performance,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:330,perform,perform,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['perform'],['perform']
Performance,"Is it that useful to see it by default? Why would you want to know unimportant genes?. IMHO it would be more interesting to know genes that have high loadings in another early PC but not in this one, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-527405641:150,load,loadings,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527405641,1,['load'],['loadings']
Performance,Is this backward compatible for cases where old AnnData object are loaded?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/957#issuecomment-567410803:67,load,loaded,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-567410803,1,['load'],['loaded']
Performance,Is this still fixed? I see that loading a 36GB h5ad file requires me to use a machine with 128GB RAM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648:32,load,loading,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648,1,['load'],['loading']
Performance,"Isaac,. this is great, thank you so much!. Regarding the default for the dataset directory. I like this solution!. Very small edits in addition to what I commented in the code:; * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`?; * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272; * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`?. Notes:; * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(â€¦)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and GÃ¶kcen favored if I'm correctly summarizing the long thread?; * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose.; * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file...; * `pyplot.rc_context` sounds awesome.; * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822:624,cache,cache,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822,4,['cache'],['cache']
Performance,It is reproduced. It is due to the `randint` producing a value outside the range of the default dtype `int32`. On windows 64 bit systems the default is `int32` despite the system being 64 bit. This is due to default for c long being `int32` on these systems. The part of the code that fails due to this is when using the context manager to perform the leiden clustering with igraph flavor.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2042435682:340,perform,perform,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2042435682,1,['perform'],['perform']
Performance,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:453,perform,performed,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173,1,['perform'],['performed']
Performance,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(); sc.pp.scale(pbmc); ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/699#issuecomment-504639751:395,perform,performed,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699#issuecomment-504639751,1,['perform'],['performed']
Performance,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:1270,cache,cache-dir,1270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497,1,['cache'],['cache-dir']
Performance,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:312,cache,cache,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,1,['cache'],['cache']
Performance,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python; def iterativley_cluster(; g: igraph.Graph,; *,; n_iterations: int = 10,; random_state: int = 0,; leiden_kwargs: dict = {}; ) -> list:; import random; random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}; _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]; for _ in range(n_iterations-1):; partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs); steps.append(partition). return steps; ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081:953,optimiz,optimization,953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081,1,['optimiz'],['optimization']
Performance,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:98,load,loaded,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645,3,"['cache', 'load']","['cache', 'loaded', 'loading']"
Performance,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:350,perform,performs,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807,1,['perform'],['performs']
Performance,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```; In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-385-66af52bcd3f3> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace); 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(; 71 obs_metrics[""total_{expr_type}""]); ---> 72 proportions = top_segment_proportions(X, percent_top); 73 # Since there are local loop variables, formatting must occur in their scope; 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns); 182 if not isspmatrix_csr(mtx):; 183 mtx = csr_matrix(mtx); --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns); 185 else:; 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0; ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450:5,load,load,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450,1,['load'],['load']
Performance,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:468,optimiz,optimized,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,1,['optimiz'],['optimized']
Performance,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:638,load,loadings,638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611,1,['load'],['loadings']
Performance,"Just to clarify, are you referring to 3 plots in the middle (PCA loading plots)? In new scanpy release, we render both positive and negative genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992:65,load,loading,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992,1,['load'],['loading']
Performance,"Looking at #842 this is possible by subsetting the data on the groups of interest and then perform the analysis (one-vs-rest). If the reference group is an aggregate of other groups, then one has to define the new labels in the subset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/984#issuecomment-656111734:91,perform,perform,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-656111734,1,['perform'],['perform']
Performance,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494:230,scalab,scalable,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494,1,['scalab'],['scalable']
Performance,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:208,load,load,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577,1,['load'],['load']
Performance,My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). @Zethson do you have a good citation for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034405597:85,perform,performance,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034405597,1,['perform'],['performance']
Performance,"My questions from #929 donâ€™t apply since you donâ€™t use numba here. Except for â€œWhat's the performance difference hereâ€:. Itâ€™s not too bad, but we should use base 2 for everything that isnâ€™t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591:90,perform,performance,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591,1,['perform'],['performance']
Performance,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:149,perform,performance,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['perform'],['performance']
Performance,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasnâ€™t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/703#issuecomment-504923377:159,load,loaded,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703#issuecomment-504923377,1,['load'],['loaded']
Performance,No idea about the error in the performance test. @flying-sheep ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626:31,perform,performance,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626,1,['perform'],['performance']
Performance,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1882,perform,performance,1882,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,1,['perform'],['performance']
Performance,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410:29,optimiz,optimize,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410,1,['optimiz'],['optimize']
Performance,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:916,cache,cache,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,1,['cache'],['cache']
Performance,"Not only yes, but actually `spatialdata_io.visium_hd` only loads data that is **not** in the Zarr format, but the native format from SpaceRanger 3.x (which includes some Zarr files, but many other file extensions). The `SpatialData` object returned by `spatialdata_io.visium_hd` can then be saved to `.zarr` following the SpatialData format (described in this [design doc](https://github.com/scverse/spatialdata/blob/main/docs/design_doc.md) and [this page](https://github.com/scverse/spatialdata-notebooks/tree/main/notebooks/developers_resources/storage_format)), and read again using `spatialdata.read_zarr` (so, no need for the `spatialdata_io` package anymore). I hope this answers your question ðŸ˜Š",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2388548992:59,load,loads,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2388548992,1,['load'],['loads']
Performance,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:; sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-529105173:11,perform,performs,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-529105173,1,['perform'],['performs']
Performance,OK got it! I still think `@jit` is too opaque â€“ how should you know that some innocent-looking change results in a loop no longer being compiled? I think we should use `@njit` to be sure we have compiled performance-critical parts going forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-461002334:204,perform,performance-critical,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-461002334,1,['perform'],['performance-critical']
Performance,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:26,cache,cache,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862,1,['cache'],['cache']
Performance,"OK, reproducible with smaller test data:. ```py; adata_file = cache.mkdir(""rank_gene_groups_violin"") / ""test_adata.h5ad""; if not Path(adata_file).exists():; ssl._create_default_https_context = ssl._create_unverified_context; urllib.request.urlretrieve(; ""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file; ); adata_full = sc.read_h5ad(adata_file); adata = ad.concat([; adata[adata.obs.cell_type == 'Naive CD4+ T cells'][:4, :4],; adata[adata.obs.cell_type == 'Naive CD8+ T cells'][:4, :4],; ], merge='unique'); adata.write(data_path / 't-cells.h5ad'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2258#issuecomment-1658188074:62,cache,cache,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2258#issuecomment-1658188074,1,['cache'],['cache']
Performance,"OK, seems like I misunderstood the point about zero inflation here. You just meant â€œlarge number of zeroesâ€ as in â€œpretty sparseâ€ then?. A factor of 10 isnâ€™t that bad for something thatâ€™s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our â€“ currently slow but weâ€™ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:231,bottleneck,bottleneck,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['bottleneck'],['bottleneck']
Performance,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-456034330:221,perform,performance,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-456034330,1,['perform'],['performance']
Performance,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619:235,load,load,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619,2,"['load', 'scalab']","['load', 'scalability']"
Performance,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938:288,perform,performance,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938,1,['perform'],['performance']
Performance,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:219,load,loaded,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672,5,"['cache', 'load']","['cache', 'load', 'loaded']"
Performance,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts?. Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478414881:43,load,loading,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478414881,1,['load'],['loading']
Performance,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the ; ```; cache: pip; ```; line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732:182,cache,cache,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732,1,['cache'],['cache']
Performance,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since itâ€™s ill-defined, however we can tell when itâ€™s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:307,optimiz,optimization,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688,5,['optimiz'],"['optimization', 'optimized', 'optimizing']"
Performance,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:111,perform,performed,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138,1,['perform'],['performed']
Performance,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesnâ€™t appear in the list of checks, and the PR thinks all checks have run. This doesnâ€™t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:80,queue,queued,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136,1,['queue'],['queued']
Performance,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console; $ git switch master; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ); 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%); 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%); 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%); 	 257.750.810.841 instructions:u # 2,44 insn per cycle; 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%); 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%); 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%); 	; 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ); ```. - this PR:. ```console; $ git switch hvg_PR_numba; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ); 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%); 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%); 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%); 	 373.047.679.552 instructions:u # 2,19 insn per cycle; 	 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266:339,Perform,Performance,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266,1,['Perform'],['Performance']
Performance,"ProgramData\Miniconda3\lib\site-packages\sklearn\base.py"", line 17, in <module>; from .utils import _IS_32BIT; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\__init__.py"", line 28, in <module>; from .fixes import np_version, parse_version; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\fixes.py"", line 20, in <module>; import scipy.stats; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\__init__.py"", line 441, in <module>; from .stats import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\stats.py"", line 43, in <module>; from . import distributions; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\distributions.py"", line 8, in <module>; from ._distn_infrastructure import (rv_discrete, rv_continuous, rv_frozen); File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\_distn_infrastructure.py"", line 24, in <module>; from scipy import optimize; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\__init__.py"", line 400, in <module>; from .optimize import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\optimize.py"", line 36, in <module>; from ._numdiff import approx_derivative; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\_numdiff.py"", line 6, in <module>; from scipy.sparse.linalg import LinearOperator; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in <module>; from .eigen import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py"", line 9, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py"", line 20, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 42, in <module>; from . import _arpack; ImportError: DLL load failed while importing _arpack: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953:2040,optimiz,optimize,2040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953,5,"['load', 'optimiz']","['load', 'optimize']"
Performance,"Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3858,load,load,3858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['load'],['load']
Performance,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:18,load,loadings,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582,5,['load'],"['loading', 'loadings']"
Performance,"See [colab notebook](https://colab.research.google.com/drive/17m_3IiZApxpKUHluieWK6C7sGj2XGgTb?usp=sharing). With random initialization it's about 10x faster than UMAP on this system. The quadratic init (default) is as fast as UMAP, but there's an opportunity to optimize that code to use the GPU.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051102663:263,optimiz,optimize,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051102663,1,['optimiz'],['optimize']
Performance,"Should be possible to turn the y ticks legends on. But I just tested it and didn't work. I will try to fix it. The syntax is:; ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain', return_fig=True).style(yticklabels=True,row_palette='muted').show(); ```. `style` needs to be used to tune the graphical parameters to avoid overcrowding the parameters list. But I am open to have a discussion on what the users think is best. Documentation is here: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536:299,tune,tune,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536,1,['tune'],['tune']
Performance,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263#issuecomment-422099872:223,load,loading,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422099872,1,['load'],['loading']
Performance,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1281,cache,cache-dir,1281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715,1,['cache'],['cache-dir']
Performance,"So assuming that we are only interested in downsampling, then I'd say `NearMiss` and related are straightforward and scalable (just need to compute a kmeans whcih is really fast)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030:117,scalab,scalable,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030,1,['scalab'],['scalable']
Performance,"So the user experience will be:. 1. Theyâ€™ll go through the example notebooks where theyâ€™ll learn how to download data. â†’ The notebooks should mention where to configure the cache directory. 2. Theyâ€™ll download data, probably not paying attention to the output immediately. â†’ We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe theyâ€™ll eventually look at the settings module in the online documentation. â†’ We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). â†’ We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:173,cache,cache,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,5,"['cache', 'load']","['cache', 'cachedir', 'loaded']"
Performance,So we still need to perform the following work around:; 1) change file name of file `tissue_positions.csv` to `tissue_positions_list.csv`; 2) delete the header in the file. Right?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607268186:20,perform,perform,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607268186,1,['perform'],['perform']
Performance,"So, we could also not early load `scanpy.testing._pytest` or load `pytest-cov` first?. I would like to keep the `xdist` support and use a similar interface.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175,2,['load'],['load']
Performance,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution; - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. ; - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png); **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336:417,perform,performed,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336,1,['perform'],['performed']
Performance,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization?. e.g.:; ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""); sc.pp.scale(adata); adata.write(""./cache/01_simple_process.h5ad""); print(sc.logging.get_operations(adata_id=id(adata))); ```; would probably forget the first set of operations?; ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""scale"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```; I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464691691:439,perform,performed,439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464691691,5,"['cache', 'perform']","['cache', 'performed']"
Performance,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:609,optimiz,optimization,609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['optimiz'],['optimization']
Performance,"Sorry for the late response, completley forgot to post my response here. @Fougere87, did that whitening issue occur with picard as well? I saw that with sklearn. I think we could get around that by whitening ourselves with ARPACK. Picard and sklearn look pretty similar to me in a quick comparison. Below are top 16/30 components (ranked by Geary's C, autocorrelation on the connectivity graph) cell loadings on the pbmc3k dataset. The umap and connectivity matrix here were computed on top of a PCA â€“ which I should maybe do differently. However I think the results are similar enough that it's probably not of consequence. <details>; <summary> sklearn FastICA </summary>. ![sklearn_ica](https://user-images.githubusercontent.com/8238804/68647787-d53a4d80-0572-11ea-8b95-cde9122824f1.png). </details>. <details>; <summary> picard ICA </summary>. ![picard_ica2](https://user-images.githubusercontent.com/8238804/68647808-e5eac380-0572-11ea-8485-71a770849cc9.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-552756716:400,load,loadings,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-552756716,1,['load'],['loadings']
Performance,"Sorry for the long delay â€“ I was away on a retreat. By the way, congrats on the spatial letter!. I'm not sure I understand why you'd report the standard modularity if the partitioning was done with multi resolution modularity or some other quality function. To me, this makes the metric being pretty disconnected from the computation that was run. It seems likely that there could be non-proportional relationships between the whatever quality function is used and unscaled modularity. For example there could be a case where: partitioning A has higher unscaled modularity than partitioning B, but B has higher multi resolution modularity quality with resolution .8 than A. If the multi resolution modularity is what was run, I think the logging should reflect that run resulted in a ""better"" optimization. Separately, if it's meant to assess the quality of the clustering, why not calculate something like silhouette? From my perspective, it would be because that's separate enough from the process of clustering that it should be run separately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547:793,optimiz,optimization,793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547,1,['optimiz'],['optimization']
Performance,"Sounds good to me. How are you thinking of handling reproducibility w.r.t. random seeds?. To me, the best solution here is to make it easy to do small multiples for categorical plots like this, but that's a big change in the kind of plot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:947,load,load,947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155,1,['load'],['load']
Performance,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:464,load,loading,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926,1,['load'],['loading']
Performance,"Sure thing. I've dumped a couple of example entries into a numpy file and attached it below as a zip. You can replicate the behaviour with the following code. ```; import numpy; import scanpy as sc; import anndata as ad. features = np.load(""example_features.npy""). #perform scaling in sklearn; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(); scaled = scaler.fit_transform(features.copy()). print(np.isnan(scaled).sum()). #perform scaling in scanpy with the default settings; adata = ad.AnnData(features); sc.pp.scale(adata). print(np.isnan(adata.X).sum()); ```. For me this results in: . <img width=""477"" alt=""Screenshot 2024-06-26 at 16 08 41"" src=""https://github.com/scverse/scanpy/assets/15019107/806d4b70-faf2-4dce-84fc-575cd86c21f6"">. [example_features.npy.zip](https://github.com/user-attachments/files/15990483/example_features.npy.zip)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191814375:235,load,load,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191814375,3,"['load', 'perform']","['load', 'perform']"
Performance,"Sure, and I'm not against supporting special cases! Could you please explain the setup?. Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. Some systems are strange. We should be nice and support those systems while still doing the correct thing by default. We shouldn't do the wrong thing by default to accommodate strange cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606:135,cache,cache,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606,2,['cache'],['cache']
Performance,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:152,load,load,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361,1,['load'],['load']
Performance,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:1072,scalab,scalability,1072,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,1,['scalab'],['scalability']
Performance,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063:119,perform,perform,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063,1,['perform'],['perform']
Performance,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:108,perform,performance,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324,1,['perform'],['performance']
Performance,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:478,perform,performs,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['perform'],['performs']
Performance,"Thank you! I think we should only use `@njit` anyway. I donâ€™t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following?. > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but canâ€™t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211:259,cache,cached,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211,2,['cache'],"['cached', 'caches']"
Performance,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498057572:249,optimiz,optimizing,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498057572,1,['optimiz'],['optimizing']
Performance,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545:674,bottleneck,bottleneck,674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545,1,['bottleneck'],['bottleneck']
Performance,"Thanks fellas, it worked. . ```py; pp.filter_genes(adata, min_counts=1); ``` ; Weird, because the other datasets, when loaded fresh have the same pattern:; ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True ; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412237509:119,load,loaded,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412237509,1,['load'],['loaded']
Performance,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:280,perform,perform,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151,1,['perform'],['perform']
Performance,Thanks for getting back @esrice! . I think I see now -- does that mean I should multiply the `X_pca_harmony` by the _original_ PC loadings to get the imputed counts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240114021:130,load,loadings,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240114021,1,['load'],['loadings']
Performance,"Thanks for opening an issue!. Many of the function in scanpy do not support being applied on a backed anndata. `highly_variable_genes` hasn't had support for out of core computation implemented, so it errors. Better out of core support is something we're working for. Is it possible to load `X` into memory here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147#issuecomment-1049155583:286,load,load,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147#issuecomment-1049155583,1,['load'],['load']
Performance,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:44,cache,cache,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241,4,['cache'],"['cache', 'cached']"
Performance,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478996906:338,cache,cache,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478996906,1,['cache'],['cache']
Performance,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020:64,bottleneck,bottleneck,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimization']"
Performance,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:377,perform,performance,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992,1,['perform'],['performance']
Performance,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:902,bottleneck,bottleneck,902,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,1,['bottleneck'],['bottleneck']
Performance,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:383,load,loaded,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['load'],['loaded']
Performance,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting?. Given my quick overview of your package, two things you should note:; 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets.; 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838:360,Perform,Performance,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271:265,perform,performant,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271,1,['perform'],['performant']
Performance,The `to_dense` function only works for csr matrices. I think we need another kernel that handles `csc` or just `.T` the resulting array if csc. I also get performance warnings about the matmul in the numba_kernel. I'll investigate this further.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205859591:155,perform,performance,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205859591,1,['perform'],['performance']
Performance,"The biggest ones are in order:. 1. [ ] `numba`: Hard to defer. Weâ€™d have to create our own `jit` decorator returning a callable object that numba-compiles and caches the real function on its first invocation; 2. ~~`pandas`~~: Used all over the place, not feasible to defer; 3. [x] `sklearn.metrics`: Easy to defer I think, letâ€™s start with this.; 4. [ ] `matplotlib.pyplot`: Shouldnâ€™t be used in a library at all. It exists to import the kitchen sink in order to be low-friction for interactive use. Hard to do since we rely on it a lot, but we should do it.; 5. [x] `networkx`: Used in DPT, paga and plotting. Pretty easy. We use pandas all over the place, and itâ€™s hard to defer loading numba as it works with decorators. /edit: shaved off another 2/5 in a7729bc61ac569a718075edb4466852b0b4a696a via `sklearn.metrics`, `scipy.stats`, and `networkx`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433:159,cache,caches,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433,2,"['cache', 'load']","['caches', 'loading']"
Performance,"The computation should be sparse. Otherwise I'd be fine with the user doing it themselves, but the main value add here would be lower memory overhead/ optimized implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2898#issuecomment-1981827424:151,optimiz,optimized,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2898#issuecomment-1981827424,1,['optimiz'],['optimized']
Performance,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:445,load,loading,445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,1,['load'],['loading']
Performance,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460942661:433,cache,cache,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460942661,1,['cache'],['cache']
Performance,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:258,cache,cache,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['cache'],['cache']
Performance,"The scanpy install directory is super wrong, as itâ€™s not writable for many people. Thereâ€™s exactly one correct way of determining a global place for cache* files like this: [`appdirs.user_cache_dir(...)`](https://pypi.org/project/appdirs/). Alex and me talked in the past and decided for a visible directory in the working directory. Iâ€™d be up for changing it to `user_cache_dir(â€¦)` for the data. *the data are cache files since reexccuting their function after deleting the files will redownload them without loss of information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564:149,cache,cache,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564,2,['cache'],['cache']
Performance,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508524376:103,load,load,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508524376,1,['load'],['load']
Performance,Then it's probably a case of having an old python3. I loaded up an environment where I have python 3.6.9 and the newest version it saw was 0.7.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650:54,load,loaded,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650,1,['load'],['loaded']
Performance,There are quite a few questions that need to be answered first:. 1. What's the performance difference here?; 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation?; 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929#issuecomment-558069573:79,perform,performance,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929#issuecomment-558069573,1,['perform'],['performance']
Performance,"There is a function called `reticulate::install_miniconda()` which [isn't mentioned in the documentation ](https://rstudio.github.io/reticulate/articles/python_packages.html) that solves this problem. ```; install_miniconda(""scanpy""); repl_python(); ```. ```; import scanpy as sc # load successfully now; exit # close the Python interpreter ; ```. Here's my `sessionInfo()` in-case it helps anyone:. ```; R version 3.6.2 (2019-12-12); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Ubuntu 18.04.3 LTS. Matrix products: default; BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1; LAPACK: /home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/libmkl_rt.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 LC_MONETARY=en_CA.UTF-8 ; [6] LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C ; [11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] stats graphics grDevices utils datasets methods base . other attached packages:; [1] reticulate_1.14. loaded via a namespace (and not attached):; [1] BiocManager_1.30.10 compiler_3.6.2 tools_3.6.2 rappdirs_0.3.1 Rcpp_1.0.3 jsonlite_1.6 packrat_0.5.0 ; [8] png_0.1-7 ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722:282,load,load,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722,2,['load'],"['load', 'loaded']"
Performance,There is a really nice version of combat from the sva package that includes a reference batch. If this was added as a feature then you could perform your corrections separately for each sample. This might be a pretty easy addition.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551:141,perform,perform,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551,1,['perform'],['perform']
Performance,"Thereâ€™s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. Theyâ€™ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (â€œFunction blah excepted a parameter foo of type Bar, but you passed a foo of type Bazâ€). iâ€™m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:550,perform,performance,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['perform'],['performance']
Performance,"These t-SNE optimizations are mentioned in the following paper. Adding it here for reference.; https://arxiv.org/abs/2212.11506; Accelerating Barnes-Hut t-SNE Algorithm by Efficient Parallelization on Multi-Core CPUs; N Chaudhary, A Pivovar, P Yakovlev, A Gorshkovâ€¦ - arXiv preprint arXiv:2212.11506, 2022",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798:12,optimiz,optimizations,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798,1,['optimiz'],['optimizations']
Performance,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856:189,perform,performance,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856,1,['perform'],['performance']
Performance,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:574,tune,tune,574,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619,1,['tune'],['tune']
Performance,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. ðŸ‘. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:417,perform,performance,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,1,['perform'],['performance']
Performance,"This issue is still persistent. I've created a colab notebook that shows the issue on a dataset we subsample to 6000 cells:. https://colab.research.google.com/drive/1QrnDFZ7nDNOLx9gr92eknhKShd2aTIdN. @gokceneraslan can you please throw a ""bug"" tag on this issue so it gets put in the queue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-635348051:284,queue,queue,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-635348051,1,['queue'],['queue']
Performance,"This line:. https://github.com/scverse/scanpy/blob/383a61b2db0c45ba622f231f01d0e7546d99566b/pyproject.toml#L162. means that pytest imports that module together with the other plugins, and *then* runs tests. That means that it will `import scanpy.testing._pytest` (i.e. `scanpy` and everthing imported in there) before pytest-cov is loaded and can do anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122:332,load,loaded,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122,1,['load'],['loaded']
Performance,"This sounds interesting. If the performance is acceptable, it might make a good addition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-558999208:32,perform,performance,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-558999208,1,['perform'],['performance']
Performance,"This will work for csc matrix; ```bash ; @numba.njit(cache=True, parallel=True); def to_dense_csc(; shape: tuple[int, int],; indptr: NDArray[np.integer],; indices: NDArray[np.integer],; data: NDArray[DT],; ) -> NDArray[DT]:; """"""\; Numba kernel for np.toarray() function; """"""; X = np.empty(shape, dtype=data.dtype). for c in numba.prange(shape[1]):; X[:,c] = 0; for i in range(indptr[c], indptr[c + 1]):; X[indices[i],c] = data[i]; return X; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205901859:53,cache,cache,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205901859,1,['cache'],['cache']
Performance,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:696,load,loading,696,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,1,['load'],['loading']
Performance,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:90,cache,cacheing,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262,1,['cache'],['cacheing']
Performance,"Very nice! Thank you for the benchmark! ðŸ™‚ It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]!. How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082:468,bottleneck,bottleneck,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082,1,['bottleneck'],['bottleneck']
Performance,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)?. Why did you decide to change the default in Leiden; (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD; > how different is that to clustering on the UMAP embedding directly?; It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484424732:868,optimiz,optimization,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484424732,1,['optimiz'],['optimization']
Performance,"We had a journal club about this recently and couldn't really come up with a good solution tbh. Mean-log is so much easier for a lot of applications. Log-transforming data also has a variance-stabilizing effect and it reduces skewness so that the data at least better approximates a normal distribution than before, which many downstream methods assume (although data is often still far from normal). So I don't see how we can forgo log transformation without modeling count data for everything directly. The effect outlined in the paper is the most pronounced for differential expression tests between groups for which size factor distributions differ... So you could check size factor distributions before performing the test to estimate whether the log-mean vs mean-log difference will affect the test. If yes, try without log transforming the data and see if the test can deal with the outliers. Especially for the t-test, the poorer approximation of normality may not have as strong an effect as the log-mean vs mean-log difference. However, in our experience size factors tend to have a range of ~100-fold difference, and not 1,000-10,000 fold as was shown in the paper. We weren't so taken with the suggestion in the paper of increasing the pseudocount as that essentially removes fold-change effects... and also removes zeros (making all matrices dense). As for embeddings... you could remove size factor outliers for the PCA calculation and do it without log-transformation. Although in practice we found it gives very similar results. Thus, our solution was to visualize size factor distributions on embeddings to see whether there is an effect. Usually you do see a count depth effect in the embedding though... and that's not that surprising for CPM normalization, as you assume that all cells are of similar molecule count, which is incorrect. With other normalization methods it shouldn't be as bad. But yeah... overall, it's a complicated problem without a good solution. I imagine it i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918:708,perform,performing,708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918,1,['perform'],['performing']
Performance,"We knocked out a gene, then wanna to reveal the difference between KO embro cells and wild type embryo cells and illustrate the function of the gene in development process. The single cell data was generated using Smart-seq2 technology, which is a low throughput technology. And besides, there were only a few cells in eary embros because they are very small.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135:252,throughput,throughput,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135,1,['throughput'],['throughput']
Performance,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:249,load,loader,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['load'],['loader']
Performance,"We're always up for improved performance! Would love to see improvements here. (Btw, I think I've already got your gist bookmarked on twitter). Do you have any benchmarks of performance here? Especially against our current implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546:29,perform,performance,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546,2,['perform'],['performance']
Performance,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:103,load,load,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,1,['load'],['load']
Performance,"Well, but the amount of memory should be a lot smaller than if you used; ```; adata = sc.read('test.h5ad'); ```; There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434#issuecomment-456003799:273,load,loaded,273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434#issuecomment-456003799,1,['load'],['loaded']
Performance,"What about comparing communities between for example CPM and RBERVertexPartition at the same resolution, using modularity score? This way we are not comparing scores obtained by optimization functions, just simple ""external"" measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127:178,optimiz,optimization,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127,1,['optimiz'],['optimization']
Performance,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:693,bottleneck,bottleneck,693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['bottleneck'],['bottleneck']
Performance,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:; `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691512482:244,load,load,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691512482,1,['load'],['load']
Performance,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:194,perform,performing,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['perform'],['performing']
Performance,"With #3056 merged, this now says. > before | after | ratio | benchmark; > --- | --- | --- | ---; > 448Â±100ms | 381Â±100ms | ~0.85 | `preprocessing_counts.time_scrublet('pbmc68k_reduced')`. but ASV seems to think thatâ€™s not enough to report. Also unclear why itâ€™s reported as taking 26 minutes by github:. > <img width=""252"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/291575/6117cece-a145-4b46-85a4-dd86a61819ef"">. I see in the server logs. > - May 14 10:15:57 scvbench benchmark[1462905]: 2024-05-14T10:15:57.547945Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting; > - [â€¦running benchmarks]; > - May 14 10:27:18 scvbench benchmark[1462905]: 2024-05-14T10:27:18.793352Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting. which means that between setting the check run to â€œrunningâ€ and to â€œdoneâ€, 11m21s passed. Maybe GitHub counts the queue time?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339:1105,queue,queue,1105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339,1,['queue'],['queue']
Performance,"Wow well that certainly makes things a lot easier, thank you for creating that code snippet! It seems as though Scanpy was smart enough to realize I hadn't performed PCA and thus did it for me using the default settings. However as you have suggested I should be using 'arpack' to make my results reproducible, will do! . Thank you for your reply!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498856082:156,perform,performed,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498856082,1,['perform'],['performed']
Performance,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>; <summary> Example </summary>. ```; A; /; C - B; ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`; * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360:188,queue,queue,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360,2,['queue'],['queue']
Performance,"Yes , the sampling is done with weights and I used the coreset technique; for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>; wrote:. > I understand the benefits of sampling regarding computational speed up.; > What I'm not clear on is how you choose your weights for the calculations; > you perform here. You mentioned that you get wrong marker gene results when; > you sample and don't use weights. That makes sense if you get a; > non-representative set of cells in your sample. I wonder how you select the; > weights to fix this. I guess you don't just try a lot of different values; > until one works, right?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494:322,perform,perform,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494,1,['perform'],['perform']
Performance,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:195,optimiz,optimizing,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291,1,['optimiz'],['optimizing']
Performance,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:100,bottleneck,bottleneck,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139,1,['bottleneck'],['bottleneck']
Performance,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:581,perform,performs,581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,1,['perform'],['performs']
Performance,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:750,perform,perform,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467,1,['perform'],['perform']
Performance,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:131,perform,performed,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,1,['perform'],['performed']
Performance,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on itðŸ˜„. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though ðŸ˜‚",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090:203,optimiz,optimize,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090,1,['optimiz'],['optimize']
Performance,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:401,tune,tune,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['tune'],['tune']
Performance,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2349,load,load,2349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :); > ; > A couple follow up points ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:1363,optimiz,optimization,1363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088,1,['optimiz'],['optimization']
Performance,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854:214,cache,cache,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854,3,['cache'],"['cache', 'cached']"
Performance,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:874,perform,performance,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['perform'],['performance']
Performance,"adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1199,cache,cache,1199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['cache'],['cache']
Performance,"ading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3585,concurren,concurrent,3585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"age](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>; <summary> code </summary>. ```python; from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:4482,optimiz,optimization,4482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345,1,['optimiz'],['optimization']
Performance,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5470,load,load,5470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"ah, yes, i confused `.scanpy` and `.write`. i think caching in a real cache directory instead of `./.write` would be better in any case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346320991:70,cache,cache,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346320991,1,['cache'],['cache']
Performance,"also, the fact that reshuflling is performed is not in docs and should be documented. @bio-la do you plan to work on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637:35,perform,performed,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637,1,['perform'],['performed']
Performance,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1613,load,load,1613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['load'],['load']
Performance,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > â€”; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2046,load,loading,2046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475,1,['load'],['loading']
Performance,"ateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4029,cache,cached,4029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4368,cache,cached,4368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:735,optimiz,optimized,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['optimiz'],['optimized']
Performance,"canpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4535,cache,cached,4535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [â€¦] Generally, I think there should be a lon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1423,cache,cache,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cache']
Performance,"che, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz.; ```. â€‹My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats???. Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:2363,cache,cache,2363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,2,['cache'],['cache']
Performance,cleaning caches did the trick. letâ€™s see if the absolute path is also needed (rebuilding master): https://travis-ci.org/github/theislab/scanpy/builds/663442852. /edit: it did,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786:9,cache,caches,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786,1,['cache'],['caches']
Performance,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4818,scalab,scalable,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['scalab'],['scalable']
Performance,da-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; debugpy 1.6.7 py310heca2aa9_0 conda-forge; decorator 5.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9878,cache,cached-property,9878,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,"dex); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1487,cache,cache,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,1,['cache'],['cache']
Performance,"e at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1124,perform,performance,1124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458,1,['perform'],['performance']
Performance,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1384,cache,cache-,1384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['cache'],['cache-']
Performance,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it canâ€™t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2420,perform,performance,2420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"eading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3510,concurren,concurrent,3510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:2638,perform,perform,2638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['perform'],['perform']
Performance,ecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.wh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1213,cache,cached,1213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4865,cache,cached-property,4865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached-property', 'cached-property-']"
Performance,"equent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1190,cache,cache,1190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,3,['cache'],['cache']
Performance,"equirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4377,cache,cached,4377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"et things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(â€¦, flavor=â€œseuratâ€)` mimics `FindVariableFeatures(â€¦, method=â€œmean.var.plotâ€)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(â€¦, flavor=â€œseurat_v3â€)` mimics `FindVariableFeatures(â€¦, method=â€œvstâ€)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=â€œseuratâ€` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seuratâ€™s gene ordering doesnâ€™t match their definition in the paper. The one in the paper makes most sense, as itâ€™s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:1106,perform,perform,1106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['perform'],['perform']
Performance,"f = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2734,perform,perform,2734,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,1,['perform'],['perform']
Performance,"fied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4451,cache,cached,4451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"g that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1190,perform,performance,1190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['perform'],['performance']
Performance,"h-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3190,cache,cached,3190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Curr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1640,optimiz,optimization,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1015,load,loaders,1015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['load'],['loaders']
Performance,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:570,tune,tune,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['tune'],['tune']
Performance,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs; - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314:498,perform,performed,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314,1,['perform'],['performed']
Performance,"hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc. pbmc <- FindVariableFeatures(pbmc, mean.function=ExpMean, selection.method = 'vst', nfeatures = 2000). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_v3.csv""); ```. *unless when using `batches`, a bug we are currently solving. Nevertheless, implementation details, choices for numeric stability and numerics of the underlying libraries are indeed a challenge and do cause some discrepancies - surely interested how your comparison looks @carversh if you give these suggestions here a try!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:4406,Load,Load,4406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,2,"['Load', 'load']","['Load', 'load']"
Performance,"i think the reason we do this is for project-specific caching, not temporary files. using the dedicated cache dir is of course preferable to using the working dir, since the OS knows about them (and can clean them once prompted or necessary), and preferable to a tempdir, as they survive restarts. we should use <code>cache_dir = Path([appdirs](https://pypi.python.org/pypi/appdirs/1.4.3).user_cache_dir('scanpy', 'F. Alex Wolf'))</code>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346303221:104,cache,cache,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346303221,1,['cache'],['cache']
Performance,"ib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4117,cache,cached,4117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2378,concurren,concurrent,2378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"ine 82, in <module>; from .base import clone; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\base.py"", line 17, in <module>; from .utils import _IS_32BIT; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\__init__.py"", line 28, in <module>; from .fixes import np_version, parse_version; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\fixes.py"", line 20, in <module>; import scipy.stats; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\__init__.py"", line 441, in <module>; from .stats import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\stats.py"", line 43, in <module>; from . import distributions; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\distributions.py"", line 8, in <module>; from ._distn_infrastructure import (rv_discrete, rv_continuous, rv_frozen); File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\_distn_infrastructure.py"", line 24, in <module>; from scipy import optimize; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\__init__.py"", line 400, in <module>; from .optimize import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\optimize.py"", line 36, in <module>; from ._numdiff import approx_derivative; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\_numdiff.py"", line 6, in <module>; from scipy.sparse.linalg import LinearOperator; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in <module>; from .eigen import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py"", line 9, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py"", line 20, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 42, in <module>; from . import _arpack; ImportError: DLL load failed while importing _ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953:1922,optimiz,optimize,1922,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953,2,['optimiz'],['optimize']
Performance,ing tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecti,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1300,cache,cached,1300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2575,optimiz,optimization,2575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,"it shouldnâ€™t affect performance at all, just readability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316#issuecomment-2437410637:20,perform,performance,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316#issuecomment-2437410637,1,['perform'],['performance']
Performance,"ixbuf2.0-common (= 2.42.6+dfsg-2),; libgfortran5 (= 11.2.0-10),; libgirepository-1.0-1 (= 1.70.0-2),; libglib2.0-0 (= 2.70.1-1),; libglpk40 (= 5.0-1),; libgmp10 (= 2:6.2.1+dfsg-2),; libgnutls30 (= 3.7.2-2),; libgomp1 (= 11.2.0-10),; libgpg-error0 (= 1.42-3),; libgraphite2-3 (= 1.3.14-1),; libgssapi-krb5-2 (= 1.18.3-7),; libgtk-3-0 (= 3.24.30-3),; libgtk-3-common (= 3.24.30-3),; libharfbuzz0b (= 2.7.4-1),; libhdf5-103-1 (= 1.10.7+repack-4),; libhdf5-hl-100 (= 1.10.7+repack-4),; libheif1 (= 1.12.0-2+b3),; libhogweed6 (= 3.7.3-1),; libicu67 (= 67.1-7),; libidn2-0 (= 2.3.2-2),; libigraph1 (= 0.8.5+ds1-1),; libimagequant0 (= 2.12.2-1.1),; libip4tc2 (= 1.8.7-1),; libisl23 (= 0.24-2),; libitm1 (= 11.2.0-10),; libjbig0 (= 2.1-3.1+b2),; libjpeg62-turbo (= 1:2.1.1-1),; libjs-jquery (= 3.5.1+dfsg+~3.5.5-8),; libjs-jquery-hotkeys (= 0~20130707+git2d51e3a9+dfsg-2.1),; libjs-jquery-isonscreen (= 1.2.0-1.1),; libjs-jquery-metadata (= 12-3),; libjs-jquery-tablesorter (= 1:2.31.3+dfsg1-2),; libjs-jquery-throttle-debounce (= 1.1+dfsg.1-1.1),; libjs-jquery-ui (= 1.13.0+dfsg-1),; libjs-sphinxdoc (= 4.2.0-5),; libjs-underscore (= 1.9.1~dfsg-4),; libjson-c5 (= 0.15-2),; libk5crypto3 (= 1.18.3-7),; libkeyutils1 (= 1.6.1-2),; libkmod2 (= 29-1),; libkrb5-3 (= 1.18.3-7),; libkrb5support0 (= 1.18.3-7),; liblapack3 (= 3.10.0-1),; liblbfgsb0 (= 3.0+dfsg.3-9),; liblcms2-2 (= 2.12~rc1-2),; libldap-2.4-2 (= 2.4.59+dfsg-1),; libllvm11 (= 1:11.1.0-4),; liblqr-1-0 (= 0.4.2-2.1),; liblsan0 (= 11.2.0-10),; libltdl7 (= 2.4.6-15),; liblz4-1 (= 1.9.3-2),; liblzf1 (= 3.6-3),; liblzma5 (= 5.2.5-2),; liblzo2-2 (= 2.10-2),; libmagic-mgc (= 1:5.39-3),; libmagic1 (= 1:5.39-3),; libmagickcore-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmagickwand-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmd0 (= 1.0.4-1),; libmount1 (= 2.37.2-4),; libmpc3 (= 1.2.1-1),; libmpdec3 (= 2.5.1-2),; libmpfr6 (= 4.1.0-3),; libncurses6 (= 6.2+20210905-1),; libncursesw6 (= 6.2+20210905-1),; libnettle8 (= 3.7.3-1),; libnghttp2-14 (= 1.43.0-1),; libnsl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:6253,throttle,throttle-debounce,6253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['throttle'],['throttle-debounce']
Performance,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:1744,load,loading,1744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297,1,['load'],['loading']
Performance,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4735,cache,cached,4735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. â€œreading cached data from ~/.cache/scanpy/paul15.h5adâ€. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of [â€¦]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1291,cache,cache,1291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],['cache']
Performance,"md64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2246,cache,cached,2246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"mp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype); 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,; 639 ordered=ordered,; --> 640 dtype=dtype); 641 if dtype.categories is None:; 642 msg = (""The categories must be provided in 'categorie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1569,load,load,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['load'],['load']
Performance,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4726,load,load,4726,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['load'],['load']
Performance,"n3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3182,optimiz,optimization,3182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,n_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1455,cache,cached,1455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"n_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Coll",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2174,cache,cached,2174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ne-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1921,cache,cached,1921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ng-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2043,concurren,concurrent,2043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3361,cache,cached,3361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"nse for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>; <summary> ðŸ </summary>. ```python; def log1p(data, copy=False, chunked=False, chunk_size=None):; """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters; ----------; data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`; The (annotated) data matrix of shape `n_obs` Ã— `n_vars`. Rows correspond; to cells and columns to genes.; copy : `bool`, optional (default: `False`); If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; if copy:; if not isinstance(data, AnnData):; data = data.astype(np.floating); data = data.copy(); elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):; raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):; if issparse(X):; np.log1p(X.data, out=X.data); else:; np.log1p(X, out=X). return X. if isinstance(data, AnnData):; if not np.issubdtype(data.X.dtype, np.floating):; data.X = data.X.astype(np.floating, copy=False); if chunked:; for chunk, start, end in data.chunked_X(chunk_size):; data.X[start:end] = _log1p(chunk); else:; _log1p(data.X); else:; _log1p(data). return data if copy else None; ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239:1453,perform,perform,1453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239,1,['perform'],['perform']
Performance,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2901,optimiz,optimization,2901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,"oftware/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session information updated at 2022-05-17 14:56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:1758,concurren,concurrent,1758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['concurren'],['concurrent']
Performance,"ogg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 key = check_bool_indexer(self.index, key); 910 ; --> 911 return self._get_with(key); 912 ; 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 return self.loc[key]; 952 ; --> 953 return self.reindex(key); 954 except Exception:; 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs); 3732 @Appender(generic.NDFrame.reindex.__doc__); 3733 def reindex(self, index=None, **kwargs):; -> 3734 return super(Series, self).reindex(index=index, **kwargs); 3735 ; 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4354 # perform the reindex on the axes; 4355 return self._reindex_axes(axes, level, limit, tolerance, method,; -> 4356 fill_value, copy).__finalize__(self); 4357 ; 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4367 ax = self._get_axis(a); 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,; -> 4369 tolerance=tolerance, method=method); 4370 ; 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 else:; 502 if not target.is_unique:; --> 503 raise ValueError(""cannot reindex with a non-unique indexer""); 504 ; 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer; ```. These t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264:1763,perform,perform,1763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264,1,['perform'],['perform']
Performance,onda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; d,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9816,cache,cachecontrol-with-filecache,9816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol-with-filecache']
Performance,"one-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3270,cache,cached,3270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"op:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:ç­‰çº¿;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:ç­‰çº¿;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2032,cache,cached-property,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['cache'],['cached-property']
Performance,"or faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz.; ```. â€‹My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1942,cache,cache,1942,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,3,['cache'],['cache']
Performance,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2549,perform,performance,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['perform'],['performance']
Performance,"ow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4278,cache,cached,4278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"p us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1469,load,loading,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['load'],['loading']
Performance,p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9769,cache,cachecontrol,9769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol']
Performance,"packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:2078,load,load,2078,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['load'],['load']
Performance,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:117,load,load,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220,1,['load'],['load']
Performance,"port scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); pri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:1405,cache,cache,1405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,1,['cache'],['cache']
Performance,pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5616,bottleneck,bottleneck,5616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,rgon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5824,cache,cachetools,5824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cachetools']
Performance,"rgs, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings; [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):; --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(; [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,; [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,; [127](https://vscode-remote+ssh-002dremote-002b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:2311,cache,cache,2311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['cache'],['cache']
Performance,"s__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2458,cache,cachetools,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cachetools']
Performance,"self, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; Fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1968,concurren,concurrent,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:69,load,loading,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158,1,['load'],['loading']
Performance,"spatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2435,cache,cached-property,2435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cached-property']
Performance,"ss CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:1212,cache,cache,1212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,4,['cache'],"['cache', 'cache-control', 'cached']"
Performance,"ssion"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1651,scalab,scalable,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['scalab'],['scalable']
Performance,"sudo rm /media/ubuntu/d0b69706-4d42-40a3-b531-382041477d35/home/cns/biosoft/cellranger/cellranger-3.0.2/deng2_count_myself -fr. At 2019-07-27 18:48:50, ""Cristian"" <notifications@github.com> wrote:. Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. This happens in the first loop to load all the datasets. If I run only one dataset the same error (unsupported operand type(s) for +: 'int' and 'str') showed up when I plot some data quality summary plots:. For instance:; p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac'); adata = adata[adata.obs['mt_frac'] < 0.2] print('Number of cells after MT filter: {:d}'.format(adata.n_obs)); sc.pp.filter_cells(adata, min_genes = 700) print('Number of cells after gene filter: {:d}'.format(adata.n_obs)). I am using data generated by 10x V3 and CellRanger v3.0.1. I really do not know where the problem is. I really appreciate any advice/help to solve this issue. Thanks in advance. â€”; You are receiving this because you are subscribed to this thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613:392,load,load,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613,2,['load'],['load']
Performance,"sure, weâ€™ll talk in 10 days or so, after my holidays ðŸ˜„. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents donâ€™t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:244,cache,cache,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,9,['cache'],['cache']
Performance,"t I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, â€¦. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesnâ€™t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers donâ€™t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, youâ€™d have $XDG_CACHE_DIR point to a separate disk that has more space and isnâ€™t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1096,cache,cached,1096,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cached']
Performance,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6041,perform,performance,6041,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,1,['perform'],['performance']
Performance,"t import NNDescent; [48](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=47) from pynndescent.distances import named_distances as pynn_named_distances. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py:39, in <module>; [25](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=24) else:; [26](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=25) return val; [29](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=28) @numba.njit(; [30](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=29) ""f4(f4[::1],f4[::1])"",; [31](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=30) fastmath=True,; [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=31) cache=True,; [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=32) locals={; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=33) ""result"": numba.types.float32,; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=34) ""diff"": numba.types.float32,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=35) ""dim"": numba.types.int32,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=36) },; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=37) ); ---> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:11618,cache,cache,11618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['cache'],['cache']
Performance,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:1642,cache,cache,1642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,2,['cache'],['cache']
Performance,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1941,perform,performs,1941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['perform'],['performs']
Performance,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1791,perform,performance,1791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['perform'],['performance']
Performance,"trix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:1692,bottleneck,bottleneck,1692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['bottleneck'],['bottleneck']
Performance,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1045,load,loaders,1045,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['load'],['loaders']
Performance,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:2139,Perform,Performance,2139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3608,cache,cached,3608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"y variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:1171,cache,cache,1171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,1,['cache'],['cache']
Performance,you can check here that the log is actually performed as you suggest: https://github.com/theislab/scanpy/blob/5533b644e796379fd146bf8e659fd49f92f718cd/scanpy/preprocessing/_recipes.py#L66-L95. I'll close this for now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149:44,perform,performed,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149,1,['perform'],['performed']
Safety," line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; Ã— python setup.py egg_info did not run successfully.; â”‚ exit code: 1; â•°â”€> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3465,avoid,avoids,3465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['avoid'],['avoids']
Safety," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways ðŸ˜„, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1527,detect,detection,1527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,2,['detect'],"['detecting', 'detection']"
Safety," we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1319,detect,detected,1319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['detect'],['detected']
Safety,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metricâ€™s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7602,redund,redundency,7602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['redund'],['redundency']
Safety,"/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1153,detect,detect,1153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,1,['detect'],['detect']
Safety,"> (a) a pair of completely correlated features, and (b) very strange count distributions. can you elaborate more on this? what does it mean ""completely correlated"", like an identical copy ?. > Once I used a proper variance stabilizing transform (arcsinh in this case) and remove redundant features. Interesting, never seen this used in scRNA-seq, is it common in IMC ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323:279,redund,redundant,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-802666323,1,['redund'],['redundant']
Safety,"> > Numba canâ€™t correctly detect when a threading backend is available; >; > Is there a numba issue for this?. https://github.com/numba/numba/issues/6108, This issue may be specific to `tbb`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150:26,detect,detect,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874667150,1,['detect'],['detect']
Safety,"> > So I wrote a wrapper around scipy.sparse to implement NumPy's **array_function** protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular scipy.sparse.; > ; > Thoughts on adding this to scipy.sparse itself so that we can avoid the wrapper? cc @rgommers. See discussion in https://github.com/scipy/scipy/issues/10362 for this. The general sentiment is: probably not the best idea, because of the matrix/ndarray semantics not being compatible. More input on that SciPy issue is very welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682:362,avoid,avoid,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557317682,1,['avoid'],['avoid']
Safety,"> @flyingsheep I can assure you, that's the normal case in academic HPC systems. I agree that this is a huge and common problem in many HPC systems. I usually install conda and R packages to non-home directories with bigger space to avoid issues on servers. One can fill up hundreds of MB by just installing a single package e.g. human genome from Bioconductor ðŸ˜„ . > Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. There is a user home and the cache is `~/.cache` and $XDG_CACHE_HOME is undefined (at least in my case). Some pip wheel files are there for example. . Although it's painful to work in such systems, I believe it's user's responsibility to fix this. One idea might be to print a warning when the cache directory is created for the first time along with the path itself to inform the user about where files are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878:233,avoid,avoid,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878,2,"['avoid', 'detect']","['avoid', 'detect']"
Safety,"> Can we keep the docs on what exactly is happening + how to troubleshoot somewhere in this doc? This means things like: How to tag + build locally, twine check, list contents of distributed file etc. Sure, as we agreed on in person, Iâ€™ll just add a section to the end of the document.; If the build process or package structure arenâ€™t touched, doing things manually isnâ€™t necessary. > We should also automate some checks to avoid broken releases. As we agreed in person: Letâ€™s postpone this. E.g. don't allow this except on specific branches + probably turn on merge queue so we know only commits that pass tests + doc builds get to those branches. This PR automatically does `twine check`, which is enough improvement over â€œtrust the person doing the release to do thatâ€ to be worth the change, even if it wasnâ€™t for the added convenience!. /edit: all addressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678:425,avoid,avoid,425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678,1,['avoid'],['avoid']
Safety,"> Completely agree, GÃ¶kcen!; > ; > How I just thought about dealing with this in the past couple of minutes: could we not make a submodule rtools? We could show the contained wrapper functions on an extra page of the API. All of the dependencies of this would be optional. In effect, this would be a very shallow wrapper that is only interesting for people who already have a working R installation etc. and use Scanpy along with R packages. As there are quite many of these people, this is definitely meaningful.; > . That'd make things a lot easier for many people (including myself ðŸ˜ƒ), I agree. However. 1) There are (and will be) so many R packages about single cell, so once we open the door, there might be so many requests about these packages so that it'd be difficult to decide what to include and what not to include. The decision might be a bit arbitrary. This is why I suggested a contrib repo, which will have everything users request (as soon as there is someone who is willing to maintain it), in a `use at your own risk` way... 2) There might be several bug reports about rpy2 itself or thin wrappers or R installation or R packages themselves. I was wondering whether this might introduce more maintenance burden, although supported packages will be limited. > The code would still look proper. Implementing tests for these wrappers is maybe not so important as these are only shallow interfaces. It would be easier to have this in the main scanpy repository than setting up a scanpy-contrib: I imagine less people will like to contribute and take the burden of maintaining another repository. PS: anndata is a different story. That's something that is meant to be so basic that it doesn't need a lot of maintenance an contributions.; > ; > What do you think?. Alternatively, we can just prepare jupyter notebooks with some Python 3 and some R cells in it (which is super easy via rpy2 magics anyway) for some R packages/functions like mnn or SIMLR and put those in scanpy_usage as a ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901:1031,risk,risk,1031,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-382002901,1,['risk'],['risk']
Safety,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:70,risk,risk,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257,1,['risk'],['risk']
Safety,"> Hm. Are all functions that you edit in 1.9.6? Then we could set the milestone to 1.9.7.; > ; > If youâ€™re not sure, itâ€™s safer to set it to 1.10.0. I think `sc.pp.normalize_per_cell` is not in the API section of the doc of 1.9.6 (intentionally?).; Other functions I modified are listed there as far as I checked.... You have `pp` or `tl` methods in mind which could conflict here?; Not 100% sure I donâ€™t miss anything, so can add 1.10.0 ðŸ‘",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814510857:122,safe,safer,122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1814510857,1,['safe'],['safer']
Safety,"> I believe a method was recently added to scanpy to use only a particular fraction of genes to calculate size factors (avoiding genes that make up >5% of the total counts). Yes, it's https://scanpy.readthedocs.io/en/latest/api/scanpy.pp.normalize_total.html with param `fraction=0.95`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488044083:120,avoid,avoiding,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488044083,1,['avoid'],['avoiding']
Safety,"> I created a new environment (see below for package details) and there everything works as it should. Can you use this new environment to do your analysis?. I expect that the previous environment managed to get into a messy state, which can lead to very strange errors. Because of this, I generally avoid trying to update old environments much and instead opt for creating fresh ones frequently.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096:300,avoid,avoid,300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-848441096,1,['avoid'],['avoid']
Safety,"> I'll definitely talk to the admin, but I am not sure he would update. An admin that doesnâ€™t take security risks seriously isnâ€™t doing their job properly. ---. > Jupyter Notebook requires JavaScript. that probably means that Jupyter notebook tries to run lynx or www or sone other text-only browser. `jupyter notebook --no-browser` is correct and the tokens arenâ€™t machine-specific. [I set up stuff differently](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html#automatic-password-setup), but @ivirshupâ€™s setup should work perfectly as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/561#issuecomment-477525477:108,risk,risks,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/561#issuecomment-477525477,1,['risk'],['risks']
Safety,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward?. Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this.; * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way.; * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678:241,avoid,avoid,241,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678,1,['avoid'],['avoid']
Safety,"> It is great that you are looking into this. Can you check if it is possible to remove the need to type 'color' and simple accept the second parameter as the 'color' parameter (eg instead of sc.pl.umap(adata, color='clusters') -> sc.pl.umap(adata, 'clusters'). ðŸ‘ . > The reason why the dimensions is a string like ""1,2"", was to avoid breaking previous usage. The idea is to parse `components`, but allow you to directly pass the indices with dimensions. > The starting number is not 0 because is consistent with usage as in 'principal component 1' or 'UMAP-1'. I don't think this should be changed even though it requires a bit of extra coding. To me, I think it makes more sense to be consistent with python. I feel like it's very clear what is happening if the default argument is `sc.pl.pca(adata, dimensions=(0, 1))`. It makes it easier to work with programmatically if the values are equivalent to what you could use to index the array directly. For example, say you find the dimension which is maximally correlated with some gene. You can just pass the result of that into dimensions without having to remember to add 1. > For the plots being the product of color and components: this was to solve the unlikely case in which you want to plot n colors using m dimensions. I don't have an opinion on this as I think is a corner case and have never used this functionality. Cool. I feel like this can be useful, but it would be useful if I could choose which arguments it worked with. I think this is a different function call though. For example, I might was to look at a gene under multiple embeddings, so pairwise combinations of `basis` and `colors`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877:329,avoid,avoid,329,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1538#issuecomment-747279877,1,['avoid'],['avoid']
Safety,"> Just to go back to your original problem, in your case you were using as mapping categories that were not present in your groupby key altogether. This is a different issue, and probably the function should have thrown an error saying var_group_labels are not present in categories. Since I just copied the example from the tutorial, I think it would be great fix the handling of heatmap there. There, the problem is sort of two-fold: 1) as you mentioned, the groupby labels NK/T-cell etc. hadn't been defined before, and 2) that only markers for only a subset of the clusters are used (5 out of 9 clusters have markers) are used with `dendrogram=True`. Both of them independently provoke the warning/issue about the reordering.; Maybe an error would be appropriate not only for undefined groupby labels (as you suggested), ; but also for the case where cluster markers for only a subset of clusters are supplied, instead of delivering a warning and a potentially incorrect ordering.; In the latter case, an error with a message suggesting to use `dendrogram=False` would be worthwhile. The danger with just printing a warning is that it might be ; missed/ignored by the user (e.g. if the function is used in a pipeline with lots of other outputs) ; and mismatching color codes might also not be apparent initially. ; For instance, it took me a while to even spot this issue, as i didn't notice the mismatching colors initially.; This confusion could be avoided if heatmap doesn't produce a plot at all when incorrect arguments are used. I agree that your workarounds also work, but they don't fully serve as substitutes to make the plots as in the tutorial yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522:1455,avoid,avoided,1455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1479#issuecomment-723071522,1,['avoid'],['avoided']
Safety,"> More interesting is that regress_out becomes lightning fast when n_jobs = 24 and with BLAS multi threading disabled:. Thats not too surprising to me. This must be significantly over scheduling your machine. --------------------------------. This got me doing a little more digging into this, and it look's like there's actually a solution now! We can use [`threadpoolctl`](https://github.com/joblib/threadpoolctl) to dynamically manage the number of threads BLAS uses via the `threadpool_limits` context manager. . I'm definitely interested in using this inside scanpy to manage the number of threads used here. Not quite sure yet what the right behaviour/ api is. Some options:. * Should all calls use 1 blas thread by default, so parallelization will only happen through the number of jobs?; * Do we only limit the number of threads if `n_jobs` is specified? ; * Do we try and be fancy, with something like `n_threads = n_cpus // n_jobs`?. *Minor update*. [If we use `joblib` (with the `loky` backend) instead of `multiprocessing`, the fancy solution will be used by default.](https://joblib.readthedocs.io/en/latest/parallel.html#avoiding-over-subscription-of-cpu-resources). I think this is what the code would look like inside of `regress_out`:. ```python; from joblib import Parallel, delayed; res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240:1135,avoid,avoiding-over-subscription-of-cpu-resources,1135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396#issuecomment-691913240,1,['avoid'],['avoiding-over-subscription-of-cpu-resources']
Safety,> Numba canâ€™t correctly detect when a threading backend is available. Is there a numba issue for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687:24,detect,detect,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874655687,1,['detect'],['detect']
Safety,"> One thing we had discussed was moving out the merge logic for multiple batches from being specified by `flavor` to being specified by a different argument, maybe `merge_flavor` or `batch_flavor`. Have you thought about/ looked into this?. I see what you mean. Not yet looked into that here, indeed. Slight risk of increasing confusion potential for users?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1892781138:308,risk,risk,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1892781138,1,['risk'],['risk']
Safety,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions?. Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019:914,recover,recover,914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602#issuecomment-763590019,1,['recover'],['recover']
Safety,"> Test failures are not mine. It's not numba, it's annoy #1638 (hadn't realized scrublet uses it too). Asking for a rebuild will make it go away, but we should see if we can (1) avoid annoy in that test or (2) disable that test on 3.6 in a separate PR. Doc builds failures do seem related to this, however. Something about the way the `pip` requirement is formatted?. ------------------------. In future, could you not force push while responding to review? It makes it difficult for me to figure out what changed since my last review. History cleanup can happen pre or post review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118:178,avoid,avoid,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-777382118,1,['avoid'],['avoid']
Safety,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons?. Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€?. Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417:829,redund,redundant,829,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610#issuecomment-484041417,2,['redund'],['redundant']
Safety,"> best palette out there. Thatâ€™s quite the generalization. | Pro | Con |; | --- | --- |; | Many colors | Ugly colors, no rhyme or reason |; | Acceptably distinguishable | Dark colors hard to distinguish on white bg, and light ones hard to distinguish on black bg |; | | Thereâ€™s always colors that are hard to see on any kind of bg (no safe bg color) |; | | Not colorblind friendly |. So Iâ€™d recommend against it whenever you can avoid it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/740#issuecomment-513174083:335,safe,safe,335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/740#issuecomment-513174083,2,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"> coverage decreased, I think not detected because some pytest.parametrize were removed?. I think codecov just hadnâ€™t updated its comment yet when you saw that. What you say canâ€™t be, it doesnâ€™t matter how a line was hit: If a line is run, itâ€™ll be reported as hit, if your changes would have caused it to no longer be it, it would have been reported as a miss.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3042#issuecomment-2196399245:34,detect,detected,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3042#issuecomment-2196399245,1,['detect'],['detected']
Safety,"> some of the members of the nth ring have more than one neighbor in the nth-1. Damn... yeah, that won't work then. Back to differences of adjacency matrices then i guess. It looks as though your functions are replicating what I assume is going on in the backend of `networkx` anyway. Are you trying to avoid the heavy dependency or why replicate the effort? I would assume that once we need things like label propagation (maybe to denoise label assignment after deconvolving spatial spots?) then `networkx` might come in handy, no?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776:303,avoid,avoid,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-705480776,1,['avoid'],['avoid']
Safety,"@Hrovatin, if you haven't read it yet I think you would find the [Hotspot](https://www.cell.com/cell-systems/fulltext/S2405-4712(21)00114-9) method from the Yosef lab interesting. It uses something similar to Morans I for feature selection and local Morans I for gene module detection. They use parametric null models to get significances for their scores, which would be significantly faster than permutation testing. I'm a little unsure of how the parametric null models correspond to the non-parametric ones since the only comparison I've found so far is some Q-Q plots in the supp.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204:275,detect,detection,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-846745204,1,['detect'],['detection']
Safety,@LuckyMD I think we cann't use Silhouette co-efficient for the data like single cell. Where the are chances we have clusters with few points and silhouette won't be able to detect it separate cluster. E.g. in Pbmc3k dataset 'Megakaryocytes' and 'Dendritic' cell type will not be marked as a separate cluster by using your suggested co-efficient.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327:173,detect,detect,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498155327,1,['detect'],['detect']
Safety,"@LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Here's an example for a BFS from a specific point (which you can expand to all points by using the identity matrix):. <img width=""447"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/95168561-360eec00-07fd-11eb-96b9-19fbf3871d02.png"">. @Koncopd, I think that should work, since you're adding a self edge so you have redundancy at each step. . A separate point of contention on handling it like this: do you want the first step neighbors to have the same weight as the second step neighbors? My assumption would be no.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147:508,redund,redundancy,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704081147,1,['redund'],['redundancy']
Safety,"@chris-rands my two cents here: Using two many PCs (50 or 100) might not be such a good idea because most PCs above the first 15-20 (rough figure) are likely to represent non-biological variability (e.g., batch effects), so their inclusion in the analysis might lead to the identification of clusters that are not biologically-relevant but are rather technical noise. That's why it is recommended to use either a Jackstraw plot or an elbow plot to identify the optimal number of PCs to avoid including PCs that correspond to technical variation rather than biological heterogeneity. Therefore, overshooting the number of PCs used from 10 to 20 is not that big a deal, but using 50 is NOT ideal by any means.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877:486,avoid,avoid,486,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822057877,1,['avoid'],['avoid']
Safety,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:589,avoid,avoided,589,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['avoid'],['avoided']
Safety,"@falexwolf I took the opportunity to add a change that I wanted with respect to the palette which is the ability to set a palette based on a matplotlib colormap. For example using `palette='tab20'`:. ![image](https://user-images.githubusercontent.com/4964309/46139067-dcf34180-c24d-11e8-892a-a6f3bbda2c4b.png). or using `palette='Set3'`. ![image](https://user-images.githubusercontent.com/4964309/46139126-feecc400-c24d-11e8-9e34-f8395c70aeb9.png). I didn't want to modify the previous code that handles setting the palette to avoid breaking other code, but if we have some tests for other functions that use that functionality I could try to update the original methods.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021:527,avoid,avoid,527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-425036021,1,['avoid'],['avoid']
