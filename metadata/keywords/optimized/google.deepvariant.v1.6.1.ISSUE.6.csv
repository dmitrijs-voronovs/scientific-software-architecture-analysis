quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Testability,I can confirm this problem on some of my samples. The program never produces the final outputs due to the post processing step ending prematurely (but no error message reported). . Normal log:; `; I0814 02:06:33.719291 140247404730176 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; 2024-08-14 02:06:33.734191: I deepvariant/postprocess_variants.cc:94] Read from: /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/bwa_hapbetter/wgs/deepvariant/tmp/tmp1gvo5vri/call_variants_output-00000-of-00001.tfrecord.gz; 2024-08-14 02:13:18.938389: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 71894602; I0814 02:35:15.649105 140247404730176 postprocess_variants.py:1313] CVO sorting took 28.698674070835114 minutes; I0814 02:35:15.649988 140247404730176 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0814 02:40:15.761767 140247404730176 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0814 05:02:43.606994 140247404730176 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 142.46444999376934 minutes; I0814 06:01:36.673851 140247404730176 postprocess_variants.py:1407] Finished writing VCF and gVCF in 58.884093316396076 minutes. real 235m30.029s; user 220m0.378s; sys 13m54.784s. `. Samples with problems:; `; I0814 16:35:25.856544 140492383778624 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; 2024-08-14 16:35:25.879399: I deepvariant/postprocess_variants.cc:94] Read from: /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/bwa_hapbetter/wgs/deepvariant/tmp/tmpfrusl15j/call_variants_output-00000-of-00001.tfrecord.gz; 2024-08-14 16:44:06.712469: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 92795573; I0814 17:09:30.584156 140492383778624 postprocess_variants.py:1313] CVO sorting took 34.078398688,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868#issuecomment-2290172397:188,log,log,188,,https://github.com/google/deepvariant/issues/868#issuecomment-2290172397,1,['log'],['log']
Testability,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 2; ```; This is my command; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-350952138:363,test,test-training-dataset,363,,https://github.com/google/deepvariant/issues/10#issuecomment-350952138,1,['test'],['test-training-dataset']
Testability,I confirmed again that `cd bin; sudo ./run-prereq.sh` exits with `$?` set to 0. Then I did another attempt with `make-examples.zip` and the test data but the result is still the same ImportError.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-360832980:140,test,test,140,,https://github.com/google/deepvariant/issues/41#issuecomment-360832980,1,['test'],['test']
Testability,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262:129,test,test,129,,https://github.com/google/deepvariant/issues/262,1,['test'],['test']
Testability,"I get a 8 VCPUs, 52 GB RAM [n1-highmem-8](https://cloud.google.com/compute/docs/machine-types#n1_high-memory_machine_types) machine to test:. ```; gcloud compute instances create ""${USER}-test-speed"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-highmem-8"" \; --zone ""us-west2-b"" \; --boot-disk-size 100G \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I installed Singularity:; ```; curl https://gist.githubusercontent.com/pichuan/7840c8ba80ad31fee9d6f8bea20edb6a/raw/cbf62eb2ea2f141351801db76781d99d04704b4e/install_singularity_3.7.0.sh | \; sed -e s'|github.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:135,test,test,135,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,3,['test'],"['test', 'test-speed']"
Testability,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED; 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though).; Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785:1564,test,test,1564,,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785,1,['test'],['test']
Testability,"I got a machine to test:. ```; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. On the machine, I install nvidia driver first:. ```; sudo yum update -y && sudo yum install -y python3; curl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py; sudo python3 install_gpu_driver.py; ```. After that, I can confirm that nvidia-smi exists:; ```; [pichuan@pichuan-gpu2 ~]$ nvidia-smi; Thu Mar 16 04:47:54 2023 ; +-----------------------------------------------------------------------------+; | NVIDIA-SMI 525.85.12 Driver Version: 525.85.12 CUDA Version: 12.0 |; |-------------------------------+----------------------+----------------------+; | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |; | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |; | | | MIG M. |; |===============================+======================+======================|; | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 |; | N/A 34C P0 29W / 250W | 0MiB / 16384MiB | 1% Default |; | | | N/A |; +-------------------------------+----------------------+----------------------+; ; +-----------------------------------------------------------------------------+; | Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:19,test,test,19,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,1,['test'],['test']
Testability,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell; `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants; ```; cd ouput; ../opt/deepvariant/bin/call_variants \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \; --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \; --checkpoint ""/opt/models/pacbio/model.ckpt"" \; --use_openvino; ```; At this point, we made progress with the following; ```; Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /output/./model.xml; [ SUCCESS ] BIN file: /output/./model.bin; [ SUCCESS ] Total execution time: 30.04 seconds. ; [ SUCCESS ] Memory consumed: 699 MB; ```; With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-763490283:39,test,tested,39,,https://github.com/google/deepvariant/issues/404#issuecomment-763490283,1,['test'],['tested']
Testability,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:381,test,testdata,381,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"I have a couple of questions about https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples,. 1. how is `examples.tfrecord@${N_SHARDS}.gz` evaluated to become `examples.tfrecord-00006-of-00008.gz`; 1. what does `--task {}` mean?. Could you please explain briefly how this command parallelize the processing? N_SHARDS docker containers have been started, and it seems each 1kbp region is sent to a different processor based on log, but I don't quite get it how each container knows which 1kbp region to parse.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428692945:472,log,log,472,,https://github.com/google/deepvariant/issues/99#issuecomment-428692945,1,['log'],['log']
Testability,"I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/516:325,log,logically,325,,https://github.com/google/deepvariant/issues/516,1,['log'],['logically']
Testability,"I have been trying to run the whole genome case study with --regions in make_example, with the following command. I have copied all command in the genome case study page and added the regions to the make example as follow.; ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --regions '""chr20:10,000,000-10,010,000""'\; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1 ; ```; I have also tried ; `` --regions ""chr20:10,000,000-10,010,000""\`` ; and ; `` --regions chr20:10,000,000-10,010,000\`` ; However, in all cases, it seems like the whole data set is used in make example instead of the specific region. I am not sure what the problem is.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72:302,log,log,302,,https://github.com/google/deepvariant/issues/72,2,['log'],['log']
Testability,"I have encountered the following error in several PacBio HiFi samples while running the docker image of deepvariant 1.4.0:. > F deepvariant/allelecounter.cc:872] Check failed: left_padding + right_padding < counts_.size() (5000 vs. 4022); Fatal Python error: Aborted. Deepvariant was run while enabling read normalization:. docker run -v ""input_path"":/input -v ""output_path"":/output google/deepvariant:1.4.0 /opt/deepvariant/bin /run_deepvariant --model_type=PACBIO --make_examples_extra_args=""normalize_reads=true"" --ref=/input/reference.fa --reads=/input/sample.bam --output_vcf=/output/sample.vcf --output_gvcf=/output/sample.gvcf --num_shards=16 --logging_dir=/output/logs. I know it is discouraged to enable read normalization due to potential excessive computational times, but I need it to make sure that I am capturing INDELs on the same conditions as my Illumina left-aligned samples. Any ideas on how to solve this issue?; Thank you,; Eugenio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/762:672,log,logs,672,,https://github.com/google/deepvariant/issues/762,1,['log'],['logs']
Testability,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437006790:1392,log,logging,1392,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790,2,['log'],"['log', 'logging']"
Testability,"I have no problem in making my .bam file public, but please help me where; to drop / upload it, to make it public, so you can test it. On Thu, Sep 23, 2021 at 4:25 AM Kirti B ***@***.***> wrote:. > I think RAM is not an issue, my machine has 2TB RAM.; >; > Coverage is 46x; >; > On Thu, Sep 23, 2021 at 4:19 AM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> Hmm, empirically we've been able to handle larger input BAM than 20GB; >> with less RAM you had.; >>; >> Another known issue (which we will fix in the next release) is that if; >> auxiliary fields are being read in, and if the BAM has many of them, it; >> could unnecessarily increase the RAM usage. However, in this case given; >> that you're using WGS model (which by default isn't parsing aux fields),; >> that shouldn't be an issue...; >>; >> @kirti141 <https://github.com/kirti141> Another question for you - is; >> there a BAM file that you can make public (with no sensitive information,; >> of course) that we can attempt to reproduce this issue? Thanks again for; >> reporting this.; >>; >> â€”; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/482#issuecomment-925383292>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/ANQXTK5JEYIGIH5HDIV742LUDJMPBANCNFSM5DR4DILA>; >> .; >> Triage notifications on the go with GitHub Mobile for iOS; >> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; >> or Android; >> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925387861:126,test,test,126,,https://github.com/google/deepvariant/issues/482#issuecomment-925387861,1,['test'],['test']
Testability,"I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing; BIN_VERSION=""1.5.0""; ```dockerfile; docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=test_data/Aligned.sortedByCoord.out.bam \; --output_vcf=output/output.vcf.gz \; --num_shards=30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --logging_dir=output/logs \; --intermediate_results_dir output/intermediate_results_dir; ``` ; Please let me know if any error in the command i ran",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/775:571,log,logs,571,,https://github.com/google/deepvariant/issues/775,1,['log'],['logs']
Testability,"I have the problem that I can't enable the GPU when I run the Docker. I am using a NVIDIA P100. nvidia-docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all -it ... This is how I call call_variant ; `(time /opt/deepvariant/bin/call_variants --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ""${EXAMPLES}"" --checkpoint ""${MODEL}"" --execution_hardware accelerator) >""${LOG_DIR}/call_variants.log"" 2>&1`. The output from the call_variant log: . > 2018-06-23 22:47:42.743518: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; WARNING: Logging before flag parsing goes to stderr.; I0623 22:47:43.324297 140315677046528 call_variants.py:329] Initializing model from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; INFO:tensorflow:Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; I0623 22:47:44.415543 140315677046528 tf_logging.py:82] Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 379, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 335, in call_variants; 'execution_hardware is set to accelerator, but no accelerator '; __main__.ExecutionHardwareError: execution_hardware is set to accelerator, but no accelerator was found; real	0m6.241s; user	0m6.872s; sys	0m2.256s. When I run the docker and check for the GPU with nvidia-smi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/81:391,log,log,391,,https://github.com/google/deepvariant/issues/81,3,"['Log', 'log']","['Logging', 'log']"
Testability,"I have tried running deepvariant (v1.6.0) using the rnaseq models using the example data with the google docker container:; `run_deepvariant --num_shards=32 --model_type=WES --customized_model=/opt/models/rnaseq/model.ckpt --regions=test.bed --ref=genome.fa --reads=hg005_gm26107.mrna.grch38.bam --output_vcf=deepvariantrna.vcf --logging_dir=logs --make_examples_extra_args=""split_skip_reads=true,channels=''""`. But it seems to get stuck at the call_variant step. **Any additional context:**. <img width=""1304"" alt=""image"" src=""https://github.com/google/deepvariant/assets/7647927/0c01d955-c1fa-4ede-9579-3f942c262c3a"">. test.bed; chr1 13670 13966; chr1 14409 14501; chr1 15005 15038; chr1 15796 15947; chr1 16607 16765; chr1 16858 17055; chr1 17233 17368; chr1 17369 17436; chr1 17606 17742; chr1 17915 18061; chr1 18268 18366; chr1 24738 24891; chr1 29534 30039; chr1 30267 30667; chr1 30976 31109; chr1 34554 35174; chr1 35245 35481; chr1 35721 36081; chr1 52473 53312; chr1 57598 57653; chr1 58700 58856; chr1 62916 64116; chr1 65419 65433; chr1 65520 65573; chr1 69037 71585; chr1 89295 91629; chr1 92091 92240; chr1 110953 111357; chr1 112700 112804; chr1 120721 120932; chr1 129055 129223; chr1 131025 134836; chr1 135141 135895; chr1 137682 137965; chr1 139790 139847; chr1 140075 140339; chr1 141474 143011; chr1 146386 149707; chr1 155767 155831; chr1 157784 157887; chr1 160446 160690; chr1 161314 161525; chr1 164263 164791; chr1 165491 165942; chr1 167129 168165; chr1 168610 168767; chr1 169049 169264; chr1 172557 172688; chr1 173753 173862; chr1 182696 182746; chr1 183132 183216; chr1 183494 183571; chr1 183740 183901; chr1 183981 184174; chr1 185217 185350; chr1 185491 185559; chr1 186317 186469; chr1 187129 187287; chr1 187376 187577; chr1 187755 187890; chr1 187891 187958; chr1 188130 188266; chr1 188439 188584; chr1 188791 188902; chr1 195263 195411; chr1 257864 259025; chr1 261550 261634; chr1 263015 268655; chr1 268667 268816; chr1 289266 289370; chr1 297345 297502; chr1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/766:233,test,test,233,,https://github.com/google/deepvariant/issues/766,3,"['log', 'test']","['logs', 'test']"
Testability,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:57,test,test,57,,https://github.com/google/deepvariant/issues/222,4,['test'],"['test', 'testdata']"
Testability,"I just started using DeepVariant 0.7.0 and I have gotten it to complete on a few exome runs. Out of the 4 exome runs I tested, I used 64 shards for the make_examples step. For 3 of the exomes, the make_examples step seemed to take about 10-15 minutes per shard. For a 4th exome, the make_examples step for one of the shards was taking much longer than 10-15 minutes; after 14 hours it was still running and I manually killed it. Now I have moved onto a whole-genome sequencing run for testing and the same thing is happening; 63/64 shards complete in about 1hour, but a straggling job has been running for 22 hours now.; ; * Have you run into this problem before and do you have suggestions for debugging?; * It occurred to me that there could be some ultra-high coverage region in my BAM file that could be slowing things down. But, I am looking for a way to investigate this. Is there an easy way to determine which region of the genome is in a particular shard?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105:119,test,tested,119,,https://github.com/google/deepvariant/issues/105,2,['test'],"['tested', 'testing']"
Testability,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230:781,test,testing,781,,https://github.com/google/deepvariant/issues/230,2,['test'],"['test', 'testing']"
Testability,"I pull the latest version of deepvariant and failed to call make_examples as follows:. ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:06:44.997696: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:06:44.997911 140034548377344 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:06:45.001199 140034548377344 make_examples.py:1080] Preparing inputs; 2018-12-20 07:06:45.001707: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:06:45.001820 140034548377344 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:06:45.002523 140034548377344 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/home/chungtsai_su/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1000, in processing_regions_from_options; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:544,test,testdata,544,,https://github.com/google/deepvariant/issues/131,2,['test'],['testdata']
Testability,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:371,log,log,371,,https://github.com/google/deepvariant/issues/488,1,['log'],['log']
Testability,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:118,test,test,118,,https://github.com/google/deepvariant/issues/226,5,['test'],"['test', 'testdata']"
Testability,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ilAriAges1.fasta.bgz"" \; --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \; --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \; --norealign_reads \; --vsc_min_fraction_indels ""0.12"" \; --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315#issuecomment-638428497:793,log,logtostderr,793,,https://github.com/google/deepvariant/issues/315#issuecomment-638428497,1,['log'],['logtostderr']
Testability,"I ran the benchmarking using RTGtool for BWA-MEM + deepvariant 1.5.0 with 35x Illumina WGS data of HG002 (used in pFDA challenge) and compared the results with pFDA v2 challenge submission (BSODP here https://doi.org/10.18434/mds2-2336). I observed that the deepvariant 1.5.0 that I run has improved precision but lower recall. . pFDA v2 submission: `Precision: 0.9940, Recall: 0.9982 and F-score:0.9961`; deepvariant v1.5.0: `Precison: 0.9991 , recall: 0.9934, and -score: 0.9962`. I am just wondering if the pFDA submission has any extra filtering used after running deepvariant. Is there any publicly available deepvaraint v1.5.0 vcf for HG002?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/626:10,benchmark,benchmarking,10,,https://github.com/google/deepvariant/issues/626,1,['benchmark'],['benchmarking']
Testability,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:954,test,testdata,954,,https://github.com/google/deepvariant/issues/530,1,['test'],['testdata']
Testability,"I ran:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; This printed False, which confirmed that this setup was indeed not seeing GPU. :(. Let me test on Ubuntu to get another data point.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471378680:164,test,test,164,,https://github.com/google/deepvariant/issues/619#issuecomment-1471378680,2,['test'],['test']
Testability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:943,log,log,943,,https://github.com/google/deepvariant/issues/102,1,['log'],['log']
Testability,"I see. @pgrosu are you saying that if we just grep the output `HG003.output.vcf.gz` file (like I did with the input), we'll see it?; I ended up not having time to test it last night. So I haven't gone through a run to check. @genieusbio if you can confirm that, that will be great! I can also check later. I still need to work on a few other things before I get to this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1719699043:163,test,test,163,,https://github.com/google/deepvariant/issues/708#issuecomment-1719699043,1,['test'],['test']
Testability,I see. Right you mentioned that before.; It could be that the versions are different. Let me see if it's possible for me to get a CentOS7 set up with CUDA version V11.8.89 and then test with that.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471281516:181,test,test,181,,https://github.com/google/deepvariant/issues/619#issuecomment-1471281516,1,['test'],['test']
Testability,"I tested with chr1 of [the WGS script](https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_case_study_docker.sh). See below for details.; With `use_openvino=true`, call_variants runs for ~15m on chr1. Without, it takes about ~21m. See commands and machine details below:; ----. # Commands. Tested on the same machine:. All below were done with command like:; ```; scripts/run_wgs_case_study_docker.sh 2>&1 | tee /tmp/openvino.log; ```; with some code diffs below:. 1. Use your Docker image, use_openvino=true; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2,test,tested,2,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,5,"['Test', 'log', 'test']","['Tested', 'log', 'testda', 'testdata', 'tested']"
Testability,I tried running DeepVariant-0.5.0+cl-183695032 on the quickstart dataset with the following command:; ```; python call_variants.zip --dataset_config_p; btxt a.pbtxt --checkpoint DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard/model.ckpt.index; ```. However it exits with the following error:; ```; KeyError: 'InceptionV3/Logits/Conv2d_1c_1x1/weights; ```,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/51:335,Log,Logits,335,,https://github.com/google/deepvariant/issues/51,1,['Log'],['Logits']
Testability,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:283,test,testing,283,,https://github.com/google/deepvariant/pull/365,1,['test'],['testing']
Testability,"I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:71,benchmark,benchmark,71,,https://github.com/google/deepvariant/issues/641,1,['benchmark'],['benchmark']
Testability,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-704106130:236,test,test,236,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130,1,['test'],['test']
Testability,I was able to run a test pipeline successfully with BED file you provided (`gs://public-debug/exomes.bed`). Could you please run your pipeline with `gs://public-debug/exomes.bed` (not `gs://canis/CNR-data/exomes.bed`) and see if it succeeds. This is what I got in the log. ```; I1112 17:29:38.624562 140623242430208 genomics_reader.py:213] Reading gs://public-debug/exomes.bed with NativeBedReader; I1112 17:30:18.942625 140623242430208 make_examples.py:1086] Writing examples to /mnt/google/.google/output/nmousavi-test/dv-test/2018-11-10/staging/examples/0/examples_output.tfrecord-00000-of-00008.gz. ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437974705:20,test,test,20,,https://github.com/google/deepvariant/issues/116#issuecomment-437974705,4,"['log', 'test']","['log', 'test']"
Testability,"I was running the whole exome example from here:; https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration. ```#!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=PROJECT_ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:944,test,testdata,944,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: Ã¢ENCFF528VXT.bamÃ¢. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - Ã¢ENCFF528VXT.bamÃ¢ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:70,test,test,70,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,7,['test'],"['test', 'testfiles']"
Testability,"I was using OpenVINO from v1.1 when it was still faster, and mainly haven't bothered to remove the flags since at worst it doesn't seem to make it slower. I didn't benchmark in v1.3 when not including the flag, so it may well be the case it isn't as helpful anymore. It would get tricky since users can easily build their own image with `DV_OPENVINO_BUILD=1`, so the flags to run with OpenVINO can't be easily removed. But if the default google/deepvariant image doesn't have OpenVINO support, it would be nice if there was more of a user warning+exit or a warning+ignore the flag and run as if `--use_openvino=False` anyway since it is currently a legitimate flag to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1153777690:164,benchmark,benchmark,164,,https://github.com/google/deepvariant/issues/541#issuecomment-1153777690,1,['benchmark'],['benchmark']
Testability,"I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363221616:394,test,test,394,,https://github.com/google/deepvariant/issues/47#issuecomment-363221616,2,['test'],"['test', 'testdata']"
Testability,"I would like to training a new model with two different dataset. There are about 8,500,00 training examples. But I used the python script shuffle_tfrecords_beam.py to shuffling these examples. I get the error that the code uses too mach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:342,log,log,342,,https://github.com/google/deepvariant/issues/91,1,['log'],['log']
Testability,"I' m using Ubuntu Virtual-Machine mount on Windows 11.; Running ""cat ${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" it displays the fasta file, but after running:; sudo docker run...., it reports -bash: --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory. Why?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1631967811:236,test,testdata,236,,https://github.com/google/deepvariant/issues/675#issuecomment-1631967811,1,['test'],['testdata']
Testability,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/370:464,log,log,464,,https://github.com/google/deepvariant/issues/370,2,['log'],['log']
Testability,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU @ 2.30GHz; Stepping: 0; CPU MHz: 2300.000; BogoMIPS: 4600.00; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 46080K; NUMA node0 CPU(s): 0-15; ```. I then get 16 messages like this:; `; Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461090328:14,test,testing,14,,https://github.com/google/deepvariant/issues/150#issuecomment-461090328,1,['test'],['testing']
Testability,"I'm going to try installing CUDA 11.3 on the CentOS7 machine first, to confirm whether that will address the issue. I followed: https://developer.nvidia.com/cuda-11.3.0-download-archive?target_os=Linux. ```; wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda_11.3.0_465.19.01_linux.run; sudo sh cuda_11.3.0_465.19.01_linux.run; ```. ```; export PATH=/usr/local/cuda-11.3/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Now, with this, I tried:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. which still gave me False! Hmm. I'll search the error on the internet to see if I can find something useful",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648:951,test,test,951,,https://github.com/google/deepvariant/issues/619#issuecomment-1471410648,1,['test'],['test']
Testability,"I'm seeing an OOM in the logs:; ```; OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]; ```; It also shows your training params:; ```; Training Examples: 8264746; Batch Size: 16384; Epochs: 1; Steps per epoch: 504; Steps per tune: 1500000; Num train steps: 504; ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696:25,log,logs,25,,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696,1,['log'],['logs']
Testability,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:115,log,logged,115,,https://github.com/google/deepvariant/issues/194,1,['log'],['logged']
Testability,"I'm unfamiliar with the bazel build environment. After a successful build with tensorflow-gpu, all tests passed, but on attempting to run a binary from bazel-bin I see. 2018-02-05 11:14:37.628020: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library; Traceback (most recent call last):; File ""/home2/bradBuild/deepvariant/bazel-bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 118, in run; argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/flags.py"", line 112, in __call__; return self.__dict__['__wrapped'].__call__(*args, **kwargs); TypeError: __call__() got an unexpected keyword argument 'known_only'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47:99,test,tests,99,,https://github.com/google/deepvariant/issues/47,1,['test'],['tests']
Testability,"I've also tried on Ubuntu 18.04 and getting similar problems. Have you tried these test on systems other than 16.04, or should I really try to downgrade to that exact version? Thanks!. `Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-54-generic x86_64)`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514398912:83,test,test,83,,https://github.com/google/deepvariant/issues/199#issuecomment-514398912,1,['test'],['test']
Testability,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479738587:1077,test,test,1077,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587,3,['test'],"['test', 'testing']"
Testability,"I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/735:580,test,testdata,580,,https://github.com/google/deepvariant/issues/735,1,['test'],['testdata']
Testability,I've got 1.1-gpu working so I don't think it's an issue with my CUDA. Testing 1.2 now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035492077:70,Test,Testing,70,,https://github.com/google/deepvariant/issues/514#issuecomment-1035492077,1,['Test'],['Testing']
Testability,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:475,log,logs,475,,https://github.com/google/deepvariant/issues/292,3,"['log', 'test']","['logs', 'testdata']"
Testability,"I've successfully run deepvariant with test data. But I keep getting the following error when extracting pileup images from my own provided BAM file. What could be the problem, please?. ```; I1003 20:27:32.183320 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1-1000 [1000 bp] [1.62s elapsed]; I1003 20:27:32.185085 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1001-2000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.186733 140083390310144 make_examples.py:825] Found 0 candidates in chr1:2001-3000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.188343 140083390310144 make_examples.py:825] Found 0 candidates in chr1:3001-4000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.189908 140083390310144 make_examples.py:825] Found 0 candidates in chr1:4001-5000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.191494 140083390310144 make_examples.py:825] Found 0 candidates in chr1:5001-6000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.193065 140083390310144 make_examples.py:825] Found 0 candidates in chr1:6001-7000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.194626 140083390310144 make_examples.py:825] Found 0 candidates in chr1:7001-8000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.196187 140083390310144 make_examples.py:825] Found 0 candidates in chr1:8001-9000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.197738 140083390310144 make_examples.py:825] Found 0 candidates in chr1:9001-10000 [1000 bp] [0.00s elapsed]; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1090, in make_examples_runner; c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99:39,test,test,39,,https://github.com/google/deepvariant/issues/99,1,['test'],['test']
Testability,I've tested your solution - it works on Ubuntu 20.04 but doesn't work on Ubuntu 18.04. Still looking for solution,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-942094050:5,test,tested,5,,https://github.com/google/deepvariant/issues/489#issuecomment-942094050,1,['test'],['tested']
Testability,"ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false; ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371096675:2175,test,testfiles,2175,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675,1,['test'],['testfiles']
Testability,"ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. Changing `--shards` to `128` changes the output in the VCF file. Running a diff between the two outputs shows th",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1242,test,testdata,1242,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created?. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:2050,test,testdata,2050,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478393809:974,test,tested,974,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809,1,['test'],['tested']
Testability,"If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1591,log,log,1591,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['log'],['log']
Testability,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728683346:68,benchmark,benchmark,68,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346,3,"['benchmark', 'test']","['benchmark', 'benchmarking', 'test']"
Testability,"Including my `$BIN_VERSION` `INPUT_DIR` and `OUTPUT_DIR` in a run script fixed this and I was able to run the test data on a t2.medium. Thanks! . ```; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". time sudo docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ucsc.hg19.chr20.unittest.fasta \; 	--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=/output/output.vcf.gz \; 	--output_gvcf=/output/output.g.vcf.gz \; 	--intermediate_results_dir /output/intermediate_results_dir \; 	--num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-867736803:110,test,test,110,,https://github.com/google/deepvariant/issues/462#issuecomment-867736803,2,['test'],"['test', 'testdata']"
Testability,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: Ã¢ENCFF528VXT.bamÃ¢; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - Ã¢ENCFF528VXT.bamÃ¢ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:60,test,testing,60,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,9,['test'],"['test', 'testfiles', 'testing']"
Testability,"Is this issue related to TF version?; Any help to fix this issue? Thanks.; ```; (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0""; (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20; INFO: Using cached SIF image; 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555:101,test,tests,101,,https://github.com/google/deepvariant/issues/555,2,['test'],['tests']
Testability,Issue testing custom model,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:6,test,testing,6,,https://github.com/google/deepvariant/issues/797,1,['test'],['testing']
Testability,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13#issuecomment-351172185:839,log,logic,839,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185,1,['log'],['logic']
Testability,"It has nothing to do with docker image. The problem is with input file (with err `Unrecognized SAM header type`). Pasting make_example worker log:. ```; Unrecognized SAM header type, ignoring:; I1108 21:36:46.455516 140363989006080 genomics_reader.py:213] Reading /input-gcsfused-2/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_pV6vDu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3088269832 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""chr1"" is 248956422 bp and IS MISSING, ""chr2"" is 242193529 bp and IS MISSING, ""chr3"" is 198295559 bp and IS MISSING, ""chr4"" is 190214555 bp and IS MISSING, ""chr5"" is 181538259 bp a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437191865:142,log,log,142,,https://github.com/google/deepvariant/issues/116#issuecomment-437191865,1,['log'],['log']
Testability,"It is at the beginning of the log. I copied it from your first post. `; Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3); I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172#issuecomment-484975077:30,log,log,30,,https://github.com/google/deepvariant/issues/172#issuecomment-484975077,1,['log'],['log']
Testability,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17#issuecomment-352085708:595,test,test,595,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708,2,['test'],"['test', 'test-data']"
Testability,"It looks like examples were created for all the samples but call_variants did not run for the child. ; Could you please check the sizes of all files with extension ```tfrecord``` for all samples? To make sure that parents' files are not empty.; Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally.; Also, could you try to run the test on Ubuntu OS?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431#issuecomment-804422288:428,test,test,428,,https://github.com/google/deepvariant/issues/431#issuecomment-804422288,1,['test'],['test']
Testability,"It might work on your test data, but not working on all my samples...I; tried on many samples. Will think of any solution for sharing my .bam file. On Thu, Sep 23, 2021 at 6:06 AM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> hm, this question (data sharing); > turns out to be more complicated than I thought. I'll have to think about; > what's a best way for this.; >; > For now, I can try to run DV 1.2 on this large BAM file:; >; > $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; > 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; >; > on a 64 cores CentOS machine, to see if I can reproduce the issue.; >; > I'll report back after I have a chance to run it.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925428637>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTK3FDM6LAWCEQH7A2MTUDJZCNANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925432605:22,test,test,22,,https://github.com/google/deepvariant/issues/482#issuecomment-925432605,3,['test'],"['test', 'testdata']"
Testability,"Just to ""update"" this one more time (haven't seen any commits referencing this issue):. The problem seems a bit more pervasive than initially thought. I have now completed two benchmarking runs (NA12878) with hap.py - one using Deepvariant (1.1), and one with Strelka (2.9), input BAM file being the same for both:. Strelka - SNP Recall: 0.995 SNP Precision: 0.997 - FN: 88 FP: 56; Deepvariant - SNP Recall: 0.951 SNP Precision: 0.953 - FN: 936 FP: 904. I checked a bunch of these FN/FP calls and found that they are mostly the above described incorrect heterozygous calls. The read length for this data set is 100bp, which is shorter than our own in-house produced data (150bp) where we do not see this issue. Maybe that explains it (more uniq mappings). . Anyway, hopefully this can be fixed soonish.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470#issuecomment-891869362:176,benchmark,benchmarking,176,,https://github.com/google/deepvariant/issues/470#issuecomment-891869362,1,['benchmark'],['benchmarking']
Testability,"K-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both approaches (DV-GLN-OPT and GATK-Joint) in parallel, and then validate both results. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:1783,assert,assertion,1783,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876,1,['assert'],['assertion']
Testability,Logged training loss does not decrease while performance improves,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:0,Log,Logged,0,,https://github.com/google/deepvariant/issues/194,1,['Log'],['Logged']
Testability,"Looking at @amy-houseman 's latest IGV in https://github.com/google/deepvariant/issues/691#issuecomment-1669163372 , it seems like the realigned BAM would see 1 read containing the alt allele C. If this is the case, I don't expect it to even trigger our candidate generation logic. (From this post, it didn't seem like @amy-houseman has specified different vsc_ thresholds). One possibility is that the realignment BAM was generated with different region compared with when the variant calling was run, like @pgrosu mentioned above.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1670038776:275,log,logic,275,,https://github.com/google/deepvariant/issues/691#issuecomment-1670038776,1,['log'],['logic']
Testability,"Looks like the problem is in make_examples. In order to debug the problem we need make_examples logs. You can run the command below. Flag ""logtostderr"" redirect logs to the console. You need to run it inside docker the same way as you ran your original command.; ``` ; /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315#issuecomment-638394420:96,log,logs,96,,https://github.com/google/deepvariant/issues/315#issuecomment-638394420,4,['log'],"['logs', 'logtostderr']"
Testability,"Looks like this is the GLnexus question. Could you please post the question at GLNexus [page ](https://github.com/dnanexus-rnd/GLnexus); Also, from the log output it looks like GLnexus was completed successfully.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815#issuecomment-2096502613:152,log,log,152,,https://github.com/google/deepvariant/issues/815#issuecomment-2096502613,1,['log'],['log']
Testability,"M13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; W1023 11:00:14.858257 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:9; W1023 11:00:14.858535 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:30; W1023 11:00:14.858640 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:63; W1023 11:00:14.858738 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:107; W1023 11:00:14.858865 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:109; I1023 11:00:14.943413 140022713169664 make_examples.py:1363] Task 0: 26 candidates (21 examples) [0.51s elapsed]; I1023 11:00:34.047770 140022713169664 make_examples.py:1363] Task 0: 100 candidates (123 examples) [19.10s elapsed]; I1023 11:00:53.403723 140022713169664 make_examples.py:1363] Task 0: 200 candidates (255 examples) [19.36s elapsed]; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2020-10-23 11:01:17.042772: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 26013000 end: 26014000; I1023 11:00:13.973792 139819486582528 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes . **Any additional context:** . Deepvariant was successful on grch37 and grch38 reference genomes. I assume that the PacBio CCS model was trained on grch37 and grch38 reference genome and might not be applicable to other reference genomes. Can someone recommend a tweak to run deepvariant on CHM13 draft genome?. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:4178,test,test,4178,,https://github.com/google/deepvariant/issues/367,2,['test'],['test']
Testability,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925379521:321,log,log,321,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521,1,['log'],['log']
Testability,"Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1411,log,log,1411,,https://github.com/google/deepvariant/issues/116,1,['log'],['log']
Testability,"Most likely what happens is that your BAM file (or truth file) has contig names that do not match contig names of the reference. Could you list all the arguments you used for running make_examples? Also, could you print the header of 'project-retraining/testdata/aligned_reads.bam'?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-445914153:254,test,testdata,254,,https://github.com/google/deepvariant/issues/128#issuecomment-445914153,1,['test'],['testdata']
Testability,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:365,log,login,365,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579,1,['log'],['login']
Testability,My apologies. Thank you for the comment. I have not yet tested but it's looking like what I was after. I very much appreciate it.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/731#issuecomment-1834714799:56,test,tested,56,,https://github.com/google/deepvariant/issues/731#issuecomment-1834714799,1,['test'],['tested']
Testability,"N=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ```bash; ulimit -u 10000; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output; ```. @Carl-labhub mentioned ""When I run it, Iâ€™m doing so from an interactive session with singularity exec"". I'm a bit confused by this. Maybe you mean `singularity shell`? So I tried:. ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:3112,test,test,3112,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['test'],['test']
Testability,"NOTE: I've checked the `alignments20_sorted.bam` file to see if the QUAL field is still set to * as @pgrosu pointed out here:. > As these are sorted reads, by just looking at the BAM file, it starts with position 60001, as shown here - which why you are getting 0 candidates:; > ; > ```; > 55ad4f97_28026_0 0 chr20 60001 50 618S27M1D8M1I6M1D7M1D5M1I9M1I5M1D20M1D8M1I24M1I35M1I9M1I2M1I1M1I52M1D3M1D15M2D4M2D12M1I21M1I5M1; > D11M1D7M1I18M1I15M1D1M1D43M1I16M1D8M1I21M1D2M1D7M1I10M1I2M1D8M1D5M1D3M1D2M1I13M1D28M1I20M1I4M1I37M1I19M1I21M1D18M1I5M1D16M1I1M1I3M1I29M1I12M1D6M1I2M1I7; > M1D1M1D2M1I4M1D22M1D18M1I4M1D12M1D4M1I1M1I3M2I17M1I1M1I44M1D3M1D2M1D10M1I11M2D1M1D9M1I19M1D2M1I32M1D2M1D8M1D20M1I14M1D6M1D15M1I7M1D3M1D25M1I6M1I8M1D11M; > 1I7M1I11M1I12M1D2M1D3M1I70M1I23M1D3M1I48M1I21M1I46M1D14M1I3M1D10M1I6M1D34M2I8M1I11M1I5M1I10M1D8M1I8M1I14M1D19M1I26M1I6M1I13M1D4M1I2M1I33M1I8M1I7M1I12M1I1M1I4M1I8M1I3M1I1M1I3M2I4M1I14M1I1M1I5M1I1M1I1M1I2M2I2M1I3M1I1M4I3M1I1M1I1M2I3M4I5M1I3M1I1M1I3M3I5M1I6M1I2M1I1M2I1M1I7M1I3M2I3M1I5M2I1M2I4M1I1M1I2M1D4M1I6M1I3M1I2M1I1M4I1M1I1M2I5M1I3M1I3M1I2M1D1M1I2M2I1M1I1M1I4M3I1M2I2M1I6M1I4M3I1M3I1M3I8M2I47M1I11M1I8M1D3M1I1M1I30M1I15M1I6M1I17M1D18M1D4M1I19M1I28M1D37M1I23M1D7M1D21M1D79M1I12M1D1M1D9M1I21M1D44M1D30M1D3M1I13M1I9M1I34M1D10M1I; > ```; > Since no PHRED value is stored with a `*`, as shown here:; > ; > ```; > AGTCTGCTTCATGCCTTTAACT * AS:i:-15583 ; > ```; > This is why the read triggers the assertion failure [here](https://github.com/google/deepvariant/blob/master/deepvariant/allelecounter.cc#L103):; > ; > ```c++; > CHECK_LE(offset + len, read.aligned_quality_size());; > ```; > ; > The preferred response that would be nice, is if the code could just identify which read (QNAME) it is referring to in order to verify this, or just have a flag to ignore reads that one has no QUAL score for.; > ; > Hope it helps,; > ~p. Viewing the file with samtools (and quickly killing the command) showed me that it is indeed still set to *, as you can see below:; ```; user@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458488164:1426,assert,assertion,1426,,https://github.com/google/deepvariant/issues/138#issuecomment-458488164,1,['assert'],['assertion']
Testability,"NP, please share workers log so we can help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437467869:25,log,log,25,,https://github.com/google/deepvariant/issues/118#issuecomment-437467869,1,['log'],['log']
Testability,"NPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:20533,log,logical,20533,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['log'],['logical']
Testability,"No YAML file is needed for running DeepVariant as we migrated to using Genomics Pipeline API v2 since DeepVariant v0.7 . Please see DeepVariant's cloud page for sample scripts. https://cloud.google.com/genomics/docs/tutorials/deepvariant. Note that we keep the cloud page updated with every release, and it is the place to look into in case of major changes to DeepVariant runner. For debugging, we found workers log more helpful in general. In your case, they are under `gs://gbsc-gcp-project-udn-dev-deep-variant/UDN668131_test/deepvariant_staging_folder/logs`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437114999:413,log,log,413,,https://github.com/google/deepvariant/issues/118#issuecomment-437114999,2,['log'],"['log', 'logs']"
Testability,"No problem. I will still want to make sure there isn't a problem on AWS though. Our team develop DeepVariant to run everywhere (with a compatible OS). We mostly test on Ubuntu 16.04 so far because we haven't had much resource to test much more broadly. But a Ubuntu either on GCP or AWS shouldn't be an issue. And if it is, I think our team needs to know why.; Once I have a chance to test and confirm, I'll report back here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480324512:161,test,test,161,,https://github.com/google/deepvariant/issues/167#issuecomment-480324512,3,['test'],['test']
Testability,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896286964:125,test,tested,125,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964,1,['test'],['tested']
Testability,"OK â€“ that proceeded further, I think. Now the error is; ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". â€”; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380197853:143,Log,Logits,143,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853,1,['Log'],['Logits']
Testability,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386513685:431,test,test,431,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685,1,['test'],['test']
Testability,"Oh I thought I was using a consistent model and codebase â€” thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461399048:307,test,test,307,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048,1,['test'],['test']
Testability,"Oh, sorry I couldn't see that you already had one `--regions` in there.; I believe this one below (taken from your attached logs) should have worked, and it works for me locally, so I'll leave this here and ask my colleagues to weigh in.; ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/Homo_sapiens_assembly19.fasta"" --reads ""/input/proper.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""3:178936057-178936106 3:178952054-178952106"" --vsc_min_fraction_snps ""0.004"" --task {}. E0416 22:39:09.366390 140299630700288 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_WCb7xE/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '3:178952054-178952106']"".; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/305#issuecomment-620732836:124,log,logs,124,,https://github.com/google/deepvariant/issues/305#issuecomment-620732836,1,['log'],['logs']
Testability,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-438034821:175,test,testdata,175,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821,2,['test'],['testdata']
Testability,"On Ubuntu 16.04 TLS, I built it with Python 2.7 and tensorflow 1.4.1. It failed all tests that import random (e.g. `deepvariant/deepvariant/core/cloud_utils_test.py`). It turned out that in `random.py`, it imports `math`, and it mistakingly imports the `math.py` in `/deepvariant/deepvariant/core/`, instead of from the standard library. Is there a fix?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32:84,test,tests,84,,https://github.com/google/deepvariant/issues/32,1,['test'],['tests']
Testability,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:271,log,login,271,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113,1,['log'],['login']
Testability,Our build and test was tested on Ubuntu 16.4. I can do a quick test run on Ubuntu 14.4.; Can you remind us what version of `bazel` you're using?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514360900:14,test,test,14,,https://github.com/google/deepvariant/issues/199#issuecomment-514360900,3,['test'],"['test', 'tested']"
Testability,Our experience is the model works well across a variety of species. But we have not tested it on bacteria or other haploid organisms so we'd love to hear about any results you get there.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/3#issuecomment-350808977:84,test,tested,84,,https://github.com/google/deepvariant/issues/3#issuecomment-350808977,1,['test'],['tested']
Testability,"PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info comma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:9775,test,test,9775,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['test']
Testability,Pacbio data test failed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:12,test,test,12,,https://github.com/google/deepvariant/issues/631,1,['test'],['test']
Testability,"Per: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#how-much-gpu-memory-is-needed-for-the-keras-models. 16GB. In our test, we observe the model occupying 16GB GPU memory. I have a GPU NVIDIA RTX A2000 with 6GB memory, is there a way I can use it to do the call variants step with no problems?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/759:130,test,test,130,,https://github.com/google/deepvariant/issues/759,1,['test'],['test']
Testability,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486163437:19,test,testing,19,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437,1,['test'],['testing']
Testability,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430171385:536,test,test,536,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385,4,"['log', 'test']","['logdir', 'logfiles', 'test']"
Testability,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. BjÃ¶rn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385485505:223,test,test,223,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505,1,['test'],['test']
Testability,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1105,sandbox,sandboxed,1105,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002,1,['sandbox'],['sandboxed']
Testability,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385874525:153,test,test,153,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525,2,['test'],"['test', 'tested']"
Testability,Please provide more information. What image did you use? Did you use the runner image or deepvarinat image directly? What command did you use? Did you get an error? Any log to share?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/81#issuecomment-409913088:169,log,log,169,,https://github.com/google/deepvariant/issues/81#issuecomment-409913088,1,['log'],['log']
Testability,"Please share the full runner log, as well as config (YAML) file used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-387572014:29,log,log,29,,https://github.com/google/deepvariant/issues/70#issuecomment-387572014,1,['log'],['log']
Testability,"Processes: |; | GPU GI CI PID Type Process name GPU Memory |; | ID ID Usage |; |=============================================================================|; | No running processes found |; +-----------------------------------------------------------------------------+; ```. Then I install cuda. This was from: https://developer.nvidia.com/cuda-downloads; ```; curl -O https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run ; export TERM=xterm; sudo sh cuda_12.1.0_530.30.02_linux.run; ```. ```; export PATH=/usr/local/cuda-12.1/bin:$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH; sudo ldconfig; ```. ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2023 NVIDIA Corporation; Built on Tue_Feb__7_19:32:13_PST_2023; Cuda compilation tools, release 12.1, V12.1.66; Build cuda_12.1.r12.1/compiler.32415258_0; ```. This is not 11.8, but is a newer version. So let's test with it. Install Singularity:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.5/scripts/install_singularity.sh; sed -i -e 's/apt-get/yum/g' install_singularity.sh; bash -x install_singularity.sh; ```. Check version:; ```; [pichuan@pichuan-gpu2 ~]$ singularity --version; singularity version 3.7.0; ```. The rest is similar to https://github.com/google/deepvariant/issues/514#issuecomment-1035630725 , but with v1.5.0. ```; # Pull the image.; BIN_VERSION=1.5.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553:2578,test,test,2578,,https://github.com/google/deepvariant/issues/619#issuecomment-1471348553,1,['test'],['test']
Testability,"Quartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_sha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2272,log,log,2272,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,1,['log'],['log']
Testability,"Quick answer: Can you try `./build_release_binaries.sh` instead of `./build_and_test.sh` in your steps above?; Then it should work.; The issue you're encountering has nothing to do with Ubuntu 18. ----. More details:. Earlier today, @akolesnikov and I were just wondering why our internal tests didn't capture this.; We have daily tests that run scripts like this:; https://github.com/google/deepvariant/blob/r0.8/scripts/run_wes_case_study_binaries.sh. After checking what that script was doing, I found out that if you run `scripts/run_wes_case_study_binaries.sh`, it'll work on both Ubuntu 16 and 18. We'll fix build_and_test.sh in the next release. Thanks for reporting! ; If using `build_release_binaries.sh` still doesn't work for you, feel free to reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514472365:289,test,tests,289,,https://github.com/google/deepvariant/issues/199#issuecomment-514472365,2,['test'],['tests']
Testability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3840,Log,Logging,3840,,https://github.com/google/deepvariant/issues/60,1,['Log'],['Logging']
Testability,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:946,log,logs,946,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['log'],['logs']
Testability,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364545868:499,log,log,499,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868,1,['log'],['log']
Testability,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-429154858:162,sandbox,sandbox,162,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858,1,['sandbox'],['sandbox']
Testability,"Right, so now update your LD_LIBRARY_PATH with this version closer to the beginning of it. Make sure you echo it first to see what it's set to via `echo $LD_LIBRARY_PATH` as you might want to include those things as well. Study the following two links for more information:. https://www.tecmint.com/understanding-shared-libraries-in-linux/; https://docs.oracle.com/cd/E19455-01/816-0559/chapter2-48927/index.html. LD_LIBRARY_PATH is a string of colon-separated paths that a program will search (from left-to-right) through for the libraries it needs. You don't have to export it if you want to test things like this:. LD_LIBRARY_PATH=....(your paths)... python $HOME/miniconda3/envs/deepVar...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453800484:594,test,test,594,,https://github.com/google/deepvariant/issues/137#issuecomment-453800484,1,['test'],['test']
Testability,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393:69,log,logging,69,,https://github.com/google/deepvariant/pull/393,2,"['log', 'test']","['logging', 'test']"
Testability,Run a interactive mode of docker container (different than deepvariant container) and plan to build the deepvariant from source:. run the build-prereq.sh and run build_and_test.sh. Then how to test deepvariant function?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/448:193,test,test,193,,https://github.com/google/deepvariant/issues/448,1,['test'],['test']
Testability,"Running make_examples locally from the Docker container. It runs without errors on the NA12878_S1.chr20.10_10p1mb.bam dataset, but fails with my BAM file. See the stack trace below. My BAM file is too large to attach here. What do you recommend? Also what are the allowed values for --logging_level? Please advise. ```; # ./opt/deepvariant/bin/make_examples --logging_level DEBUG --mode calling --ref /dv2/reference/CFSAN000189.fasta --reads /dv2/samples/CFSAN000211/reads.sorted.bam --examples output.examples.tfrecord; WARNING: Logging before flag parsing goes to stderr.; I1228 21:10:23.407845 140668049200896 client.py:1004] Timeout attempting to reach GCE metadata service.; W1228 21:10:23.408325 140668049200896 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 969, in main; options = default_options(add_flags=True, flags=FLAGS); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 207, in default_options; sample_name = extract_sample_name_from_reads(flags.reads); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 406, in extract_sample_name_from_reads; raise ValueError('Expected a single sample, found {}'.format(samples)); ValueError: Expected a single sample, found set([]). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:530,Log,Logging,530,,https://github.com/google/deepvariant/issues/28,1,['Log'],['Logging']
Testability,"Running with -v, everything looks normal except 2 error messages:. 1st error message:; ==============; ===> LINKING PACKAGE: conda-forge::linecache2-1.0.0-py_1 <===; prefix=/mnt/home/mansourt/miniconda3/envs/deepVar; source=/mnt/home/mansourt/miniconda3/pkgs/linecache2-1.0.0-py_1. pyc file failed to compile successfully; python_exe_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/python2.7; py_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py; pyc_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.pyc; compile rc: 1; compile stdout: Compiling lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py ...; File ""lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py"", line 102; def keyworded(*arg1, arg2=1):; ^; SyntaxError: invalid syntax. compile stderr:. 2nd error message:; ==============; $ bash -x /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> cwd: /mnt/home/mansourt/miniconda3/envs/deepVar/bin <==; ==> exit code: 1 <==; ==> stdout <==; b'Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/local/lmod/lmod/init/bash)\nShell debugging restarted\n'; ==> stderr <==; b'+ \'[\' -z \'\' \']\'\n+ case ""$-"" in\n+ __lmod_vx=x\n+ \'[\' -n x \']\'\n+ set +x\n+ unset __lmod_vx\n+ set -eu -o pipefail\n+ MODEL_VERSION=0.7.2\n+ GSUTIL=/mnt/home/mansourt/miniconda3/envs/deepVar/bin/gsutil\n+ for MODEL_TYPE in wgs wes\n+ MODEL_NAME=DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ GSREF=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ OUTDIR=/mnt/home/mansourt/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ mkdir -p /mnt/home/mansourt/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-451711664:499,test,tests,499,,https://github.com/google/deepvariant/issues/137#issuecomment-451711664,4,['test'],['tests']
Testability,"R}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:3244,log,log,3244,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['log'],['log']
Testability,"S-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ``",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3933,test,test,3933,,https://github.com/google/deepvariant/issues/429,1,['test'],['test']
Testability,"S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:3920,test,testdata,3920,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"SORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test NO STATUS; //deepvariant:model_train_test NO STATUS; //deepvariant:modeling_test NO STATUS; //deepvariant:pileup_image_test NO STATUS; //deepvariant:postprocess_variants_lib_test NO STATUS; //deepvariant:postprocess_variants_test NO STATUS; //deepvariant:tf_utils_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:3223,test,test,3223,,https://github.com/google/deepvariant/issues/6,1,['test'],['test']
Testability,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1780922055:56,sandbox,sandbox,56,,https://github.com/google/deepvariant/issues/722#issuecomment-1780922055,1,['sandbox'],['sandbox']
Testability,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well.; * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```; sudo docker run \; -v ""/root/quickstart-testdata"":""/input"" \; -v ""/root/quickstart-output"":""/output"" \; google/deepvariant:latest \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \; --examples ""/output/make_examples.tfrecord@1.gz"" \; --gvcf ""/output/gvcf.tfrecord@1.gz"" \; --regions ""chr20:10,000,000-10,010,000""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325#issuecomment-659076993:376,test,testdata,376,,https://github.com/google/deepvariant/issues/325#issuecomment-659076993,1,['test'],['testdata']
Testability,"Sorry about the the confusion of <sample_name>, I edited out the actual name. The name was appeared correctly and It was being generated at the right path too (files were deleted by snakemake due to incomplete workflow). . It seems like both VCF and gVCF were generated successfully from the log but if failed to run vcf_stats_report.py: . ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/project/pi_robertmills_umass_edu/databases/bacteroides_genome/<ref_genome>"" --infile ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output.tfrecord.gz"" --outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --cpus ""6"" --gvcf_outfile ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --nonvariant_site_tfrecord_path ""../../results/deepVariant/KO_PV/<sample_name>/intermediate/gvcf.tfrecord@6.gz"". I0626 19:01:26.925776 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; 2024-06-26 19:01:26.928061: I deepvariant/postprocess_variants.cc:94] Read from: ../../results/deepVariant/KO_PV/<sample_name>/intermediate/call_variants_output-00000-of-00001.tfrecord.gz; 2024-06-26 19:01:26.930065: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 407; I0626 19:01:26.930917 139684790912832 postprocess_variants.py:1313] CVO sorting took 6.503661473592123e-05 minutes; I0626 19:01:26.931080 139684790912832 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0626 19:01:26.931126 139684790912832 postprocess_variants.py:1318] Using 6 CPUs for parallelization of variant transformation.; I0626 19:01:26.954008 139684790912832 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0626 19:01:26.991115 139684790912832 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.00046567519505818686",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2195634219:292,log,log,292,,https://github.com/google/deepvariant/issues/839#issuecomment-2195634219,1,['log'],['log']
Testability,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844185505:177,test,test,177,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505,1,['test'],['test']
Testability,"Sorry to bother, but could you please provide me with the original path for downloading the data in the exome case study? I am interested in exploring some related data. Thank you very much. . Here's the code related to the data in case study:; ```; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/842:318,test,testdata,318,,https://github.com/google/deepvariant/issues/842,1,['test'],['testdata']
Testability,"Sorry, I've been traveling the past week without a solid network connection. I've uploaded my simg to google drive at the address below. This will likely not be a permanent link, but you can use it for testing now. https://drive.google.com/open?id=12NZKJwRTroKofGB6KrZ4JVyN36DRbPuF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-488824724:202,test,testing,202,,https://github.com/google/deepvariant/issues/178#issuecomment-488824724,1,['test'],['testing']
Testability,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-350909874:492,test,test-training-dataset,492,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874,1,['test'],['test-training-dataset']
Testability,"Sort of a follow-up question, on a related application: can DeepVariant take an RNA-seq bam file obtained via GATK's best practices (link [here](https://software.broadinstitute.org/gatk/documentation/article.php?id=3891)), and get a trustworthy output? . A quick check on a specific locus indicates that DeepVariant's WGS applied to RNA-seq retrieves a few more variants than GATK's ""HaplotypeCaller"". Can we trust it? Has anybody done more comprehensive testing of this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-457703903:455,test,testing,455,,https://github.com/google/deepvariant/issues/115#issuecomment-457703903,1,['test'],['testing']
Testability,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774282552:57,test,test,57,,https://github.com/google/deepvariant/issues/419#issuecomment-774282552,1,['test'],['test']
Testability,"Sure! please find attached the logs in the terminal.; In the meantime, I will run the simple case study.; Thank you!; [output.log](https://github.com/google/deepvariant/files/15052710/output.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270:31,log,logs,31,,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270,3,['log'],"['log', 'logs']"
Testability,TEST 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/1#issuecomment-348330972:0,TEST,TEST,0,,https://github.com/google/deepvariant/issues/1#issuecomment-348330972,1,['TEST'],['TEST']
Testability,"TP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265#issuecomment-580549772:1687,Test,Test,1687,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772,1,['Test'],['Test']
Testability,"TTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:4111,test,test,4111,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['test'],['test']
Testability,"TY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3261,test,test,3261,,https://github.com/google/deepvariant/issues/797,1,['test'],['test']
Testability,"T_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances reset --zone us-west1-b pichuan-gpu2; ```. and then ssh back to the machine. ```; BIN_VERSION=1.5.0; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```. Oh wow, that actually worked. --> it returns True. Although, now I'm seeing @pgrosu 's comment above, and started wondering if it could be that restarting means I reset $LD_LIBRARY_PATH (because now it's empty , and the command above still worked). So, my new hypothesis that ""nvidia-modprobe helped"" might not be true. Need to test with clean setup again :). But at least something works now! I just don't exactly know what helped yet :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:2367,test,test,2367,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355,2,['test'],['test']
Testability,Test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426#issuecomment-782440933:0,Test,Test,0,,https://github.com/google/deepvariant/issues/426#issuecomment-782440933,1,['Test'],['Test']
Testability,"Thank a lot for your answer. Now the are public for real!. Luisa. 2018-03-07 16:53 GMT+01:00 Mark DePristo <notifications@github.com>:. > Thanks Luisa! It seems those s3 files aren't public:; >; > wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 07:28:01-- http://dv-testfiles.s3.; > amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)...; > 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80...; > connected.; > HTTP request sent, awaiting response... 403 Forbidden; > 2018-03-07 07:28:01 ERROR 403: Forbidden.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371184018>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QUPeGVGaxtvJH4LH2Ygb1cSQEtjlks5tcAKUgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371186430:212,test,testfiles,212,,https://github.com/google/deepvariant/issues/52#issuecomment-371186430,6,['test'],['testfiles']
Testability,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480271688:114,test,test,114,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688,1,['test'],['test']
Testability,"Thank you @ASLeonard for sharing your current solution. On my end I'll continue to try to reproduce the issue. If I'm unable to, I might need to ask you to help test out my updated code after I have it. I might reach out again later on this thread. For now, I'll close this issue. Please feel free to update with more information, or submit another issue if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-763821009:161,test,test,161,,https://github.com/google/deepvariant/issues/404#issuecomment-763821009,1,['test'],['test']
Testability,"Thank you @AndrewCarroll and @pichuan for the clarification. The calibration makes sense, and could be intriguing for inspecting DNN-resiliency. Having the same underlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:277,log,logits,277,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040,5,"['log', 'test']","['logits', 'test', 'testing']"
Testability,"Thank you @Zer0day-0 . I went back and looked at my notes on the last time I tried to install with Conda: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. I noticed that you're using CentOS. I think I might be testing with CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. Version:. ```; $ uname -a; Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. Install conda:. ```bash; curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:235,test,testing,235,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,2,['test'],"['test', 'testing']"
Testability,Thank you @carsonhh ! I'll test this and make a change accordingly.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/789#issuecomment-1992745319:27,test,test,27,,https://github.com/google/deepvariant/issues/789#issuecomment-1992745319,1,['test'],['test']
Testability,"Thank you @githubtefo for the log. The last step certainly looks strange to me:. ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/reference/Homo_sapiens.GRCh37.dna.primary_assembly.fa"" --infile ""/tmp/tmpba2iuryg/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""/output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpba2iuryg/gvcf.tfrecord@16.gz"". I0418 11:23:10.756783 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control; 2024-04-18 11:23:10.761841: I deepvariant/postprocess_variants.cc:94] Read from: /tmp/tmpba2iuryg/call_variants_output-00000-of-00001.tfrecord.gz; 2024-04-18 11:23:39.285004: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 8753635; I0418 11:24:21.877126 140607260936000 postprocess_variants.py:1313] CVO sorting took 1.1852648933728536 minutes; I0418 11:24:21.877437 140607260936000 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0418 11:24:21.877472 140607260936000 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0418 11:24:35.259627 140607260936000 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: 15337_Control. real	2m10.376s; user	1m41.800s; sys	0m24.640s; ```. I would expect `postprocess_variants` to be doing more. I believe @lucasbrambrink is looking into another issue in `postprocess_variants` related to multiprocessing (https://github.com/google/deepvariant/issues/804), but I'm not sure if this relevant. For now, can you try disable multiprocessing in the `postprocess_variants` step by setting `--cpus 0`. If you're using the run_deepvariant one-step script, you can add:. ```bash; --postprocess_variants_extra_args=""cpus=0""; ```. and see if that works for you. Please let us know. We'll try to make this more robust in the future!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2068144877:30,log,log,30,,https://github.com/google/deepvariant/issues/810#issuecomment-2068144877,1,['log'],['log']
Testability,"Thank you Maria -- that's definitely a more elegant starting point. Hi Fra,. The reason why you are seeing that is because there are additional characters after some of the backslash characters (`\`) at the end of each line. So you need to make sure there are no spaces or characters after each backslash (i.e. it needs to be the last character on those lines) like this:. ```; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; ```. The strings that starts with ` **Replace this string...`, ` **Optional.` or ` **How many cores...` need to be removed, as they are there in the documentation only to provide additional clarity regarding those parameters. Otherwise Bash interprets the next line as a separate new command to run, like this:. ```; paul$ --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; -bash: --ref=/mountpoint/fastQ/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; paul$; ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1632163617:811,test,testdata,811,,https://github.com/google/deepvariant/issues/675#issuecomment-1632163617,2,['test'],['testdata']
Testability,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:163,test,test,163,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,2,['test'],"['test', 'tested']"
Testability,"Thank you for help, with your tweak build and tests passed.; But I get different results on test data - described [here](https://github.com/google/deepvariant/issues/239#issuecomment-558061938).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-558062729:46,test,tests,46,,https://github.com/google/deepvariant/issues/236#issuecomment-558062729,2,['test'],"['test', 'tests']"
Testability,"Thank you for looking into this. I'm running DeepVariant on a docker image. Below is my command line. My reference genome, reads, and truth VCF are 300Mb, so I will send via email if that is okay.; ```; ref=H37Rv.fa; BAM=GCA_000193185.1_1_1.bowtie2.rmdup.bam; TRUTH_VCF=test.vcf.gz; base=${BAM%.rmdup.bam}. /opt/deepvariant/bin/make_examples --mode training --ref ${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions confidence.bed . ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71#issuecomment-387870861:270,test,test,270,,https://github.com/google/deepvariant/issues/71#issuecomment-387870861,1,['test'],['test']
Testability,Thank you for providing the run-prereq.sh information. I found a 2.1.0 version of intervaltree in ${HOME}/.local/lib/python2.7/site-packages that was overriding the site-packages directory within the container. The test ran successfully once I eliminated the conflict.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255#issuecomment-570610250:215,test,test,215,,https://github.com/google/deepvariant/issues/255#issuecomment-570610250,1,['test'],['test']
Testability,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-489377484:317,test,test,317,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484,1,['test'],['test']
Testability,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty); [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log); ; I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674:154,test,tested,154,,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674,4,"['log', 'test']","['log', 'tested']"
Testability,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311#issuecomment-636996628:171,test,testing,171,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628,3,"['Test', 'test']","['Testing', 'testing', 'tests']"
Testability,"Thank you for the reply. I have included the "" "" in the code however it appears missing at runtime.It worked when I replace that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72#issuecomment-413179094:471,log,log,471,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094,1,['log'],['log']
Testability,"Thank you for the response ; I reordered the conda channels to match yours and installed gsutil but using conda and thus I did not need to edit the path. This is the set of commands I used:. ```; conda create -n deepvariant python=2.7; source activate deepvariant; conda install -c conda-forge google-cloud-sdk; conda install -v -y deepvariant &> deepvariant_insatll.log; ```. I got a successful installation inspite of the first error message just like you; However, running the code is producing another error:. ```; python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4i44qy/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_intern",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:367,log,log,367,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,1,['log'],['log']
Testability,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-603439134:192,test,testing,192,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134,1,['test'],['testing']
Testability,"Thank you for your reply!; 1ã€My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2ã€I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1267,sandbox,sandbox,1267,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260,1,['sandbox'],['sandbox']
Testability,"Thank you so much.; In my current understanding, if I use the pre-trained parameters for DeepVariant in Keras or other deep learning frameworks, building the equivalent model in each library is required.; Anyway, I'll try which approach is the easiest for my aim. Please let another confirmation.; For saving the checkpoints ""DeepVariant-inception_v3-0.7.0+data-wgs_standard"", did you used the function in this?:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/deepvariant/testing/tf_test_utils.py#L48. Best.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-446044386:510,test,testing,510,,https://github.com/google/deepvariant/issues/127#issuecomment-446044386,1,['test'],['testing']
Testability,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:546,sandbox,sandbox,546,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620,1,['sandbox'],['sandbox']
Testability,"Thank you very much, that finally resolved the issue. What happens now is that after performing the evaluation (the training, test, and validation sets are the sequencing of the NA12878 sample from the Coriell Institute sequenced three times), Iâ€™m getting low recall and precision values. For indels, recall is 0.41 and precision is 0.24, and for SNPs, recall is 0.57 and precision is 0.72. I tried the model you provided for exome sequencing, but I didnâ€™t achieve better results (which is why I decided to create my own model). However, with typical tools (like GATK HaplotypeCaller), I get much better results (indels with 0.8 recall and 0.62 precision, and SNPs with 0.89 recall and 0.97 precision). Do you have any idea why this might be happening and any advice on how to solve it? I really believe that using a variant caller trained with my data should yield better results than GATK HaplotypeCaller",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827:126,test,test,126,,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827,1,['test'],['test']
Testability,"Thank you! Although while testing I will continue to use Docker, changing the code might become important to me ðŸ‘",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/42#issuecomment-360789056:26,test,testing,26,,https://github.com/google/deepvariant/issues/42#issuecomment-360789056,1,['test'],['testing']
Testability,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877:203,test,test,203,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877,2,"['log', 'test']","['log', 'test']"
Testability,Thank you! I'll test that out.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71#issuecomment-387952593:16,test,test,16,,https://github.com/google/deepvariant/issues/71#issuecomment-387952593,1,['test'],['test']
Testability,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728645413:169,test,test,169,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413,3,"['benchmark', 'test']","['benchmark', 'test']"
Testability,Thank you. Do you have a timeline on when the benchmarks on GLnexus to combine gVCFs would be completed ?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-461139788:46,benchmark,benchmarks,46,,https://github.com/google/deepvariant/issues/142#issuecomment-461139788,1,['benchmark'],['benchmarks']
Testability,"Thank you. I want to use a PacBio model trained on v3.3.2 benchmark variants. I assumed that the following model: deepvariant/models/DeepVariant/0.9.0/DeepVariant-inception_v3-0.9.0+data-pacbio_standard from DeepVariant bucket released in 2019 is trained on v3.3.2. However, when I run this model in call_variants using examples created by make_examples from DeepVariant v1.0.0, I get the following error:. `ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 6 channels while the examples have 8.`. Does this mean this model is not compatible with DeepVariant v1.0.0? If I recreate examples with 6 channels using DeepVariant v0.9, will that have a significant drop in accuracy? Also, is there a trained model on v3.3.2 of benchmark variants that is compatible with 8 channels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-727285166:58,benchmark,benchmark,58,,https://github.com/google/deepvariant/issues/381#issuecomment-727285166,2,['benchmark'],['benchmark']
Testability,Thanks @AndrewCarroll! Happy to test beta version when you produce them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1086175908:32,test,test,32,,https://github.com/google/deepvariant/issues/518#issuecomment-1086175908,1,['test'],['test']
Testability,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866205653:433,test,test,433,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653,1,['test'],['test']
Testability,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-964833815:203,log,logging,203,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815,1,['log'],['logging']
Testability,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:73,log,log,73,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705,2,['log'],['log']
Testability,"Thanks @akolesnikov for responding! . I'm running Python `Python 2.7.15+`. This was on a fresh Ubuntu 14.4 install, using your default settings in DeepVariant v0.8/master and running `./build-prereq.sh` followed by `./build_and_test.sh`. What I'm confused by is that `bazel-bin/deepvariant/make_examples_test` runs and everything passes... ```; $ bazel-bin/deepvariant/make_examples_test; Running tests under Python 2.7.15: /usr/bin/python; ...; .; ----------------------------------------------------------------------; Ran 101 tests in 6.501s. OK; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514034399:397,test,tests,397,,https://github.com/google/deepvariant/issues/199#issuecomment-514034399,2,['test'],['tests']
Testability,"Thanks @chrisfleisch for following up on this issue.; If I understand correctly, you're also talking specifically about the `call_variants` step, not the `make_examples` step, right?. I can try to see if I can get a AMD machine to test it out. I actually have not made any progress on this issue yet.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-597978325:231,test,test,231,,https://github.com/google/deepvariant/issues/274#issuecomment-597978325,1,['test'],['test']
Testability,"Thanks @dkurt ! ; I'll take a look next week and make sure I can incorporate the changes internally.; Can you confirm again that you're ok with us doing that? (I'll add a pointer to this PR when I make the internal change, so that the commit log will have the information. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442#issuecomment-821760836:242,log,log,242,,https://github.com/google/deepvariant/pull/442#issuecomment-821760836,1,['log'],['log']
Testability,"Thanks @edg1983 ; Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**; 1. make_examples runtime: 290/198 = about 1.5 times; 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries; 3. #entries in gVCF: 531371190/213244705 = about 2.5 times; 4. call_variants runtime: 1494/456 = about 3.3 times; 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:; (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs.; (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me.; (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-700190764:90,TEST,TEST,90,,https://github.com/google/deepvariant/issues/346#issuecomment-700190764,2,"['TEST', 'benchmark']","['TEST', 'benchmarking']"
Testability,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30#issuecomment-355031587:320,test,tested,320,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587,1,['test'],['tested']
Testability,"Thanks @pgrosu !. Follow up with you @ruolin , ; I started a machine with:; `gcloud compute instances create --project OUR_PROJECT --zone us-west1-b --image-project ubuntu-os-cloud --image-family ubuntu-1804-lts --machine-type custom-64-131072 --min-cpu-platform ""Intel Skylake"" --boot-disk-size 300G pichuan-test-20210425`. which is a 64 core, 128 GB RAM machine, and the command finished running without any issue.; The command I ran was:. ```; sudo docker run \; -v /home/pichuan/pacbio-case-study/input/data:/input \; -v /home/pichuan/pacbio-case-study/output:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/Homo_sapiens_assembly19.fasta \; --reads=/input/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam \; --output_vcf=/output/deepvariant.output.vcf.gz \; --output_gvcf=/output/deepvariant.output.g.vcf.gz \; --num_shards=64 \; --logging_dir=/output/logs \; --runtime_report --use_hp_information=true; ```. I can try another run with 20 cores, and I can also try with smaller disks to see if that would produce a similar issue. @ruolin let us know if you have any other ideas for us to reproduce this issue. I haven't tried Terra, but there's someone I can contact there for DeepVariant setup, feel free to let me know too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446#issuecomment-826959857:309,test,test-,309,,https://github.com/google/deepvariant/issues/446#issuecomment-826959857,2,"['log', 'test']","['logs', 'test-']"
Testability,"Thanks @pgrosu for helping out!. One thing to note is that DeepVariant isn't tested on Mac, and it's not currently something that we officially support. But good to know that there seems to be workarounds. I'll keep this open for a bit longer in case @heznanda wants to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1622159614:77,test,tested,77,,https://github.com/google/deepvariant/issues/657#issuecomment-1622159614,1,['test'],['tested']
Testability,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:; ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; 21; ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; <MagicMock name='mock.contig().n_bases' id='140373647899088'>; >>> int(res); 1; ```. And I accidentally run the above command with Python3, and it throw the following error:. ```; >>> res = min(end + bufsize, contig_nbp); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; TypeError: '<' not supported between instances of 'MagicMock' and 'int'; ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464961152:242,mock,mock,242,,https://github.com/google/deepvariant/issues/154#issuecomment-464961152,5,['mock'],['mock']
Testability,"Thanks Luisa! It seems those s3 files aren't public:. wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 07:28:01-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 403 Forbidden; 2018-03-07 07:28:01 ERROR 403: Forbidden.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371184018:69,test,testfiles,69,,https://github.com/google/deepvariant/issues/52#issuecomment-371184018,6,['test'],['testfiles']
Testability,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437503051:362,test,testdata,362,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051,2,"['log', 'test']","['logs', 'testdata']"
Testability,"Thanks Paul for pointing this out,. The reason why I used Parabricks is to add optional parameters and run DeepVariant in one command. However based on what you said, I will run 3 steps (make_examples, call_variants and post_process_variants) separately so I can add optional parameters in `make_examples` step. While doing `call_variants` step, it generates a lot of errors. One of the error points out that `input depth must be evenly divisible by filter depth: 6 vs 7`. What does it mean?. I also attached the error log [error.log](https://github.com/google/deepvariant/files/12731863/error.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1736300897:519,log,log,519,,https://github.com/google/deepvariant/issues/706#issuecomment-1736300897,3,['log'],['log']
Testability,Thanks for bringing this to our attention!; I'm running our tests with `nvidia/cuda:12.1.1-cudnn8-devel-ubuntu20.04` now and will update here when I have confirmed whether it's working.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506:60,test,tests,60,,https://github.com/google/deepvariant/issues/676#issuecomment-1631277506,1,['test'],['tests']
Testability,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909:198,test,tests,198,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909,1,['test'],['tests']
Testability,"Thanks for confirmation. I have done additional testings, and the conclusion is that the underlying htslib used in DeepVariant is the culprit. I have created a issue for my self to fix it (#119). Meanwhile, if it's possible please put your BED file into a public bucket and rerun DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-438403390:48,test,testings,48,,https://github.com/google/deepvariant/issues/116#issuecomment-438403390,1,['test'],['testings']
Testability,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549166457:51,log,log,51,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457,1,['log'],['log']
Testability,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330:104,test,testing,104,,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330,1,['test'],['testing']
Testability,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-462075143:319,test,tests,319,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143,1,['test'],['tests']
Testability,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood itâ€™s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if theyâ€™re in the working directory? Thereâ€™s a lot of regex going on Iâ€™d love to see documented (I guess maybe itâ€™s in the codebase but if it could be explicitly written on the documentation thatâ€™d be awesome) â€” I understand that there is this cloud runner but considering these other components are exposed I feel like I donâ€™t have a good sense of the way they should be appropriately called individually. . Iâ€™m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like Iâ€™m almost there â€” if I canâ€™t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461398822:1177,test,test,1177,,https://github.com/google/deepvariant/issues/151#issuecomment-461398822,1,['test'],['test']
Testability,Thanks for the pull request. I've updated this in our codebase and will point to this PR in the commit log.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/472#issuecomment-879985843:103,log,log,103,,https://github.com/google/deepvariant/pull/472#issuecomment-879985843,1,['log'],['log']
Testability,"Thanks for the quick reply, @pichuan . First of all, I followed the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md, but it gives the same error. 1. I got the image from ""docker://google/deepvariant:1.6.0"" I tried both: locally download the image and use the image without downloading. But they gave the same error.; 2. Follow the instructions in the link above to run the program. Here is the script that I used ; ```; #!/bin/bash. BIN_VERSION=""1.6.0"". INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746#issuecomment-1846470975:531,test,testdata,531,,https://github.com/google/deepvariant/issues/746#issuecomment-1846470975,2,['test'],['testdata']
Testability,"Thanks for the quick reply:; For me, make_examples.tfrecord@8.gz file is not generated and the following is the `make_examples.log` . ```{bash}; I0330 15:47:21.754682 140654028756736 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756700 140654028756736 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435#issuecomment-810383024:127,log,log,127,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024,1,['log'],['log']
Testability,"Thanks for the quick response. This is the command and log file.; [stderr.log](https://github.com/google/deepvariant/files/14670380/stderr.log); ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=~{model} \ ; --ref= ~{ref_fasta} \; --reads= ~{align_bam} \; --make_examples_extra_args=~{MAKE_EXAMPLE_ARGS} \; --output_vcf= ~{sampleID}.vcf.gz \; --output_gvcf= ~{output_file_name} \; --num_shards= ~{cpu_per_task} \; --haploid_contigs=""chrX,chrY"" \ ; --par_regions_bed= ~{par_bed} \ ; --postprocess_variants_extra_args=~{POSTPROCESS_VARIANTS_ARGS} \; --regions= ~{regions}```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783#issuecomment-2010160828:55,log,log,55,,https://github.com/google/deepvariant/issues/783#issuecomment-2010160828,3,['log'],['log']
Testability,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742700347:688,test,testing,688,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347,1,['test'],['testing']
Testability,"Thanks for the updates @fo40225 . We've also been communicating through emails. To give a quick update here: after a few internal testing, it seems like the current implementation of the async writer is missing records. We're still looking through this with @fo40225 . Hopefully it'll get resolved eventually. When we can confirm the results are consistent as before, we will be able to add this to our codebase.; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-477842467:130,test,testing,130,,https://github.com/google/deepvariant/pull/152#issuecomment-477842467,1,['test'],['testing']
Testability,"Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data.; I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data.; Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/617:151,test,testing,151,,https://github.com/google/deepvariant/issues/617,1,['test'],['testing']
Testability,Thanks for your reminder @scott7z ; I ignored what you mentioned.; But for now I do not plan to make any further changes.; I've converted to a platform that supports AVX2 and completed a 'quick-start' test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-352945209:201,test,test,201,,https://github.com/google/deepvariant/issues/16#issuecomment-352945209,1,['test'],['test']
Testability,Thanks for your reply and i will try DeepVariant v1.5.0 to benchmark the HG002 Revio data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641#issuecomment-1535591093:59,benchmark,benchmark,59,,https://github.com/google/deepvariant/issues/641#issuecomment-1535591093,1,['benchmark'],['benchmark']
Testability,"Thanks for your reply, but it didn't work. If I type --regions ""3:178936057-178936106"" --regions ""3:178952054-178952106"", then the deepvariant only accepts the second region as an input. I have also tried several ways, such as --regions=""3:178936057-178936106"" ""3:178952054-178952106"" , --regions=""3:178936057-178936106 3:178952054-178952106"", --regions=3:178936057-178936106 3:178952054-178952106, --regions=3:178936057-178936106 178952054-178952106, --regions ""3:178936057-178936106 178952054-178952106"", --regions ""3:178936057-178936106,178952054-178952106"", and none of them works. The log is attached here. Thanks a lot!; [problemetic logs with 'regions'.txt](https://github.com/google/deepvariant/files/4546376/problemetic.logs.with.regions.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/305#issuecomment-620645675:590,log,log,590,,https://github.com/google/deepvariant/issues/305#issuecomment-620645675,3,['log'],"['log', 'logs']"
Testability,"Thanks nmousavi. I have the yaml file only for v0.6.1 and not for v_0.7.0. However, thanks for pointing toward workers log folder. This gives a better idea in debugging. Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437461639:119,log,log,119,,https://github.com/google/deepvariant/issues/118#issuecomment-437461639,1,['log'],['log']
Testability,"Thanks paul for the detailed analysis and explanation of how you went about it. To confirm this analysis I tried out by changing TF_WHL_VERSION and other related symbols to 1.10.1 in settings.sh so that build-prereq.sh installs the matching tensorflow. Now, the build and test passes and I am able to run variant calling. I presume this is a good workaround to address this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423268455:272,test,test,272,,https://github.com/google/deepvariant/issues/94#issuecomment-423268455,1,['test'],['test']
Testability,"Thanks!. ×‘×ª××¨×™×š ×™×•× ×‘×³, 21 ×‘× ×•×‘×³ 2022, 23:50, ×ž××ª Pi-Chuan Chang â€<; ***@***.***>:. > Before you proceed, if you can't use Docker because of root permission, I; > recommend that you try Singularity:; > https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity; >; > If you don't have root permission, you won't be able to install necessary; > things before running the binaries either.; > ------------------------------; >; > Here is what I did:; >; > Get a machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:578,test,test,578,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['test'],['test']
Testability,"Thanks, @pichuan. Always happy to contribute to the open source community.; I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a; [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log); lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space.; Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2085656965:240,log,log,240,,https://github.com/google/deepvariant/issues/810#issuecomment-2085656965,3,['log'],['log']
Testability,"Thanks, just to followup: my `call_variants` step eventually finished after 1d2h31m. The last lines in my call_variants.log file read:. <pre>; 2018-10-17T14:30:33.396510159Z statfs 424901734400 available 4378992640 used 429280727040 total -- interval 10.0000 seconds 0 used; 2018-10-17T14:30:33.786897949Z I1017 14:30:33.786412 140161207068416 call_variants.py:359] Processed 10551585 examples in 329738 batches [0.904 sec per 100]; 2018-10-17T14:30:34.069377504Z I1017 14:30:34.068934 140161207068416 call_variants.py:359] Processed 10551617 examples in 329739 batches [0.904 sec per 100]; 2018-10-17T14:30:34.164880374Z I1017 14:30:34.164383 140161207068416 call_variants.py:359] Processed 10551649 examples in 329740 batches [0.904 sec per 100]; 2018-10-17T14:30:34.166325693Z I1017 14:30:34.166042 140161207068416 call_variants.py:361] Done evaluating variants; </pre>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430723173:120,log,log,120,,https://github.com/google/deepvariant/issues/105#issuecomment-430723173,1,['log'],['log']
Testability,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1733,test,test,1733,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377,1,['test'],['test']
Testability,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423408772:63,log,logs,63,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772,1,['log'],['logs']
Testability,"The 12.1.1 base image did NOT work. I tested `nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04` instead, which did work correctly. . # Please use `nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/676#issuecomment-1634645510:38,test,tested,38,,https://github.com/google/deepvariant/issues/676#issuecomment-1634645510,1,['test'],['tested']
Testability,"The Bam file was downloaded from. ```; https://www.encodeproject.org/files/ENCFF528VXT/@@download/ENCFF528VXT.bam; ```. Then, since it was missing the @RG line, I added it manually just to test using picard:; ```; java -jar /picard.jar AddOrReplaceReadGroups I=ENCFF528VXT.bam O=ENCFF528VXT.bam RGID=4 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=20; ```; The output of runninng; ```; samtools view -H ENCFF528VXT.bam; ```; is the following :; ```; @HD VN:1.5 SO:coordinate; @SQ SN:chr1 LN:249250621; @SQ SN:chr2 LN:243199373; @SQ SN:chr3 LN:198022430; @SQ SN:chr4 LN:191154276; @SQ SN:chr5 LN:180915260; @SQ SN:chr6 LN:171115067; @SQ SN:chr7 LN:159138663; @SQ SN:chr8 LN:146364022; @SQ SN:chr9 LN:141213431; @SQ SN:chr10 LN:135534747; @SQ SN:chr11 LN:135006516; @SQ SN:chr12 LN:133851895; @SQ SN:chr13 LN:115169878; @SQ SN:chr14 LN:107349540; @SQ SN:chr15 LN:102531392; @SQ SN:chr16 LN:90354753; @SQ SN:chr17 LN:81195210; @SQ SN:chr18 LN:78077248; @SQ SN:chr19 LN:59128983; @SQ SN:chr20 LN:63025520; @SQ SN:chr21 LN:48129895; @SQ SN:chr22 LN:51304566; @SQ SN:chrX LN:155270560; @SQ SN:chrY LN:59373566; @SQ SN:chrM LN:16571; @RG ID:4 LB:lib1 PL:illumina SM:20 PU:unit1; @PG ID:bwa PN:bwa VN:0.7.10-r789 CL:/usr/local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371096675:189,test,test,189,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675,1,['test'],['test']
Testability,"The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") ; -- Configuring done; -- Generating done; -- Build files have been written to: /root/clif/build; ```; which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash; root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build; root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm ha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:2694,Test,Test,2694,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,2,['Test'],['Test']
Testability,"The TF/Cuda/cuDNN versions being used may be incompatible with your GPU. A few questions for you:. * Does the quickstart without GPU run without errors? Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; * What OS are you running on?; * Command to reproduce your issue?; * Are you able to run TF + GPU separately from DeepVariant?; * Do older versions of DeepVariant run without errors? I recommend trying 1.0.0 or 0.10.0. We recommend using the latest version for best results, but this is just for debugging.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-833796921:160,test,test,160,,https://github.com/google/deepvariant/issues/452#issuecomment-833796921,1,['test'],['test']
Testability,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355032456:63,test,test-shan,63,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456,2,"['log', 'test']","['logs', 'test-shan']"
Testability,"The code in [Download binaries, models, and test data](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#download-binaries-models-and-test-data) the of the DV quick start guide ran successfully. However, running [make_examples](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#make_examples) using the quickstart-testdata failed with the following error:; ```; Traceback (most recent call last):; attila-ThinkS:~/tools/deepvariant$ python bin/make_examples.zip \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/make_examples.py"", line 45, in <module>; from deepvariant import variant_caller; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/variant_caller.py"", line 50, in <module>; from deepvariant.python import variant_calling; ImportError: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41:44,test,test,44,,https://github.com/google/deepvariant/issues/41,3,['test'],"['test', 'test-data', 'testdata']"
Testability,"The command I ran was on version 0.10.0; `sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/input/GRCh38_latest_genomic.fna --reads=/input/SOL235_sorted.bam --regions ""NC_000007.14"" --output_vcf=/output/SOL235output.vcf.gz --output_gvcf=/output/SOL235output.g.vcf.gz --num_shards=1 \ `. I am running deep variant on MacOS Catalina Version 10.15.4 (19E287); Attached you'll find the full dump of the terminal from running the above command all the way until it errors out. Thanks for your help!; [DeepVariantDump.log](https://github.com/google/deepvariant/files/4545525/DeepVariantDump.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-620560432:629,log,log,629,,https://github.com/google/deepvariant/issues/304#issuecomment-620560432,2,['log'],['log']
Testability,"The command for installing DeepVariant? . `singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1`. Or the full command that is written to stdout when DeepVariant runs? For this latter case, after installing nucleus the full command for deepvariant is not written to output. The last line of output is `KeyError: 'SerializedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```; for bam in $READS; do; 	echo ""running deepvariant on $bam""; 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES; 	echo ""finished with $bam""; done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam; # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****; # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946:390,test,test,390,,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946,2,"['LOG', 'test']","['LOGDIR', 'test']"
Testability,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777:95,test,test,95,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777,1,['test'],['test']
Testability,"The errors (part of them). + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (00:49:22) INFO: Current date is 2018-01-27; (00:49:22) Loading:; (00:49:22) Loading: 0 packages loaded; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:96:; 1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=fals; e to temporarily disable this check.; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:98:; 1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:100; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:102; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:104; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:106; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:108; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:110; :1: name 're2_test' is not defined (did you mean 'ios_test'?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43:51,test,test,51,,https://github.com/google/deepvariant/issues/43,1,['test'],['test']
Testability,The following are full runner log. ; [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log](https://github.com/google/deepvariant/files/1986206/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stderr.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log](https://github.com/google/deepvariant/files/1986207/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl-stdout.log); [ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log](https://github.com/google/deepvariant/files/1986208/ENHvj4m0LBjriPO-qfLP3oABIOjMyffEGCoPcHJvZHVjdGlvblF1ZXVl.log). Please find the yaml file and the runner bash script here: https://github.com/ding-lab/RegulatoryGermline/tree/master/deepvariant. Thanks.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-387605005:30,log,log,30,,https://github.com/google/deepvariant/issues/70#issuecomment-387605005,7,['log'],['log']
Testability,"The full logs are attached for exit status 16 and 20: ; [Logs_deepvariant.txt](https://github.com/google/deepvariant/files/3801760/Logs_deepvariant.txt). I have found, that the Deepvariant works fine for the same .bam file, but with another target region .bed file (both are attached). Again, the target file that does not work for some samples, works fine for other samples. [Works_fine_Twist_Exome_Target.txt](https://github.com/google/deepvariant/files/3801765/Works_fine_Twist_Exome_Target.txt); [does_not_work_Trusight_one_bed.txt](https://github.com/google/deepvariant/files/3801766/does_not_work_Trusight_one_bed.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-549153164:9,log,logs,9,,https://github.com/google/deepvariant/issues/232#issuecomment-549153164,1,['log'],['logs']
Testability,"The issue stems from a mismatch between the set of channels the model was trained on and the channels in the examples generated during `run_deepvariant`. The important bit in the logs you posted is:; ```; From /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58/example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6].; I0327 22:12:15.248034 139725850806080 dv_utils.py:365] From /local/scratch/haley.arnold/14698718/tmpg5h0cte0/make_examples.tfrecord-00000-of-00001.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; W0327 22:12:15.248203 139725850806080 call_variants.py:541] Input shape [100, 221, 7] and model shape [100, 221, 6] does not match.; W0327 22:12:15.248327 139725850806080 call_variants.py:549] Input channels [1, 2, 3, 4, 5, 6, 19] and model channels [1, 2, 3, 4, 5, 6] do not match.; ```. Your customized model was trained on `[1, 2, 3, 4, 5, 6]` (the `BASE_CHANNELS`) but the examples in `make_examples.tfrecord-00000-of-00001.gz` have an extra channel, 19 (`insert_size`), which gets [added to the WGS model preset](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deepvariant.py#L369). . You can either:; a) include `--channels ""insert_size""` when [generating the training data](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#training-set); b) don't set `--model_type WGS` when you call `run_deepvariant` (which you may not need to do regardless if you provide a `customized_model`). . The choice comes down to if you want to include the channel or not. Experiments have shown it provides a slight accuracy boost for WGS, but its not strictly necessary.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2027543371:179,log,logs,179,,https://github.com/google/deepvariant/issues/797#issuecomment-2027543371,1,['log'],['logs']
Testability,"The parameter (see the flags at the top of make_examples.py) is ""realign_reads"", and the way to set it to false is to pass `--norealign_reads`. As stated in the usage you quoted above, reads longer than 500 bp are never realigned (this was added in v1.0). We still add `--norealign_reads` to PacBio runs, but if you forget to add it, there's likely not a big difference since very few pacbio reads will be below 500 bp. If you want to try a shorter run before the full one to test your setup, you can always run DeepVariant all the way through with a small region, like `--regions ""chr20:10,000,000-10,010,000""`, which should only take a few minutes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/437#issuecomment-816788129:476,test,test,476,,https://github.com/google/deepvariant/issues/437#issuecomment-816788129,1,['test'],['test']
Testability,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416#issuecomment-889821634:223,test,tested,223,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634,1,['test'],['tested']
Testability,"This directory here contains `dockerfile` and other files that need to be `ADD` to the image; ![image](https://user-images.githubusercontent.com/25972546/33970607-df14b3b2-e0ae-11e7-8aea-0509ae0c9c61.png); `docker build -f dockerfile -t deepvariant_1214 .`; `Successfully built b829b45a9401`. This directory contains the deepvariant `models` and `quickstart-testdata` files downloaded from [https://console.cloud.google.com/storage/browser/deepvariant](url); ![image](https://user-images.githubusercontent.com/25972546/33970996-1a4e378a-e0b1-11e7-8852-9b63cb477f47.png); `docker run --name deepvariant_test_1214 -v /***/bioinfo/wangwd/workflow/deepvariant/data:/data -it b829b45a9401 /bin/bash`; Then I `cd /opt/deepvariant/bin/`; ![image](https://user-images.githubusercontent.com/25972546/33971209-4789bc8c-e0b2-11e7-833c-95f440712240.png); `./make_examples \; --mode calling \; --ref /data/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /data/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples /data/quickstart-output/examples.tfrecord.gz`; When I finished executing this command, I did not get any result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-351583972:358,test,testdata,358,,https://github.com/google/deepvariant/issues/16#issuecomment-351583972,3,['test'],['testdata']
Testability,This is a test.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426:10,test,test,10,,https://github.com/google/deepvariant/issues/426,1,['test'],['test']
Testability,"This seems to run fine when using the following script:; ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402#issuecomment-756252754:123,test,testdata,123,,https://github.com/google/deepvariant/issues/402#issuecomment-756252754,1,['test'],['testdata']
Testability,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355522771:156,Test,Tests,156,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771,2,"['Test', 'test']","['Tests', 'test']"
Testability,"Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:1140,log,logging,1140,,https://github.com/google/deepvariant/issues/730,1,['log'],['logging']
Testability,"Update: I've done some investigation, and I think we need to change the logic in make_examples, specifically somewhere around here:. https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/variant_caller.py#L207-L230. I've done a prototype but the behavior isn't quite what I expected yet. I'll continue to work on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2297681816:72,log,logic,72,,https://github.com/google/deepvariant/issues/811#issuecomment-2297681816,1,['log'],['logic']
Testability,"Updating my previous response a bit. Could you share the output of the two echo commands below? I want to make sure there aren't any subtle differences with how quoting is done between both sets of commands. I don't immediately see what else could be causing this issue. Thanks!. ```; BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". echo ""singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1"". echo ""singularity -s exec --cleanenv --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402#issuecomment-757012167:335,test,testdata,335,,https://github.com/google/deepvariant/issues/402#issuecomment-757012167,1,['test'],['testdata']
Testability,Using PAR region flag seems to log NativeBedReader endlessly,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748:31,log,log,31,,https://github.com/google/deepvariant/issues/748,1,['log'],['log']
Testability,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; RogÃ©rio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:2372,test,tested,2372,,https://github.com/google/deepvariant/issues/223,2,['test'],"['testdata', 'tested']"
Testability,"V_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19389,test,test,19389,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['test'],['test']
Testability,Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 h1234567_1 ; libgomp 11.2.0 h1234567_1 ; libstdcxx-ng 11.2.0 h1234567_1 ; libuuid 1.41.5 h5eee18b_0 ; markdown-it-py 3.0.0 pypi_0 pypi; mdurl 0.1.2 pypi_0 pypi; mkl 2023.1.0 h213fc3f_46344 ; mkl-service 2.4.0 py310h5eee18b_1 ; mkl_fft 1.3.8 py310h5eee18b_0 ; mkl_random 1.2.4 py310hdb19cb5_0 ; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.4.0 pypi_0 pypi; mock 5.1.0 pypi_0 pypi; msgpack 1.0.8 pypi_0 pypi; ncurses 6.4 h6a678d5_0 ; nest-asyncio 1.6.0 pypi_0 pypi; numpy 1.24.3 py310h5f9d8c6_1 ; numpy-base 1.24.3 py310hb5e798b_1 ; openssl 3.0.14 h5eee18b_0 ; opt-einsum 3.3.0 pypi_0 pypi; optax 0.2.3 pypi_0 pypi; orbax-checkpoint 0.5.23 pypi_0 pypi; packaging 24.1 pypi_0 pypi; pip 24.0 py310h06a4308_0 ; protobuf 3.13.0 pypi_0 pypi; pygments 2.18.0 pypi_0 pypi; python 3.10.14 h955ad1f_1 ; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h5eee18b_0 ; rich 13.7.1 pypi_0 pypi; scipy 1.14.0 pypi_0 pypi; setuptools 69.5.1 py310h06a4308_0 ; six 1.16.0 pypi_0 pypi; sortedcontainers 2.1.0 pypi_0 pypi; sqlite 3.45.3 h5eee18b_0 ; tbb 2021.8.0 hdb19cb5_0 ; tensorstore 0.1.64 pypi_0 pypi; tf-slim 1.1.0 pypi_0 pypi; tk 8.6.14 h39e8969_0 ; toolz 0.12.1 pypi_0 pypi; typing-extensions 4.12.2 pypi_0 pypi; tzdata 2024a h04d1e81_0 ; wheel 0.43.0 py310h06a4308_0 ; wrapt 1.16.0 pypi_0 pypi; xz 5.4.6 h5eee18b_1 ; zipp 3.19.2 pypi_0 pypi; zlib 1.2.13 h5eee18b_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:12726,mock,mock,12726,,https://github.com/google/deepvariant/issues/859,1,['mock'],['mock']
Testability,"Versions; - singularity version 3.6.4-1.el7; - CentOS Linux release 7.9.2009; - Kernel: Linux 3.10.0-1160.6.1.el7.x86_64. The command I was running has some differences in the singularity setup, as I had to explicitly bind the scratch directory etc on the compute nodes. When running the command that **did** work for you, I get the following error immediately `OSError: [Errno 30] Read-only file system: '/output'`, due to the binding issue.; ; ```; singularity run -B ${INPUT}:/input,${OUTPUT}:/output,${OUTPUT}/intermediate_results_dir:/output/intermediate_results_dir,$TMPDIR:$TMPDIR \; deepvariant_1.1.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""PACBIO"" \; --ref /input/asm.fasta \; --reads /input/hifi.bam \; --output_vcf /output/asm.output.vcf.gz \; --output_gvcf /output/asm.output.g.vcf.gz \; --num_shards ""${THREADS}"" \; --call_variants_extra_args ""use_openvino=true"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. This correctly makes the examples and saves the results to the *intermediate_results_dir*, the error happens at the start of call_variants, when the openvino model wants to write to the read-only container. I tried making a `models/pacbio` file and dowloaded the ckpt files, and then made a bind to `opt/models/pacbio/`, but also same error on the read only system. I've also tried running the `bin/call_variants` command from the login nodes and ran into the same error, which was surprising as I have write permissions to more locations on those nodes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-762181198:1397,log,login,1397,,https://github.com/google/deepvariant/issues/404#issuecomment-762181198,1,['log'],['login']
Testability,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5#issuecomment-350478903:83,Test,Test,83,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903,2,"['Test', 'test']","['Test', 'test']"
Testability,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255#issuecomment-568131537:997,test,testdata,997,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537,2,['test'],['testdata']
Testability,"We created a CPU image on an Amazon instance as described here and the example code was running fine. Then we tried to use the same image on a super computer (where we do not have root privileges), we get this error: . singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ deepvariant.simg /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2019-04-14 11:40:35.568961: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real 0m2.359s; user 0m0.767s; sys 0m0.949s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482956117:737,test,testdata,737,,https://github.com/google/deepvariant/issues/132#issuecomment-482956117,2,['test'],['testdata']
Testability,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.; In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_author_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/125#issuecomment-445377405:278,log,login,278,,https://github.com/google/deepvariant/pull/125#issuecomment-445377405,1,['log'],['login']
Testability,"We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s). If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.; In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_author_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/55#issuecomment-371302921:260,log,login,260,,https://github.com/google/deepvariant/pull/55#issuecomment-371302921,2,['log'],['login']
Testability,"We use the testdata from DeepVariant source (https://github.com/google/deepvariant/tree/r0.6/deepvariant/testdata) .The test_nist.b37_chr20_100kbp_at_10mb.bed is really small, so we just can set one example in that examples.tfrecord.gz. . **test_nist.b37_chr20_100kbp_at_10mb.bed file:**; > chr20	10000846	10002407; chr20	10002520	10004171; chr20	10004273	10004964; chr20	10004994	10006386; chr20	10006409	10007800; chr20	10007824	10008018; chr20	10008043	10008079; chr20	10008100	10008707; chr20	10008808	10008897; chr20	10009002	10009791; chr20	10009933	10010531. We will try to train model with our realistic WES data, and set with at least 10,000 examples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69#issuecomment-386202451:11,test,testdata,11,,https://github.com/google/deepvariant/issues/69#issuecomment-386202451,2,['test'],['testdata']
Testability,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-597715853:14,test,testing,14,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853,1,['test'],['testing']
Testability,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:1390,test,tests,1390,,https://github.com/google/deepvariant/issues/59,1,['test'],['tests']
Testability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:948,test,test,948,,https://github.com/google/deepvariant/issues/552,2,['test'],['test']
Testability,"With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc; [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file.; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc; [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'; ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```; # Copyright 2017 Google LLC.; #; # Redistribution and use in source and binary forms, with or without; # modification, are permitted provided that the following conditions; # are met:; #; # 1. Redistributions of source code must retain the above copyright notice,; # this list of conditions and the following disclaimer.; #; # 2. Redistributions in binary form must reproduce the above copyright; # notice, this list of conditions and the following disclaimer in the; # documentation and/or other materials provided with the distribution.; #; # 3. Neither the name of the copyright holder nor the names of its; # contributors may be used to endorse or promote products derived from this; # software without specific prior written permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:145,test,test,145,,https://github.com/google/deepvariant/issues/145,1,['test'],['test']
Testability,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433250645:163,test,tests,163,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645,3,"['benchmark', 'test']","['benchmarks', 'tests']"
Testability,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725:62,log,log,62,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725,3,"['log', 'test']","['log', 'test']"
Testability,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:; ```; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371288942:183,Log,Logging,183,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942,1,['Log'],['Logging']
Testability,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0""; BASE=""${PWD}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""10consensus.fasta""; BAM=""268_041_m10.sorted.bam""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""M10.output.vcf.gz""; OUTPUT_GVCF=""M10.output.g.vcf.gz""; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-697073304:82,test,test,82,,https://github.com/google/deepvariant/issues/290#issuecomment-697073304,1,['test'],['test']
Testability,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:15,log,log,15,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106,2,"['log', 'sandbox']","['log', 'sandbox']"
Testability,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351271748:134,sandbox,sandboxing,134,,https://github.com/google/deepvariant/issues/12#issuecomment-351271748,1,['sandbox'],['sandboxing']
Testability,"You are right, there is no motivation to move this into a separate thread due current logging is already optimal! Benchmarked master and proposed branches and there is no time difference between them. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393#issuecomment-743129654:86,log,logging,86,,https://github.com/google/deepvariant/pull/393#issuecomment-743129654,2,"['Benchmark', 'log']","['Benchmarked', 'logging']"
Testability,"You are totally right! I should have tested it before creating an issue... I also tested DeepVariant using `cram` and `crai` index files and it worked also. Maybe that information could be explicitly added to the documentation to prevent other (lazy) users to report same ""issues"". Thanks a million!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/481#issuecomment-920082344:37,test,tested,37,,https://github.com/google/deepvariant/issues/481#issuecomment-920082344,2,['test'],['tested']
Testability,"You need to specify all the variable in the same script. For example:. ```; #!/usr/bin/zsh; BIN_VERSION=""0.8.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/195#issuecomment-509390976:142,test,testdata,142,,https://github.com/google/deepvariant/issues/195#issuecomment-509390976,2,['test'],['testdata']
Testability,"[300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./Dockerfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3573,test,testdata,3573,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"[log1.txt](https://github.com/google/deepvariant/files/2393958/log1.txt); Hi Paul,. Attached is the log with --verbose_failures. Yes I am using a node on our HPC cluster. Some more details about my environment:; 1. gcc version 4.8.5; 2. centos 7; 3. protobuf version 3.5.1 installed as module with PATH and LD_LIB_PATH set accordingly.; 4. clif installed under HOME directory and WORKSPACE updated accordingly. Thank you; Ram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422487671:100,log,log,100,,https://github.com/google/deepvariant/issues/94#issuecomment-422487671,1,['log'],['log']
Testability,\; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:1744,log,logs,1744,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,"\; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21:24:43.683619 140528910345984 make_ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:1699,log,log,1699,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:4415,test,test,4415,,https://github.com/google/deepvariant/issues/413,1,['test'],['test']
Testability,"] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:4822,test,testdata,4822,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,"](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""ver",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:2144,test,testdata,2144,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiments for validating previous results -- possibly becoming circular. On another note, I'm sure your already know this regarding IGV colors, you can get complete breakdown of their meaning at the following site: . https://software.broadinstitute.org/software/igv/AlignmentData. Hope it helps,; ~p. #### References. 1. [GenomeScope 2.0 and Smudgeplot for reference-free profiling of polyploid genomes](https://www.nature.com/articles/s41467-020-14998-3); 2. [GenomeScope: fast reference-free genome profiling from short reads](https://academic.oup.com/bioinformatics/article/33/14/2202/3089939?login=false); 3. [Measuring Genome Sizes Using Read-Depth, k-mers, and Flow Cytometry: Methodological Comparisons in Beetles (Coleoptera)](https://academic.oup.com/g3journal/article/10/9/3047/6060154); 4. [Kmer2SNP: reference-free SNP calling from raw reads based on matching](https://www.biorxiv.org/content/10.1101/2020.05.17.100305v1.full)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:3963,log,login,3963,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,1,['log'],['login']
Testability,"_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:2341,test,test,2341,,https://github.com/google/deepvariant/issues/678,2,['test'],['test']
Testability,"_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq 0 $((N_SHARDS-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""hs37d5.fa.gz"" \; --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \; --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:; ```; ls HG002.examples.tfrecord*.gz | wc -l; ```; I see 64 of them here.; A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:3482,log,log,3482,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['log'],['log']
Testability,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:2074,test,test,2074,,https://github.com/google/deepvariant/issues/214,1,['test'],['test']
Testability,_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http:/,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3431,test,test,3431,,https://github.com/google/deepvariant/issues/89,1,['test'],['test']
Testability,"_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. And I did same things as you to check the versions. ```; singularity --version; singularity version 3.8.5-2.el7. uname -a; Linux sumner098 3.10.0-1062.1.2.el7.x86_64 #1 SMP Mon Sep 30 14:19:46 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux. singularity exec deepvariant_1.6.0.sif pip freeze | grep numpy; numpy==1.22.1. singularity shell -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:1.6.0; Singularity> python -c ""import numpy; print(numpy.__version__)""; 1.22.1; Singularity> python3 -c ""import numpy; print(numpy.__version__)""; 1.22.1. singularity exec --bind ${PWD}/quickstart-testdata/,${PWD}/quickstart-output/,/usr/lib/locale/:/usr/lib/locale/ deepvariant_1.6.0.sif python -c ""import numpy; print(numpy.__version__)""; 1.22.1; ```. Thanks a lot!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746#issuecomment-1846470975:2689,test,testdata,2689,,https://github.com/google/deepvariant/issues/746#issuecomment-1846470975,1,['test'],['testdata']
Testability,_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Tas,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10502,test,testdata,10502,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11619,test,testdata,11619,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"_deepvariant/deepvariant/make_examples_core.py"", line 472, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_43f_hbd8/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: OUT_OF_RANGE: EOF; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref GRCh38_no_alt_analysis_set.fasta --reads data/hg005_gm26107.mrna.grch38.bam --examples output/intermediate_results_dir/make_examples.tfrecord@8.gz --channels '' --regions data/chr20_CDS_3x.bed --split_skip_reads --task 3. real	0m9.092s; user	0m3.463s; sys	0m0.757s. I use the case study with all test files(https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) put in it and now I remove the space that you tell me about this , what is the problem ?; @danielecook",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1303761079:3361,test,test,3361,,https://github.com/google/deepvariant/issues/581#issuecomment-1303761079,1,['test'],['test']
Testability,"_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3548,test,test,3548,,https://github.com/google/deepvariant/issues/717,5,['test'],['test']
Testability,"_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y install parallel; sudo apt-get -y install aria2; ## Download models, and test data; # Copy the model files to your local disk.; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:1906,test,testdata,1906,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['test'],['testdata']
Testability,_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_ev,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121355,log,log,121355,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"_future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355522771:1192,log,log,1192,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771,1,['log'],['log']
Testability,"_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2482,test,test,2482,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Hum,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3484,sandbox,sandbox,3484,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['sandbox'],['sandbox']
Testability,"_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]; ...; ```. The strange thing is: ; There seems to be a long lag from the timestamp 04:32 to 09:14, it blocked for almost 5 hours? ; But after that , the ""sec per 100"" log seems MUCH faster than a regular run without OpenVINO. However, because of that strange long lag, the over time runtime seems worse with OpenVINO. Any idea on what's happening here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-712402658:2604,log,log,2604,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658,1,['log'],['log']
Testability,"_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. I cannot figure out the reason why this divided approach succeeded. Could you tell me any suggestions of points to check?; Because I'm using personal data under strict data management, I cannot provide or share our tfrecords. I used apache-beam v2.9.0 in python 2.7.5 (not in docker of DeepVariant v0.7.1).; If some version dependencies exist, I'd like to get some dockerfile like your environment. Best regards,. Masaru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1875,log,log,1875,,https://github.com/google/deepvariant/issues/133,3,['log'],['log']
Testability,"_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; I1218 13:54:44.727533 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:2479,test,testdata,2479,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:3084,test,testdata,3084,,https://github.com/google/deepvariant/issues/471,4,['test'],"['test', 'testdata']"
Testability,_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119255,log,log,119255,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"_run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3168,test,test,3168,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72587,test,testlogs,72587,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_ro,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120409,log,log,120409,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123647,log,log,123647,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"`[bazel release 0.21.0]` -- should I try a more recent version?. I believe this was prescribed by default `settings.sh`.; https://github.com/google/deepvariant/blob/r0.8/settings.sh. Is there a good review stable release from which to try? Sorry. Really at a loss here, as tests pass.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514397821:273,test,tests,273,,https://github.com/google/deepvariant/issues/199#issuecomment-514397821,1,['test'],['tests']
Testability,"``. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2936,log,log,2936,,https://github.com/google/deepvariant/issues/602,2,['log'],"['log', 'logs']"
Testability,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:559,log,logs,559,,https://github.com/google/deepvariant/issues/435,3,['log'],"['log', 'logs']"
Testability,"```; I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found; []; ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900:1103,test,testing,1103,,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900,1,['test'],['testing']
Testability,"```; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output""; ```. `echo $INPUT_DIR`; returns /ybod2/cavery/deepvariant_run/quickstart-testdata. I am executing commands from the ""deepvariant_run"" directory. I made the change you suggested and received a new error:; `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/output"": invalid volume specification: ':/output'.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262#issuecomment-574900293:34,test,testdata,34,,https://github.com/google/deepvariant/issues/262#issuecomment-574900293,2,['test'],['testdata']
Testability,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2087375034:413,test,test,413,,https://github.com/google/deepvariant/issues/813#issuecomment-2087375034,1,['test'],['test']
Testability,"a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:5352,test,test,5352,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,a-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-ng: 9.2.0-hdf63c60_0 conda-forge; libgfortran-ng: 7.3.0-hdf63c60_2 conda-forge; liblapack: 3.8.0-14_openblas conda-forge; libopenblas: 0.3.7-h5ec1e0e_5 conda-forge; libpng: 1.6.37-hed695b0_0 conda-forge; libprotobuf: 3.11.1-h8b12597_0 conda-forge; libssh2: 1.8.2-h22169c7_2 conda-forge; libstdcxx-ng: 9.2.0-hdf63c60_0 conda-forge; linecache2: 1.0.0-py_1 conda-forge; markdown: 3.1.1-py_0 conda-forge; mock: 3.0.5-py27_0 conda-forge; ncurses: 6.1-hf484d3e_1002 conda-forge; numpy: 1.14.6-py27h95a1406_1201 conda-forge; oauth2client: 1.5.2-py27_0 bioconda ; openjdk: 8.0.192-h14c3975_1003 conda-forge; openssl: 1.1.1d-h516909a_0 conda-forge; parallel: 20160622-1 bioconda ; pbr: 5.4.2-py_0 conda-forge; perl: 5.26.2-h516909a_1006 conda-forge; perl-threaded: 5.26.0-0 bioconda ; pip: 19.3.1-py27_0 conda-forge; prettytable: 0.7.2-py_3 conda-forge; protobuf: 3.11.1-py27he1b5a44_0 conda-forge; psutil: 5.6.7-py27h516909a_0 conda-forge; pyasn1: 0.4.8-py_0 conda-forge; pyasn1-modules: 0.2.7-py_0 conda-forge; pycparser: 2.19-py27_1 conda-forge; pyopenssl: 19.1.0-py27_0 conda-forge; pyparsing: 2.4.5-py_0 conda-forge; pyperclip: 1.7.0-py_0 conda-forge; pysocks: 1.7.0-py27_0 conda-forge; python: 2.7.15-h5a48372_1009 conda-forge; pyyaml: 5.2-py27h516909a_0 conda-forge; readline: 8.0-hf8c457e_0 conda-forge; requests: 2.22.0-py27_1 conda-forge; rsa: 3.1.4-py27_0 bioconda ; scipy: 1.2.1-py27h921218d_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566427577:2402,mock,mock,2402,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577,1,['mock'],['mock']
Testability,a/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10253,test,testdata,10253,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,a024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/a,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121296,test,testlogs,121296,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,ache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121978,test,testlogs,121978,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ad apptainer/1.2.5; module load clusterbasics; module load samtools; module load bedtools. OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0 candidates (0 examples) [0.06s elapsed]; I0812 17:25:01.273164 139627217889088 genomics_reader.py:222] Reading ../mapped/SRR18493715.RNA-Seq.Camellia_sp._multipetala.leaf/Aligned.sortedByCoord.out.bam with NativeSamReader; I0812 17:25:01.308415 139627217889088 make_examples_core.py:301] Task 18/32: Writing gvcf records to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/gvcf.tfrecord-00018-of-00032.gz; I0812 17:25:01.325705 139627217889088 make_examples_core.py:301] Task 18/32: Writing examples to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:1641,log,log,1641,,https://github.com/google/deepvariant/issues/867,1,['log'],['log']
Testability,"aded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. Thanks a lot,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371096675:3067,Log,Logging,3067,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675,1,['Log'],['Logging']
Testability,ader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] T,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10988,test,testdata,10988,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"age when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106990,log,log,106990,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"age; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors: ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0Ul6DZ/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 43, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265#issuecomment-580549772:1734,test,test,1734,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772,1,['test'],['test']
Testability,"ages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3128,test,test,3128,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ah, forgot to mention, I filtered chimeric reads because I had around 60% of them in every tested sample, was curious how much it affects my results (using samplix enrichment + pacbio hifi)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/863#issuecomment-2277597820:91,test,tested,91,,https://github.com/google/deepvariant/issues/863#issuecomment-2277597820,1,['test'],['tested']
Testability,"ake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <24>(char const (&) [24])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; collect2: error: ld returned 1 exit status; make[3]: *** [clif/backend/CMakeFiles/clif-matcher.dir/build.make:147: clif/backend/clif-matcher] Error 1; make[2]: *** [CMakeFiles/Makefile2:1342: clif/backend/CMakeFiles/clif-matcher.dir/all] Error 2; make[1]: *** [CMakeFiles/Makefile2:1349: clif/backend/CMakeFiles/clif-matcher.dir/rule] Error 2; make: *** [Makefile:617: clif-matcher] Error 2; ```; I had the same error last time but it somehow worked magically when I removed `build/` folder after a while. I think these are the major changes I have made apart from the changes to the `build_release_binaries.sh`. Please feel free to let me know if anything looks weird to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:6168,Log,LogMessage,6168,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,['Log'],['LogMessage']
Testability,"ake_examples_runner(options); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 953, in processing_regions_from_options; options.exclude_calling_regions),; File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 549, in build_calling_regions; regions = ranges.RangeSet.from_contigs(contigs); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 126, in from_contigs; return cls(make_range(contig.name, 0, contig.n_bases) for contig in contigs); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 86, in __init__; self._by_chr[range_.reference_name].addi(range_.start, range_.end, None); File ""/usr/local/lib/python2.7/dist-packages/intervaltree/intervaltree.py"", line 330, in addi; return self.add(Interval(begin, end, data)); File ""/usr/local/lib/python2.7/dist-packages/intervaltree/intervaltree.py"", line 313, in add; "" {0}"".format(interval); ValueError: IntervalTree: Null Interval objects not allowed in IntervalTree: Interval(0, 0); parallel: This job failed:; python bin/make_examples.zip --mode calling --ref ../Falcon_Unzip/all_p_ctg.fa --reads ../Falcon_Unzip/out.bam --sample_name FalconSet --examples quickstart-output/examples.tfrecord@40.gz --task 1; seq 0 $((N_SHARDS-1)) 0,00s user 0,00s system 0% cpu 0,003 total; parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python 2,40s user 0,92s system 109% cpu 3,034 total. ```. Is there something wrong with the bam file? My reads were mapped using bwa mem ; `bwa mem -p -t 48 all_p_ctg.fa ../ARC_MA_sequencing/94_Fastq_files_GC047403/GC047403.170925_ARC_.fq.gz|samtools sort -@ 48 -m 5G -o out.bam -`. thanks for any help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77:6740,LOG,LOGDIR,6740,,https://github.com/google/deepvariant/issues/77,3,"['LOG', 'log']","['LOGDIR', 'log']"
Testability,"al = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:85322,log,log,85322,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"aligner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --channels insert_size --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m5.584s; user 0m13.426s; sys 0m0.563s; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; I am running this on an AWS graviton4 machine (aarch64 architecture). The Dockerfile does not work for me directly. I had to run `build-prereq.sh` and resolve all the errors manually within a docker container. But I did run build_and_test.sh to make sure all tests passed and the binaries were also built successfully.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:5097,test,test,5097,,https://github.com/google/deepvariant/issues/879,3,['test'],"['test', 'tests']"
Testability,"all last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty. real 3m2.335s; user 0m7.450s; sys 0m4.274s`. **Does the quick start test work on your system?**; Yes",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:7696,test,test,7696,,https://github.com/google/deepvariant/issues/869,1,['test'],['test']
Testability,"all_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/wgs/model.ckpt --use_openvino --batch_size 512 --include_debug_info --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./NA12878_S1.vcf --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" --gvcf_outfile ./NA12878_S1.gvcf.gz ) > ./postprocess_variants.log 2>&1. Is there something I am missing?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:2719,log,log,2719,,https://github.com/google/deepvariant/issues/625,3,['log'],['log']
Testability,"alling_exome_regions_configuration. ```#!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=PROJECT_ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1093,test,testdata,1093,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,another test by not-Pi-Chuan,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426#issuecomment-782471153:8,test,test,8,,https://github.com/google/deepvariant/issues/426#issuecomment-782471153,1,['test'],['test']
Testability,"ant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124113,test,testlogs,124113,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125988,test,testlogs,125988,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,ant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118197,log,log,118197,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 wa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1327,log,logging,1327,,https://github.com/google/deepvariant/issues/116,1,['log'],['logging']
Testability,"ap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42233,test,testlogs,42233,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100662,test,testlogs,100662,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"apan.; I also asked in https://github.com/google/deepvariant/issues/127 ; but this is a different type of issue. I also want to train DeepVariant model from the WGS data in our laboratory using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_L",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1048,log,log,1048,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:3195,log,log,3195,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['log'],['log']
Testability,"ariant.simg /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2019-04-14 11:40:35.568961: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real 0m2.359s; user 0m0.767s; sys 0m0.949s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482956117:1288,log,login,1288,,https://github.com/google/deepvariant/issues/132#issuecomment-482956117,1,['log'],['login']
Testability,"ariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125463,test,testlogs,125463,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,ariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121575,log,log,121575,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally.; There were tests whose specified size is too big. Use the --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126220,log,log,126220,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"at . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1508,log,log,1508,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,atchNorm/moving_variance|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean|InceptionV3/Logits/Conv2d_1c_1x1/biases|InceptionV3/Conv2d_1a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:26279,Log,Logits,26279,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"ation for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165:1074,test,test,1074,,https://github.com/google/deepvariant/issues/165,2,"['benchmark', 'test']","['benchmarks', 'test']"
Testability,"azel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Source data:; [test_seq_data.tar.gz](https://github.com/google/deepvariant/files/3611565/test_seq_data.tar.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:5558,test,test,5558,,https://github.com/google/deepvariant/issues/219,2,['test'],['test']
Testability,azel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_de,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121526,test,testlogs,121526,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,b 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1550,test,test,1550,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"b/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80521,log,log,80521,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (s,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107339,log,log,107339,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124518,log,log,124518,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1759,log,log,1759,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107687,log,log,107687,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124866,log,log,124866,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107861,log,log,107861,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125040,log,log,125040,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107513,log,log,107513,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124692,log,log,124692,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1933,log,log,1933,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2349,log,log,2349,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:2318,test,test,2318,,https://github.com/google/deepvariant/issues/419,2,['test'],['test']
Testability,"bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:1899,log,logging,1899,,https://github.com/google/deepvariant/issues/60,1,['log'],['logging']
Testability,bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119457,log,log,119457,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/v,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120346,test,testlogs,120346,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"bdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00003-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00025-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00047-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00004-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00026-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00048-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00005-of-00064.gz PE150",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1579,log,log,1579,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701,1,['log'],['log']
Testability,"bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepvariant_log.txt 2>&1. Command exit status:; 127. Command output:; (empty). Command error:; docker: Error response from daemon: open /var/lib/docker/overlay2/fe3663cd03e849890d83be14603f217249f3f43f9585b554df599d0318909f21/.tmp-committed2046174062: no such file or directory.; See 'docker run --help'. Work dir:; /home/ubuntu/dd/nextflow2/work/ea/9ecd306270fe3f00d9b73f8261fe89. Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`. -- Check '.nextflow.log' file for details",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:3205,log,log,3205,,https://github.com/google/deepvariant/issues/883,1,['log'],['log']
Testability,"bedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably tru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14363,log,logging,14363,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['log'],"['logging', 'logs']"
Testability,beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115213,Log,Logits,115213,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268#issuecomment-586584341:3736,log,logs,3736,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341,2,['log'],"['log', 'logs']"
Testability,"blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" --examples ""/tmp/tmpj5fx0phm/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmpj5fx0phm/gvcf.tfrecord@1.gz"" --task {} ). [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: /input/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:08:49.655763 139654431344384 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:49.663154 139654431344384 make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:1495,log,log,1495,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['log'],['log']
Testability,"bove command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored:; <tf.Variable 'classification/kernel:0' shape=(2048, 3) dtype=float32, numpy=_. ------------; Please note that I used the same checkpoint path as I was using while running _run_deepvariant.py_.; Could you please help me to resolve the issue and suggest if I am missing anything while using _call_varaint_keras.py_?. Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:2104,Assert,AssertionError,2104,,https://github.com/google/deepvariant/issues/636,2,['Assert'],['AssertionError']
Testability,"build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [IntelÂ® XeonÂ® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1069,test,test,1069,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['test'],['test']
Testability,"but the docker install commands failed:. sudo apt-get -qq -y update; E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. sudo apt-get -qq -y install docker-ce; E: Package 'docker-ce' has no installation candidate. So instead I ran an alternate docker installation, which succeeded (sudo apt install docker.io). I don't know if this is the ultimate problem. The first command within the docker seems to complete with no errors:. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hs37d5.fa.gz"" --reads ""/input/HG002_NIST_150bp_chr20_downsampled_30x.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --regions ""20"" --task {}. ...omitting much output... It does take 40 minutes as opposed to the advertised 8, though. I was using pre-emptible instances so perhaps this caused delay, but I did test it 3 times, and it is reliably ~40 mins each time. The second command within the docker dies, this is all the output:. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] St",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749313156:1100,test,test,1100,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156,1,['test'],['test']
Testability,"c.): Singularity (v3.10.0); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:1507,log,log,1507,,https://github.com/google/deepvariant/issues/833,1,['log'],['log']
Testability,cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepva,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118617,log,log,118617,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4527,sandbox,sandbox,4527,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['sandbox'],['sandbox']
Testability,"cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1584,log,log,1584,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1410,log,log,1410,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123167,log,log,123167,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Ta",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9385,test,testdata,9385,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,che/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121748,test,testlogs,121748,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10016,test,testdata,10016,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"ckages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30696,log,log,30696,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,ckages; ++ export DV_GPU_BUILD=0; ++ DV_GPU_BUILD=0; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: Fir,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3176,test,test,3176,,https://github.com/google/deepvariant/issues/19,1,['test'],['test']
Testability,"com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the followin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1034,test,testdata,1034,,https://github.com/google/deepvariant/issues/640,1,['test'],['testdata']
Testability,com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16685,log,logging,16685,,https://github.com/google/deepvariant/issues/19,1,['log'],['logging']
Testability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20543,test,testing,20543,,https://github.com/google/deepvariant/issues/19,4,['test'],"['testing', 'tests']"
Testability,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486821218:161,test,testing,161,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218,1,['test'],['testing']
Testability,"containers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:4203,test,testdata,4203,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,"cord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2019-04-14 11:40:35.568961: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real 0m2.359s; user 0m0.767s; sys 0m0.949s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482956117:2684,test,testdata,2684,,https://github.com/google/deepvariant/issues/132#issuecomment-482956117,2,['test'],['testdata']
Testability,core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]; I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s; user 0m37.643s; sys 0m6.426s. ***** Running the command:*****; time /opt/deepvariant/bin/post,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11857,test,testdata,11857,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. I sshed into my machine:. ```bash; gcloud compute ssh pichuan-deepvariant-vm --zone us-west1-b; ```. I ran this with my own `YOUR_PROJECT` and `OUTPUT_GCS_BUCKET` setting.; Then the following is basically just copy/paste from the doc:. ```; BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:1388,log,logs,1388,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['log'],['logs']
Testability,"cratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3378,test,test,3378,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL; > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks; > Ordering chunks by estimated depth; > Setup complete, beginning run; > Polishing 3% complete (46/1342). Estimated time remaining: unknown; > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s; > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s; > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:6587,log,log,6587,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"cs/deepvariant-training-case-study.md. ---. I got a CPU machine:; ```; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west2-b"" \; --min-cpu-platform ""Intel Skylake""; ```. Set variables; ```; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:1578,log,logs,1578,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['log'],['logs']
Testability,"ctory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help; CPU Builds a DeepVariant Docker image for CPU usage.; GPU Builds a DeepVariant Docker image for GPU usage.; Runner Builds a DeepVariant Docker image for large-scale analysis run; using the Genomics Pipelines API. $; ```. Even `Runner` is a bit too general, so maybe calling it `LargeScaleAnaly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1518,test,test,1518,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,1,['test'],['test']
Testability,"cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; make_examples.tfrecord-00005-of-00008.gz.example_info.json; make_examples.tfrecord-00006-of-00008.gz; make_examples.tfrecord-00006-of-00008.gz.example_info.json; make_examples.tfrecord-00007-of-00008.gz; make_examples.tfrecord-00007-of-00008.gz.example_info.json; postprocess_variants.log",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2555,log,log,2555,,https://github.com/google/deepvariant/issues/818,2,['log'],['log']
Testability,"d and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:1034,test,testdata,1034,,https://github.com/google/deepvariant/issues/530,1,['test'],['testdata']
Testability,"d of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls. Is there a way to filter my variants from DeepTrio further?. Thank you!. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-820564569:1894,test,test,1894,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569,1,['test'],['test']
Testability,"d request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2006,log,log,2006,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"d"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1612,test,testdata,1612,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['test'],['testdata']
Testability,"d-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 993, in processing_regions_from_options",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437503051:1723,test,test,1723,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051,1,['test'],['test']
Testability,d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Conv2d_2b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:43576,Log,Logits,43576,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights|InceptionV3/Conv2d_2b_3x3/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:64412,Log,Logits,64412,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"d_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71244,log,log,71244,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1039,log,logs,1039,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cach,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119925,log,log,119925,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124344,log,log,124344,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1689,log,log,1689,,https://github.com/google/deepvariant/issues/225,1,['log'],['log']
Testability,"derlying Inception V3 network architecture for both PacBio and WGS, a point of natural comparability would be the logits kernel across all three genotypes:. ![image](https://github.com/pgrosu/test/assets/6555937/e8ebb437-0132-474e-9ada-c64256aeb791). ![image](https://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels bein",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1239,test,tested,1239,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040,1,['test'],['tested']
Testability,"details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I sshed into the machine. ```bash; gcloud compute ssh pichuan-cpu --zone us-west1-b; ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.5; ```. And I confirmed the version:. ```; pichuan@pichuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'proto",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:1522,test,test,1522,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['test'],['test']
Testability,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5753,Test,Test,5753,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,4,"['Test', 'log']","['Test', 'log']"
Testability,"doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; I1218 13:54:44.727533 140314139805440 genomics_reader.py:174] Reading projec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1591,test,testdata,1591,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2493,Log,Log,2493,,https://github.com/google/deepvariant/issues/876,1,['Log'],['Log']
Testability,"ds /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2312,test,test,2312,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"dule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44714,log,log,44714,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"dule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:49148,log,log,49148,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"dule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57884,log,log,57884,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"dule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55735,log,log,55735,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"dule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:60033,log,log,60033,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77525,log,log,77525,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82749,test,testlogs,82749,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:90094,test,testlogs,90094,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:94991,test,testlogs,94991,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"e initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [IntelÂ® XeonÂ® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1149,test,test,1149,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['test'],['test']
Testability,"e lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:3000,log,log,3000,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"e native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71419,log,log,71419,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"e specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:10783,test,test,10783,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['test']
Testability,"e the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78104,test,testdata,78104,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testdata']
Testability,"e-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16963,log,log,16963,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4',",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1820,log,logs,1820,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,1,['log'],['logs']
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:19094,log,log,19094,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:28432,log,log,28432,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35711,log,log,35711,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37866,log,log,37866,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:51299,log,log,51299,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64476,log,log,64476,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66631,log,log,66631,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68786,log,log,68786,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"e_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3183,Assert,AssertionError,3183,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,1,['Assert'],['AssertionError']
Testability,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13153,Test,Test,13153,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,4,"['Test', 'log']","['Test', 'log']"
Testability,"ealign script. Same script worked fine when I ran pre-compiled binary from the Docker container ðŸ¤·â€â™‚ï¸. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:1221,test,testdata,1221,,https://github.com/google/deepvariant/issues/199,1,['test'],['testdata']
Testability,"ecision, recall, f1) for either het or homalt. Or more specifically they are all 0.0. Eval stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1238,test,testing,1238,,https://github.com/google/deepvariant/issues/876,1,['test'],['testing']
Testability,"ecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo dock",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2698,Log,Log,2698,,https://github.com/google/deepvariant/issues/151,1,['Log'],['Log']
Testability,"ed object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42711,test,testing,42711,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,ed) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/ba,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118819,log,log,118819,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"eeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:2727,test,testdata,2727,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"eepvaraint on a wgs file. â€˜[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.â€™ I have googled it but I am not sure what is causing this issue and how to resolve it. I will appreciate your help. ; I have attached the full error log, bash and yaml files. . error:; code: 10; message: |-; 11: Docker run failed: ASS""\ncalls {\n info {\n key: ""AD""\n value {\n values {\n int_value: 8\n }\n values {\n int_value: 7\n }\n }\n }\n info {\n key: ""DP""\n value {\n values {\n int_value: 15\n }\n }\n }\n info {\n key: ""GQ""\n value {\n values {\n int_value: 12\n }\n }\n }\nâ€¦â€¦.; â€¦â€¦â€¦.; â€¦â€¦â€¦..Reading /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfReader\nI0806 01:37:07.984452 140434439055104 postprocess_variants.py:593] Writing output to VCF file: /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf\nI0806 01:37:08.052251 140434439055104 genomics_writer.py:118] Writing /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfWriter\n**[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.** See logs at gs://gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/deepvariant_staging_folder/logs/']]. Operation ID: ENqznd7QLBi1jMjtufCo3ckBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl. Best,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University. [UDN631726_gvcf_error_08052018.log](https://github.com/google/deepvariant/files/2263671/UDN631726_gvcf_error_08052018.log); Since GIT does not allow me to attach .sh or .yaml files, I am saving them as text files and attaching.; [deepvariant_v0.6.1_UDN631726.yaml.txt](https://github.com/google/deepvariant/files/2263680/deepvariant_v0.6.1_UDN631726.yaml.txt); [deepvariant_v0.6.1_UDN631726.sh.txt](https://github.com/google/deepvariant/files/2263674/deepvariant_v0.6.1_UDN631726.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86:1280,log,logs,1280,,https://github.com/google/deepvariant/issues/86,4,['log'],"['log', 'logs']"
Testability,"eepvariant/issues/127 ; but this is a different type of issue. I also want to train DeepVariant model from the WGS data in our laboratory using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1066,log,log,1066,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"eference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:1546,log,logx,1546,,https://github.com/google/deepvariant/issues/653,1,['log'],['logx']
Testability,"egions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m0.146s`. ```; $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ```bash; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took `5m25.905s`. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. # This parts starts shuffling... ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:4047,log,log,4047,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['log'],['log']
Testability,"el_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1146,log,log,1146,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"elapsed]; I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]; I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]; Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]; parallel: This job failed:; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:9764,test,test,9764,,https://github.com/google/deepvariant/issues/794,2,['test'],['test']
Testability,eler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120879,log,log,120879,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"eloper.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,; Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one).; 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:4348,test,test,4348,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,1,['test'],['test']
Testability,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1117,log,log,1117,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,1,['log'],['log']
Testability,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1866,test,testdata,1866,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,1,['test'],['testdata']
Testability,"enerate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123109,test,testlogs,123109,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106412,test,testlogs,106412,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ent in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://acade",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:1363,benchmark,benchmark,1363,,https://github.com/google/deepvariant/issues/475,1,['benchmark'],['benchmark']
Testability,"ep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:8306,log,log,8306,,https://github.com/google/deepvariant/issues/129,2,['log'],"['log', 'logs']"
Testability,"epvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1481,log,logging,1481,,https://github.com/google/deepvariant/issues/214,1,['log'],['logging']
Testability,"er); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15228,log,log,15228,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['log'],['log']
Testability,"er@deepvariant:~/data$ samtools sort -f alignments20.bam alignments20_sorted.bam`; This command produced a sorted file with size of 7.75GB. 3. I tried running the `make_examples` script again with the new `alignments20_sorted.bam` file.; The command:; ```; python bin/make_examples.zip \; --mode training \; --ref ""data/chr20.fa"" \; --reads ""data/alignments20_sorted.bam"" \; --examples ""training-examples/training_set.with_label.tfrecord.gz"" \; --confident_regions ""data/NA12878.sorted.bed"" \; --truth_variants ""data/NA12878.sorted.vcf.gz"" \; --regions ""chr20"" \; --norealign_reads; ```; And The output: (receiving the same QUAL field missing error); ```; 2019-01-29 11:46:16.329383: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.333216: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.334961 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.337215 140471159555840 make_examples.py:1024] Preparing inputs; 2019-01-29 11:46:16.340804: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.344462: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.346041 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.360527 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:16.361952 140471159555840 make_examples.py:946] Common contigs are [u'chr20']; I0129 11:46:16.501434 140471159555840 make_examples.py:1030] Writing examples to prj-NA12878/training-examples/training_set.with_label.tfrecord.gz; 2019-01-29 11:46:16.502209: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:1542,test,testdata,1542,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['test'],['testdata']
Testability,"ere are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:2493,test,testdata,2493,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['test'],['testdata']
Testability,"error pops out when I switch to my BAM. ```; [05/03/2018 23:30:18 INFO discovery.py] URL being requested: GET https://genomics.googleapis.com/v1alpha2/operations?filter=projectId+%3D+isb-cgc-06-0004+AND+labels.job-id+%3D+call-varia--root--180503-233007-45&alt=json&pageSize=256; call-varia--root--180503-233007-45: FAILURE; [u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/03/2018 23:30:18 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-XX-XXXX', '--logging', 'gs://XXXXX/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:1099,log,logging,1099,,https://github.com/google/deepvariant/issues/70,1,['log'],['logging']
Testability,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:1947,mock,mockbuild,1947,,https://github.com/google/deepvariant/issues/391,1,['mock'],['mockbuild']
Testability,"es, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2528,log,log,2528,,https://github.com/google/deepvariant/issues/876,1,['log'],['log']
Testability,"es/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374, in call_variants; raise ValueError(f'Shape mismatch in {example_info_json} and '; ValueError: Shape mismatch in ./examples.tfrecord-00000-of-00032.gz.example_info.json and /opt/models/pacbio/model.ckpt.example_info.json.; ```. My command line looks like this:; `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --add_hp_channel --add_hp_channel --alt_aligned_pileup diff_channels --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/HG002.merged.bam --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --realign_reads --regions 20 --sample_name HG002 --split_skip_reads --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --batch_size 512 --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:2527,LOG,LOGDIR,2527,,https://github.com/google/deepvariant/issues/628,6,"['LOG', 'log']","['LOGDIR', 'log', 'logs']"
Testability,es_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorf,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11131,test,testdata,11131,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"esented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be teste",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1380,Test,Test,1380,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,2,"['Test', 'test']","['Test', 'test']"
Testability,"et.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '; '--intermediate_results_dir /ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2402,log,log,2402,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,1,['log'],['log']
Testability,"etails:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.UnexpectedExitStatusE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1343,log,log,1343,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['log'],['log']
Testability,"etch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; #################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16414,mock,mock,16414,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['mock'],['mock']
Testability,"evel. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14978,test,testdata,14978,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['test'],['testdata']
Testability,"examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:7375,log,logs,7375,,https://github.com/google/deepvariant/issues/129,1,['log'],['logs']
Testability,"examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA1287",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14038,log,logged,14038,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['log'],['logged']
Testability,examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1388,test,test,1388,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"f the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜logsâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:2956,test,tested,2956,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['test'],['tested']
Testability,"f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/wgs/model.ckpt --use_openvino --batch_size 512 --include_debug_info --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:2002,LOG,LOGDIR,2002,,https://github.com/google/deepvariant/issues/625,6,"['LOG', 'log']","['LOGDIR', 'log', 'logs']"
Testability,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3313,sandbox,sandbox,3313,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761,1,['sandbox'],['sandbox']
Testability,"fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/GCF_000001405.26_GRCh38_genomic1.fa.gz --reads /input/1115492_23181_0_0.cram --examples /tmp/tmpf8z5m2q; 0/make_examples.tfrecord@5.gz --channels insert_size --gvcf /tmp/tmpf8z5m2q0/gvcf.tfrecord@5.gz --regions chr3:10,049,322-10,156,156 --task 4. I didn't try the quick start test.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:2650,test,test,2650,,https://github.com/google/deepvariant/issues/588,1,['test'],['test']
Testability,"fda.gov/challenges/truth/results-explore)"", and select â€œ**func_cds**â€ (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the â€œtruthâ€ set was defined. They mentioned that they **focused on regions where variants could be made most confidently** (genome-wide), and Iâ€™m assuming that is why most of the numbers are so high. Otherwise, they are more in the range of using [my same WGS sample and variant caller (DeepVariant) while only changing the alignment](https://precision.fda.gov/comparisons/3437), which isnâ€™t really an independent verification (matching my original concern that the percentages being reported seemed unrealistically high). **In other words, I would say DeepVariant performed well, *along with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to creat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:6589,test,tested,6589,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['test'],['tested']
Testability,"ference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1324,test,testPartExecutor,1324,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['test'],['testPartExecutor']
Testability,"file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:75476,test,testing,75476,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,files and parameters not found when running test-data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/558:44,test,test-data,44,,https://github.com/google/deepvariant/issues/558,1,['test'],['test-data']
Testability,"flow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:40156,log,log,40156,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"flow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:75024,log,log,75024,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,frecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1226,test,test,1226,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"g data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1576,log,log,1576,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['log'],['log']
Testability,"g for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:15082,test,testing,15082,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,"g/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; I1218 13:54:44.727533 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 455, in _ensure_consistent_con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:2598,test,testdata,2598,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"ged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_ID; PROBAND_ID=${Case_ID}_proband; MOTHER_ID=${Case_ID}_mother; FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:7556,test,test,7556,,https://github.com/google/deepvariant/issues/518,1,['test'],['test']
Testability,"get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; make_examples.tfrecord-00005-of-00008.gz.example_info.json; make_examples.tfrecord-00006-of-00008.gz; make_examples.tfrecord-00006-o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2223,log,log,2223,,https://github.com/google/deepvariant/issues/818,1,['log'],['log']
Testability,"github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:986,test,testdata,986,,https://github.com/google/deepvariant/issues/678,1,['test'],['testdata']
Testability,"gle/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1147,test,testdata,1147,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"gle_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:108035,log,log,108035,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"gle_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2107,log,log,2107,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"gner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122793,test,testlogs,122793,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""ve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3495,test,testdata,3495,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,gs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121799,log,log,121799,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"h test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜logsâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4218,log,login,4218,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['log'],['login']
Testability,hNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Conv2d_4a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:88743,Log,Logits,88743,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"ha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:2229,test,testing,2229,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['test'],['testing']
Testability,hap.py benchmarking,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/896:7,benchmark,benchmarking,7,,https://github.com/google/deepvariant/issues/896,1,['benchmark'],['benchmarking']
Testability,"he file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: Ã¢ENCFF528VXT.bamÃ¢; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - Ã¢ENCFF528VXT.bamÃ¢ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:1007,test,testfiles,1007,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,1,['test'],['testfiles']
Testability,"he terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/op_def_pb2.py"", line 210, in <module>; serialized_options=None, file=DESCRIPTOR),; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 534, in __new__; return _message.default_pool.FindFieldByName(full_name); KeyError: ""Couldn't find field tensorflow.OpDef.control_output"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:2588,test,test,2588,,https://github.com/google/deepvariant/issues/342,2,['test'],['test']
Testability,"hecked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:1610,test,test,1610,,https://github.com/google/deepvariant/issues/550,2,['test'],['test']
Testability,"hello, ; I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s; user 112m32.542s; sys 0m37.407s; I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720:11,test,tested,11,,https://github.com/google/deepvariant/issues/720,1,['test'],['tested']
Testability,"hello,; I am testing RNA-seq data with deepvarian1.4. ; I have some questions aboutï¼š; 1. Is GATK SplitNCigarReads necessary for input bam?; 2. Could you explain in detail what the parameter ""--make_examples_extra_args=""split_skip_reads=true, channels=''"" dose?; 3. Does the RNAseq model only apply to the CDS region?; Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/635:13,test,testing,13,,https://github.com/google/deepvariant/issues/635,1,['test'],['testing']
Testability,"hello,; I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening?; ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b); ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf); ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed); Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655:10,test,tested,10,,https://github.com/google/deepvariant/issues/655,1,['test'],['tested']
Testability,"hello,; When analyzing PacBio data, I encountered some problems.; I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error.; Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam; My cmd:; 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'; 2. /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref hs37d5.fasta \; --reads n1000.subreads_to_ccs_aligned.bam \; --output_vcf n1000.depv.vcf.gz \; --output_gvcf n1000.depv.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir intermediate_results_dir; Error:; ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem?; Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672:68,test,tested,68,,https://github.com/google/deepvariant/issues/672,1,['test'],['tested']
Testability,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456:19,log,log,19,,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456,1,['log'],['log']
Testability,"hi @sounkou-bioinfo,. Thank you for bringing up this issue. We have not worked on this specifically but I will try to give this a shot in our next release. It's a reasonable request and we may have the right logics to fix this in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/875#issuecomment-2322170478:208,log,logics,208,,https://github.com/google/deepvariant/issues/875#issuecomment-2322170478,1,['log'],['logics']
Testability,"hink Nucleous library (that is used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1274,log,logs,1274,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['log'],['logs']
Testability,"his TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5396,log,log,5396,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability,"his prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also noticed there are several deprecation warnings in the log file",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:5104,log,log,5104,,https://github.com/google/deepvariant/issues/269,1,['log'],['log']
Testability,hiï¼Œ; Thanks for your reply. I mean BQSR. I tested on WES data. The result : ; 1. deal with deduplication and BQSRï¼š; ![1682582003166](https://user-images.githubusercontent.com/70870741/234796615-a5ce10c7-da7b-4542-8717-b2cde03fc478.jpg); 2. only deal with deduplication:; ![1682582061403](https://user-images.githubusercontent.com/70870741/234796818-f7bb9a58-dc50-4a96-8848-6bd785b0b136.jpg). Can I think BQSR processed results are better?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/638#issuecomment-1525033039:43,test,tested,43,,https://github.com/google/deepvariant/issues/638#issuecomment-1525033039,1,['test'],['tested']
Testability,"ho ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-rep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2238,test,tests,2238,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['test'],['tests']
Testability,"homas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK â€“ that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > â€”; > You are receiving this because you authored th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:1352,Log,Logits,1352,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['Log'],['Logits']
Testability,"hon/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123821,log,log,123821,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"hon/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87998,log,log,87998,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"hot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <24>(char const (&) [24])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; collect2: error: ld returned 1 exit status; make[3]: *** [clif/backend/CMakeFiles/clif-matcher.dir/build.make:147: clif/backend/clif-matcher] Error 1; make[2]: *** [CMakeFiles/Makefile2:1342: clif/backend/CMakeFiles/clif-matcher.dir/all] Error 2; make[1]: *** [CMak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:5599,Log,LogMessage,5599,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,['Log'],['LogMessage']
Testability,"hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; ```. However, If I run the follow bash script, it can not find `ref_idx` through `ls` command, whereas shell condition expression ` [ -f $ref_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:1766,log,logdir,1766,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761,2,['log'],"['logdir', 'logs']"
Testability,"https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100720,log,log,100720,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"hub.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜/mnt/cdw-genomeâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5720,log,login,5720,,https://github.com/google/deepvariant/issues/171,1,['log'],['login']
Testability,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/319:866,log,log,866,,https://github.com/google/deepvariant/issues/319,2,['log'],['log']
Testability,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2064,log,log,2064,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,"['benchmark', 'log']","['benchmark', 'log']"
Testability,"iant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125813,test,testlogs,125813,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"iants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:2833,log,log,2833,,https://github.com/google/deepvariant/issues/429,1,['log'],['log']
Testability,"ibcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106760,test,testlogs,106760,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"icense of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:5216,test,test,5216,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"ign(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign; reads = reservoir_sample_reads(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads; return utils.reservoir_sample(iterable_of_reads, k, random); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample; for i, item in enumerate(iterable):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:3785,test,testdata,3785,,https://github.com/google/deepvariant/issues/812,1,['test'],['testdata']
Testability,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1726,sandbox,sandboxed,1726,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937,1,['sandbox'],['sandboxed']
Testability,"iled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2291,log,logs,2291,,https://github.com/google/deepvariant/issues/722,1,['log'],['logs']
Testability,"iles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. [deepvariant_v0.7.0_UDN668131_test.sh.txt](https://github.com/google/deepvariant/files/2567583/deepvariant_v0.7.0_UDN668131_test.sh.txt). Thanks,; Shruti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437503051:4997,test,test,4997,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051,4,['test'],['test']
Testability,ileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepva,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119859,test,testlogs,119859,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"image](https://github.com/google/deepvariant/assets/30355684/4c6fdc69-f881-44aa-8935-8c627a591f6f). Example of reads that were moved:; Original alignment; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993994 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE XA:Z:X,+140993784,79M,2; PG:Z:MarkDuplicates AS:i:74 XS:i:69 MD:Z:17C61 NM:i:1 RG:Z:DM_23_2198; NB501857:464:HH7FWBGXV:2:23210:26812:14806 147 X 140994064 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA XA:Z:X,-140994589,50M3D29M,4; PG:Z:MarkDuplicates AS:i:79 XS:i:67 MD:Z:79 NM:i:0 RG:Z:DM_23_2198; Local realignment; X:140993145-140994144/ X_140993145_140994144realigned_reads.bam X_140993145_140994144realigned_reads.bam.bai; frmascla@frt:DeepV-TEST$ samtools view Local/X_140993145_140994144realigned_reads.bam | grep NB501857:464:HH7FWBGXV:2:23210:26812:14806; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993784 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE; NB501857:464:HH7FWBGXV:2:23210:26812:14806 19 X 140993854 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA. Original alignment, bam file; https://www.dropbox.com/scl/fi/c9tc01sdtf2sroxj3u3bj/original_alignment.bam?rlkey=jgxnyhyse2ekcu6t1s3l3lnnl&dl=0; Local realignment, bam file; https://www.dropbox.com/scl/fi/oqhny0s7h9hu3zcyrprig/X_140993145_140994144realigned_reads.bam?rlkey=zmbon72t19vjlcdht1zt6m5xg&dl=0. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; Yes. **Setup**; - Op",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763:1914,TEST,TEST,1914,,https://github.com/google/deepvariant/issues/763,1,['TEST'],['TEST']
Testability,impl_linux-64 2.40 h41732ed_0 conda-forge; libblas 3.9.0 17_linux64_openblas conda-forge; libcblas 3.9.0 17_linux64_openblas conda-forge; libcurl 7.87.0 h6312ad2_0 conda-forge; libdeflate 1.18 h0b41bf4_0 conda-forge; libedit 3.1.20191231 he28a2e2_2 conda-forge; libev 4.33 h516909a_1 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.1.0 he5830b7_0 conda-forge; libgfortran-ng 13.1.0 h69a702a_0 conda-forge; libgfortran5 13.1.0 h15d22d2_0 conda-forge; libgomp 13.1.0 he5830b7_0 conda-forge; liblapack 3.9.0 17_linux64_openblas conda-forge; libnghttp2 1.51.0 hdcd2b5c_0 conda-forge; libnsl 2.0.0 h7f98852_0 conda-forge; libopenblas 0.3.23 pthreads_h80387f5_0 conda-forge; libprotobuf 3.18.0 h780b84a_1 conda-forge; libsqlite 3.42.0 h2797004_0 conda-forge; libssh2 1.10.0 haa6b8db_3 conda-forge; libstdcxx-ng 13.1.0 hfd8a6a1_0 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; lz4-c 1.9.3 h9c3ff4c_1 conda-forge; markdown 3.4.3 pyhd8ed1ab_0 conda-forge; markupsafe 2.0.1 py36h8f6f2f9_0 conda-forge; mock 5.0.2 pyhd8ed1ab_0 conda-forge; multidict 5.2.0 py36h8f6f2f9_0 conda-forge; ncurses 6.4 hcb278e6_0 conda-forge; numpy 1.16.6 py36h2aa4a07_0 conda-forge; oauth2client 4.1.3 py_0 conda-forge; oauthlib 3.2.2 pyhd8ed1ab_0 conda-forge; openjdk 8.0.332 h166bdaf_0 conda-forge; openssl 1.1.1u hd590300_0 conda-forge; opt_einsum 3.3.0 pyhd8ed1ab_1 conda-forge; pandas 1.1.5 py36h284efc9_0 conda-forge; parallel 20230522 ha770c72_0 conda-forge; perl 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:4475,mock,mock,4475,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['mock'],['mock']
Testability,"in 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] T",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:8754,test,testdata,8754,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:1555,log,log,1555,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"ine 1007, in main; FLAGS.outfile, header=header, round_qualities=True) as vcf_writer, \; File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/genomics_writer.py"", line 170, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 316, in _native_writer; exclude_header=exclude_header); File ""/tmp/Bazel.runfiles_gmddntqf/runfiles/com_google_deepvariant/third_party/nucle us/io/vcf.py"", line 288, in __init__; writer_options); ValueError: Unknown: Could not open variants_path: /scratch/moldach/bin/quickstart-out put/output.vcf.gz. real 0m3.763s; user 0m2.899s; sys 0m0.651s; I0327 13:32:43.946818 47794500922048 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --inf ile ""/tmp/tmp63xxmwmi/call_variants_output.tfrecord.gz"" --outfile ""/scratch/moldach/bi n/quickstart-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp63xxmwmi/g vcf.tfrecord@1.gz"" --gvcf_outfile ""/scratch/moldach/bin/quickstart-output/output.g.vcf .gz""' returned non-zero exit status 1.; [moldach@cdr767 bin]$ ^C; [moldach@cdr767 bin]$ exit; exit; srun: error: cdr767: task 0: Exited with exit code 130; ```. Any idea what could be causing this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:13660,test,testdata,13660,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"ing back to me! Any suggestions would be great. . 1. Your OS version.; NAME=""CentOS Linux""; VERSION=""7 (Core)""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""7""; PRETTY_NAME=""CentOS Linux 7 (Core)""; ANSI_COLOR=""0;31"". 2. Your Singularity version.; singularity version 3.4.1-1.2.el7; 3. Your numpy version (I'm not sure whether this affects singularity); 1.17.5; 4. Just to confirm, which *simg file are you using? The command you run? Was this with or without GPU?. I tried using the deepvariant-0.9.0.simg image from here with and without GPU: `https://storage.googleapis.com/deepvariant/singularity_images`; # Pull Singularity images; INPUT_DIR='singularity'; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/singularity_images""; # Non-gpu image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0.simg; # GPU image; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/deepvariant-0.9.0-gpu.simg. # Test Singularity DeepVariant0.9.0 image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz . # Errors:. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-31 01:37:29.333483: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . # Test Singularity DeepVariant0.9.0 GPU image on test data; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; ${INPUT_DIR}/deepvariant-${BIN_VERSION}-gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265#issuecomment-580549772:979,test,test,979,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772,1,['test'],['test']
Testability,"int; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6592,log,logs,6592,,https://github.com/google/deepvariant/issues/129,4,['log'],['logs']
Testability,"internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32847,log,log,32847,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100416,test,testlogs,100416,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash; python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; ```; should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash; source settings.sh; ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872:1508,test,test,1508,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872,1,['test'],['test']
Testability,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):; Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; Â Â Â  tf.app.run(); Â  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; Â Â Â  _sys.exit(main(_sys.argv[:1] + flags_passthrough)); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main; Â Â Â  make_examples_runner(options); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner; Â Â Â  regions = processing_regions_from_options(options); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options; Â Â Â  options.min_shared_contigs_basepairs); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:1015,test,testdata,1015,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['test'],['testdata']
Testability,"iving the same QUAL field missing error); ```; 2019-01-29 11:46:16.329383: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.333216: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.334961 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.337215 140471159555840 make_examples.py:1024] Preparing inputs; 2019-01-29 11:46:16.340804: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.344462: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.346041 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.360527 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:16.361952 140471159555840 make_examples.py:946] Common contigs are [u'chr20']; I0129 11:46:16.501434 140471159555840 make_examples.py:1030] Writing examples to prj-NA12878/training-examples/training_set.with_label.tfrecord.gz; 2019-01-29 11:46:16.502209: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-01-29 11:46:16.550218: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.554270: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.556066 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.571476 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:20.140141 140471159555840 make_examples.py:7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:2157,test,testdata,2157,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['test'],['testdata']
Testability,"ject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <24>(char const (&) [24])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:5272,Log,LogMessage,5272,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,['Log'],['LogMessage']
Testability,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:2177,log,log,2177,,https://github.com/google/deepvariant/issues/243,1,['log'],['log']
Testability,"ke sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y install parallel; sudo apt-get -y install aria2; ## Download models, and test data; # Copy the model files to your local disk.; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.data-00000-of-00001; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.index; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.meta. # Copy the data; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/agilent_sureselect_human_all_exon_v5_b37_targets.bed; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.fai; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gz.gzi; aria2c -c -x10 -s10 ${DATA_HTTP_DIR}/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:2072,test,test,2072,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['test'],['test']
Testability,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>; 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>; I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>; I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs; 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:2388,log,log,2388,,https://github.com/google/deepvariant/issues/677,1,['log'],['log']
Testability,"king fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1177,log,log,1177,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"l be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be rem>; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; 2024-08-28 10:40:49.893797: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.; I0828 10:40:49.947995 140318776715072 train.py:316]; ```. And here is an excerpt of from a later portion of the log file including some training and tuning steps, where you can see the 0.0 for het and homalt eval stats. . ```; I0829 07:13:59.098341 140305134778112 logging_writer.py:48] [13700] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.552009105682373, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_; weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5520098805427551, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0,; train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:18:37.609363 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 60.6% (13778/22724), ETA: 8h52m; I0829 07:18:37.611700 140305134778112 logging_writer.py:48] [13778] steps_per_sec=0.280118; I0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:6308,log,log,6308,,https://github.com/google/deepvariant/issues/876,1,['log'],['log']
Testability,"l first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post; File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty; INFO: Cleaning up image...; ```. Please let me know if I could provide the input BAM for testing/debugging. Thank you for your time. Best regards,; Louis",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:4886,test,testing,4886,,https://github.com/google/deepvariant/issues/833,1,['test'],['testing']
Testability,"l nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1673,log,logs,1673,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,2,['log'],['logs']
Testability,"l_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72412,test,testlogs,72412,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"l_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72237,test,testlogs,72237,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,l_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_trai,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72062,test,testlogs,72062,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,l_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvari,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119203,test,testlogs,119203,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:4309,test,test,4309,,https://github.com/google/deepvariant/issues/844,2,['test'],['test']
Testability,"lafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:; It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8?. ---. Here is what I tried. On a GCE instance, I ran:. ```bash; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. Then, I ran:. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```; (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/; call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip; call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh; deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip; freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip; ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521:1818,test,test,1818,,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521,1,['test'],['test']
Testability,"las.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71362,test,testlogs,71362,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ld' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:6434,test,tests,6434,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,5,['test'],"['test', 'tests']"
Testability,"le>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25616,log,log,25616,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"le>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3094,test,test,3094,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"le>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32781,test,testlogs,32781,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"led filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldnâ€™t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out â€œcomplex variantsâ€ (with more than one variant at a position), but .vcf files containing those variants werenâ€™t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://pre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1364,test,test,1364,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['test'],['test']
Testability,"led_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2709,log,logs,2709,,https://github.com/google/deepvariant/issues/722,1,['log'],['logs']
Testability,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488147736:2087,log,logic,2087,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736,2,"['Benchmark', 'log']","['Benchmarks', 'logic']"
Testability,ler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121115,log,log,121115,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1976,log,logs,1976,,https://github.com/google/deepvariant/issues/465,4,['log'],"['log', 'logs']"
Testability,les.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1307,test,test,1307,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:2466,log,log,2466,,https://github.com/google/deepvariant/issues/458,3,['log'],['log']
Testability,"les_runner(options); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2003, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1472, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1534, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5. **Does the quick start test work on your system?**; it work. **Any additional context:**; I also run the mistaken command directly inside docker container, . [gosadmin@node3 trio]$ docker run -it -v ""${DIR}"":""/data"" google/deepvariant:${VERSION} /bin/bash. root@d2911291b750:/data# ls -alh; total 42G; drwxrwxr-x 2 1000 1000 4.0K May 8 03:39 .; drwxr-xr-x 1 root root 29 May 8 08:21 ..; -rw-rw-r-- 1 1000 1000 6.9M Apr 30 08:19 HG002.bai; -rw-rw-r-- 1 1000 1000 9.7G Apr 30 08:19 HG002.bam; -rw-rw-r-- 1 1000 1000 9.6M Apr 30 09:01 HG002_truth.bed; -rw-rw-r-- 1 1000 1000 147M Apr 30 09:00 HG002_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:00 HG002_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 6.8M Apr 30 08:33 HG003.bai; -rw-rw-r-- 1 1000 1000 8.4G Apr 30 08:33 HG003.bam; -rw-rw-r-- 1 1000 1000 9.2M Apr 30 09:03 HG003_truth.bed; -rw-rw-r-- 1 1000 1000 129M Apr 30 09:02 HG003_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:02 HG003_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 7.0M May 7 07:42 HG004.bai; -rw-rw-r-- 1 100",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:3010,test,test,3010,,https://github.com/google/deepvariant/issues/455,1,['test'],['test']
Testability,"licable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1945,test,testdata,1945,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"ll be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3702,log,log,3702,,https://github.com/google/deepvariant/issues/151,1,['log'],['log']
Testability,"lledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3518,test,test,3518,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-???",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1478,log,log,1478,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"llo, . Previously I had an issue where the parameters I was using were not producing checkpoints in the model training step. I know that choosing parameters has a component of guesswork and iteration, and I was wondering if there are recommendations anywhere on how to choose a starting point for model training parameters, or if there are descriptions somewhere of what changing a particular parameter is likely to do. In the run described below, I am attempting to train the model on an individual using a second individual for the training data and a third individual for the validation data, but my goal is to use multiple individuals for both the training and validation sets, akin to the project described [here](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). . Secondly, I've been having an issue lately where I submit scripts to my computing cluster, and though they are granted resources and produce a log file, the log file is empty after several days of the code running indicating no progress has been made or that the program has even initialized. I am also asking my cluster resources about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:1000,log,log,1000,,https://github.com/google/deepvariant/issues/840,2,['log'],['log']
Testability,"llready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1160,log,log,1160,,https://github.com/google/deepvariant/issues/677,2,['log'],"['log', 'logs']"
Testability,"local/bin/bwa0.7.10 sampe -P reference_files/male.hg19.fa.gz ENCFF182MTO.sai ENCFF949NMY.sai ENCFF182MTO.fastq.gz ENCFF949NMY.fastq.gz; @PG ID:MarkDuplicates PN:MarkDuplicates VN:1.92() CL:net.sf.picard.sam.MarkDuplicates INPUT=[ENCFF182MTOENCFF949NMY.raw.srt.filt.srt.bam] OUTPUT=ENCFF182MTOENCFF949NMY.raw.srt.dupmark.bam METRICS_FILE=ENCFF182MTOENCFF949NMY.raw.srt.dup.qc REMOVE_DUPLICATES=false ASSUME_SORTED=true VALIDATION_STRINGENCY=LENIENT PROGRAM_RECORD_ID=MarkDuplicates PROGRAM_GROUP_NAME=MarkDuplicates MAX_SEQUENCES_FOR_DISK_READ_ENDS_MAP=50000 MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=8000 SORTING_COLLECTION_SIZE_RATIO=0.25 READ_NAME_REGEX=[a-zA-Z0-9]+:[0-9]:([0-9]+):([0-9]+):([0-9]+).* OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 VERBOSITY=INFO QUIET=false COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false; ```. I uploaded the modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371096675:2202,test,testfiles,2202,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675,1,['test'],['testfiles']
Testability,"local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:3457,Assert,AssertionError,3457,,https://github.com/google/deepvariant/issues/857,4,"['Assert', 'test']","['AssertionError', 'test']"
Testability,"long with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). Y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:7603,test,test,7603,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['test'],['test']
Testability,"low.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14332,test,testlogs,14332,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"lp and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11546,test,testlogs,11546,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ls.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash; python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; ```; should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash; source settings.sh; ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash; bazel build -c opt ${DV_COPT_FLAGS} deepvariant/...; ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872:1763,test,tested,1763,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872,1,['test'],['tested']
Testability,ls_test NO STATUS; //deepvariant/core:errors_test NO STATUS; //deepvariant/core:genomics_io_gcs_test NO STATUS; //deepvariant/core:genomics_io_noplugin_test NO STATUS; //deepvariant/core:genomics_io_test NO STATUS; //deepvariant/core:hts_test NO STATUS; //deepvariant/core:hts_verbose_test NO STATUS; //deepvariant/core:io_utils_test NO STATUS; //deepvariant/core:py_math_test NO STATUS; //deepvariant/core:py_utils_test NO STATUS; //deepvariant/core:ranges_test NO STATUS; //deepvariant/core:reader_base_test NO STATUS; //deepvariant/core:reference_fai_test NO STATUS; //deepvariant/core:sam_reader_test NO STATUS; //deepvariant/core:samplers_test NO STATUS; //deepvariant/core:variantutils_test NO STATUS; //deepvariant/core:vcf_reader_test NO STATUS; //deepvariant/core:vcf_writer_test NO STATUS; //deepvariant/core/python:hts_verbose_test NO STATUS; //deepvariant/core/python:math_wrap_test NO STATUS; //deepvariant/core/python:reference_wrap_test NO STATUS; //deepvariant/core/python:sam_reader_wrap_test NO STATUS; //deepvariant/core/python:vcf_reader_wrap_test NO STATUS; //deepvariant/core/python:vcf_writer_wrap_test NO STATUS; //deepvariant/environment_tests:env_smoke_test NO STATUS; //deepvariant/environment_tests:protobuf_implementation_test NO STATUS; //deepvariant/python:allelecounter_wrap_test NO STATUS; //deepvariant/python:variant_calling_wrap_test NO STATUS; //deepvariant/realigner:aligner_test NO STATUS; //deepvariant/realigner:realigner_test NO STATUS; //deepvariant/realigner:ssw_test NO STATUS; //deepvariant/realigner:window_selector_test NO STATUS; //deepvariant/realigner/python:debruijn_graph_wrap_test NO STATUS; //deepvariant/realigner/python:ssw_misc_test NO STATUS; //deepvariant/realigner/python:ssw_wrap_test NO STATUS; //deepvariant/testing:gunit_extras_test NO STATUS; //deepvariant/vendor:statusor_test NO STATUS; //deepvariant/vendor:timer_test NO STATUS; //deepvariant/vendor/python:statusor_examples_test NO STATUS; `; how to fix it ? I am not a root users.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:6276,test,testing,6276,,https://github.com/google/deepvariant/issues/6,1,['test'],['testing']
Testability,"lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord-00000-of-00001.gz; I1202 23:23:46.248160 46912500266816 make_examples_core.py:301] Writing examples to /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord-00000-of-00001.gz; I1202 23:23:46.248263 46912500266816 make_examples_core.py:301] Overhead for preparing inputs: 0 seconds; RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .; Traceback (most recent call last):; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/flashscra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:2325,test,testdata,2325,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"m"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord-00000-of-00001.gz; I1202 23:23:46.248160 46912500266816 make_examples_core.py:301] Writing examples to /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord-00000-of-00001.gz; I1202 23:23:46.248263 46912500266816 make_examples_core.py:301] Overhead for preparing inputs: 0 seconds; RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .; Traceback (most recent call last):; File ""/flashscratch/kimkw/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:2181,test,testdata,2181,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"m/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-01",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:976,test,test,976,,https://github.com/google/deepvariant/issues/717,2,['test'],['test']
Testability,"m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29163785/119_9105: Not found: Could not read base quality scores; 2020-02-10 20:52:23.440373: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (4265 vs. 0). The packages work with the test Illumina files. After testing several options, I notice what I got as raw data is either the bam file (without quality scores, only ""!"") or a fasta files. However, it seems that using those reads during the mapping wuth BWA (or with pbmm2) produces bam files without mapping scores which seems to be the problem. . This is how I mapped reads:. bwa mem -x pacbio -t 8 -R $ReadGroup $1 $2 | java -jar /sw/apps/bioinfo/picard/2.20.4/rackham/picard.jar SortSam \; INPUT=/dev/stdin \; OUTPUT=""$sample.bwa.picardSort.bam"" \; SORT_ORDER=coordinate. I feel this is a very basic question, but it is actually possible to run DeepVariant maping PB reads but in fasta format? Am I missing somthing to get the mapping scores in the bam file?. Thanks a lot,. Cheers,. /Sergio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270:2235,test,test,2235,,https://github.com/google/deepvariant/issues/270,2,['test'],"['test', 'testing']"
Testability,"mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1350,test,testdata,1350,,https://github.com/google/deepvariant/issues/223,1,['test'],['testdata']
Testability,"me/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very mu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13039,LOG,LOGDIR,13039,,https://github.com/google/deepvariant/issues/166,2,"['LOG', 'log']","['LOGDIR', 'logs']"
Testability,"ment, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarke",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1337,test,test,1337,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"mmon reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71537,test,testlogs,71537,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantMod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1564,log,log,1564,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123343,log,log,123343,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117061,test,tests,117061,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['tests']
Testability,"mon contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:17154,test,testdata,17154,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['test'],['testdata']
Testability,must be specified in calling mode.; I0330 15:47:21.755398 140432560695040 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757477 140432560695040 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.770883 139863230490368 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.773075 139863230490368 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139747089467136 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.756903 139747089467136 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.754880 139944273491712 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757000 139944273491712 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.759158 140716713432832 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.761278 140716713432832 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.755259 140202003052288 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.757451 140202003052288 errors.py:61] sample_name must be specified in calling mode.; I0330 15:47:21.765991 139705794897664 genomics_reader.py:223] Reading /input/sample.bam with NativeSamReader; E0330 15:47:21.768276 139705794897664 errors.py:61] sample_name must be specified in calling mode.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ref.fa --reads /input/sample.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@8.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --task 2. ```; I think the bam file is not corrected - would you please let me know which is the best suitable tool for aligning sequencing reads?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435#issuecomment-810383024:2049,log,logs,2049,,https://github.com/google/deepvariant/issues/435#issuecomment-810383024,1,['log'],['logs']
Testability,"n I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \; --mode calling \; --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs; 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155:1052,test,testdata,1052,,https://github.com/google/deepvariant/issues/155,1,['test'],['testdata']
Testability,"n Power8 & Redhat 7.5. 1. Build the following packages with ""/usr/bin/gcc""; cmake 3.13.3; Protobuf 3.6.1 C++ (static build with --enable-static for bazel); bazel 0.15.0. 2. Install Advance Toolchain 11.0 and build the following packages with /opt/at11.0/bin/gcc; Python 2 and Pip 19.0.2; Protobuf 3.6.1 C++ (uninstall static and build shared); Protobuf 3.6.1 Python (should build and install from source or CLIF will fail); TensorFlow 1.12.0 (fix floatn.h error with the link Floatn.h error: https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f); CLIF; Opencv-python 3.4.5.20 (for tensor2tensor install). Then build DeepVariant will pass, and results here (excepted //deepvariant/labeler:haplotype_labeler_test tracked in issue 154). ```; ================================================================================; (05:42:50) INFO: Elapsed time: 715.015s, Critical Path: 689.68s; (05:42:50) INFO: 1835 processes: 1835 local.; (05:42:50) INFO: Build completed, 1 test FAILED, 2433 total actions; //deepvariant:allelecounter_test PASSED in 0.1s; //deepvariant:call_variants_test PASSED in 59.8s; //deepvariant:data_providers_test PASSED in 11.8s; //deepvariant:dv_vcf_constants_test PASSED in 0.5s; //deepvariant:exclude_contigs_test PASSED in 1.6s; //deepvariant:haplotypes_test PASSED in 1.7s. â–½; //deepvariant:modeling_test PASSED in 48.2s; //deepvariant:pileup_image_test PASSED in 1.8s; //deepvariant:postprocess_variants_lib_test PASSED in 0.1s; //deepvariant:postprocess_variants_test PASSED in 4.8s; //deepvariant:resources_test PASSED in 1.8s; //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-464686381:1139,test,test,1139,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381,1,['test'],['test']
Testability,"n Singularity"" section in quick start test (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1084,test,testdata,1084,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"n asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71594,log,log,71594,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"n.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)"";",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5911,mock,mock,5911,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['mock'],['mock']
Testability,"n_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2277,test,testda,2277,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['test'],['testda']
Testability,nce/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branc,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:59566,Log,Logits,59566,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"ncy region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiment",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:2366,test,test,2366,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,1,['test'],['test']
Testability,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:3151,test,test,3151,,https://github.com/google/deepvariant/issues/485,1,['test'],['test']
Testability,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1609,test,test,1609,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,4,"['log', 'test']","['logic', 'logs', 'test', 'tests']"
Testability,"neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/ki",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1487,test,testdata,1487,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"nfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_googl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:2483,log,logs,2483,,https://github.com/google/deepvariant/issues/435,1,['log'],['logs']
Testability,"ngularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:1611,test,testdata,1611,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"nherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:6384,test,tests,6384,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['tests']
Testability,"ning \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know wh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:1540,test,testing,1540,,https://github.com/google/deepvariant/issues/251,1,['test'],['testing']
Testability,"nomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 byte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:17018,test,testdata,17018,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['test'],['testdata']
Testability,"non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency estimate for each alternate allele"">; ##INFO=<ID=AQ,Number=A,Type=Integer,Description=""Allele Quality score reflecting evidence for each alternate allele (Phred scale)"">; ##INFO=<ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##FILTER=<ID=MONOALLELIC,Description=""Site represents one ALT allele in a region with multiple variants that could not be unified into non-overlapping multi-allelic sites"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype Likelihoods"">; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:4883,assert,assertion,4883,,https://github.com/google/deepvariant/issues/633,1,['assert'],['assertion']
Testability,nput/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21:24:43.683619 140528910345984 make_examples.py:648] Task 0/8: 5700 candidates (6127 examples) [24.04s elapsed]; I0622 21:24:44.784906 139897026688768 make_examples.py:648] Task 4/8: 6002 candidates (6422 examples) [6.72s elapsed]; I0622 21:24:46.344424 139897026688768 make_examples.py:648] Task 4/8: Found 6004 candidate variants; I0622 21:24:46.344626 139897026688768 make_examples.py:648] Task 4/8: Created 6424 examples; I0622 21:24:58.252706 140528910345984 make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]; I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants; I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples; ```. Here is a snapshot,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:2439,log,log,2439,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability,"nsorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcuda.so.1: cannot open shared object file: No such file or directory. I additionally tried using docker to run the docker images (which worked for me with DeepVariant v.7.0.0, but not 9.0.0): ; `# Pull the deep variant docker image.; sregistry pull docker://gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; â€‹. # Test running docker interactively w/Singularity. ; BIN_VERSION=""0.9.0""; singularity shell --bind '/labs/jandr/walter/tb/test' /home/kwalter/.singularity/shub/deepvariant-docker-deepvariant:${BIN_VERSION}.simg;. /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=ucsc.hg19.chr20.unittest.fasta \; --reads=NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; `; I got the same errors with this test. It seems like there is some issue with numpy/tensorflow in this most recent version of the image that makes it incompatible with running via Singularity. Any suggestions or advice would be great. Thank you in advance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265#issuecomment-580549772:3678,Test,Test,3678,,https://github.com/google/deepvariant/issues/265#issuecomment-580549772,3,"['Test', 'test']","['Test', 'test']"
Testability,"nsorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103498,log,log,103498,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"nt it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory â€˜logsâ€™: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3836,log,logs,3836,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['log'],['logs']
Testability,"nt/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126163,test,testlogs,126163,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"nt/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:1864,test,test,1864,,https://github.com/google/deepvariant/issues/592,2,['test'],['test']
Testability,"nt/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main; write_variants_to_vcf(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf; for variant in variant_iterable:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? ; No, the quick start and also chr22 from the same sample ran through. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:5715,test,test,5715,,https://github.com/google/deepvariant/issues/517,2,['test'],['test']
Testability,"nt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; parallel: This job failed:; Using mount point: /input-gcsfused-48; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; mkdir -p ./input-gcsfused-48 && gcsfuse --implicit-dirs gbsc-gcp-project-udn-dev-deep-variant /input-gcsfused-48 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/examples/examples_output.tfrecord@64.gz --reads /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam --ref /mnt/google/.google/input/gbsc-gcp-project-udn-dev-test/hs37d5_ref/hs37d5.fa --task 48 --regions gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed. operation id: 3776708258517585322. $ gsutil ls gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed; gs://gbsc-gcp-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437503051:4296,test,test,4296,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051,1,['test'],['test']
Testability,"nt/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 455, in _ensure_consistent_contigs; min_coverage_fraction); File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 527, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 4641652 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""NC_000913.3"" is 4641652 bp and IS MISSING; ```. I am attaching here all the files I used (the BAM file is too big so I attach only the header). [sequence.fasta.txt](https://github.com/google/deepvariant/files/2690298/sequence.fasta.txt); [sequence.fasta.fai.txt](https://github.com/google/deepvariant/files/2690309/sequence.fasta.fai.txt); [aligned_reads_header.sam.txt](https://github.com/google/deepvariant/files/2690312/aligned_reads_header.sam.txt); [variants.bed.txt](https://github.com/google/deepvariant/files/2690313/variants.bed.txt); [variants.vcf.gz](https://github.com/google/deepvariant/files/2690314/variants.vcf.gz); [make_examples.log](https://github.com/google/deepvariant/files/2690369/make_examples.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:4682,log,log,4682,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,2,['log'],['log']
Testability,"nt/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78694,test,testing,78694,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,nt/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118779,test,testlogs,118779,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"nt_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123591,test,testlogs,123591,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"nternal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71069,log,log,71069,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ntig names should match. Looking at [VCF Specs](https://samtools.github.io/hts-specs/VCFv4.2.pdf) it looks like '#contig' header is optional, but I think Nucleous library (that is used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1163,test,testdata,1163,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"o docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s; user 0m0.037s; sys 0m0.013s; ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:2447,log,log,2447,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,2,['log'],['log']
Testability,"oad the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106816,log,log,106816,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"odule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82806,log,log,82806,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"odule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:90151,log,log,90151,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"odule>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:95048,log,log,95048,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"oject ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1363,log,log,1363,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['log'],['log']
Testability,"oject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; parallel: This job failed:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:5491,test,testdata,5491,,https://github.com/google/deepvariant/issues/678,1,['test'],['testdata']
Testability,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:; ```; pichuan@pichuan-gpu:~$ uname -a; Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux; ```. # Install GPU driver and Singularity on the machine:; ```; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash; ```. Singularity version:; ```; pichuan@pichuan-gpu:~$ singularity --version; singularity version 3.7.0; ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:757,test,test,757,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725,2,['test'],['test']
Testability,oke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118575,test,testlogs,118575,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ome common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106934,test,testlogs,106934,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"omwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL; > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks; > Ordering chunks by estimated depth; > Setup complete, beginning run; > Polishing 3% complete (46/1342). Estimated time remaining: unknown; > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s; > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s; > Polishing",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:6564,log,logs,6564,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,"on us-east1; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be havi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6181,test,test,6181,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['test'],['test']
Testability,"on"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3404,test,testdata,3404,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"on3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 444, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2865, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3697, in _create_op_internal; ret = Operation(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). real	41m45.880s; user	1063m44.358s; sys	25m21.900s; INFO: Cleaning up image...; ERROR: failed to delete container image tempDir /tmp/pbs.1173981.omics/rootfs-2853380811: unlinkat /tmp/pbs.1173981.omics/rootfs-2853380811/tmp-rootfs-1307439201/opt/traps/lib/libmodule64.so: permission denied; singularity/3.10.0 is unloaded . - `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No; **Any additional context:**; Input BAM file seems valid. Checked with samtools quickcheck -v command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:17439,test,test,17439,,https://github.com/google/deepvariant/issues/564,2,['test'],['test']
Testability,"on?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:1097,test,test,1097,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,1,['test'],['test']
Testability,"ons_to_include, contig_dict)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; ```. I've added the BED file to the public bucket:; gs://public-debug/exomes.bed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437689349:4118,test,testdata,4118,,https://github.com/google/deepvariant/issues/116#issuecomment-437689349,1,['test'],['testdata']
Testability,"ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log; -------; STARTING DEEPVARIANT; I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/; I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****; ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:22608,log,log,22608,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"oot 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1879,test,test,1879,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"oot root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1960,test,test,1960,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"oot/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2293,test,testlogs,2293,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ope-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2577,log,logs,2577,,https://github.com/google/deepvariant/issues/129,1,['log'],['logs']
Testability,"or PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1269,log,log,1269,,https://github.com/google/deepvariant/issues/677,3,"['benchmark', 'log']","['benchmark', 'benchmarks', 'log']"
Testability,"or message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:75710,test,testing,75710,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,"or. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1197,log,logs,1197,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,"or/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12476,log,log,12476,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,ore.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9873,test,testdata,9873,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"ore/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8094,test,test,8094,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,['test'],['test']
Testability,"orflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106586,test,testlogs,106586,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"orflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71012,test,testlogs,71012,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"orflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87947,test,testlogs,87947,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880); [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1); [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_; [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:5671,log,login,5671,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,1,['log'],['login']
Testability,"ot/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124169,log,log,124169,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ot/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125520,log,log,125520,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ot/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally.; There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126395,log,log,126395,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,17,"['log', 'test']","['log', 'test', 'testlogs', 'tests']"
Testability,"ot/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122861,log,log,122861,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"otnote6)</sup>2 HG001, 3 HG002, 3 HG003, 3 HG004, 7 HG005, 6 HG006, 6 HG007, 2 NA12891, 2 NA12892 | 457,420,038; ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>9 HG001, 7 HG002, 7 HG003, 7 HG004, 8 HG005, 8 HG006, 8 HG007, 9 NA12891, 9 NA12892 | 27,783,324 |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | <sup>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:1977,test,testdata,1977,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"ouble-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3327,log,log,3327,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['log'],['log']
Testability,"ow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77447,test,testlogs,77447,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71187,test,testlogs,71187,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106468,log,log,106468,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"p.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106642,log,log,106642,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30650,test,testlogs,30650,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 312 in <module>; parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_sampl. real 1154m27.406s; user 287m31.460s; sys 2m20.121s; I0509 05:42:42.322269 47221860157248 run_deeptrio.py:674] None. Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""/paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta"" --reads_parent1 ""/paedyl01/disk1/yangyxt/wesplus/50_samples_; Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error; `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:4831,test,test,4831,,https://github.com/google/deepvariant/issues/646,2,['test'],['test']
Testability,parallel: This job failed reproducing the test example,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:42,test,test,42,,https://github.com/google/deepvariant/issues/325,1,['test'],['test']
Testability,"pes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""e2-medium"" \; --zone ""us-west1-b""; ```. After ssh into the machine, I ran:. ```; sudo apt -y update && sudo apt -y install docker.io; ```. And then followed the steps here:. https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432#issuecomment-806341687:1249,log,log,1249,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687,1,['log'],['log']
Testability,"piens_assembly38.fasta"" --reads ""/hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v4/R202302180101/R202302180101.pbmm2.CCS.alignment.sort.bam"" --examples ""/tmp/tmp8_1neaqr/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmp8_1neaqr/gvcf.tfrecord@20.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_nwff5xo0/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; warning: /tmp/Bazel.runfiles_4ji1hg9j/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so: write error (disk full?). Continue? (y/n/^C) ; warning: /tmp/Bazel.runfiles__5bs6aw8/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:2240,Assert,AssertionError,2240,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,1,['Assert'],['AssertionError']
Testability,"pileup_image_height? Yes, our depths can be as much as a few thousands. I havenâ€™t looked at the data set yet, but thatâ€™s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. â€”; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-365324431:432,test,test,432,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431,1,['test'],['test']
Testability,"pileup_images( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 534, in create_pileup_images ; pileup = _pileup_for_pair_of_alts(alts) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 486, in _pileup_for_pair_of_alts ; ref_image = self.build_pileup( ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 435, in build_pileup ; build_pileup_for_one_sample(reads_for_samples[i], sample)) ; File ""/tmp/Bazel.runfiles_gd__toh4/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 355, in build_pileup_for_one_sample ; rows = ([self._encoder.encode_reference(refbases)] * ; ImportError: numpy.core.multiarray failed to import ; I0516 14:57:26.170286 139707932989248 make_examples_core.py:257] Task 0/2: Writing example info to ./make_examples.tfrecord-00000-of-00002.gz.example_info.json ; I0516 14:57:26.170386 139707932989248 make_examples_core.py:2273] example_shape = None ; I0516 14:57:26.170525 139707932989248 make_examples_core.py:2274] example_channels = [1, 2, 3, 4, 5, 6, 19] ; I0516 14:57:26.170763 139707932989248 make_examples_core.py:257] Task 0/2: Found 0 candidate variants ; I0516 14:57:26.170813 139707932989248 make_examples_core.py:257] Task 0/2: Created 0 examples ; parallel: This job failed: ; /opt/deepvariant/bin/make_examples --mode calling --ref genome.fasta --reads test.paired_end.sorted.cram --examples ./make_examples.tfrecord@2.gz --channels ; insert_size --gvcf ./gvcf.tfrecord@2.gz --regions genome.bed --task 1 ; ```. And the command is; ```bash; /opt/deepvariant/bin/run_deepvariant \; --ref=genome.fasta \; --reads=test.paired_end.sorted.cram \; --output_vcf=test_out.vcf.gz \; --output_gvcf=test_out.g.vcf.gz \; --model_type=WGS \; --regions genome.bed \; --intermediate_results_dir=. \; --num_shards=2; ```. Deepvariant version: 1.5.0 (the official docker container); Ran on Ubuntu 20.04 LTS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1549844072:2955,test,test,2955,,https://github.com/google/deepvariant/issues/640#issuecomment-1549844072,2,['test'],['test']
Testability,"ples_runner; regions, calling_regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes, the quick test run as normal.; ```. 3. reference index does; ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:7033,test,test,7033,,https://github.com/google/deepvariant/issues/653,2,['test'],['test']
Testability,"portError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:70737,test,tests,70737,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"portError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106140,test,tests,106140,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"put ${outdir2}/${sample}_haplotagged.bam \ #generate ""C1_haplotagged.bam""ã€""F1_haplotagged.bam""ã€""M1_haplotagged.bam""; --reference ${ref}; ${outdir2}/${sample}_deepvariant1.phased.vcf.gz; ${bam}. samtools index ${outdir2}/${sample}_haplotagged.bam. â‘£The final result of variant calling; #DeepTrio version:1.2.0; /opt/deepvariant/bin/deeptrio/run_deeptrio; --model_type PACBIO; --ref ${ref}; --reads_child ${outdir2}/C1_haplotagged.bam; --reads_parent1 ${outdir2}/F1_haplotagged.bam; --reads_parent2 ${outdir2}/M1_haplotagged.bam; --output_vcf_child ${outdir4}/C1.output.vcf.gz; --output_vcf_parent1 ${outdir4}/F1.output.vcf.gz; --output_vcf_parent2 ${outdir4}/M1.output.vcf.gz; --sample_name_child 'C1'; --sample_name_parent1 'F1'; --sample_name_parent2 'M1'; --num_shards 8; --output_gvcf_child ${outdir4}/C1.g.vcf.gz; --output_gvcf_parent1 ${outdir4}/F1.g.vcf.gz; --output_gvcf_parent2 ${outdir4}/M1.g.vcf.gz; --use_hp_information. **However,the log file of DeepTrio still contains errorsï¼š**(The result files have been generated); cat log |grep -i error; W1008 21:26:50.592375 47245352568640 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 09:06:50.674110 47029842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:1582,log,log,1582,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,2,['log'],['log']
Testability,"put. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1187,log,logs,1187,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['log'],['logs']
Testability,"pvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72645,log,log,72645,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"pvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107165,log,log,107165,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,pvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_tra,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71887,test,testlogs,71887,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122035,log,log,122035,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:16095,log,log,16095,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,"python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46943,test,testlogs,46943,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53530,test,testlogs,53530,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,p|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights/RMSProp|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weigh,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:52227,Log,Logits,52227,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]; I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s; user 0m37.643s; sys 0m6.426s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878; 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz; 2023-05-19 16:22:51.173038: I deepvariant/postprocess_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:12889,test,testdata,12889,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"r _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in off-target regions was noticeably lower). That said, I think comparisons with my own data are in the ballpark of what they were describing in what I quoted above, although I would have expected the indel accuracy to be in-between (perhaps something more like 90-95%). Nevertheless, even though I am trying to learn more about the DeepVariant, I apologize that I should have taken more time to consider my phrasing. For _point 4_, I think I understand your response, but that will probably be more clear as I do some testing with DeepVariant. Thank you again for your time and detailed response. So, please feel free to close this ticket. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:2549,test,testing,2549,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,1,['test'],['testing']
Testability,"r job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is there a better place to ask questions like this?. This is a good place to ask :); It's a public forum, so our team and everyone in the community can see and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1790,log,log,1790,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['log'],['log']
Testability,"r updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op.; I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p.; I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA...; I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]; I0327 13:32:39.551621 47138345245376 call_variants.py:404] Done evaluating variants. real 0m29.939s; user 0m20.774s; sys 0m5.396s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/scratch/moldach/bin/quickstart- testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --outfile ""/scratch/moldach/bin/quickstart-output/output.vcf.gz"" --non variant_site_tfrecord_path ""/tmp/tmp63xxmwmi/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/scra tch/moldach/bin/quickstart-output/output.g.vcf.gz"". 2020-03-27 13:32:43.691530: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/ tmp63xxmwmi/call_variants_output.tfrecord.gz; 2020-03-27 13:32:43.692692: I deepvariant/postprocess_variants.cc:97] Done reading: /t mp/tmp63xxmwmi/call_variants_output.tfrecord.gz. #entries in single_site_calls = 86; 2020-03-27 13:32:43.692820: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 86; 2020-03-27 13:32:43.692941: I deepvariant/postprocess_variants.cc:103] Start SortSingl eSiteCalls; 2020-03-27 13:32:43.693087: I deepvariant/postprocess_variants.cc:105] Done SortSingle SiteCalls; I0327 13:32:43.6935",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:10106,test,testdata,10106,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2122,test,test,2122,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"r1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4152,log,log,4152,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['log'],['log']
Testability,"r20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2647,test,testdata,2647,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"r20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python bin/make_examples.zip \; --mode calling \; --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; --channels ""insert_size""; ```. (To figure out which flags you need to add for each model, you can read https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253 . Sorry that we don't have better documentation than that right now). For how to run this with multiple shards, and how to run the rest of the commands, please read https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md. I just tested the steps above and confirmed that it worked for me on v1.4.0, at least for the make_examples step.; If you encounter more issues with other steps, please feel free to ask again. I'd be happy to help. Note that I don't plan to put this into an official documentation page now, because that adds to our maintenance burden to keep it up to date. Given that we have the Docker/Singularity solution that works generally well for our users, I don't expect many of our users to need to use pre-built binaries. @zivlang thank you for your question so I have a chance to test it again and document it here. Hopefully this is helpful for you. Happy to answer more questions if you encounter more problems.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:3227,test,tested,3227,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,2,['test'],"['test', 'tested']"
Testability,rAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; environment: {}; flags:; - ALWAYS_RUN; imageUri: google/cloud-sdk:alpine; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; environment: {}; resources:; projectId: valis-194104; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 10; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks: []; labels: {}; machineType: n1-standard-1; nvidiaDriverVersion: ''; preemptible: false; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/cloud-platform; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; zones:; - us-west1-b; timeout: 604800s; startTime: '2018-11-08T14:27:06.604193Z'; name: projects/valis-194104/operations/12097970745380060156; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:8495,log,logs,8495,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,2,['log'],"['log', 'logs']"
Testability,"re --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:10373,test,test,10373,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],['test']
Testability,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:3649,test,test,3649,,https://github.com/google/deepvariant/issues/446,5,"['log', 'test']","['log', 'logging', 'test', 'tested']"
Testability,"reating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1492,log,logs,1492,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:; - **v0.9.0**: 290+1494+281 = 2065min (~34h); - **v1.0.0**: 335+1487+300 = 2122min (~35h); - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:; - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall); - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall); - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):; ```; singularity exec \; --bind /data/ref/genomes/GRCh38:/genomes \; --bind /data/projects/HICF2_project/BAM:/bam_files \; /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \; --reads=""/bam_files/${bamfile}"" \; --output_vcf=""VCF/${sampleID}.vcf.gz"" \; --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \; --intermediate_results_dir=""tmp_data"" \; --num_shards=10; ```. I've uploaded the log files from the 3 runs if you want to take a look:; [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log); [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log); [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-698786904:1735,log,log,1735,,https://github.com/google/deepvariant/issues/346#issuecomment-698786904,7,['log'],['log']
Testability,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:3942,test,test,3942,,https://github.com/google/deepvariant/issues/374,2,['test'],['test']
Testability,"remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 24æ—¥ æ˜ŸæœŸäº” 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scip",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:14010,mock,mock,14010,,https://github.com/google/deepvariant/issues/89,1,['mock'],['mock']
Testability,"remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018å¹´ 08æœˆ 28æ—¥ æ˜ŸæœŸäºŒ 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10739,mock,mock,10739,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['mock'],['mock']
Testability,"requency). The neural net assigns a genotype probability for all of these positions. The confidence that the neural network has in the variant call is represented in the GQ field (which is the phred-encoded probability that the assigned genotype is correct). When the probability for reference is >50% and 99% (GQ20), the genotype assigned is ./. when the probability for reference is >99% (GQ20+) the genotype assigned is 0/0. In your gVCF calls, only this row is a REF call made by the neural net:; ```; 1	69897	.	T	C,<*>	0.7	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:8:6:2,4,0:0.666667,0:0,8,13,990,990,990; ```. The second way that DeepVariant makes a no-call/reference-call is in the process of gVCF generation. The gVCF was designed as a way to encode the probability of a region with no variant call information is a reference, so that later in joint genotyping of many samples, you can potentially change the genotype call based on what you see in the population. In your gVCF calls all other REF calls fall into this category. DeepVariant makes a gVCF with REF-CALL blocks over stretches where it observes no candidates, with a heuristic logic based on coverage and support determining the probability of these calls. In calls of this nature, there is a different logic that does not apply the GQ20 threshold. In joint genotyping, GLnexus will use the probabilities to determine the VCF call. . In summary, a variant (0/1 or 1/1) will have a variant call if it is the most likely genotype at the position.; A position will receive a 0/0 call if the model observed a site with >GQ20. It can make sense to filter DeepVariant results if you have a substantial preference for precision. In addition to higher overall accuracy, we try to make DeepVariant report well-calibrated confidence probabilities. If you need higher precision for the variant calls, filtering on the call GQ is the best field. This was a long answer. Please let me know what areas remain unclear after reading it. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/639#issuecomment-1526164168:1560,log,logic,1560,,https://github.com/google/deepvariant/issues/639#issuecomment-1526164168,2,['log'],['logic']
Testability,rev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110444,Log,Logits,110444,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"rflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100240,test,testlogs,100240,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"rflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8046,test,test,8046,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"rflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46999,log,log,46999,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"rflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53586,log,log,53586,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"riant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124287,test,testlogs,124287,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"riant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125638,test,testlogs,125638,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"riant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; Using mount point: /input-gcsfused-0; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437596560:3647,test,testdata,3647,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560,1,['test'],['testdata']
Testability,"rm"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. ```; python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1532,test,testdata,1532,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['test'],['testdata']
Testability,"rmTMP/JobSpecificFolder"" (${TMPDIR}); ```. cd ${TMPDIR}; BIN_VERSION=""1.6.1""; module load singularity/3.5.2. #####################################################################; # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**; # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]; then; cp ""${THEREF}""* ./; cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* .; chmod 666 `basename ""${THEREF}""`*; chmod 666 ""${ALIGNMENTNAME}.bam""*; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=`basename ""${THEREF}""` \; --reads=""${ALIGNMENTNAME}.bam"" \; --sample_name=${SAMPLENAME} \; --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \; --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \; --intermediate_results_dir . \; --num_shards=8 \; --logging_dir=.; ; if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]; then; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \; /opt/deepvariant/bin/postprocess_variants \; --ref=`basename ""${THEREF}""` \; --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \; --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \; --cpus ""8"" \; --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \; --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \; --sample_name=${SAMPLENAME}; fi; cp *.log ${WORKINDIR}/; cp ""./${ALIGNMENTNAME}.deepVariant.vcf.gz""* ${WORKINDIR}/; else; cp ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz""* .; fi. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2106784467:2010,log,log,2010,,https://github.com/google/deepvariant/issues/818#issuecomment-2106784467,1,['log'],['log']
Testability,"rnal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14381,log,log,14381,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1819,log,log,1819,,https://github.com/google/deepvariant/issues/225,1,['log'],['log']
Testability,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:108221,test,testlogs,108221,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2024,log,log,2024,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,1,['log'],['log']
Testability,"rror_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14106,log,logging,14106,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['log'],['logging']
Testability,"runcated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 13979615457049",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16663,test,testdata,16663,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['test'],['testdata']
Testability,"runcated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688877 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16799,test,testdata,16799,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['test'],['testdata']
Testability,"running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your script to explicitly use the 0.8.0 image at ` gcr.io/deepvariant-docker/deepvariant:0.8.0`, which is now tagged as `latest`. Models are included in this image. The WES model checkpoint for this image is at `/opt/models/wes/model.ckpt`. To run the v0.8.0 container in interactive mode, you will need to modify the entrypoint as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:1898,test,tests,1898,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,1,['test'],['tests']
Testability,"runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:7674,test,test,7674,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"ry using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --inpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1203,log,log,1203,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"s able to build deepvariant! Tests failed, below, and I am happy to open a separate issue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```; FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1062,log,log,1062,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2643,test,test,2643,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100,1,['test'],['test']
Testability,"s like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > An",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1263,log,logs,1263,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['log'],['logs']
Testability,"s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7139,test,test,7139,,https://github.com/google/deepvariant/issues/171,2,['test'],['test']
Testability,s to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1423,test,test,1423,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4777,test,test,4777,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['test'],['test']
Testability,"s(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads; return utils.reservoir_sample(iterable_of_reads, k, random); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample; for i, item in enumerate(iterable):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:3875,test,testdata,3875,,https://github.com/google/deepvariant/issues/812,1,['test'],['testdata']
Testability,"s.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/comp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:4872,test,test,4872,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"s/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolera",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:15771,log,log,15771,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,"s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14004,Log,Logging,14004,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['Log'],['Logging']
Testability,s_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_cor,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10739,test,testdata,10739,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,s_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11369,test,testdata,11369,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"s_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:1580,test,test,1580,,https://github.com/google/deepvariant/issues/818,2,['test'],['test']
Testability,"scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3464,test,test,3464,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"se the r1.5 one (the doc is mostly the same , but I'll remember to use the 1.5 code): https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md. To get a machine, I used a command here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I sshed into the machine. ```bash; gcloud compute ssh pichuan-cpu --zone us-west1-b; ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.5; ```. And I confirmed the version:. ```; pichuan@pichuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:1216,log,log,1216,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['log'],['log']
Testability,"sfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:; ```; -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1055,test,test,1055,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"sg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16653,test,tests,16653,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"sg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30383,test,tests,30383,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3237,test,testdata,3237,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8686,benchmark,benchmark-test,8686,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,['benchmark'],['benchmark-test']
Testability,"sk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is one that I recommend you actually read. The last line gave you an example of how you'd run `call_variants`. Something like:. ```bash; python3 bazel-out/k8-opt/bin/deepvariant/call_variants.zip --help; ```; should work. ---. # Further development and debugging. And, from here, if you want to continue to iterate, modify code and re-build. . You can try:. ```bash; source settings.sh; ```. This should allow you to use `bazel` in your terminal. At this point if you run `bazel --help` you should see the usage. If you make any changes, you can run:. ```bash; bazel build -c opt ${DV_COPT_FLAGS} deepvariant/...; ```. to re-build all binaries. (And can use `bazel test` to run unit tests). If you're specially looking into just one build target, you can also run something like:. ```bash; bazel build -c opt ${DV_COPT_FLAGS} deepvariant/call_variants; ```. And, if you want to run the built binary, in addition to the one of using python to run, you can also directly run something like. ```bash; bazel-bin/deepvariant/call_variants --help; ```. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872:2664,test,test,2664,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872,2,['test'],"['test', 'tests']"
Testability,"sk=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resourc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2049,log,log,2049,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"sl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```. _Originally posted by @aderzelle in https://github.com/google/deepvariant/issues/268#issuecomment-586584341_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:3900,log,logs,3900,,https://github.com/google/deepvariant/issues/273,2,['log'],"['log', 'logs']"
Testability,"sorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:74978,test,testlogs,74978,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding alleleÂ isÂ called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; â€“ | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:3931,assert,assertion,3931,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954,1,['assert'],['assertion']
Testability,"sr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:4780,test,test,4780,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ss.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1276,log,logged,1276,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,2,['log'],['logged']
Testability,"ss.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Ð”Ð¸ÑÑÐµÑ€Ñ‚Ð°Ñ†Ð¸Ñ/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **k",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3369,test,test,3369,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hope it helps and have a great weekend!; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:12308,test,tests,12308,,https://github.com/google/deepvariant/issues/50,1,['test'],['tests']
Testability,"st probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:1461,assert,assertion,1461,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954,1,['assert'],['assertion']
Testability,"st-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8615,test,test,8615,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"start-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test PASSED in 49.7s; Stats over 10 runs: max = 49.7s, min = 2.6s, avg = 9.1s, dev = 13.6s; //deepvariant:model_train_test PASSED in 127.0s; Stats over 10 runs: max = 127.0s, min = 2.7s, avg = 42.5s, dev = 47.3s. Executed 38 out of 38 tests: 37 tests pass and 1 fails locally.; (06:34:35) INFO: Build completed, 1 test FAILED, 2471 total actions; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:5826,test,testlogs,5826,,https://github.com/google/deepvariant/issues/131,9,"['log', 'test']","['log', 'test', 'testlogs', 'tests']"
Testability,"stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had hig",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1292,test,test,1292,,https://github.com/google/deepvariant/issues/876,1,['test'],['test']
Testability,"su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA128",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:3849,test,testdata,3849,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,"sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main; proto_utils.uses_fast_cpp_protos_or_die(); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die; raise ValueError('Expected to be using C++ protobuf implementation '; ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:5477,test,test,5477,,https://github.com/google/deepvariant/issues/499,2,['test'],['test']
Testability,"sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374, in call_variants; raise ValueError(f'Shape mismatch in {example_info_json} and '; ValueError: Shape mismatch in ./examples.tfrecord-00000-of-00032.gz.example_info.json and /opt/models/pacbio/model.ckpt.example_info.json.; ```. My command line looks like this:; `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --add_hp_channel --add_hp_channel --alt_aligned_pileup diff_channels --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/HG002.merged.bam --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --realign_reads --regions 20 --sample_name HG002 --split_skip_reads --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --batch_size 512 --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes ok. Do you have some input on this?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:3166,log,log,3166,,https://github.com/google/deepvariant/issues/628,3,['log'],['log']
Testability,"t (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78345,test,testdata,78345,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testdata']
Testability,"t *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100298,log,log,100298,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"t and log file. code is running but neither output generating or error throwing just running.; please see below code and log file. ###### code #############; #!/usr/bin/env nextflow. nextflow.enable.dsl=2; params.outdir = '/home/deepak/integration/resu1'; params.data_dir = '/home/deepak/integration/resu1/4.markDupliM'; params.refhg38 = '/home/deepak/integration/hg381_22XYM'; params.bed = '/home/deepak/integration'. workflow {; // Define channels for input data; Channel; .fromPath(""${params.data_dir}/*_sorted_md.bam""); .map { file -> ; def sample_id = file.baseName.replace('_sorted_md', ''); return [sample_id, file]; }; .set { read_pairs }; /// Step 1. DeepVariant; DeepVariant(read_pairs, params.refhg38, params.bed); }. process DeepVariant {; tag ""deepavar on ${sample_id}""; publishDir ""${params.outdir}/5.finaleepvar"", mode: 'copy'; cpus 4; //BIN_VERSION 1.6.1. input:; tuple val(sample_id), path(read_files); val(params.refhg38); val(params.bed); ; output:; //tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), path(""${sample_id}_rawd.gvcf.gz""), emit: raw_vcfs; tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), emit: raw_vcfs. script:; """"""; docker run \; -v ""${params.data_dir}"":/opt/bam -v ""${params.refhg38}"":/opt/refhg38 -v ""${params.bed}"":/opt/bed \; google/deepvariant:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /opt/refhg38/Homo_sapiens_assembly38cleaned.fasta \; --reads /opt/bam/${read_files} \; --regions /opt/bed/hg38_exomeY.bed \; --output_vcf /opt/bam/${sample_id}_rawd.vcf.gz \; --num_shards ${task.cpus}; """"""; }. ######## code ################. terminal:; (base) deepak@ubuntu22:~/integration$ nextflow run final_deepvarian.nf . N E X T F L O W ~ version 24.04.4. Launching `final_deepvarian.nf` [hungry_stonebraker] DSL2 - revision: 4dab17f4f2. executor > local (1); [dd/64034b] DeepVariant (deepavar on SRR26512958) [ 0%] 0 of 2. log file attached; [nextflow.log](https://github.com/user-attachments/files/17008943/nextflow.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013:1984,log,log,1984,,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013,3,['log'],['log']
Testability,"t data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; T",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1360,test,testdata,1360,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"t-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:2005,test,testdata,2005,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:ðŸ†‘:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:2180,test,test,2180,,https://github.com/google/deepvariant/issues/380,2,['test'],['test']
Testability,"t/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125695,log,log,125695,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"t/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122603,log,log,122603,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,t/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119417,test,testlogs,119417,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"t/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/po",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:2509,log,log,2509,,https://github.com/google/deepvariant/issues/435,1,['log'],['log']
Testability,"t/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1702,test,testlogs,1702,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,t/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (sh,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1528,test,testlogs,1528,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"t/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8244,test,test,8244,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,"t_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/D",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2398,test,test,2398,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"tart running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3637,Log,Log,3637,,https://github.com/google/deepvariant/issues/151,1,['Log'],['Log']
Testability,"tch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; export BAZEL_PYTHON=/home/qilibj/inst/bin/python; export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11""; # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:18723,test,test,18723,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['test'],['test']
Testability,"te-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100474,log,log,100474,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"te-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Trace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16918,test,testlogs,16918,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; Fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92575,test,testlogs,92575,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92619,log,log,92619,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"tern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. I cannot figure out the reason why this divided approach succeeded. Could you tell me any suggestions of points to check?; Because I'm using personal data under strict data management, I cannot provide or share our tfrecords. I used apache-beam v2.9.0 in python 2.7.5 (not in docker of DeepVariant v0.7.1).; If some version dependencies exist, I'd like to get some docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1857,log,log,1857,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"ternal PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1072,benchmark,benchmark,1072,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,1,['benchmark'],['benchmark']
Testability,test,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426:0,test,test,0,,https://github.com/google/deepvariant/issues/426,1,['test'],['test']
Testability,test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118152,test,testlogs,118152,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,test 2,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426#issuecomment-782298000:0,test,test,0,,https://github.com/google/deepvariant/issues/426#issuecomment-782298000,1,['test'],['test']
Testability,test issue,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/1:0,test,test,0,,https://github.com/google/deepvariant/issues/1,1,['test'],['test']
Testability,test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1469,test,test,1469,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from skle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21004,assert,assertEqual,21004,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['assert'],['assertEqual']
Testability,test_make_labeler_ref fail due to mock data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:34,mock,mock,34,,https://github.com/google/deepvariant/issues/154,1,['mock'],['mock']
Testability,"tet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2331,log,log,2331,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,2,"['benchmark', 'log']","['benchmark', 'log']"
Testability,"thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1370,log,log,1370,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['log'],['log']
Testability,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):; Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; Â Â Â  tf.app.run(); Â  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; Â Â Â  _sys.exit(main(_sys.argv[:1] + flags_passthrough)); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main; Â Â Â  make_examples_runner(options); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner; Â Â Â  regions = processing_regions_from_options(options); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options; Â Â Â  options.min_shared_contigs_basepairs); Â  File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:183,Log,Logging,183,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,2,"['Log', 'test']","['Logging', 'testdata']"
Testability,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-902138384:568,test,test,568,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384,1,['test'],['test']
Testability,"thanks for your responses. command lineï¼š; docker run \; -v /sfs-grand-med-research/:/sfs-grand-med-research/ \; swr.cn-north-1.myhuaweicloud.com/grand-med-clinical/deepvariant:""1.0.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=human_g1k_v37.main_chrom.fasta \; --reads=202022.hg19.pbmm2.sort.MT.bam \; --output_vcf=202022.MT.vcf.gz \; --num_shards=16 \; --intermediate_results_dir=/tmp/. logfileï¼š; [hg19_MT_deepvariant.log](https://github.com/google/deepvariant/files/5436542/hg19_MT_deepvariant.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/366#issuecomment-716272982:417,log,logfile,417,,https://github.com/google/deepvariant/issues/366#issuecomment-716272982,3,['log'],"['log', 'logfile']"
Testability,"that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1555,Benchmark,Benchmark,1555,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,4,"['Benchmark', 'test']","['Benchmark', 'test', 'tested']"
Testability,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1687,test,test,1687,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,2,['test'],['test']
Testability,"these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4291,test,testing,4291,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,1,['test'],['testing']
Testability,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:21329,test,testlogs,21329,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_inte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23505,log,log,23505,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:62264,test,testlogs,62264,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_inte",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:110392,log,log,110392,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2986,Log,Log,2986,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['Log'],['Log']
Testability,"thub.com/sylabs|github.com/hpcng|' | \; bash -x; ```. Here's the version:; ```; pichuan@pichuan-test-speed:~$ singularity --version; singularity version 3.7.0; ```. I followed:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; to download the data. And then:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions input/idt_capture_novogene.grch38.bed \; --output_vcf output/HG003.output.vcf.gz \; --output_gvcf output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir output/intermediate_results_dir 2>&1 | tee /tmp/all.log; ```. I'll paste part of the log of each step so that you can compare. ## make_examples; make_examples speed is roughly:; ```; I0622 21:19:25.373434 140610510067456 make_examples.py:648] Task 7/8: 4900 candidates (5187 examples) [27.99s elapsed]; I0622 21:19:35.260825 139809239041792 make_examples.py:648] Task 1/8: 4809 candidates (5065 examples) [32.79s elapsed]; I0622 21:19:37.868103 139727062120192 make_examples.py:648] Task 2/8: 4900 candidates (5208 examples) [37.92s elapsed]; I0622 21:19:37.739557 139786800707328 make_examples.py:648] Task 6/8: 5100 candidates (5441 examples) [29.08s elapsed]; I0622 21:19:44.484720 140667007305472 make_examples.py:648] Task 5/8: 4902 candidates (5241 examples) [37.78s elapsed]; ```. Here are the last few lines from the log:; ```; I0622 21:24:34.005878 140667007305472 make_examples.py:648] Task 5/8: Created 6240 examples; I0622 21:24:38.061186 139897026688768 make_examples.py:648] Task 4/8: 5906 candidates (6318 examples) [17.72s elapsed]; I0622 21",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:1666,log,log,1666,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability,"ticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz; SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:1302,test,test,1302,,https://github.com/google/deepvariant/issues/518,1,['test'],['test']
Testability,timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:ha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118985,test,testlogs,118985,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ting system: Ubuntu 22.04.2 LTS; > * DeepVariant version: 1.6.1; > * Installation method (Docker, built from source, etc.): Docker; > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format.; > ; > **Steps to reproduce:**; > ; > * Command: sudo docker run ; > -v ""${INPUT_DIR}"":""/input"" ; > -v ""${OUTPUT_DIR}"":""/output"" ; > google/deepvariant:""${BIN_VERSION}"" ; > /opt/deepvariant/bin/run_deepvariant ; > --model_type=PACBIO ; > --ref=/input/RILWLs1.fasta ; > --reads=/input/Out.fastq ; > --output_vcf=/output/output.vcf.gz ; > --output_gvcf=/output/output.g.vcf.gz ; > --intermediate_results_dir /output/intermediate_results_dir ; > --num_shards=15; > * Error trace: (if applicable); > ; > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.gtqyKpmVpHdy0Yw9XgACJqtqoRcB3SuNknzCYOE8y-g); > ; > Is there any way to reproduce the issue by using the quick start?; > ; > **Any additional context:**. Its a 256GB RAM syst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080:1212,Test,Test,1212,,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080,1,['Test'],['Test']
Testability,tionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Logits/Conv2d_1c_1x1/biases/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:42445,Log,Logits,42445,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 190, in write_tfrecords; for proto in protos:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 111, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in _transform_call_variants_output_to_variants; yield _transform_call_variant_group_to_output_variant(**cvo_group_kwargs); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1036, in _transform_call_variant_group_to_output_variant; return add_call_to_variant(; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 434, in add_call_to_variant; gq, variant.quality = compute_quals(predictions, index); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 469, in compute_quals; genomics_math.ptrue_to_bounded_phred(predictions[prediction_index]); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py"", line 143, in ptrue_to_bounded_phred; raise ValueError('ptrue must be between zero and one: {}'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. I'm running this code on an H100 GPU running nvidia driver - `535.183.06` and CUDA version is `12.2`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:5558,test,test,5558,,https://github.com/google/deepvariant/issues/901,2,['test'],['test']
Testability,"tobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7391,test,test,7391,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['test'],['test']
Testability,"tps://genomics.googleapis.com/v1alpha2/operations?filter=projectId+%3D+isb-cgc-06-0004+AND+labels.job-id+%3D+call-varia--root--180503-233007-45&alt=json&pageSize=256; call-varia--root--180503-233007-45: FAILURE; [u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/03/2018 23:30:18 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-XX-XXXX', '--logging', 'gs://XXXXX/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:1152,log,logs,1152,,https://github.com/google/deepvariant/issues/70,1,['log'],['logs']
Testability,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:1603,test,test,1603,,https://github.com/google/deepvariant/issues/475,3,['test'],"['test', 'testdata', 'testing']"
Testability,"ts from 1342 chunks.; > Merging took 7s; > Merge cleanup took 0s; Separated reads with divisions: H1 475116, H2 453908, and H0 159194; > Wrote haplotyped bams in 1m 43s; > Finished phasing in 18m 46s. real	18m47.373s; user	245m51.236s; sys	2m8.903s; mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file; [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log; -------; [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED; [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET.; [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305; [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/; [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:; [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376; [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:9764,log,log,9764,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"ts20_sorted.bam with NativeSamReader; I0129 11:46:16.360527 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:16.361952 140471159555840 make_examples.py:946] Common contigs are [u'chr20']; I0129 11:46:16.501434 140471159555840 make_examples.py:1030] Writing examples to prj-NA12878/training-examples/training_set.with_label.tfrecord.gz; 2019-01-29 11:46:16.502209: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-01-29 11:46:16.550218: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.554270: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.556066 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.571476 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:20.140141 140471159555840 make_examples.py:782] Found 0 candidates in chr20:1-1000 [3.64s elapsed]; I0129 11:46:20.141022 140471159555840 make_examples.py:782] Found 0 candidates in chr20:1001-2000 [0.00s elapsed]; I0129 11:46:20.141855 140471159555840 make_examples.py:782] Found 0 candidates in chr20:2001-3000 [0.00s elapsed]; I0129 11:46:20.142673 140471159555840 make_examples.py:782] Found 0 candidates in chr20:3001-4000 [0.00s elapsed]; I0129 11:46:20.143475 140471159555840 make_examples.py:782] Found 0 candidates in chr20:4001-5000 [0.00s elapsed]; I0129 11:46:20.144277 140471159555840 make_examples.py:782] Found 0 candidates in chr20:5001-6000 [0.00s elapsed]; I0129 11:46:20.145082 140471159555840 make_examples.py:782] Found 0 candidates in chr20:6001-7000 [0.00s elapsed]; I0129 11:46:20.145899 140471159555840 make_examples.py:782] Found 0 candidates in chr20:7001-8000 [0.00s elapsed]; I0129 11:46:20.146692 140",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:3043,test,testdata,3043,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['test'],['testdata']
Testability,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355522771:1978,test,testing,1978,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771,4,['test'],['testing']
Testability,"tup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-10-31 20:36:34.101375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:1342,test,testdata,1342,,https://github.com/google/deepvariant/issues/901,1,['test'],['testdata']
Testability,"t}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/output/{output_file}.vcf '; '--output_gvcf=/output/{output_file}.g.vcf '; '--num_shards={threads} '; '--make_examples_extra_args min_mapping_quality=1,keep_supplementary_alignments=true '; '--intermediate_results_dir /output/{file_tmp} 1>{log} 2>{log}'); shell(""{bcftools} view -Oz -o {output.vcf_gz} {output_dir}/{output_file}.vcf""); shell(""{bcftools} view -Oz -o {output.gvcf_gz} {output_dir}/{output_file}.g.vcf""). rule samtools_index:; input:; ""{preifx}.bam""; output:; ""{preifx}.bam.bai""; run:; shell(""{samtools} index {input}""); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:3444,log,log,3444,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,2,['log'],['log']
Testability,"u"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Run make_examples:. ```; OUTPUT_DIR=""${PWD}/quickstart-ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1486,test,test,1486,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['test'],['test']
Testability,ue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```; FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_t,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1180,test,testlogs,1180,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"uf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:1372,sandbox,sandbox,1372,,https://github.com/google/deepvariant/issues/580,1,['sandbox'],['sandbox']
Testability,uild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9623,test,testdata,9623,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,uild_artifacts/h5py_1604753633596/work; httplib2 @ file:///home/conda/feedstock_root/build_artifacts/httplib2_1679483503307/work; idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1663625384323/work; idna-ssl @ file:///home/conda/feedstock_root/build_artifacts/idna_ssl_1636483491140/work; importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1630267465156/work; intervaltree @ file:///home/conda/feedstock_root/build_artifacts/intervaltree_1683532206518/work; Jinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1636510082894/work; jsonschema @ file:///home/conda/feedstock_root/build_artifacts/jsonschema_1634752161479/work; Keras-Applications==1.0.8; Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work; Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1679584000376/work; MarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1621455668064/work; mock @ file:///home/conda/feedstock_root/build_artifacts/mock_1681654098624/work; multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1633329770033/work; numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1607958944856/work; oauth2client==4.1.3; oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work; opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work; pandas==1.1.5; protobuf==3.18.0; psutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1610127101219/work; pyasn1==0.4.8; pyasn1-modules==0.2.7; pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work; PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1683676063469/work; pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1663846997386/work; pyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work; pyrsiste,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:4621,mock,mock,4621,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,1,['mock'],['mock']
Testability,"ujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:8992,test,testdata,8992,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"umina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:1946,test,testdata,1946,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability,"unfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz; gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json; gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz; gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json; gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz; gvcf.tfrecord-00005-of-00016.gz make_examples.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:3449,test,test,3449,,https://github.com/google/deepvariant/issues/733,2,['test'],['test']
Testability,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s; user	0m3.036s; sys	0m0.866s; ```. **Setup**; - Operating system: CentOS Linux release 8.2.2004 (Core); - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity/Docker; - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**; The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:5610,test,test,5610,,https://github.com/google/deepvariant/issues/533,2,['test'],['test']
Testability,"up>[(6)](#vfootnote6)</sup>6 HG002, 6 HG003, 6 HG004, 8 HG005, 8 HG006, 8 HG007 | 13,039,595 |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"":",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:2317,test,testdata,2317,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"urces.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; # internvaltree v3 has some API changes with v2; ##########################################################################; pip install 'intervaltree==2.1.0'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:22176,test,test,22176,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,3,"['mock', 'test']","['mock', 'test']"
Testability,"urope-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1565,log,log,1565,,https://github.com/google/deepvariant/issues/214,1,['log'],['log']
Testability,"ut of the reads:. ```; 2018-12-10 17:04:46.071140: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.071218 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1210 17:04:46.072298 140037815584512 make_examples.py:1024] Preparing inputs; 2018-12-10 17:04:46.072535: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.072572 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; I1210 17:04:46.072917 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 455, in _ensure_consistent_con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128:1133,test,testdata,1133,,https://github.com/google/deepvariant/issues/128,1,['test'],['testdata']
Testability,"ut you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:; ```; ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz""; ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:4832,log,log,4832,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['log'],['log']
Testability,"ut/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78406,test,testing,78406,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability,"variant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122325,log,log,122325,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"variants.bed.txt contigs have name 'chr' when BAM file has contig name 'NC_000913.3'; > Contig names should match. Looking at [VCF Specs](https://samtools.github.io/hts-specs/VCFv4.2.pdf) it looks like '#contig' header is optional, but I think Nucleous library (that is used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1039,test,testdata,1039,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}' returned non-zero exit status 2. Do anyone know how to handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:2067,test,testdata,2067,,https://github.com/google/deepvariant/issues/226,2,['test'],['testdata']
Testability,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:2828,test,test,2828,,https://github.com/google/deepvariant/issues/339,2,['test'],['test']
Testability,"vidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3345,log,log,3345,,https://github.com/google/deepvariant/issues/844,1,['log'],['log']
Testability,"w I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is there a better place to ask questions like this?. This is a good place to ask :); It's a public forum, so our team and everyone in the community can ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1708,log,logs,1708,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['log'],['logs']
Testability,"w_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:21386,log,log,21386,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"w_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:62321,log,log,62321,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"we recommend using the `--norealign_reads` flag with PacBio reads. Realigning will take way too long as the reads are very long. So the command you posted should be modified as follows:. ```; python bin/make_examples.zip \; --mode training \; --ref ""data/chr20.fa"" \; --reads ""data/sorted_final_merged.bam"" \; --examples ""training-examples/training_set.with_label.tfrecord.gz"" \; --confident_regions ""data/NA12878.sorted.bed"" \; --regions ""chr20"" \; --truth_variants ""data/NA12878.sorted.vcf.gz"" \; --norealign_reads; ```. When running with this flag, I run into an error that is due to the `QUAL` field missing (output shown below). I am not sure why you are not seeing this error, but you may want to check your data files. You could try the following:; * Regenerate `data/sorted_final_merged.bam.bai` and `data/NA12878.sorted.vcf.gz.tbi` as these seem to be older than the corresponding data files. ; * Check the BAM file. Are you able to view the contents using `samtools`? I mention this because the log shows the following line: `[W::bam_hdr_read] EOF marker is absent. The input is probably truncated`. Hope this helps!. ```; 2019-01-15 00:54:33.940093: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0115 00:54:33.942667 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; I0115 00:54:33.945646 140481635538688 make_examples.py:1024] Preparing inputs; 2019-01-15 00:54:33.956298: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0115 00:54:33.958478 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; I0115 00:54:33.978430 140481635538688 genomics_reader.py:174] Reading data/NA12878.sorted.vcf.gz with NativeVcfReader; I0115 00:54:33.980473 140481635538688 make_examples.py:946] Common contigs are [u'chr20']; I0115 00:54:34.150244 140481635538688 make_examples.py:1030] Writing examples to /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-454225947:1403,log,log,1403,,https://github.com/google/deepvariant/issues/138#issuecomment-454225947,1,['log'],['log']
Testability,"with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.; The former has 77+69+25+23=194 total FNs+FPs, and the latter has 53+78+21+35=187. Overall there are fewer errors, even though it is true that not all the error types were better on DeepTrio. We hope to improve all our models in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:1245,benchmark,benchmark,1245,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,2,['benchmark'],"['benchmark', 'benchmark-output']"
Testability,"xamples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Deepvariant previously work well with data aligned to linear reference genome. So, I think it could be some issue with pangenome-aligned data. I can share the cram (e.g., chr22) if you would like to test. **Any additional context:**; Code I used to align to a haplotype sampled pangenome graph:; ```; # vg giraffe to align reads to graph; rg=""ID:1\tLB:lib_${sample}\tSM:${sample}\tPL:illumina\tPU:lib_${sample}.1""; time \; vg giraffe \; --progress \; --read-group $rg \; --sample ${sample} \; --output-format gam \; -f ${fqpath}/${sample%.hap*}_1_paired.fq.gz \; -f ${fqpath}/${sample%.hap*}_2_paired.fq.gz \; -Z ${gbzpath}/${sample}.gbz \; -t $threads > ${outpath}/${sample}.gam. # vg surject to convert gam to bam; time \; vg surject \; -F $path_list \; -x ${gbzpath}/${sample}.gbz \; -t $(($threads - 10)) \; --sam-output \; --read-group '1' \; --sample ${sample} \; --prune-low-cplx \; --interleaved \; --max-frag-len 3000 \; ${outpath}/${sample}.gam | \; sed 's/CHM13#0#//g' - | \; samtools view -hb - | \; samtools reheader ${outpath}/${sample}.tmp.header.sam - | \; samtools sort --write-index -@ 10 -m 4G -O CRAM --reference $ref -T ${outpath}/${sample}.cram -o ${outpath}/${samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:2371,test,test,2371,,https://github.com/google/deepvariant/issues/847,1,['test'],['test']
Testability,"xome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1426,log,log,1426,,https://github.com/google/deepvariant/issues/129,1,['log'],['log']
Testability,"xsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --intermediate_results_dir /output/intermediate_results_dir \; > --num_shards=1 \; >. Status: Downloaded newer image for google/deepvariant:1.0.0; I0911 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1872,Test,Test,1872,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['Test'],['Test']
Testability,"y (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5496,log,log,5496,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability,"y run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:3259,sandbox,sandbox,3259,,https://github.com/google/deepvariant/issues/602,1,['sandbox'],['sandbox']
Testability,"y"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114947,test,testlogs,114947,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"y:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, some very similar samples keep running",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:12496,test,test,12496,,https://github.com/google/deepvariant/issues/855,1,['test'],['test']
Testability,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355036534:103,test,test,103,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534,1,['test'],['test']
Testability,"yes @pichuan scripts/run_wes_case_study_binaries.sh does complete with those warnings, just wondering about why the output is binary when the postprocess command is invoked with tfrecord in name [examples.tfrecord.zip](https://github.com/google/deepvariant/files/3456033/examples.tfrecord.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.tfrecord.vcf`. than without [examples.zip](https://github.com/google/deepvariant/files/3456034/examples.zip). `python ./bazel-bin/deepvariant/postprocess_variants.zip --ref quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --infile quickstart-output/examples.tfrecord.cvo.gz --outfile quickstart-output/examples.vcf`. and what does that binary file represent?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-517213905:369,test,testdata,369,,https://github.com/google/deepvariant/issues/199#issuecomment-517213905,2,['test'],['testdata']
Testability,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355996061:340,test,test,340,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061,1,['test'],['test']
Testability,"ython/google/protobuf/pyext/_message.so is probably truncated; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_i3h8s325/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; Traceback (most recent call last):; File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 392, in <module>; File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 326, in Main; AssertionError: Cannot exec() '/tmp/Bazel.runfiles_8twzrz82/runfiles/com_google_deepvariant/deepvariant/make_examples.py': file not found.; parallel: Error: Output is incomplete. Cannot append to buffer file in /tmp. Is the disk full?; parallel: Error: Change $TMPDIR with --tmpdir or use --compress.; Warning: unable to close filehandle properly: No space left on device during global destruction. real 0m2.302s; user 0m1.215s; sys 0m0.687s. ## command-line plan B:; /share/app/singularity/3.8.1/bin/singularity exec \; --bind /usr/lib/locale/:/usr/lib/locale/ \; --bind $ccsbam:$ccsbam \; --bind $ccsbam.bai:$ccsbam.bai \; --bind $fasta:$fasta \; --bind $fasta.fai:$fasta.fai \; --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output \; /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/ou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300:3717,Assert,AssertionError,3717,,https://github.com/google/deepvariant/issues/679#issuecomment-1636707300,1,['Assert'],['AssertionError']
Testability,"ython/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: /usr/local/cuda-10.0/lib64/libcublas.so.9.0: version `libcublas.so.9.0' not found (required by /root/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. However:. 1. I specified CUDA 10 in `settings.sh`; 1. CUDA 9 is not required for TensorFlow `r1.12`. Additionally:. 1. CUDA 9 is not available for my system, Ubuntu 18; 1. Symlinking to the correct file names as suggested [elsewhere](https://github.com/tensorflow/tensorflow/issues/15604) did not work; 1. I have built TensorFlow `r1.12` (and master) for CUDA 10 (and 9) in my environment previously. Questions:. 1. Does deepvariant have a requirement for CUDA 9(.0?)?; 1. How would you recommend proceeding?. _________. ```; Linux localhost 4.15.0-1032-aws #34-Ubuntu SMP Thu Jan 17 15:18:09 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux; ```. Full log:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=1; ++ TF_ENABLE_XLA=1; ++ export TF_NEED_CUDA=1; ++ TF_NEED_CUDA=1; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=1; ++ TF_NEED_MKL=1; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=1; ++ TF_NEED_S3=1; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=10.0; ++ TF_CUDA_VERSION=10.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=7; ++ TF_CUDNN_VERSION=7; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:4726,log,log,4726,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"ython/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:40100,test,testlogs,40100,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23465,test,testlogs,23465,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:110352,test,testlogs,110352,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"ywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42275,log,log,42275,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"z""` in the `make_examples` step generated ; `test.examples.tfrecord-000??-of-00064.gz` (shards from 00 to 63). (2) From the command you pasted, it seems like you're directly running inside docker.; If that is the case, you don't need the [-v flags](https://docs.docker.com/storage/volumes/), but you still need to make sure that the files are actually visible when/where you run call_variants.; If you're still running call_variants inside docker (like you did for your make_examples step), please make sure when you can see `test.examples.tfrecord-000??-of-00064.gz` from where you run the command. So, `ls test.examples.tfrecord-000??-of-00064.gz` should see the files. The command you pasted from our run_wgs_case_study_docker.sh example is actually different from the make_examples command you posted first. If you're using that, you need to make sure you're using the [-v flags](https://docs.docker.com/storage/volumes/) correctly and make sure the files are visible to it. (3) I tested with `sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2` first and directly running the commands inside. I modified from the WES example because it's faster, but WGS should be the same. I can confirm that I was able to run the following steps without any issues:. 1. Start interactive mode so I can use command similar to yours. I'm not considering how I'm getting the data out.; ```; sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.2; ```. 2. Inside the interactive mode, run the following:; ```; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wes_standard""; DATA_HTTP_DIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; N_SHARDS=""64"". ## Download extra packages; sudo apt-get -y update; sudo apt-get -y install parallel; sudo apt-get -y install aria2; ## Download models, and test data; # Copy the model files to your local disk.; aria2c -c -x10 -s10 ""${MODEL_HTTP_DIR}""/model.ckpt.d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:1164,test,tested,1164,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['test'],['tested']
Testability,"zed SAM header type, ignoring:; I0129 11:46:16.346041 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.360527 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:16.361952 140471159555840 make_examples.py:946] Common contigs are [u'chr20']; I0129 11:46:16.501434 140471159555840 make_examples.py:1030] Writing examples to prj-NA12878/training-examples/training_set.with_label.tfrecord.gz; 2019-01-29 11:46:16.502209: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-01-29 11:46:16.550218: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.554270: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.556066 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.571476 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:20.140141 140471159555840 make_examples.py:782] Found 0 candidates in chr20:1-1000 [3.64s elapsed]; I0129 11:46:20.141022 140471159555840 make_examples.py:782] Found 0 candidates in chr20:1001-2000 [0.00s elapsed]; I0129 11:46:20.141855 140471159555840 make_examples.py:782] Found 0 candidates in chr20:2001-3000 [0.00s elapsed]; I0129 11:46:20.142673 140471159555840 make_examples.py:782] Found 0 candidates in chr20:3001-4000 [0.00s elapsed]; I0129 11:46:20.143475 140471159555840 make_examples.py:782] Found 0 candidates in chr20:4001-5000 [0.00s elapsed]; I0129 11:46:20.144277 140471159555840 make_examples.py:782] Found 0 candidates in chr20:5001-6000 [0.00s elapsed]; I0129 11:46:20.145082 140471159555840 make_examples.py:782] Found 0 candidates in chr20:6001-7000 [0.00s elapsed]; I0129 11:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:2906,test,testdata,2906,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['test'],['testdata']
Testability,"zel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122525,test,testlogs,122525,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"zelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --act",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:1834,benchmark,benchmarks,1834,,https://github.com/google/deepvariant/issues/608,4,"['benchmark', 'test']","['benchmarks', 'tests']"
Testability,"{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0; ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.0"", ""shape"": [100, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3915,test,testdata,3915,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,5,['test'],['testdata']
Testability,"{outpath}/${sample}.g.vcf.gz \; --num_shards=${threads} \; --intermediate_results_dir ${TMPDIR} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"". ```; - Error trace: (if applicable); ```; I0706 12:06:34.396180 140036894541632 make_examples_core.py:301] Task 16/40: 20600183 candidates (20662757 examples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Deepvariant previously work well with data aligned to linear reference genome. So, I think it could be some issue with pangenome-aligned data. I can share the cram (e.g., chr22) if you would like to test. **Any additional context:**; Code I used to align to a haplotype sampled pangenome graph:; ```; # vg giraffe to align reads to graph; rg=""ID:1\tLB:lib_${sample}\tSM:${sample}\tPL:illumina\tPU:lib_${sample}.1""; time \; vg giraffe \; --progress \; --read-group $rg \; --sample ${sample} \; --output-format gam \; -f ${fqpath}/${sample%.hap*}_1_paired.fq.gz \; -f ${fqpath}/${sample%.hap*}_2_paired.fq.gz \; -Z ${gbzpath}/${sample}.gbz \; -t $threads > ${outpath}/${sample}.gam. # vg surject to convert gam to bam; time \; vg surject \; -F $path_list \; -x ${gbzpath}/${sample}.gbz \; -t $(($threads - 10)) \; --sam-output",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:1977,test,test,1977,,https://github.com/google/deepvariant/issues/847,2,['test'],['test']
Testability,"} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:3483,test,tested,3483,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['test'],['tested']
Testability,"}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1; . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main; intermediate_results_dir = check_or_create_intermediate_results_dir(; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir; os.makedirs(intermediate_results_dir); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); [Previous line repeated 1 more time]; File ""/usr/lib/python3.8/os.py"", line 223, in makedirs; mkdir(name, mode); OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**; The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:3668,test,test,3668,,https://github.com/google/deepvariant/issues/530,2,['test'],['test']
Testability,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7075,log,log,7075,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,2,"['log', 'test']","['log', 'tests']"
Testability,"}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1274,test,test,1274,,https://github.com/google/deepvariant/issues/223,1,['test'],['test']
Usability," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:1427,simpl,simply,1427,,https://github.com/google/deepvariant/issues/243,1,['simpl'],['simply']
Usability," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716:1909,learn,learning,1909,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716,1,['learn'],['learning']
Usability," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476392585:2159,feedback,feedback,2159,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585,1,['feedback'],['feedback']
Usability," `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0710 12:09:25.874915 139944337696576 postprocess_variants.py:1318] Using 19 CPUs for parallelization of variant transformation.; I0710 12:09:45.096508",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1784,learn,learning-container-license,1784,,https://github.com/google/deepvariant/issues/849,1,['learn'],['learning-container-license']
Usability," for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2532,learn,learning,2532,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,1,['learn'],['learning']
Usability," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1301,learn,learn,1301,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,"['learn', 'simpl']","['learn', 'simplify']"
Usability," price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2026,simpl,simplified,2026,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,1,['simpl'],['simplified']
Usability," this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1630,guid,guide,1630,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,1,['guid'],['guide']
Usability," was not used. These are the output (part of the output were removed due to the limit of the characters of this post):. ```; âžœ t7 apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:2632,Learn,Learning,2632,,https://github.com/google/deepvariant/issues/774,1,['Learn'],['Learning']
Usability,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440:219,guid,guidelines,219,,https://github.com/google/deepvariant/issues/440,1,['guid'],['guidelines']
Usability,"""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:3404,simpl,simple,3404,,https://github.com/google/deepvariant/issues/610,1,['simpl'],['simple']
Usability,"(sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transform",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1620,Learn,Learning,1620,,https://github.com/google/deepvariant/issues/849,1,['Learn'],['Learning']
Usability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2855,learn,learning,2855,,https://github.com/google/deepvariant/issues/845,1,['learn'],['learning']
Usability,") that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1613,guid,guides,1613,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,1,['guid'],['guides']
Usability,"), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21778,learn,learn,21778,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['learn'],['learn']
Usability,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:28,clear,clear,28,,https://github.com/google/deepvariant/issues/367,1,['clear'],['clear']
Usability,"**Describe the issue:**; (A clear and concise description of what the issue is.); I have no ""sudo"" authority; I think deepvariant tool have to run ""sudo"" authority ; and I did this command; `sudo docker pull google/deepvariant:""0.10.0""`. as far as I understand, this next step is ; `sudo docker run \; -v ""${INPUT_DIR}"":""/3.Sort"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${ref_fasta}"" \; --reads=""/3.Sort/$1_Markdup_sort.bam"" \; --output_vcf=/output/$1_Deepvariant.output.vcf.gz \; --output_gvcf=/output/$1_Deepvariant.output.g.vcf.gz \; --num_shards=${N_SHARDS}; `; is that right?; But I can't use ""sudo"" every time. how can I run 'deepvariant' without 'sudo' ?? . **Setup**; - Operating system: Ubuntu; - DeepVariant version:0.10.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/335:28,clear,clear,28,,https://github.com/google/deepvariant/issues/335,1,['clear'],['clear']
Usability,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/333:682,Clear,Clearly,682,,https://github.com/google/deepvariant/issues/333,2,"['Clear', 'simpl']","['Clearly', 'simply']"
Usability,"**Describe the issue:**; Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio?. **Setup**; - Operating system: Cent OS; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/532:136,guid,guides,136,,https://github.com/google/deepvariant/issues/532,1,['guid'],['guides']
Usability,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:50,guid,guidelines,50,,https://github.com/google/deepvariant/issues/471,1,['guid'],['guidelines']
Usability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam âœ…; 2. non-parallel + cram âœ… ; 3. parallel + bam âœ… ; 4. non-parallel + cram ðŸ”´ . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:87,simpl,simple,87,,https://github.com/google/deepvariant/issues/786,1,['simpl'],['simple']
Usability,"**Describe the issue:**; Since I couldn't run DeepVariant with Docker, I thought I'd try the prebuilt binaries version, but I couldn't find a guide on how to use the prebuilt binaries DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590:142,guid,guide,142,,https://github.com/google/deepvariant/issues/590,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459:120,clear,clear,120,,https://github.com/google/deepvariant/issues/459,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:237,guid,guide,237,,https://github.com/google/deepvariant/issues/462,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:120,clear,clear,120,,https://github.com/google/deepvariant/issues/480,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:124,clear,clear,124,,https://github.com/google/deepvariant/issues/522,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/525:120,clear,clear,120,,https://github.com/google/deepvariant/issues/525,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:; YES. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: ubuntu **16.04**; - DeepVariant version: **1.1.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than â€œ**AD**â€œ did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than â€œ**AD**â€œ did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/529:125,clear,clear,125,,https://github.com/google/deepvariant/issues/529,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:124,clear,clear,124,,https://github.com/google/deepvariant/issues/592,2,['clear'],"['clear', 'clearly']"
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated.; (A clear and concise description of what the issue is.). **Setup**; - Operating system:Mac OS ; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true; - Error trace: No error.(if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561:240,clear,clear,240,,https://github.com/google/deepvariant/issues/561,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:120,clear,clear,120,,https://github.com/google/deepvariant/issues/550,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:120,clear,clear,120,,https://github.com/google/deepvariant/issues/545,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:154,guid,guide,154,,https://github.com/google/deepvariant/issues/580,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants.; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - 150bp paired-end Illumina data. **Steps to reproduce:**; - Command: ; `/opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; --make_examples_extra_args=""normalize_reads=true"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \; --num_shards=${threads}`; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):; ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/612:405,clear,clear,405,,https://github.com/google/deepvariant/issues/612,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; At the call_variants.py step, running into error that tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; (A clear and concise description of what the issue is.). **Setup**; - Operating system:CentOS7 ; - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.):singularity run with SIF image pulled from docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:292,clear,clear,292,,https://github.com/google/deepvariant/issues/564,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:124,clear,clear,124,,https://github.com/google/deepvariant/issues/640,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651:120,clear,clear,120,,https://github.com/google/deepvariant/issues/651,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:120,clear,clear,120,,https://github.com/google/deepvariant/issues/717,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**; - Operating system: linuxï¼ˆCentosï¼‰; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**; - Error trace: (if applicable); <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:120,clear,clear,120,,https://github.com/google/deepvariant/issues/668,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Hi developers,; I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:120,clear,clear,120,,https://github.com/google/deepvariant/issues/653,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:205,clear,clear,205,,https://github.com/google/deepvariant/issues/678,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes.; **Describe the issue:**; (A clear and concise description of what the issue is.); I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Operating system:; Redhat enterprise v7.9, x86_64; **Steps to reproduce:**; - Command:; BIN_VERSION=""1.5.0""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/HG38.fa \; --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=4 \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \; --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2""; I allocated 4 cores and 70GBs to run that program.; I added the VAF thresholds for SNPs and Indels because I read the reported issues:; https://github.com/google/deepvariant/issues/578; - Error trace: (if applicable); And here are some most recent results I got from stdout:; I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]; I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]; I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]; I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:126,clear,clear,126,,https://github.com/google/deepvariant/issues/683,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**; - Operating system: Cent; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**; - Command:; ```bash; #!/bin/bash; sample=$1; threads=$2. chr=$3; indir=""01.mapping""; outdir=""02.snps""; sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}""; rm -rf ${outdir}/${sample}_chr${chr}; ```; - Error trace: (if applicable); ```bash; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]; I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]; I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]; I0325 17:32:27.143459 47041007318848 mak",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:126,clear,clear,126,,https://github.com/google/deepvariant/issues/794,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:122,clear,clear,122,,https://github.com/google/deepvariant/issues/856,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:177,guid,guide,177,,https://github.com/google/deepvariant/issues/879,2,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); `run_deepvariant` is erroring out in the `postprocess_variants` step. **Setup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:127,clear,clear,127,,https://github.com/google/deepvariant/issues/901,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; Although some variants are clearly heterozygous in IGV, deepvariant GT shows a homozygous genotype. **Setup**; - Operating system: linux; - DeepVariant version: 1.6.1; - Installation method : Docker; - Type of data: illumina, WES, hg38. **Steps to reproduce:**; ```; docker run --rm -i \; -v ${ref_dir}:/opt/ref \; -v ${kit_dir}:/opt/kit \; -v ${input_dir}:/opt/sample \; ${deepvariant_docker} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/opt/ref/${ref_fasta}"" \; --reads=""/opt/sample/${input_bam_file}"" \; --regions=""/opt/kit/${kit_bed_file}"" \; --output_vcf=""/opt/sample/${input_bam_file/.bam/.dv.vcf}"" \; --num_shards=""${threads}""; ```. Here are 5 selected variants called by 5 different versions of deepvariant:; ```; v0.10.0 chr12 11353713 . T C 57.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:54:43:27,15:0.348837:57,0,55; v1.1.0 chr12 11353713 . T C 36.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:34:44:28,15:0.340909:36,0,38; v1.4.0 chr12 11353713 . T C 23.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:28,15:0.340909:23,0,25; v1.5.0 chr12 11353713 . T C 24.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:17:44:28,15:0.340909:24,0,17; v1.6.1 chr12 11353713 . T C 24.6 PASS . GT:GQ:DP:AD:VAF:PL 1/1:5:44:28,15:0.340909:22,3,0; -----------------------------; v0.10.0 chr3 195779035 . G A 3.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:33:24,8:0.242424:1,0,38; v1.1.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:41:33:24,8:0.242424:0,43,45; v1.4.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:23:33:24,8:0.242424:0,32,22; v1.5.0 chr3 195779035 . G A 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:16:33:24,8:0.242424:0,26,16; v1.6.1 chr3 195779035 . G A 13.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:14:33:24,8:0.242424:13,28,0; -----------------------------; v0.10.0 chr6 159711482 . C T 46.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:36:70:38,32:0.457143:46,0,36; v1.1.0 chr6 159711482 . C T 6.9 PASS . GT:GQ:DP:A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/824:151,clear,clearly,151,,https://github.com/google/deepvariant/issues/824,1,['clear'],['clearly']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:342,clear,clear,342,,https://github.com/google/deepvariant/issues/865,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** ; (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:125,clear,clear,125,,https://github.com/google/deepvariant/issues/733,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:120,clear,clear,120,,https://github.com/google/deepvariant/issues/743,1,['clear'],['clear']
Usability,". 27.87% , 0.05% ,python ,libssw.so ,[.] ssw_align; | ; --27.82%--ssw_align; | ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6008,learn,learning,6008,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,".; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; # internvaltree v3 has some API changes with v2; ###########################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21865,learn,learn,21865,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['learn'],['learn']
Usability,".com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; google/deepvariant:1.5.0-gpu google/deepvariant:1.6.1-gpu docker images run as CPU-only because they are using ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1064,Learn,Learning,1064,,https://github.com/google/deepvariant/issues/844,1,['Learn'],['Learning']
Usability,".g.vcf.gz|grep -C 3 ""10764356"" . chromosome_1	10764353	.	C	T,<*>	27.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:27:162:100,62,0:0.382716,0:27,0,51,990,990,990; chromosome_1	10764354	.	A	<*>	0	.	END=10764354	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764355	.	C	T,<*>	26.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:162:100,62,0:0.382716,0:26,0,60,990,990,990; chromosome_1	10764356	.	A	T,<*>	26.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:26:161:98,63,0:0.391304,0:26,0,64,990,990,990; chromosome_1	10764357	.	T	<*>	0	.	END=10764357	GT:GQ:MIN_DP:PL	0/0:50:162:0,300,2999; chromosome_1	10764358	.	T	C,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:172:98,63,0:0.366279,0:33,0,69,990,990,990; chromosome_1	10764359	.	T	<*>	0	.	END=10764381	GT:GQ:MIN_DP:PL	0/0:50:169:0,300,2999. ```; To me in the 3 samples it's the same site that is present. . Here another one (where the SNP is RefCall in one sample and PASS in another). ```; zgrep -w ""chromosome_2"" output.g.vcf.gz|grep -C 2 ""9780248""; chromosome_2	9780195	.	T	<*>	0	.	END=9780244	GT:GQ:MIN_DP:PL	0/0:50:294:0,300,2999; chromosome_2	9780245	.	GGT	G,<*>	37.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:294:161,131,0:0.445578,0:37,0,41,990,990,990; chromosome_2	9780248	.	A	<*>	0	.	END=9780249	GT:GQ:MIN_DP:PL	0/0:50:163:0,270,2939; chromosome_2	9780250	.	T	TTG,<*>	36.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:298:161,133,0:0.446309,0:36,0,34,990,990,990; chromosome_2	9780251	.	A	<*>	0	.	END=9780281	GT:GQ:MIN_DP:PL	0/0:50:272:0,300,2999; ```; In this second case, I don't understand why DeepVariant did not even consider there might be a variant there, as the bam clearly shows many reads mapping in that position with a variant site (it's the middle T flanked by 2 homozygous T sites). ![example2](https://user-images.githubusercontent.com/23341393/75358443-2ddbef00-58b3-11ea-9170-dd996a53386b.png). Now I know that calling SNP (in the sense of single nucleotide) variation in the vicinity of more complex events is known to be tricky, therefore this might not be an issue with DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/278#issuecomment-591476042:3374,clear,clearly,3374,,https://github.com/google/deepvariant/issues/278#issuecomment-591476042,1,['clear'],['clearly']
Usability,".tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1.0, train/recall_homalt=0.8789808750152588, train/recall_homref=0.7945205569267273, train/true_negatives=498.0, train/true_positives=222.0; I1031 11:18:53.873582 14055859",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1858,learn,learning,1858,,https://github.com/google/deepvariant/issues/904,1,['learn'],['learning']
Usability,"/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; > --ref; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; > --report_title MITO60_Stats --sample_name MITO60 --output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > --model_type ONT_R104; >; >; > On Wed, Jun 12, 2024 at 12:03â€¯PM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> And, just in case the documentation isn't clear:; >>; >> This part:; >>; >> sudo docker run \; >> -v ""${INPUT_DIR}"":""/input"" \; >> -v ""${OUTPUT_DIR}"":""/output"" \; >> google/deepvariant:""${BIN_VERSION}"" \; >> ...; >>; >> The variable BIN_VERSION was specified in earlier in the steps:; >>; >> BIN_VERSION=""1.6.1""; >>; >> So, in Unix command it's equivalent to:; >>; >> google/deepvariant:""1.6.1"" \; >>; >> â€”; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; >> .; >> You are receiving this because you were mentioned.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508:4235,clear,clear,4235,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508,1,['clear'],['clear']
Usability,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1678,learn,learning,1678,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524,1,['learn'],['learning']
Usability,"/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6092,simpl,simpler,6092,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,1,['simpl'],['simpler']
Usability,"0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13897,simpl,simple,13897,,https://github.com/google/deepvariant/issues/166,1,['simpl'],['simple']
Usability,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749327121:376,clear,clear,376,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121,1,['clear'],['clear']
Usability,"1. The type of the data is ONT_R9 simplex.; 2. The basecaller is guppy. ; 3. Threads are set to 48.; I also provide the code as below. Thank you for your help. ; `BIN_VERSION=""1.6.1""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; INPUT_DIR=""/home/user/MasaÃ¼stÃ¼/BEYZA/input"" OUTPUT_DIR=""/home/user/MasaÃ¼stÃ¼/BEYZA/output"" THREADS=48 MODEL_NAME=""ONT_R104""; sudo docker run -v ""/home/massive/Desktop/beyza/input"":""/input"" -v ""/home/massive/Desktop/beyza/OUTPUTDEEP"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=ONT_R104 --ref=""/input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads=""/input/NA12878-minion-ul_GRCh38.bam"" â€“output_vcf=""/output/deep.vcf"" --threads=48; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166:34,simpl,simplex,34,,https://github.com/google/deepvariant/issues/814#issuecomment-2089912166,1,['simpl'],['simplex']
Usability,"2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1776,learn,learning,1776,,https://github.com/google/deepvariant/issues/904,1,['learn'],['learning']
Usability,"3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4068,Simpl,SimpleNamespace,4068,,https://github.com/google/deepvariant/issues/469,1,['Simpl'],['SimpleNamespace']
Usability,"4.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:4440,simpl,simpler,4440,,https://github.com/google/deepvariant/issues/611,1,['simpl'],['simpler']
Usability,"5208 . C T 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:44:42:22,20:0.47619:44,0,56; Chrom_6 5425277 . G A 32 PASS . GT:GQ:DP:AD:VAF:PL 0/1:32:45:28,17:0.377778:31,0,57; Chrom_6 **5425401** . G T 30.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:60:46,14:0.233333:30,0,50; Chrom_6 **5425402** . G T 18.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:59:46,13:0.220339:18,0,47; Chrom_6 **5425403** . T C 13.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:14:59:46,13:0.220339:13,0,45; Chrom_6 5425421 . T C 6.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:7:63:50,13:0.206349:5,0,40; Chrom_6 5425460 . G A 8.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:8:61:48,12:0.196721:7,0,40; Chrom_6 5425539 . A C 14.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:45:36,9:0.2:14,0,45; Chrom_6 5425555 . G A 20.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:34,10:0.227273:20,0,50; ```; As you see, the event which is a change of 3 consecutive nucleotides, is called in one sample as a single T to TTC event and in the other sample as 3 SNPs. ; This is annoying if one wants to compare the 2 samples, as simple comparison scripts will not understand those events are in fact identical. . We ran GATK in parallel and GATK calls the event in both sample as T to TTC. . The only difference between the 2 samples is the sequencing coverage which is higher in Sample1. ; I also looked into the gVCF and the same phenomenom happens.; Sample 1:; ```; Chrom_6	5425343	.	G	<*>	0	.	END=5425398	GT:GQ:MIN_DP:PL	0/0:50:104:0,300,2999; Chrom_6	5425399	.	TGG	T,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:108:47,58,0:0.537037,0:33,0,61,990,990,990; Chrom_6	5425402	.	G	<*>	0	.	END=5425402	GT:GQ:MIN_DP:PL	0/0:50:106:0,180,2759; Chrom_6	**5425403**	.	T	TTC,<*>	36.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:105:47,57,0:0.542857,0:36,0,61,990,990,990; Chrom_6	5425404	.	A	<*>	0	.	END=5425420	GT:GQ:MIN_DP:PL	0/0:50:106:0,300,2999; Chrom_6	5425421	.	T	C,<*>	39.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:39:105:46,59,0:0.561905,0:39,0,62,990,990,990; Chrom_6	5425422	.	C	<*>	0	.	END=5425459	GT:GQ:MIN_DP:PL	0/0:50:105:0,180,2759; Chrom_6	5425460	.	G	A,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/202:2184,simpl,simple,2184,,https://github.com/google/deepvariant/issues/202,1,['simpl'],['simple']
Usability,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12736,simpl,simply,12736,,https://github.com/google/deepvariant/issues/614,1,['simpl'],['simply']
Usability,"9%--__memcpy_sse2_unaligned; | ; --1.36%--ssw_init; | ; --0.89%--qP_byte. 27.87% , 0.05% ,python ,libssw.so ,[.] ssw_align; | ; --27.82%--ssw_align; | ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:5940,learn,learning,5940,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"9%--__memcpy_sse2_unaligned; | ; --1.38%--ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hop",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:11454,learn,learning,11454,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2509,learn,learning,2509,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491,1,['learn'],['learning']
Usability,":__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7600,learn,learning,7600,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"> 1. Any idea how I might estimate the expected run time?. The run time of `make_examples` step can be affected by many factors, such as the coverage(depth) of the input. One component in make_examples is local realignment, which can be affected by things like depth, read length, etc. Currently I think that might be causing the biggest variance of the runtime of make_examples. This makes it hard for me to give general estimation guidelines.; Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?; If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:433,guid,guidelines,433,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,1,['guid'],['guidelines']
Usability,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735703602:546,simpl,simpler,546,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602,1,['simpl'],['simpler']
Usability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:261,learn,learned,261,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761,1,['learn'],['learned']
Usability,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328#issuecomment-663306398:1031,learn,learning,1031,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398,1,['learn'],['learning']
Usability,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1213,clear,clear,1213,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840,1,['clear'],['clear']
Usability,"> Hi @themkdemiiir,; > ; > Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker.; > ; > * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @ to the file name and add `--task` flag that specifies the task number for each shard.; > * call_variants will be run with the same number of shards.; > * postprocess_variants has to be run in a single process.; > ; > Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:; > ; > ```; > bin/make_examples \; > --examples /tmn/your_examples.tfrecord@200.gz \; > --mode calling \; > --reads /tmp/your_input_bam.bam \; > --realign_reads \; > --ref=/tmp/your_reference.fna \; > --task=11; > ; > # Input for each instance of call_variants is the output of one instance of make_examples:; > bin/call_variants.par \; > --batch_size=32 \; > --checkpoint <Path to the model checkpoint or saved model>.ckpt \; > --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; > --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz; > ; > # Input for for postprocess would be the output of all instances of call_variants:; > /tmp/your_call_variants_output.cvo.tfrecord@200.gz; > ```. So how could I merge the output of the call_variants step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624:412,simpl,simply,412,,https://github.com/google/deepvariant/issues/744#issuecomment-1855569624,1,['simpl'],['simply']
Usability,"> Hi @vera-gomes , One possible way to get around the RAM issue is to split the run into two or more, using the `--regions` flag (For example, one run can run the first 8 chromosomes, and the second run can run the rest, or something like that). And then at the end you can combine the VCFs. Perhaps a user-friendly way to implement this is to let deepvariant split the job by chromosomes/regions automatically? If this mainly affects the post processing step, perhaps just make this step automatically process the output by chromosome/regions of a fixed window size?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868#issuecomment-2293266591:302,user-friendly,user-friendly,302,,https://github.com/google/deepvariant/issues/868#issuecomment-2293266591,1,['user-friendly'],['user-friendly']
Usability,"> I'm not very familiar with SINGULARITY_CACHEDIR. But, in your command, if you're running it 3 times, you should use a different --intermediate_results_dir. Output of `make_examples` will be written to that directory. So, if you use the same intermediate_results_dir, that might explain why your data is corrupted. Thank you. I will try and feedback to you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564#issuecomment-1253259881:342,feedback,feedback,342,,https://github.com/google/deepvariant/issues/564#issuecomment-1253259881,1,['feedback'],['feedback']
Usability,"> Is there a way that I could just modify the gcp_deepvariant_runner.py script . Yes, you can easily make deepvariant_runner to generate gVCF output by using `--gvcf_outfile` flag, please refer to [our documentation](https://cloud.google.com/genomics/docs/tutorials/deepvariant#genomic_vcf_gvcf_configuration) for more details. > I'm guessing I would need to fork the gcp-deepvariant-runner repo. That's one way to do it, however, the easier way is to use our docker image; in that case launching DeepVariant will be as simple as running a `gcloud ...` command. Again please refer to [our documentations](https://cloud.google.com/genomics/docs/tutorials/deepvariant#before_you_begin) for more details.; The only issue at the moment is that deepvariant_runner is still working with previous version of DeepVariant (0.7.2) and we are in the process of releasing a new docker image that will be compatible with the latest DeepVairant (0.8.8). Please let me know if you have any difficulties with deepvariant_runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/193#issuecomment-508203315:520,simpl,simple,520,,https://github.com/google/deepvariant/issues/193#issuecomment-508203315,1,['simpl'],['simple']
Usability,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896293199:1547,clear,clear,1547,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199,1,['clear'],['clear']
Usability,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:53,clear,clearer,53,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,2,"['clear', 'learn']","['clearer', 'learns']"
Usability,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/318#issuecomment-645516372:162,clear,clear,162,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372,1,['clear'],['clear']
Usability,">::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7684,learn,learning,7684,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,@AndrewCarroll I am working with human data for clinical diagnosis of rare disease. There is great interest in the clinical community to call de novo variants in the child from trio data. There is data about the rates of those events. . In 1% of the human genome you should find zero de novo in most cases. In less than 50% of the cases there are one or two that are real. I can run it on some curated data and provide feedback.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450#issuecomment-830492777:419,feedback,feedback,419,,https://github.com/google/deepvariant/issues/450#issuecomment-830492777,1,['feedback'],['feedback']
Usability,"@AndrewCarroll Thanks for your response and the VCF example I have shared is my own data and not public data. Could you please refer to which part of my query you have answered? Maybe, I am not clear with your replies and which part does it address?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/318#issuecomment-645829089:194,clear,clear,194,,https://github.com/google/deepvariant/issues/318#issuecomment-645829089,1,['clear'],['clear']
Usability,@AndrewCarroll and @pgrosu thanks for your view on things and @pgrosu especially for your fantastic explanation of the model!. From a data scientist perspective this makes totally sense. I fully agree that one should always gather as much data as possible since one can always remove data that seems to be not useful afterwards. My intention was not to say that one do not need those data or that you should not use the information which those HomRef sites emerged from. I talked to some of my colleagues yesterday and they agreed that they would expect a variant caller to produce variant calls and information/statistics related to those and all further information would be opt-in since they would have no need for this. But my team could just be the minority here and simply an opt-out option for this behavior would be highly recommended!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612713066:772,simpl,simply,772,,https://github.com/google/deepvariant/issues/666#issuecomment-1612713066,1,['simpl'],['simply']
Usability,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1002,learn,learning,1002,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,1,['learn'],['learning']
Usability,"@Axze-rgb Great to hear from you! How have you been? I only want to hear good news :) Thank you kindly for the opportunity, but unfortunately my time is a bit dominated by a few things these days. A collaboration is a big commitment in order for me to assist properly, so for now I prefer to just provide guidance as necessary. Before you sequence too much, it's best to practice training a model with some data on your own to get a feel for what's happening. You have a great tutorial (including the data) on how to train a model [at the following link](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). Again it has germline diploid assumptions built into it. If your data varies a lot, it will take some time for the weights to shift to your data's behavior in order to achieve good accuracy (when possible). Regarding paralogs, you might need to align to a pangenome graph via giraffe, like in [the following paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365333/). Again you are adding complexities that require careful mastery of nuances you need to recognize when issues come up. Start playing with simple things first to get a good grasp of what's going on. So my humble recommendation is to take baby steps. Try to play with training a model first. Regarding learning about machine learning/deep neural networks (DNNs), find a book/youtube videos you can practice from - which matches your learning style and makes you code - to better understand how Deep Neural Networks work. Ideally sit in a class, so you are forced to do the homework, and begin with the fundamentals before eventually reaching DNNs (which have a prerequisite of some fundamentals of machine learning concepts as background). Most importantly, have fun and keep us posted :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163:305,guid,guidance,305,,https://github.com/google/deepvariant/issues/682#issuecomment-1722685163,6,"['guid', 'learn', 'simpl']","['guidance', 'learning', 'simple']"
Usability,"@CWYuan08 , for Nanopore R9.4.1, we suggest using [PEPPER](https://github.com/kishwarshafin/pepper). DeepVariant supports R10.4 simplex and duplex modes. Thank you for confirming that it is working for you now. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/673#issuecomment-1638414183:128,simpl,simplex,128,,https://github.com/google/deepvariant/issues/673#issuecomment-1638414183,1,['simpl'],['simplex']
Usability,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836:78,guid,guide,78,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836,4,['guid'],"['guide', 'guides']"
Usability,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255:38,simpl,simpler,38,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255,1,['simpl'],['simpler']
Usability,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:343,simpl,simply,343,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106,1,['simpl'],['simply']
Usability,"@MediciPrime We've updated the [quickstart guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md) based on your feedback to make it more clear that setting up a Cloud account and enabling billing isn't required to run DeepVariant. Take the new wording in that doc out for a spin and let us know what you think. Cheers,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-352087449:43,guid,guide,43,,https://github.com/google/deepvariant/issues/9#issuecomment-352087449,3,"['clear', 'feedback', 'guid']","['clear', 'feedback', 'guide']"
Usability,"@NourMarzouka ,. Closing this issue due to inactivity. Please feel free to reopen. . The conclusion is that if you have the intermediate files, then you can resume. Otherwise, you have to restart the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/825#issuecomment-2158916014:157,resume,resume,157,,https://github.com/google/deepvariant/issues/825#issuecomment-2158916014,1,['resume'],['resume']
Usability,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-961546511:394,simpl,simple,394,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511,1,['simpl'],['simple']
Usability,"@Zjianglin So the last thing you are missing is that you are not binding your `lustre` folder. Basically the `/lustre/Data/toolsDB/HostRefs/Human_hs37d5/` folder is not seen within your Singularity container at runtime, so you will need to bind that folder like this:. ```; singularity run -B /lustre/Data/toolsDB/HostRefs/Human_hs37d5/:/lustre/Data/toolsDB/HostRefs/Human_hs37d5/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/*; ```. For more information on binding paths, below is a link that expands on how it works:. https://docs.sylabs.io/guides/3.0/user-guide/bind_paths_and_mounts.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575943033:585,guid,guides,585,,https://github.com/google/deepvariant/issues/653#issuecomment-1575943033,2,['guid'],"['guide', 'guides']"
Usability,"@aalfi ,. Please follow the instructions here: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-pacbio-model-case-study.md. I see that you have `--dry_run=true` which would not run anything and would simply print out the commands:; ```; --[no]dry_run: Optional. If True, only prints out commands without executing them.; (default: 'false'); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561#issuecomment-1234749060:216,simpl,simply,216,,https://github.com/google/deepvariant/issues/561#issuecomment-1234749060,1,['simpl'],['simply']
Usability,"@ardoli, @ink1's suggestion is great, and there are two more:. * You can try [Udocker](https://blog.utar.co/blog/udocker) so that root privileges are not required for a Docker image.; * Tweak the DeepVariant source code to make it compile in user-space for your environment. It's a bit hairy, but can be done. The plus side is that you'll see DeepVariant is a pretty simple and modular pipeline, which you can tweak for your preferred analysis. Deep neural networks can be applied to many areas in the -Omics space besides just variant analysis.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372515026:367,simpl,simple,367,,https://github.com/google/deepvariant/issues/6#issuecomment-372515026,1,['simpl'],['simple']
Usability,"@bkurtoglu , can you please let us know what datatype you are using? The current model only works with ONT R10.4 simplex and duplex data. Can you provide some more details about your data and run:. 1) What chemistry is your ONT data?; 2) What basecaller did you use?; 3) How many cpus are you using to run make_examples?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2089293222:113,simpl,simplex,113,,https://github.com/google/deepvariant/issues/814#issuecomment-2089293222,1,['simpl'],['simplex']
Usability,"@chrisfleisch ; We have released v0.8.0 today :). This time, I made sure to try out Singularity as well. Here are some my personal notes when I tried to pull our Docker image and build Singularity image, based on the suggestion on your comment above. I'm still new to Singularity, so I'd really appreciate more feedback if any of the following doesn't make sense, or if any of these can be improved to be more similar to what users are used to. Once this becomes more mature (and if they become a popular use case), our team can consider adding them for official support in the future as well. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU. If you don't have singularity on your computer, install it first:; https://singularity.lbl.gov/install-linux. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; sudo docker pull gcr.io/deepvariant-docker/deepvariant:0.8.0; sudo docker tag gcr.io/deepvariant-docker/deepvariant:0.8.0 localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482430728:311,feedback,feedback,311,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728,1,['feedback'],['feedback']
Usability,"@claudiologiudice although this issue was closed some time ago, we have just released a new RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md) for Illumina data. . Please take a look if you are still considering this and let us know if you have any feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477:316,feedback,feedback,316,,https://github.com/google/deepvariant/issues/283#issuecomment-1281193477,1,['feedback'],['feedback']
Usability,"@crazysummerW ; I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir.; If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733:398,simpl,simple,398,,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733,1,['simpl'],['simple']
Usability,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452:17,intuit,intuition,17,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452,1,['intuit'],['intuition']
Usability,"@dennishendriksen ,. 1) Totally understandable, please give it a try when you have time and let us know if you are still facing issues. ; 2) Those are R9.4 data. The ONT model only supports R10.4 simplex or duplex. R9.4 has elevated error rate that causes a ton of candidates. DeepTio wouldn't work with R9 data. I am closing this issue. Feel free to reopen in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724#issuecomment-1819466106:196,simpl,simplex,196,,https://github.com/google/deepvariant/issues/724#issuecomment-1819466106,1,['simpl'],['simplex']
Usability,@depristo thank you for taking the time to help me out. . When I was trying to build DeepVariant on Ubuntu 16.04 it required that I install Google Cloud SDK because it needed the 'gsutil' command. Before installing Google Cloud SDK I started reading the 'deepvariant-quick-start.md' file and it talked a lot about using Google Cloud SDK and setting up a Google Cloud account and 'enable billing for you account'. . Based on your comment I am assuming that Google Cloud Platform is not needed and we can simply ignore 'gsutil' once the build process is complete?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-351577610:503,simpl,simply,503,,https://github.com/google/deepvariant/issues/9#issuecomment-351577610,1,['simpl'],['simply']
Usability,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480563989:339,usab,usability,339,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989,1,['usab'],['usability']
Usability,"@githubtefo It might be worth trying a smaller chromosome (like chr20, or try Quick Start) to make sure that things work end-to-end on your machine first.; If you use `--postprocess_variants_extra_args=""cpus=0""` , it's only the last step (`postprocess_variants`) that will be without multiprocessing. That step takes < 1hr for our PacBio BAM - you can see in https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#runtime-2 (this is before multiprocessing was used in postprocess_variants. @githubtefo We understand that speeding up DeepVariant is very important. We're actively making improvements! Thank you for reporting the issue. Our future releases will be better because of your feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953:695,feedback,feedback,695,,https://github.com/google/deepvariant/issues/810#issuecomment-2069997953,1,['feedback'],['feedback']
Usability,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461160956:132,simpl,simply,132,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956,1,['simpl'],['simply']
Usability,"@hangy1 , can you please run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) by simply copy-pasting the commands in your system? That way we could pinpoint the issue in a controlled case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2207033890:132,simpl,simply,132,,https://github.com/google/deepvariant/issues/839#issuecomment-2207033890,1,['simpl'],['simply']
Usability,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563:741,simpl,simply,741,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563,1,['simpl'],['simply']
Usability,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746:37,clear,clear,37,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746,1,['clear'],['clear']
Usability,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. ; And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```; name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668:655,clear,clear,655,,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668,1,['clear'],['clear']
Usability,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767885510:648,clear,clear,648,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510,1,['clear'],['clear']
Usability,"@japhill But for now, I also wonder if you can use the `-B` option in Singularity?; https://sylabs.io/guides/3.1/user-guide/bind_paths_and_mounts.html#user-defined-bind-paths; Can you try it and let me know if it works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1076923302:102,guid,guides,102,,https://github.com/google/deepvariant/issues/530#issuecomment-1076923302,2,['guid'],"['guide', 'guides']"
Usability,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251#issuecomment-566180701:216,clear,clear,216,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701,1,['clear'],['clear']
Usability,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925047566:52,clear,clear,52,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566,1,['clear'],['clear']
Usability,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430711053:316,clear,clear,316,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053,1,['clear'],['clear']
Usability,"@nvnieuwk from my understanding, I would be surprised if the numpy error is related to the region. I feel that might be a red herring. Unfortunately I don't have a clear answer for you because I can't reproduce your exact setting.; If you think it can still be related to the small region, the next thing I'd suggest you try is: Start from the setting that worked before (CRAM file that contains the whole chromosome 21). First confirm that still works, and just change one thing: restrict to a smaller region and see if that still works or fails with the numpy error again. Sorry that I don't have better suggestions for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697:164,clear,clear,164,,https://github.com/google/deepvariant/issues/640#issuecomment-1550815697,1,['clear'],['clear']
Usability,@pgrosu ; I am sorry if I was not clear enough. I tried to say that I could not install glibc locally on my system. ; I started the contact with the system administrator to see what they can do.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-454145983:34,clear,clear,34,,https://github.com/google/deepvariant/issues/137#issuecomment-454145983,1,['clear'],['clear']
Usability,"@pgrosu @pichuan thanks for your continues help in various questions I raised in this thread. Really helpful. How likely is it to generate a DeepTrio model in the future to use such a data combination? I guess it is much more than simply training a model on such pedigrees, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1615137619:231,simpl,simply,231,,https://github.com/google/deepvariant/issues/666#issuecomment-1615137619,1,['simpl'],['simply']
Usability,"@pgrosu Thank you for your guidance!!; So this is what I did for the sections you mentioned:; ```; # note_build_stage ""Install TensorFlow pip package"". # if [[ ""${DV_USE_PREINSTALLED_TF}"" = ""1"" ]]; then; # echo ""Skipping TensorFlow installation at user request; will use pre-installed TensorFlow.""; # else; # # Also pip install the latest TensorFlow with cpu support. We don't build the; # # full TF from source, but instead using prebuilt version. However, we still; # # need the full source version to build DeepVariant. # # Gets the nightly TF build: https://pypi.python.org/pypi/tf-nightly which is; # # necessary right now if we aren't pinning the TF source. We have observed; # # runtime failures if there's too much skew between the released TF package and; # # the source.; # if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly_gpu; # else; # echo ""Installing CPU-only TensorFlow nightly wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade tf_nightly; # fi; # else; # # Use the official TF release pip package.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # echo ""Installing GPU-enabled TensorFlow ${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow-gpu==${DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION}""; # elif [[ ""${DV_USE_GCP_OPTIMIZED_TF_WHL}"" = ""1"" ]]; then; # echo ""Installing Intel's CPU-only MKL TensorFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:27,guid,guidance,27,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['guid'],['guidance']
Usability,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:427,guid,guidance,427,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888,1,['guid'],['guidance']
Usability,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361436703:299,simpl,simplifications,299,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703,1,['simpl'],['simplifications']
Usability,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/360#issuecomment-713149241:589,simpl,simply,589,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241,3,['simpl'],['simply']
Usability,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:93,clear,clearly,93,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780,1,['clear'],['clearly']
Usability,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512127253:765,simpl,simply,765,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253,2,"['guid', 'simpl']","['guidance', 'simply']"
Usability,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/109#issuecomment-431125198:183,learn,learned,183,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198,1,['learn'],['learned']
Usability,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/376#issuecomment-720216342:255,feedback,feedback,255,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342,1,['feedback'],['feedback']
Usability,"@pichuan Thanks for the feedback -- I have altered my WDL (I'm building this as a method on FireCloud) to include the lscpu command, and have run with 1, 4 and 64 cores/shards so hopefully I will get a better sense of what's going on and report back here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461030330:24,feedback,feedback,24,,https://github.com/google/deepvariant/issues/150#issuecomment-461030330,1,['feedback'],['feedback']
Usability,@pichuan and @pgrosu : Thank you for your feedback. Will keep you posted on the issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-418211657:42,feedback,feedback,42,,https://github.com/google/deepvariant/issues/90#issuecomment-418211657,1,['feedback'],['feedback']
Usability,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683:401,learn,learn,401,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683,1,['learn'],['learn']
Usability,"@poddarharsh15, can you please check if the files were downloaded correctly and if their sizes look good. The case studies are designed in a way that you can simply copy-paste the commands and it should work. I just tested the case study and it worked on my end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950:158,simpl,simply,158,,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950,1,['simpl'],['simply']
Usability,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353510712:367,simpl,simplify,367,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712,1,['simpl'],['simplify']
Usability,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603:912,learn,learning,912,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603,1,['learn'],['learning']
Usability,"@sophienguyen01 it looks like you are trying to call somatic mutations here. If this is the case, please have a look at https://github.com/google/deepsomatic. DeepSomatic is still experimental, and we don't currently support retraining. However, it is designed to call somatic variants. . We would be very interested in the results you observe with this dataset if you are able to provide feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/728#issuecomment-1802319218:389,feedback,feedback,389,,https://github.com/google/deepvariant/issues/728#issuecomment-1802319218,1,['feedback'],['feedback']
Usability,@ssm0808 FYI on how to shuffle on Spark: http://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/rdd-programming-guide.html#shuffle-operations,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91#issuecomment-423040145:131,guid,guide,131,,https://github.com/google/deepvariant/issues/91#issuecomment-423040145,1,['guid'],['guide']
Usability,"@xunjieli yes, I can. I only extracted the variants where the analyzed sample had the alternative allele (genotype 1/1,0/1,./1). I did this for the single vcf obtained from deepvariant and for the combined vcf (obtained from gatk using the gvcf created with deepvariant).; Then, I compared only the variants (chromosome, position, reference and alternate allele) of this two sets. Is it clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170#issuecomment-482316115:387,clear,clear,387,,https://github.com/google/deepvariant/issues/170#issuecomment-482316115,1,['clear'],['clear']
Usability,"@ziphra ,. You are correct. It does seem like something went wrong with your mapping and you have reads without base-qualities. My suspicion was that the aligner was removing base-qualities from non-primary reads, but that's clearly not the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539#issuecomment-1144255541:225,clear,clearly,225,,https://github.com/google/deepvariant/issues/539#issuecomment-1144255541,1,['clear'],['clearly']
Usability,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-354748344:759,feedback,feedback,759,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344,1,['feedback'],['feedback']
Usability,A quick start guide issue,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:14,guid,guide,14,,https://github.com/google/deepvariant/issues/44,1,['guid'],['guide']
Usability,"Ah, understood, and thanks very much for the guidance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/519#issuecomment-1054593204:45,guid,guidance,45,,https://github.com/google/deepvariant/issues/519#issuecomment-1054593204,1,['guid'],['guidance']
Usability,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479522653:62,learn,learning,62,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653,3,"['clear', 'learn']","['clear', 'learning']"
Usability,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-482393946:112,simpl,simple,112,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946,1,['simpl'],['simple']
Usability,"And, just in case the documentation isn't clear:. This part:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ...; ```. The variable BIN_VERSION was specified in earlier in the steps:. ```; BIN_VERSION=""1.6.1""; ```. So, in Unix command it's equivalent to:. ```; google/deepvariant:""1.6.1"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763:42,clear,clear,42,,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763,1,['clear'],['clear']
Usability,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:647,resume,resumed,647,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['resume'],['resumed']
Usability,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:164,guid,guide,164,,https://github.com/google/deepvariant/issues/248,1,['guid'],['guide']
Usability,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486#issuecomment-984133927:793,clear,clear,793,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927,1,['clear'],['clear']
Usability,Assigning to @nmousavi for some feedback on why this path changed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-453148437:32,feedback,feedback,32,,https://github.com/google/deepvariant/issues/132#issuecomment-453148437,1,['feedback'],['feedback']
Usability,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/45#issuecomment-363913008:198,simpl,simply,198,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008,1,['simpl'],['simply']
Usability,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372953552:64,simpl,simple,64,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552,1,['simpl'],['simple']
Usability,Can you produce a small snippet of your BAM file and an associated command line that reproduces the issue and share it with us? We'd be happy to debug but it'd be great to have a simple example that causes the problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-370805635:179,simpl,simple,179,,https://github.com/google/deepvariant/issues/52#issuecomment-370805635,1,['simpl'],['simple']
Usability,Closing this. DeepVariant ran 'successfully' after splitting fastq files - although the output was bizarre. It is pretty clear that using DeepVariant with older PacBio data is not worthwhile -- my impression is that older PacBio data is only useful for finding larger SVs and worth using only if there is nothing better (e.g. PacBio HiFi or ONT).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614#issuecomment-1457774639:121,clear,clear,121,,https://github.com/google/deepvariant/issues/614#issuecomment-1457774639,1,['clear'],['clear']
Usability,"Dear @gunjanbaid . thanks for your reply,. I am using the docker version, and the quick start with gpu does not work. ; I have Ubuntu 20.04.; I used the exact same command as quickstart. the issue is the TF and CUDA version which is not matched with the deep variant current ubuntu version. I am using RTX 3090 and this card needs a higher version of TF. I tried to make a Docker-based on the versions that I need but unfortunately, this failed too,. Would it be possible to have an additional docker for these gpu cards?. I followed the exact libraries mentioned in this link to make the docker; https://www.fatalerrors.org/a/rtx3090-ubuntu-20.04-tensorflow-2.4.0-installation-guide.html. update:. The issue is with the CUDA version, most recent GPU cards need CUDA 11. ; Is there any plan for an update? . Thanks in advance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-834942601:678,guid,guide,678,,https://github.com/google/deepvariant/issues/452#issuecomment-834942601,1,['guid'],['guide']
Usability,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486:672,feedback,feedback,672,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486,1,['feedback'],['feedback']
Usability,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and; resequencing projects will move towards HiFi reads rather than CLR reads.; However, there is a lot of CLR sequencing data that has been generated in; the past couple of years and continues to be produced currently and could; still be useful for groups without the means to resequence using the novel; HiFi reads. So, I definitely see a niche in a large part of the; bioinformatics community that do a lot of data reusing (nowadays data; parasites). So, if there is anything we can do to help you n development,; please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<; notifications@github.com>) escribiÃ³:. > Hi @jdmontenegro <https://github.com/jdmontenegro>; >; > For the question about multi-allelic heterozygous calls - yes, DeepVariant; > is able to all 1/2 events, and will represent these in one line as a GT 1/2; > call in the VCF.; >; > For CLR calling in DeepVariant. It is theoretically possible for us to; > make a model for DeepVariant that can call CLR data. However, this requires; > us to write a special candidate generation logic to deal with the higher; > error rate. Based on what we perceive for the direction of future use in; > the genomics community, we think that data generated will be increasingly; > HiFi, so we have not been able to highly prioritize CLR models. Feedback; > from users like yourself will be useful to us in evaluating if that; > prioritization makes sense. For now, I can't commit to a timeframe under; > which we would support a PacBio CLR model.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/347#issuecomment-693080237:1470,Feedback,Feedback,1470,,https://github.com/google/deepvariant/issues/347#issuecomment-693080237,1,['Feedback'],['Feedback']
Usability,"Dear Paul,. Thank you for you suggestions about STR analysis! . Please do not apologize, as you have indeed helped me a lot and saved me a lot of time. Thank you for providing very professional guidance on Deepvariant. I am still unable to reach a conclusion on whether deepvariant can be used in some way on logically paired samples, so I will take a look at the HipSTR and deep repeat you provided, which may be helpful for my research. Thank you very much!. Wich you have a nice day!. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683275474:194,guid,guidance,194,,https://github.com/google/deepvariant/issues/697#issuecomment-1683275474,1,['guid'],['guidance']
Usability,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249:51,guid,guide,51,,https://github.com/google/deepvariant/issues/249,1,['guid'],['guide']
Usability,Deepvariant fails without clear reason - with PacBio data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:26,clear,clear,26,,https://github.com/google/deepvariant/issues/614,1,['clear'],['clear']
Usability,"Deepvariant fails without clear reason. . **Setup**; JHU Rockfish HPC; Singularity 3.8.7; singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:; ```; #!/bin/bash; #SBATCH --job-name=deep64_13448198; #SBATCH --time=24:00:00; #SBATCH --nodes=2; #SBATCH --ntasks-per-node=1; #SBATCH --cpus-per-task=32; #SBATCH --mem=0. ml anaconda; conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:26,clear,clear,26,,https://github.com/google/deepvariant/issues/614,1,['clear'],['clear']
Usability,"Do you have CUDA installed on your machine?. Check whether CUDA is installed on your machine. For example, run:; ```; rpm -qa | grep cuda; ```; or; ```; nvcc --version; ```. If you don't have CUDA installed, please follow the instructions on https://docs.nvidia.com/cuda/cuda-installation-guide-linux/ to make sure you install it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664:289,guid,guide-linux,289,,https://github.com/google/deepvariant/issues/619#issuecomment-1471268664,1,['guid'],['guide-linux']
Usability,Excellent! Please let us know if you have any other feedback. I will close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624#issuecomment-1496232959:52,feedback,feedback,52,,https://github.com/google/deepvariant/issues/624#issuecomment-1496232959,1,['feedback'],['feedback']
Usability,Feedback on poor results in family variation detection and GIAB dataset evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:0,Feedback,Feedback,0,,https://github.com/google/deepvariant/issues/689,1,['Feedback'],['Feedback']
Usability,Filtering clear false positives / low-confidence sites,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:10,clear,clear,10,,https://github.com/google/deepvariant/issues/645,1,['clear'],['clear']
Usability,"From the help page of Singularity Container [link](https://sylabs.io/guides/3.1/user-guide/cli/singularity_run.html):; ```; -B, --bind strings a user-bind path specification. spec has the format src[:dest[:opts]], where src and dest are outside and inside paths. If dest is not given, it is set equal to src. Mount options ('opts') may be specified as 'ro' (read-only) or 'rw' (read/write, which is the default). Multiple bind paths can be given by a comma separated list.; ```. Basically, this option binds your local directory to the directory inside a container. Also see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md#notes-on-singularity",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/506#issuecomment-1017088492:69,guid,guides,69,,https://github.com/google/deepvariant/issues/506#issuecomment-1017088492,2,['guid'],"['guide', 'guides']"
Usability,"Further to this question, I am trying to run 2 chromosome for the whole genome case study. . The user guide said that we can do ```--regions "" chr20 chr21"" ``` to run both chromosome, however, I get the following error when I run the make sample with ``` --regions ""chr20 chr21"" ```. I have also tried ```--regions ""20 21""```, but none of these works. . ```; E0814 18:04:05.051175 140174423590656 errors.py:64] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_D92FlN/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '21']"".; parallel: This job failed:; python /HD_disk/real-case-study-chr/input/bin/make_examples.zip --mode calling --ref /HD_disk/real-case-study-chr/input/data/hs37d5.fa.gz --reads /HD_disk/real-case-study-chr/input/data/HG002_NIST_150bp_50x.bam --examples /HD_disk/real-case-study-chr/output_1_2_3/HG002.examples.tfrecord@8.gz --regions 20 21 --gvcf /HD_disk/real-case-study-chr/output_1_2_3/HG002.gvcf.tfrecord@8.gz --task 2; ```. Would you please suggest how I should specify the region to get multiple chromosome running at the same time. For single chromosome, ``` --regions ""20"" ``` works fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72#issuecomment-412946034:102,guid,guide,102,,https://github.com/google/deepvariant/issues/72#issuecomment-412946034,1,['guid'],['guide']
Usability,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/317#issuecomment-644965403:183,Learn,Learning,183,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403,4,"['Learn', 'learn', 'simpl']","['Learning', 'learn', 'learns', 'simple']"
Usability,"Great! Thank you for letting us know!; If you're interested in sharing more feedback to us, that will be great too. :); I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1317489201:76,feedback,feedback,76,,https://github.com/google/deepvariant/issues/581#issuecomment-1317489201,1,['feedback'],['feedback']
Usability,"Great! The logs you posted confirmed that the checkpoints were not being written, but it's not clear _why_ that was the case. I will close this issue for now, but please don't hesitate to reopen if you encounter it again!. To your second question, that's correct! In 1.6, we migrated our training and inference platform from Slim to Keras, and as part of this effort we combined `model_train` and `model_eval` with a single executable `train` to make training easier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2033038202:95,clear,clear,95,,https://github.com/google/deepvariant/issues/797#issuecomment-2033038202,1,['clear'],['clear']
Usability,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054:69,clear,clear,69,,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054,1,['clear'],['clear']
Usability,"Hello @AndrewCarroll I launched DeepVariant as suggested, and then plotted the GQ distrubution. Here is a Gaussian kernel I fitted on it to better see (some people saw 2 peaks, others 4). So, here is what GauÃŸ would see . ; ![density_gaussian](https://github.com/google/deepvariant/assets/81575666/eeb28721-204b-47b6-9ea0-750aca1ba21f). Clearly 4 peaks (github horribly compresses the plot, actually here it's hard to see the 2 peaks on the right end)... now I am gonna investigate in relation to the AF. I already had a quick look at the data themselves, that's gonna be fun ... some AF are so small. . I will keep you posted. Don't hesitate to comment. EDIT: I changed the plot type, it's much better now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1647816652:337,Clear,Clearly,337,,https://github.com/google/deepvariant/issues/682#issuecomment-1647816652,1,['Clear'],['Clearly']
Usability,"Hello @kishwarshafin, . Thank you for your response and for the useful tips!. It appears that I have several types of flags for my reads presenting no quality score: ; ```; 0; 16; 2048; 2064; 4; ```; All the above flags are also present in my reads having a quality score sequence, except for the flag `4` (= read unmapped), which is absent. Also, it seems that in this case flags should be written without the `0x` prefix, which converts them to hexadecimal when they are written in decimal in the sam file, as I understand. ; `0x16` in https://broadinstitute.github.io/picard/explain-flags.html output a flag that cannot be set when read is not paired, and my read are not paired. However, it now seems clear that something went wrong with the alignment since I have all types of reads with no quality score sequence, not only `0x904` type reads (which are read unmapped `0x4`, not primary alignment `0x100`, and supplementary alignment `0x800`).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539#issuecomment-1141201135:705,clear,clear,705,,https://github.com/google/deepvariant/issues/539#issuecomment-1141201135,1,['clear'],['clear']
Usability,"Hello @mosh305 . I would like to learn more about what you would like to do with this sample, and, if possible, propose some alternatives that are more likely to succeed. I don't believe that the NA12878 Mt.Sinai set here can be reliably processed. This is a non-CCS PacBio dataset, so there will be far too many candidate examples generated to process efficiently. Also, the DeepVariant models are not trained for non-CCS PacBio reads. May I recommend that instead you consider the CCS dataset for HG002 that was submitted to genome in a bottle:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/. If this does sound interesting to you, we can provide to you a model trained for the CCS data type. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458681829:33,learn,learn,33,,https://github.com/google/deepvariant/issues/138#issuecomment-458681829,1,['learn'],['learn']
Usability,"Hello @pgrosu ; Thanks for the bcftools suggestion, exactly what's needed. I will try to digest your comment about how the model was trained. Thank you for explaining! I will report back. ; @AndrewCarroll I am gonna send you the vcf file, from a protonmail address (the proton domain is sometimes blocked by servers). . Thanks everyone for the great disussion, I hav learned a lot.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1649296164:367,learn,learned,367,,https://github.com/google/deepvariant/issues/682#issuecomment-1649296164,1,['learn'],['learned']
Usability,Hello @pichuan I will try your suggestion and let you know the feedback. As to the SIF conversion its just simple command using singularity (version 3.7.2) pull docker://google/deepvariant:1.4 (Now I use 1.5).,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1547543795:63,feedback,feedback,63,,https://github.com/google/deepvariant/issues/646#issuecomment-1547543795,2,"['feedback', 'simpl']","['feedback', 'simple']"
Usability,"Hello Andrew,. >Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. I have tried to verify the idea and plotted normalized histograms for all DeepTrio calls, filtered DeNovo-like (0/1-0/0-0/0) calls and true DeNovo calls (verified in IGV). Please see an attached picture. True DeNovo calls are still in the middle-low part of proband GQ range for DeNovo-like calls. I have tried to look at several DeNovo-like variants with a higher proband GQ, which were not in my list of true DeNovos, but all of them were false-positives. ![DeepTrio_GQproband2](https://user-images.githubusercontent.com/22089494/115329837-ea041300-a160-11eb-8788-31136171e157.png). I have prepared a few examples of DeepTrio variants in IGV (squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as fat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-822947862:131,learn,learned,131,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862,1,['learn'],['learned']
Usability,"Hello Andrew,. Thank you for your quick reply!. > 1. Are you taking the values from the QUAL field of the multisample glnexus VCF. Yes, exactly!. > 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. These are PCGC data, TPs were determined by combination of methods and manually curated. We expect an accuracy of the; found TPs to be > 95% (based on PCR for a similar dataset), although we might still miss some TP calls. > Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. I am sorry, I did not mention it. Yes, we are looking for denovos in trios. We are comparing efficiency of a few methods to create a pipeline for a big dataset. I thought we might use the QUAL score from DeepTrio to filter calls found by GATK4 pipeline.; If we use GQ fields for further filtering what values do you recommend for parents and proband?. Now, I use the following filters to retrieve denovo calls from the multisample glnexus VCF:; - Heterozygous ratio of proband = 0.2-0.8; - Homozygous ratio of parents <= 0.1; - ALT allele depth of proband >= 7; - Genotype quality of proband >= 60; - Read depth >= 7; - Allele count = 1; - Some regional filters were applied to remove noisy regions; - Common variants were removed based on 1000genome and gnomad population frequencies; Also, I had to split multiallelic calls and recalculate genotypes based on AD fields as I had a lot of ./. and 0/1 for Homozygous reference calls in parents. As results, I obtained 909 SNPs and 1,236 indels for my 10 test trios. My list of TPs contains 698 SNPs and 61 indels. So, I still have a lot of false-positives calls",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-820564569:661,learn,learned,661,,https://github.com/google/deepvariant/issues/440#issuecomment-820564569,1,['learn'],['learned']
Usability,"Hello DV team, and thanks for creating such a great tool! . I am currently trying to retrain the wgs model for a new species (a fish) however, during training, I see no evaluation statistics (precision, recall, f1) for either het or homalt. Or more specifically they are all 0.0. Eval stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:483,simpl,simple,483,,https://github.com/google/deepvariant/issues/876,1,['simpl'],['simple']
Usability,"Hello Maria,. Thanks for your reply, this is very clear.; I have such a depth because only a small region of my genome is amplified (~15kb) and several DNA are pooled by ""sample"". As the aim is to find if one out of the pooled DNA contains SNPs, I need to use all PacBio sequences instead of a subset.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/496#issuecomment-992270390:50,clear,clear,50,,https://github.com/google/deepvariant/issues/496#issuecomment-992270390,1,['clear'],['clear']
Usability,"Hello Pi-Chuan, thanks for answer above. Just to make it clear to me... I obtained a VCF file using version 1.4.0, but I cannot see the phase data (e.g., ""|"") in my sample. Does this mean that I need to call variants again using version 1.5.0 and the ""--phase_reads"" flag? Or does DeepVariant not explicitly provide this data, requiring me to run Whatshap + DeepVariant to visualize it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1547108448:57,clear,clear,57,,https://github.com/google/deepvariant/issues/649#issuecomment-1547108448,1,['clear'],['clear']
Usability,"Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/709:233,clear,clear,233,,https://github.com/google/deepvariant/issues/709,1,['clear'],['clear']
Usability,"Hello again,. actually the evidence bam folder is borderline unusable, there are so many folders that the file explorer tries to commit suicide every time I attempt to navigate it, same for the ""built-in"" explorer of bam viewers, for example Tablet is simply unusable, it systematically crashes. . I don't know how feasible this is computationally but maybe it would be best if DeepVariant produced a single bam per sample. Or even a single bam per sample per chromosome. If the user wants to subsequently split the bam that's not too complicated (otherwise the bam files themselves are neat I am happy with the results ^^). . Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/297#issuecomment-613950795:252,simpl,simply,252,,https://github.com/google/deepvariant/issues/297#issuecomment-613950795,1,['simpl'],['simply']
Usability,"Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error?. My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/139:126,learn,learned,126,,https://github.com/google/deepvariant/issues/139,2,['learn'],"['learned', 'learning']"
Usability,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/398:25,simpl,simple,25,,https://github.com/google/deepvariant/issues/398,2,"['simpl', 'user experience']","['simple', 'user experience']"
Usability,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479:224,guid,guide,224,,https://github.com/google/deepvariant/issues/479,1,['guid'],['guide']
Usability,"Hello, . I write here again to say 2 things; - first I apologize for having be somewhat agressive towards the person who came in the discussion, it seems to me it was just a curious person after all and not a troll. So sorry about that. I apologize @Joe-r-code ; - We are going to do high coverage ONT sequencing, which should allow for a better SNP calling. If it works well, would you be interested to train DeepVariant on my data set? That would make it able to manage high heterozygous genomes. You have been very helpful and I really appreciated it. So, I think we could plan to collaborate on training deepvariant on a highly heterozygous genome with many paralogs? I also ask this, because honestly I would love to understand better Neural Network and I learn better by doing. . The sequencing will take some time though.; Cheers. . EDIT: by ""curious"" I mean someone wanting to understand, I don't mean weird. In French curious can mean weird, so I don't know if this is the case in English.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1722449849:761,learn,learn,761,,https://github.com/google/deepvariant/issues/682#issuecomment-1722449849,1,['learn'],['learn']
Usability,"Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1046,guid,guidance,1046,,https://github.com/google/deepvariant/issues/793,1,['guid'],['guidance']
Usability,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/303:505,clear,clear,505,,https://github.com/google/deepvariant/issues/303,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Hello, ; a rather simple issue, I am following this tutorial ; https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-docker.md. and at this step ; `gsutil -m cp gs://deepvariant/quickstart-testdata/* input/; `; I get; `zsh: no matches found: gs://deepvariant/quickstart-testdata/*`. Thanks a lot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/61:18,simpl,simple,18,,https://github.com/google/deepvariant/issues/61,1,['simpl'],['simple']
Usability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:621,learn,learn,621,,https://github.com/google/deepvariant/issues/123,1,['learn'],['learn']
Usability,"Hello, I am trying to make a variant calling analysis with ONT data for Homo Sapiens. However, it is still running even though I started this analysis 6 days ago and it is still making examples. Could anyone help me to understand whether it is normal or I should re-run the analysis? The computer has 64-core Linux. It should be able to run the analysis for nearly one to two days based on my experiences with other variant callers. I would be very happy to get feedbacks from you! ; Thanks a lot!. Deep Variant/ Variant Calling; BIN_VERSION=1.6.1; Installation via Docker; Homo Sapiens Oxford Nanopore Whole Genome; ![Screenshot 2024-05-01 220709](https://github.com/google/deepvariant/assets/74244954/93bf098a-083d-40f5-ba41-9d876a836be3)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814:462,feedback,feedbacks,462,,https://github.com/google/deepvariant/issues/814,1,['feedback'],['feedbacks']
Usability,"Hello, I have installed all the binaries and ran all the shell scripts to install tensorflow and bazel, but after that I could not follow how to actually train the model or how to identify the snps for my files. I am sorry I am very new to deep learning. Any help would be greatly appreciate",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/63:245,learn,learning,245,,https://github.com/google/deepvariant/issues/63,1,['learn'],['learning']
Usability,"Hello, thanks for your answer. Actually my organism is an ancient tetraploid; it's quite old but we are able to find back the old duplicates easily (they just look like alleles with high divergence). The coverage in those regions doesn't deviate from what is expected. . I would bet the rotifers are variant dense, their genome is small (100 Mb) and there doesn't seem to be a lot of repeats. Somehow, it's quite the exact opposite of the human genome (large and full of repeats). ; The thing is, as it is an asexual, I can't replicate the trio strategy, as I only have a mother and a descendant. So I am unsure about what to do here. I was planning to call the variant in the mother and the daughter, assuming there should be all identical. Any new variant in the daughter I would regard as calling errors. (and I see new variants, though most of the variant in the daughter are the same as in the mother). I am unsure if retraining would make sense here. . EDIT: more simply, does the concept of a pedigree make any sense in a clonal organism?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/266#issuecomment-580767025:970,simpl,simply,970,,https://github.com/google/deepvariant/issues/266#issuecomment-580767025,1,['simpl'],['simply']
Usability,"Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. ; Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607:246,guid,guide,246,,https://github.com/google/deepvariant/issues/607,1,['guid'],['guide']
Usability,"Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:389,learn,learning,389,,https://github.com/google/deepvariant/issues/166,1,['learn'],['learning']
Usability,"Hello,. I have learned that DeepVariant is three steps at runtime.; To better understand DeepVariant, I want to know if there is a way to parse the output file of call_variants like the output file of make_examples ([Visualizing DeepVariant examples](https://github.com/google/deepvariant/blob/r1.6.1/docs/visualizing_examples.ipynb)), so that it becomes human readable instead of a binary string like below. ```; tf.Tensor(b'\nc2\x01T:\x01AZQ\x12\n\n\x02DP\x12\x04\n\x028\x1c\x12\x0e\n\x02AD\x12\x08\n\x028\x15\n\x028\x04\x12\x12\n\x03VAF\x12\x0b\n\t\x11\x92$I\x92$I\xc2?:\x14\xff\xff\xff\xff\xff\xff\xff\xff\xff\x01\xff\xff\xff\xff\xff\xff\xff\xff\xff\x01J\tHG001_10Xh\xf7Nr\x011\x80\x01\xf6N\x12\x03\n\x01\x00\x1a\x18\x00\x00\x00\x00\xb6\xf8\xef?\x00\x00\x00@\x9e\x0fH?]F\xb5\xb9\x86a$?', shape=(), dtype=string); ```. Sorry for the interruption!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/887:15,learn,learned,15,,https://github.com/google/deepvariant/issues/887,1,['learn'],['learned']
Usability,"Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/526:41,guid,guidelines,41,,https://github.com/google/deepvariant/issues/526,1,['guid'],['guidelines']
Usability,"Hello,. some of you might remember me. I know Deepvariant works well in human and in some species like rice, if I recall well. In short, all species with (very) low heterozygousity. I wonder if you see a use for Deepvariant in other species, like, there are marine species that are so ancient, diverse, widespread, you can have 5% heterozygosity, in shorts, SNPs everywhere. In such cases, Deepvariant has a tendency to ""ditch"" apparently at random (Sample1 Chrom3:20456 called, Sample2 same position not called, despite obvious evidence from mapping and support from long reads). Probably because it didn't learn what to do with so many SNPs. You know the issue because of your mosquito blog spot. And I have seen other issues (including mine) talking about that. The issue is to have a gold standard like in human, or trio data like in the mosquito, you need specific conditions, it seems difficult to imagine this could be doable with, let's say, a deep sea coral (just random example, I don't actually know what's their genome like). . Could a synthetic dataset help here? What if we feed Deepvariant a genome we made up based on what we can observed visually? I am aware if we make an error it will learn errors, but I wanted your opinion, because the lack of high quality reference dataset for many species, seems to be a serious limitation for this kind of program. Thanks a lot. Since it's not the first time I bring this out, I understand if you would simply close this. Have a good week everyone.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872:608,learn,learn,608,,https://github.com/google/deepvariant/issues/872,3,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hello,; It seems that 1 shard = 1 thread, however at some point in its run it seems that DeepVariant takes more threads N_SHARD is set to 20, no one else is using the computer at this moment and clearly more than 20 threads are taken. ![higdpgpplenjbaao](https://user-images.githubusercontent.com/23341393/74519762-a12a4c00-4f16-11ea-986c-b785044a618b.png). Is it expected?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/271:195,clear,clearly,195,,https://github.com/google/deepvariant/issues/271,1,['clear'],['clearly']
Usability,"Here I'll try to run Singularity on CentOS7 to see if I can reproduce the issue.; ; # Get a CentOS 7 machine; ```; gcloud compute instances create ""${USER}-centos"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"" \; --boot-disk-size ""200"" ; ```. # Install Singularity 3.5; Following steps here https://sylabs.io/guides/3.5/admin-guide/installation.html#installation-on-linux. Then I have:; ```; [pichuan@pichuan-centos singularity]$ singularity --version; singularity version 3.5.2; ```. # Run Quick Start with Singularity. I downloaded the data from [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). and then I tried:; ```; singularity pull docker://google/deepvariant:1.1.0"". singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --regions ""chr20:10,000,000-10,010,000"" \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --num_shards 24 -v 2; ```. This finished running and output the output.vcf.gz file without an issue. ---. Then, I was searching for ""status 252"" and found this earlier issue: https://github.com/google/deepvariant/issues/345 . At the end the issue seems to be that the CPU didn't have AVX instructions. Specifically, see @tedyun 's comment: https://github.com/google/deepvariant/issues/345#issuecomment-690820723. @williamrowell Can you check whether your CPU supports AVX instruction?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774634611:437,guid,guides,437,,https://github.com/google/deepvariant/issues/419#issuecomment-774634611,2,['guid'],"['guide', 'guides']"
Usability,"Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you wan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1738,Learn,Learning,1738,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['Learn'],['Learning']
Usability,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841:282,feedback,feedback,282,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841,1,['feedback'],['feedback']
Usability,"Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149:1208,simpl,simply,1208,,https://github.com/google/deepvariant/issues/149,1,['simpl'],['simply']
Usability,"Hi . I am a student learning about bioinformatics. I am able to us the docker image to run the quick start data, however, I am not able to run on my data. . I used samtools 1.9 to filter out low quality reads as follows. ```; quality=11; samtools view -bSq ${quality} ""${originalBAMFile}"" > ""${filteredBAMFile}""; samtools index ""${filteredBAMFile}""; ```. Note the output from the docker run script bellow is a little misleading. I copied all the data from an aws s3 buck to the local disk before the run. Also, the data is RNA not DNA. I was asked to ""see if it would just work"". I think maybe we have to change Uracil to look like Thymine. . Any suggestions would be greatly appreciated. Happy Holidays. Andy; ```; ubuntu@ip-172-31-1-186:~$ cat nohup.out ; + BIN_VERSION=0.9.0; + s3Root=/data; + INPUT_DIR=/data/aligned; + OUTPUT_DIR=/data/output; + quality=q11; + ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; + reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam; + date +%Y-%m-%d-%H.%M.%S-%Z%n; + dateStamp=2019-12-19-00.20.49-UTC; + output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz; + output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz; + nproc; + num_shards=4; + sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam"" --examples ""/tmp/deepvariant_tmp_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/254:20,learn,learning,20,,https://github.com/google/deepvariant/issues/254,1,['learn'],['learning']
Usability,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/451#issuecomment-830342295:563,learn,learned,563,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295,1,['learn'],['learned']
Usability,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-457857605:809,feedback,feedback,809,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605,1,['feedback'],['feedback']
Usability,"Hi @AndrewCarroll and @pgrosu. Thank you for your clear explanation. I understand the this case now and I am looking forward to seeing your new methods for handling these cases, as I believe it will be a significant improvement. I will explore using the method @pgrosu provided to temporarily process these varaints and ensure their uniformity. I will upgrade the version of DeepVariant in the next release of our project ([Chinese Quartet](https://github.com/xjtu-omics/ChineseQuartetGenome)). Furthermore, I noticed some information about DeepTrio in deepvariant homepage. Does DeepTrio support joint calling for quartet families (parents and **two** children)?. Best! ; Peng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902:50,clear,clear,50,,https://github.com/google/deepvariant/issues/660#issuecomment-1591182902,1,['clear'],['clear']
Usability,"Hi @AndrewCarroll the output for `samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep @SQ` is. ```; @SQ SN:I LN:15072434; @SQ SN:II LN:15279421; @SQ SN:III LN:13783801; @SQ SN:IV LN:17493829; @SQ SN:V LN:20924180; @SQ SN:X LN:17718942; @SQ SN:MtDNA LN:13794; ```. The alignment was done by a summer student working under the guidance of a post-doc (she recently had a baby so I didn't want to bother her) and it's possible a different reference version was used. In the error above I had used **c_elegans.PRJEB28388.WS274.genomic.fa**. We have another reference genome on our server so I tried it in-case this was the issue. Using **c_elegans.PRJNA13758.WS265.genomic.fa** deepvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-608553986:338,guid,guidance,338,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986,1,['guid'],['guidance']
Usability,"Hi @AndrewCarroll,. Thank you very much for this detailed explanation, it is clear to me what is happening now. Unfortunately, this is rather problematic for my use case because I am processing a cohort (DeepVariant + GLnexus) and I impute the variants afterwards. The imputation is based on the PL values: If there are sites where some samples have an actual variant but most of the other samples have a no call (hence with a discrepancy between GQ and PLs), those sites will end up with a low imputation score. Since my reads are ONT corrected, I rely very much on the imputation scores as a post-filtering step to guarantee the quality of my set. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403#issuecomment-760221333:77,clear,clear,77,,https://github.com/google/deepvariant/issues/403#issuecomment-760221333,1,['clear'],['clear']
Usability,"Hi @Asppagh ,. It can be many different reasons. ; Your machine setup definitely could be one of the factors.; And, not all inputs will take the same amount of time to run. For example, some regions in some BAMs might take longer to realign, etc. In DeepVariant, we tried to empirically set some thresholds so we hope that even the slowest cases are not too slow. But it's always useful to learn from our users what edge cases might still cause the DeepVariant to be slow. If your input BAM file is publicly sharable, you can also point us to it, and I'm happy to give it try and see if I can identify any reasons why it might be particularly slow. But it's also possible that your data is not publicly available. If that's the case, to diagnose your machine setup, you can start by running on some of our publicly shared data used in https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md. Specifically under:; https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md#how-to-reproduce-the-metrics-on-this-page. For example, you can run on our WGS BAM file: gs://deepvariant/case-study-testdata/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam on your cluster using singularity , and see what runtime you're getting. And, one more question that will help us provide better support:; Do you know if make_examples finish running on your machine? If so, how long it took on how many cores? If make_examples finished, then what's the runtime on call_variants and postprocess_variants?; (One possible issue we've seen before is that the call_variants stage is slow if users run on CPUs without acceleration)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-864304812:390,learn,learn,390,,https://github.com/google/deepvariant/issues/463#issuecomment-864304812,1,['learn'],['learn']
Usability,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870041849:404,clear,clear,404,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849,2,['clear'],['clear']
Usability,"Hi @Axze-rgb . To date, I've been hesitant to include simulated data as I'm very confident that we don't know all of the diverse and complex ways that errors happen to be able to model them. To a limited extent it might be possible to supplement training, but I'm fairly pessimistic on the approach. . For the skepticism component, one of the things we try to work on is investigating explainability - cases where the ML model has learned something that might not have been immediately obvious to a human. Hopefully we'll have some things to share there soon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872#issuecomment-2329912331:431,learn,learned,431,,https://github.com/google/deepvariant/issues/872#issuecomment-2329912331,1,['learn'],['learned']
Usability,"Hi @Axze-rgb and Andrew,. Good catch on the newline character â€“ itâ€™s hard to slow down at times when things are fun :) . Before having a presentation -- as my bandwidth is a bit tight these days (though I usually love presentations) -- I would be curious to see what you first get from Clair3 and Andrew's candidate region, as well as any additional insight Andrew might get from the VCF. I have this funny feeling of what the outcome might be â€“ and we can have the presentation later among the three of us (and anyone else that's interested) as things emerge more clearly. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468:565,clear,clearly,565,,https://github.com/google/deepvariant/issues/682#issuecomment-1652518468,1,['clear'],['clearly']
Usability,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:20,clear,clearly,20,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,1,['clear'],['clearly']
Usability,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291#issuecomment-607407000:983,simpl,simplest,983,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000,1,['simpl'],['simplest']
Usability,"Hi @FarmOmics ,; I'll close this issue. We would love to hear your feedback about the RNAseq caller. Please don't hesitate to reach out again if you have more questions or feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/572#issuecomment-1284746356:67,feedback,feedback,67,,https://github.com/google/deepvariant/issues/572#issuecomment-1284746356,2,['feedback'],['feedback']
Usability,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423:589,feedback,feedback,589,,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423,1,['feedback'],['feedback']
Usability,"Hi @GuillaumeHolley . This is a complicated issue, and though I've looked into it, I'm not 100% sure the following is correct, but I am reasonably confident:. DeepVariant is consists of some human-written heuristics which are used to identify positions that are candidate to be variant. Identified candidates are given to the neural network for classification. After this classification, another set of human-written heuristics converts the output probabilities of the network to VCF and gVCF entries. . As part of this process, there are positions that were never proposed as candidates because they do not have enough support to reach the candidate generation threshold (for the PacBio defaults, this is at least 2 reads which support an alternate event with a minimum ALT fraction of 0.12, and where the reads used for the calculation have MAPQ>=5). When this threshold is not met, the site is always considered either a reference or a no-call, and human-written heuristics are used to make a determine the genotype quality of the position for binning in the gVCF. This logic is fairly simple, and can sometimes result in a calculation of HET being the most likely, even if DeepVariant's neural network never made a call. The GQ should be set to 0 in these cases. This occurs because we value the neural network's output much more highly, and would like only it to make calls. . Hopefully this helps clarify what is going on here. The situation and explanation is complicated, so please feel free to ask further questions. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403#issuecomment-759187363:1089,simpl,simple,1089,,https://github.com/google/deepvariant/issues/403#issuecomment-759187363,1,['simpl'],['simple']
Usability,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474#issuecomment-883613731:437,simpl,simple,437,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731,1,['simpl'],['simple']
Usability,"Hi @JakeHagen . I took one of our 50x NovaSeq samples that are 150bp PE reads and trimmed the reads into 4 WGS consisting of a sample each for:. 1. first 100bp of the reads; 2. last 100bp of the reads; 3. first 75bp of the reads; 4. last 75bp of the reads. I wasn't able to replicate the effect that you see in any off the output reports. In your approach, did you trim the reads from the end (simply truncating to the first 75bp)? If so, I wasn't able to replicate the effect you see. There might be something more complicated about your sample. One possible explanation is that you have a run with lower sequencing quality and trimming to the first 75bp reads removes some lower quality parts which look suspicious to DeepVariant. If so, I wonder if your results would differ if you retained only the last 75bp reads. But I am not quite sure how to further diagnose. Here are my plots:. <img width=""1264"" alt=""HG003 novaseq 50x_only_first75bp"" src=""https://user-images.githubusercontent.com/583711/205179365-6a05941b-b6ba-47fc-b778-f970a9850651.png"">. <img width=""1266"" alt=""HG003 novaseq 50x_only_last75bp"" src=""https://user-images.githubusercontent.com/583711/205179387-7814e398-1b48-4476-9d35-6f93fafa8fbd.png"">. <img width=""1267"" alt=""HG003 novaseq 50x_only_first100bp"" src=""https://user-images.githubusercontent.com/583711/205179398-b2841a19-cb63-4ea1-9c7e-dfd79fad774d.png"">. <img width=""1286"" alt=""HG003 novaseq 50x_only_last100bp"" src=""https://user-images.githubusercontent.com/583711/205179415-a723bab5-e011-421b-a6a5-a7a750871159.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1334570619:394,simpl,simply,394,,https://github.com/google/deepvariant/issues/586#issuecomment-1334570619,1,['simpl'],['simply']
Usability,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338:368,clear,clear,368,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338,1,['clear'],['clear']
Usability,"Hi @JakeHagen ; Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/344#issuecomment-689698461:235,learn,learning,235,,https://github.com/google/deepvariant/issues/344#issuecomment-689698461,1,['learn'],['learning']
Usability,"Hi @NIBIL401 . I don't see any other specific issues in your command. Without knowing more about the specific types of differences, it's difficult to give advice on what might be missing. One observation that we do have is that DeepVariant has learned not to call RNA editing events as variants. These are post-transcription changes to the RNA sequence. Those edits appear as A->G and T->C in sequencing data. To give more advice beyond this, I think I would need to know more about the sequencing (the most ideal would be to have some a BAM file or snippet with a variant call not being made that we can diagnose why). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/775#issuecomment-1962219387:244,learn,learned,244,,https://github.com/google/deepvariant/issues/775#issuecomment-1962219387,1,['learn'],['learned']
Usability,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:52,clear,clear,52,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301,1,['clear'],['clear']
Usability,"Hi @Npaffen , I'm late to this thread. If I'm missing some context please feel free to remind me. Regarding your question ""**why the homref variants and the missings are added to the vcf in the first place**"":. DeepVariant starts with a set of candidates. These candidates came from a set of heuristics that propose a bunch of sites that potentially have variants.; You can find some thresholds we use for the heuristics here: https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L194; which basically means - if there is an alt allele that has a certain number of reads supporting it, and a certain % of reads supporting it, it will be proposed as a potential candidate. Then, DeepVariant applies a classifier on these candidates. To learn more about the representation that DeepVariant uses, https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/ is a good explanation. Each of the example (""image"") has a probability distribution for 3 classes. Based on the probability, the GT is assigned. As mentioned in previous answers, when the probability distribution from DeepVariant shows that it's not as confident, the GT is set to `./.`. By default, DeepVariant outputs the candidates even when they're classified as `0/0`, or when they're set to `./.`. ; This won't affect downstream tools like hap.py, though. Because these are not considered when tools like hap.py calculates accuracy. What DeepVariant outputs complies with the VCF spec, which says ""FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position."" If you look at our VCF, you'll notice that all variants (not including `0/0` and `./.`) should have `PASS`. Hopefully this helps. @Npaffen let me know if there's anything else that's unclear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430:772,learn,learn,772,,https://github.com/google/deepvariant/issues/666#issuecomment-1610105430,1,['learn'],['learn']
Usability,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347:770,simpl,simpler,770,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347,1,['simpl'],['simpler']
Usability,"Hi @Npaffen . Yes, in phase variants, the reads are assigned a haplotag value. Briefly, in this process, a set of potential variants are scored with heuristics (no neural network) on the likelihood that they are heterozygous variants. A cluster of such variants forms a candidate seed for a haplotype. The evidence from multiple reads across multiple positions are used to identify the putative variants on that haplotype, and then reads are scored based on whether they fall into one of the haplotypes, the other, or cannot be phased. Because this haplotagging uses information from much longer stretches and more candidate variants than the individual process of variant calling, it has the advantage of a broader set of information. This haplotagging is used to populate the information in the ""haplotype channel"" which is one of the inputs for DeepVariant long read data. We [wrote a blog](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/) describing this channel and its impact. Note that this process is only used to provide the information to the neural network for consider, the neural network will be able to learn when this channel is or is not reliable based on genome context, coverage, etc... the network's call on the genotype is what finally goes into a variant. As a result, haplotag is not used as input to generate the non-ref blocks of the gVCF, and as the final variants called are still from the neural network, the definition of a variant remains the same - a position with an ALT allele that receives a non-reference (0/0 or ./.) call. We are currently working on a deeper description of the phasing logic used in DeepVariant, which may help understand or reproduce the haplotag method more easily. Please let me know if anything in the explanation is unclear or can be elaborated further.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672:1146,learn,learn,1146,,https://github.com/google/deepvariant/issues/666#issuecomment-1602118672,1,['learn'],['learn']
Usability,"Hi @Npaffen,. That's great to hear that it worked! Let me answer each question individually:. `1` DeepVariant for the PACBIO model enables `--phase-reads` as shown below, but it is only used internally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:616,guid,guide,616,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318,1,['guid'],['guide']
Usability,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:294,simpl,simpler,294,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['simpl'],"['simpler', 'simplified']"
Usability,"Hi @Phillip-a-richmond . I understand agree that this workflow is cumbersome and far from ideal. Here is the course of action that we'll propose to take:. Within the next 2 weeks, we anticipate that we'll likely to have GIAB labels for X and Y. The first course of action that we'll take is to incorporate those and attempt to train a new model with this. Based on whether this looks promising, we may ask if you are interested to test a Docker image with this model and provide feedback on it. I can't guarantee that this will work, but I think it has reasonable odds. If it does not, we do have other ideas for X and Y calling, but they would take a bit more time. I will plan to reach back out in 2 weeks with an update about the labels and a refined estimate for when we might have the model. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831:479,feedback,feedback,479,,https://github.com/google/deepvariant/issues/518#issuecomment-1050404831,1,['feedback'],['feedback']
Usability,"Hi @PlatonB ,; In v1.6, we have added documentation on how to run on MGI data. Please see the links ""Complete Genomics data:Â [T7 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md);Â [G400 case study](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-g400-case-study.md)"" which was also linked from our GitHub main page. And let us know if you encounter any issues or have any feedback. Hopefully the customized models will give you better results!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/538#issuecomment-1817559748:452,feedback,feedback,452,,https://github.com/google/deepvariant/issues/538#issuecomment-1817559748,1,['feedback'],['feedback']
Usability,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:838,intuit,intuitive,838,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303,1,['intuit'],['intuitive']
Usability,"Hi @SHuang-Broad . By default, DeepVariant only looks at the content of the QUAL field (column 11) in order to populate the quality values. DeepVariant is able to look at and read in arbitrary additional tags (e.g. we have used the HP tag for phasing in the past). We have not previously experimented with BAQ, but with the framework above it would not be hard to look at it if you have an intuition that it might help. If you think it is promising, we could either do this investigation ourselves, or we could try to give you some instructions on how to do an experimental training if you are interested. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/565#issuecomment-1251305150:390,intuit,intuition,390,,https://github.com/google/deepvariant/issues/565#issuecomment-1251305150,1,['intuit'],['intuition']
Usability,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:125,resume,resume,125,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059,2,['resume'],['resume']
Usability,"Hi @Stikus , ; actually , it seems like simply removing the line; ```; #include <optional>; ```; will build. From the code, we're using ""optional"" from tensorflow::gtl::optional. So we don't really need the #include here. I have confirmed that removing this line builds on Ubuntu14.04. Please give that a try. If it doesn't work, let me know. I will make an internal fix, which will come out in the next release. For now, please make a local edit before you build.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236#issuecomment-557265475:40,simpl,simply,40,,https://github.com/google/deepvariant/issues/236#issuecomment-557265475,1,['simpl'],['simply']
Usability,"Hi @Suke-fudan , if your confusion is mainly about this line `The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). `, please ignore it for now. Internally we plan to remove that warning because it is confusing. To be clear:; - Our DeepTrio WGS and PACBIO models are trained with child height=60, and parents height=40. (Therefore 140 total).; - Our DeepTrio WES model was trained with child height and parents height=100, which is 300 total. If you're running PACBIO or WGS, you will see the (incorrect) warning about 140 isn't standard. If that's the case, please feel free to ignore that warning. We will improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-940505896:262,clear,clear,262,,https://github.com/google/deepvariant/issues/488#issuecomment-940505896,1,['clear'],['clear']
Usability,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308#issuecomment-628304654:871,guid,guide,871,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654,1,['guid'],['guide']
Usability,"Hi @Wenfei-Xian,. A max MAPQ score of 42 will likely have some effect, but I expect not an enormous one. I suspect that MAPQ at the lower end of the ranges would be more important, since if well-calibrated a difference between PHRED=42 and PHRED=60 is a very low additional absolute error probability. I have some bowtie mapped reads handy for a GIAB sample. I think I can conduct a quick experiment to see if that intuition is right.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/809#issuecomment-2067533385:415,intuit,intuition,415,,https://github.com/google/deepvariant/issues/809#issuecomment-2067533385,1,['intuit'],['intuition']
Usability,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328#issuecomment-663252998:981,learn,learning,981,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998,1,['learn'],['learning']
Usability,"Hi @Zjianglin , I took a quick look of the script and I'm not sure I fully understand what you're testing here. I tried a simplified version on my side. (The following steps has nothing to do with DeepVariant anymore. I'm mostly just testing `ls` and `singularity` right now). First I got these files in my /tmp; ```; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P /tmp ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; ```. Then I made my deepvariant.sif. ```; singularity build deepvariant.sif docker://google/deepvariant:1.5.0; ```. First, I check that I have the files; ```; REF=/tmp/ucsc.hg19.chr20.unittest.fasta; ls -al ${REF}*; ```; This worked.; (Note, you were doing something like `ls -al ""${ref_idx}*""`. Don't add the double quotes around the *. That didn't work for me. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; ls -al ${REF}*; ```. This also worked fine for me. I can see the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242:122,simpl,simplified,122,,https://github.com/google/deepvariant/issues/653#issuecomment-1573188242,1,['simpl'],['simplified']
Usability,"Hi @ZuyaoLiu ,; To confirm what you said -- do you mean that you train your own model (on your own data) and was able to get a much lower Mendelian violation rate? If so, that's great!!. Given that this is a non-human sample (not what we trained on), it seems like the right way to proceed is either to use our DeepTrio model, or like you said, to use your own customized model. So far both approaches seem like they would produce quite decent Mendelian violation rate. In the future, our team is interested in thinking more about making our model more generally robust to all non-human species as well. So thank you for your feedback. Hopefully the two options (either DeepTrio or your own model) will work for you for now. I'll close this issue now, but please feel free to share more thoughts.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/726#issuecomment-1852999418:626,feedback,feedback,626,,https://github.com/google/deepvariant/issues/726#issuecomment-1852999418,1,['feedback'],['feedback']
Usability,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/266#issuecomment-580713806:158,learn,learned,158,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806,1,['learn'],['learned']
Usability,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-553647872:1428,feedback,feedback,1428,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872,1,['feedback'],['feedback']
Usability,"Hi @aderzelle ; First of all, when I first think of ""deterministic"" behavior, I usually think about whether the exact same input yields the same results. Which we've made effort to make sure that running on the same input on the same machine should have deterministic behavior. If you have noticed non-deterministic behavior on the same sample, please do let us know. That said, you're absolutely right. These two represent the exact same event, but simple comparison scripts will not understand those events are in fact identical. ```; 90123; TGGGT; T--GTTC <-- Sample 1; TGTTC <-- Sample 2; ```. I think other users have reported to us in the past. So far we haven't looked closely into the code logic yet, because we mostly rely on tools such as `hap.py` for normalizing these during evaluation. So, this is a known behavior but haven't been investigated yet. And I do agree that it'll be good if we can behavior more consistently. . I will file an internal issue to track this (if we don't have one already). I can't guarantee when it'll be addressed though. If you have more suggestions on whether one representation might be better than the other, let me know. I'm closing this issue, but feel free to add your thoughts here. Thank you for reporting this!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/202#issuecomment-517123277:450,simpl,simple,450,,https://github.com/google/deepvariant/issues/202#issuecomment-517123277,1,['simpl'],['simple']
Usability,"Hi @aderzelle ; thanks for your feedback. If you have a chance to try out the two images I shared:; ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Please let me know whether they work for you or not. If you see any issues, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-562237254:32,feedback,feedback,32,,https://github.com/google/deepvariant/issues/243#issuecomment-562237254,1,['feedback'],['feedback']
Usability,"Hi @aditya-88, I looked into this issue a bit further. Unfortunately, I don't think using a `disutils.spawn`-style solution will be possible here. There are a few different things to point out. First, we need not just the `pyclif` binary, but other files that get created when you install CLIF. Here are all the files present in my `clif` directory:. ```; $ ls clif; bin clang examples include lib local pip-selfcheck.json python share; ```. Second, even if you did obtain all the needed CLIF files, we still cannot check for them to present at other paths. In our [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L78) for Bazel, we create a `new_local_repository` for CLIF. Creating this `new_local_repository` requires specifying a full absolute path in advance (more details in [the docs](https://docs.bazel.build/versions/master/be/workspace.html#new_local_repository)). Two solutions for you to get around this would be:; 1. Create a symlink to your CLIF directory via `ln -s $SOME_PATH/clif /usr/local/clif`; OR; 2. Modify the path in the [`WORKSPACE` file](https://github.com/google/deepvariant/blob/r0.7/WORKSPACE#L81). For example, if your files are at `/home/my_account/clif/`, you can change the path to `/home/my_account`. We can definitely make this more clear in our documentation in the future.; @pichuan Feel free to add on if I missed anything.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160#issuecomment-472209625:1296,clear,clear,1296,,https://github.com/google/deepvariant/issues/160#issuecomment-472209625,1,['clear'],['clear']
Usability,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458#issuecomment-844317545:132,simpl,simplify,132,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545,1,['simpl'],['simplify']
Usability,"Hi @amy-houseman . The strand of the read is one of the input channels in DeepVariant, so it is able to see that information and to learn the effects of strand bias on variant calling during training. For more information on what data is seen by DeepVariant, you can see the blog [Looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). We don't write the strand information itself in the variant calls. For filtering, we instead recommend using the GQ field, which we find to be well calibrated with the probability of genotype error. Because DeepVariant sees the strand information, it will incorporate this into its confidence about the variant. If you have some specific aspect of your problem which you think the strand of reads will behave different from the genomes and exomes DeepVariant is trained on, and you want to do independent filtering, you would have to find some other method to annotate the strand information as DeepVariant does not write this directly to the VCF.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/719#issuecomment-1767795328:132,learn,learn,132,,https://github.com/google/deepvariant/issues/719#issuecomment-1767795328,1,['learn'],['learn']
Usability,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383#issuecomment-727745570:327,learn,learn,327,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570,1,['learn'],['learn']
Usability,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-546050008:577,learn,learn,577,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008,1,['learn'],['learn']
Usability,"Hi @anands-repo ; Our codebase still supports TPU like before!; We decided to change our training tutorial to use GPU and CPU because it's a more general setting. Even though our code still supports TPU, we think it's better to simplify our tutorial.; If you encounter any issues when using DeepVariant with TPU, and if you suspect the issue might be in our codebase, please let us know.; (Btw, out of curiosity : have you been training DeepVariant model with TPUs? If you are using that, I would actually really love to hear your feedback. I didn't think we have that many external users with this functionality yet.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/376#issuecomment-720015500:228,simpl,simplify,228,,https://github.com/google/deepvariant/issues/376#issuecomment-720015500,2,"['feedback', 'simpl']","['feedback', 'simplify']"
Usability,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356#issuecomment-698549305:213,simpl,simplest,213,,https://github.com/google/deepvariant/issues/356#issuecomment-698549305,1,['simpl'],['simplest']
Usability,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-467081816:404,simpl,simplify,404,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816,1,['simpl'],['simplify']
Usability,"Hi @anitagh ,; the release won't come out within a week. Sorry for the inconvenience. For now you'll have to run make_examples separately and add the `--sample_name` flag. Another way to fix this is to make sure your BAM file header has one SM tag in it. If your BAM file is not very big, using `samtools reheader` to add a proper SM tag might also be a reasonable solution for now. If you're using GCP, you can try out the Google Cloud version that @samanvp pointed to in earlier comments. (If you have any feedback on that tool, please report to https://github.com/googlegenomics/gcp-deepvariant-runner/issues.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535654563:508,feedback,feedback,508,,https://github.com/google/deepvariant/issues/222#issuecomment-535654563,1,['feedback'],['feedback']
Usability,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161:234,learn,learn,234,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161,1,['learn'],['learn']
Usability,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844209552:439,simpl,simplified,439,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552,2,['simpl'],"['simpler', 'simplified']"
Usability,"Hi @annabeldekker . I'll paste some similar information from my answer in the other issue: https://github.com/google/deepvariant/issues/458#issuecomment-844317545. Hopefully my answer below will help you as well:. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. This `--use_hp_information` flag in the one-step `run_deepvariant` command actually controls both `sort_by_haplotypes` and `parse_sam_aux_fields` in the make_examples stage. If you set `--use_hp_information` to true in the one-step `run_deepvariant` command, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to true in make_examples stage. And if you set `--use_hp_information` to false, that means `sort_by_haplotypes` and `parse_sam_aux_fields` are both set to false in make_examples stage. In both cases, if you're running for PacBio, you always have to set `--add_hp_channel` to true in make_examples stage make sure the last channel is added. (If you're using the one-step `run_deepvariant` command, `--add_hp_channel` is automatically added). We tried our best to encaspulate these 3 flags into just one `--use_hp_information` in our one-step `run_deepvariant` command. However, I understand this might have caused further confusion when people tried to use the make_examples binary on its own.; You can find the logic here: ; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L240-L242. I will try to update our deepvariant-pacbio-model-case-study.md file to document this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-845522247:301,simpl,simplify,301,,https://github.com/google/deepvariant/issues/457#issuecomment-845522247,1,['simpl'],['simplify']
Usability,"Hi @brentp @kokyriakidis ,. We (genomics team in Google Health) have released DeepVariant 0.10 on 3/26, which includes improving consistency and accuracy by turning off `ws_use_window_selector_model` by default, along with the following other changes:; - Updated to Python3 and TensorFlow2; - Improved PacBio model for amplified libraries. For more information, please read our release notes: https://github.com/google/deepvariant/releases/tag/v0.10.0 ; If you have any feedback about your experience with v0.10, please let us know. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-605207780:470,feedback,feedback,470,,https://github.com/google/deepvariant/issues/272#issuecomment-605207780,1,['feedback'],['feedback']
Usability,"Hi @chapmanb , another update:. I went through a lot of hacky steps and built CLIF. I'm actually not sure whether it's actually usable or not, so if you have a setup that quickly give it a try, that will be great. Here's the instruction on how to get `pyclif` to run on a CentOS 6 machine:; ```; # Get a machine; gcloud beta compute instances create ""${USER}-centos6"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-6"" --image-project ""centos-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --zone ""us-west1-b"". # ssh into it; gcloud compute ssh ${USER}-centos6 --zone us-west1-b; ```. ```; ##### On the GCE instance #####; # Install Python 2.7; sudo yum install -y centos-release-SCL; sudo yum install -y python27; source /opt/rh/python27/enable. gsutil -m cp gs://deepvariant/packages/oss_clif/oss_clif.centos-6.9.latest.tgz /tmp/; (cd / && sudo tar xzf ""/tmp/oss_clif.centos-6.9.latest.tgz""); sudo ldconfig # Reload shared libraries.; ```; (I had to build with Python 2.7. Didn't figure out how to build with 2.6. Let me know if you actually need Python 2.6?). Once you do this, you can run `/usr/local/clif/bin/pyclif` and should see the usage:; ```; $ /usr/local/clif/bin/pyclif; usage: pyclif [-h] [--py3output] [--matcher_bin MATCHER_BIN] [--nc_test]; [--dump_dir DUMP_DIR] [--binary_dump] [--modname MODNAME]; [--prepend PREPEND] [--include_paths INCLUDE_PATHS]; [--ccdeps_out MODNAME.cc] [--ccinit_out MODNAME_init.cc]; [--header_out MODNAME.h] [--cc_flags CC_FLAGS] [--indent INDENT]; input_filename; pyclif: error: too few arguments; ```. Please let me know once you have a chance to try it.; CentOS 6 is tricky. It feels like everything is old :(; Let me know what other things are blocking you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385864674:128,usab,usable,128,,https://github.com/google/deepvariant/issues/29#issuecomment-385864674,1,['usab'],['usable']
Usability,"Hi @charlesfeigin,. It's great that things are being run as a non-root user. So to simplify the steps for reaching a solution, I just need three things from you:. 1. The complete set of commands that were typed for launching DeepVariant.; 2. The directories where the input files are located.; 3. The complete output of errors that were seen after DeepVariant was launched. This way we can reconstruct a runnable state. If you don't know everything, that's fine but at least steps 1 & 3 are necessary to begin to reconstruct step 2. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255:83,simpl,simplify,83,,https://github.com/google/deepvariant/issues/184#issuecomment-1699480255,1,['simpl'],['simplify']
Usability,"Hi @colsen ; thanks for confirming that the file was there. At this point I don't have a clear next guess on what could have been wrong. But can you check these two things:. 1. Confirm that docker see this file:; Run a similar command:. docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" ls /input/hg19.fasta*. This can confirm that docker actually sees the file. (should have both hg19.fasta and hg19.fasta.fai). 2. Run another program to make sure the the fasta file is well-formed. Given that you have tried samtools faidx it without issue, I think this is probably not the issue, but might still be good to check. @Roj4ck if you have more suggestions on this, please feel free to chime in!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/310#issuecomment-637679152:89,clear,clear,89,,https://github.com/google/deepvariant/issues/310#issuecomment-637679152,1,['clear'],['clear']
Usability,"Hi @crazysummerW , ; in your first step, you mentioned ""pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@rg\tID:test1\tSM:test1'"". Can you explain to me what you're trying to do in this step?. You also mentioned DeepConsensus. Are you planning to start from your own PacBio subreads BAM?; Unless you're starting with subreads, you don't need to apply DeepConsensus. If you are starting with PacBio HiFi reads (which I think is most of the DeepVariant users), you should directly map your FASTQ files using pbmm2, which gives you a BAM file, then you apply DeepVariant on the BAM. However, if you are indeed starting from PacBio subreads, you will need to follow https://github.com/google/deepconsensus/blob/r1.2/docs/quick_start.md to obtain the FASTQ file (which is the output that DeepConsensus gives you). Then, you map the FASTQ file, which gives you a BAM file, then you apply DeepVariant on the BAM. From your original question, it isn't quite clear to me what you're trying to do here. Can you elaborate?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1615164671:982,clear,clear,982,,https://github.com/google/deepvariant/issues/672#issuecomment-1615164671,1,['clear'],['clear']
Usability,"Hi @crazysummerW . Thank you for the pileup and for the trace. The trace makes it clear there isn't a variant here. . With respect to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this loo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:82,clear,clear,82,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832,1,['clear'],['clear']
Usability,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822613451:807,simpl,simply,807,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451,1,['simpl'],['simply']
Usability,"Hi @dhwani2410 . Is there any chance that this is from a sample for which publicly available data is present (e.g. HG001). I could look at pileups in the region for a more informed opinion. To your question about different variants - DeepVariant classifies event on a position-by-position basis, so the classifier does not have information about what output it gave at nearby variants. This can cause you to know a bit more than the classifier when looking at its full output. But it explains why the classifier can have a different result when two variants are putatively phased. One phenomenon that we observe with DeepVariant is that it often calls positions with substantial support as reference when they are nearby to other variants. This seems to be related to regions of segmental duplication, where the apparent variants come from reads that are mismapped to the region. We have a poster which documents this effect: https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096. So DeepVariant seems to learn something of the signature of segmental duplication and is likely not calling these positions as variant for this reason. On average, DeepVariant seems to be correct in these cases (that they are not true variants) more often than it is incorrect, which is why it has learned this pattern. However, it will not be correct in all cases. Either way, when you observe this pattern, it is a good idea to look at the region and consider whether the apparent variants may come from a duplication of related sequence.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/318#issuecomment-645810441:1021,learn,learn,1021,,https://github.com/google/deepvariant/issues/318#issuecomment-645810441,2,['learn'],"['learn', 'learned']"
Usability,"Hi @dmarkie . Thank you for your feedback. We did struggle and discuss internally on the representation that we should use for the hemizygous calls to make. Ultimately, we decided to use 0/0 and 1/1 for our presumption that this would break fewer downstream methods. However, some of that is a subjective judgement. The compromise of having the option in postprocess to handle this is an interesting one. We'll talk internally about the amount of effort and maintenance to support this. I can't make a commitment to anything now, but it is a very reasonable proposal.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/751#issuecomment-1854763028:33,feedback,feedback,33,,https://github.com/google/deepvariant/issues/751#issuecomment-1854763028,1,['feedback'],['feedback']
Usability,"Hi @elcortegano . Just to clarify, are you referring to the QUAL field of the VCF (the 6th column of tab-file itself), or the GQ field (the 10th column of a single sample). If you are referring to QUAL as the 6th column, this observation is expected. QUAL measures the probability that the ALT field has at least one allele with the ALT base. So you can think of it as p(HET) + p(HOM), or alternatively as 1 - p(REF). For homozygous positions, they look more clearly non-reference as in many cases they may not have any reference bases. . Heterozygous positions likely have at least some evidence for the Ref allele, which suggests a higher probability that the position might be Ref. If you are interested in filtering, we often recommend that the GQ field in the samples is preferable, as this is a measure of the genotype call itself being correct. There may be some differences between HET and HOM for this due to differences in difficulty in making those types of calls. However, it should be lower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/547#issuecomment-1194472227:459,clear,clearly,459,,https://github.com/google/deepvariant/issues/547#issuecomment-1194472227,1,['clear'],['clearly']
Usability,"Hi @fardokhtsadat . I realize in looking at the VCF entry, I was mistaken in saying DeepVariant sees a SNP and an Indel. In fact, it sees the candidates as you wrote. CAGCAGCGCT -> C; CAGCAGCGCT -> T. That's entirely on me, it should be clear from the VCF, and I don't have an excuse to make such a basic mistake. The call looks correct, it does look like a HET deletion of AGCAGCGCT. It looks like the realigner has decided to place a gap at this position and left-align the trailing T at the edge of the deletion so that when it is present it looks like a SNP. It's hard to authoritatively say why the realigner is going something without getting the actual BAM snippet and looking at the realignment. That part is in the heuristics of DeepVariant not the neural net. As a result of the re-alignment, DeepVariant sees the T SNP as a candidate but seems to correctly reject it. I think that's what's going on. In any case, a candidate here for the SNP seems to be a function of the realigner? . The behavior should not have changed from DeepVariant v1.2 to v1.5. In general, uncalled multiallelic events should be a relatively rare phenomenon. . Generally, if you need to, you can prune the alleles that are not called in DeepVariant VCF as long as the result of pruning stays compliant with the VCF spec expected by any downstream tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150:237,clear,clear,237,,https://github.com/google/deepvariant/issues/618#issuecomment-1470986150,1,['clear'],['clear']
Usability,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/200#issuecomment-513411694:502,clear,clear,502,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694,1,['clear'],['clear']
Usability,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301#issuecomment-617907163:372,clear,clear,372,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163,1,['clear'],['clear']
Usability,"Hi @genieusbio ,. One thing to note:. In https://gist.github.com/pichuan/baba6ee9bd9890be2a45076a4934dd38 , when you run `run_deepvariant`, this flag is specified: `--regions /input/idt_capture_novogene.grch38.bed`. Which means only the variants within the regions specified in the BED is used. If you look at the BED file:. ```; $ head -5 ./input/idt_capture_novogene.grch38.bed; chr1 69090 70008; chr1 450739 451678; chr1 685715 686654; chr1 925941 926013; chr1 930154 930336; ```. which does not include the region you're looking for. If you want to force call everything in that BAM file, you can simply remove the flag `--regions /input/idt_capture_novogene.grch38.bed` from your run. The downside is that will take longer. Or, if you just want to make sure that particular region is covered, then you can use @pgrosu 's suggestion and specify a small region that covers that range. Hopefully this is clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1720730115:601,simpl,simply,601,,https://github.com/google/deepvariant/issues/708#issuecomment-1720730115,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Hi @gevro . DeepVariant has two steps in calling variants. In the first (**make_examples**) a fairly simple, human-written heuristic identifies positions that are potentially variant and creates pileup examples of them. In the second stage (**call_variants**), a neural network classifies whether those positions are real variants or not and genotypes them. A RefCall entry occurs when a candidate variant is proposed and then is specifically rejected as non-variant. In addition, the model provides an estimate of its confidence (expressed as the QUAL and GQ fields for the entry). In the gVCF, a separate process determines the confidence for regions of the genome where no candidate is proposed and combines this with the information of the positions that have received a RefCall. For GLnexus, the genotyping is able to include the knowledge that these positions have a proposed alternate allele, but received a reference call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/386#issuecomment-728248806:101,simpl,simple,101,,https://github.com/google/deepvariant/issues/386#issuecomment-728248806,1,['simpl'],['simple']
Usability,"Hi @githubtefo ,; When you run the command, you should have logs in the terminal.; Can you provide those logs?. And, can you try something simple like https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md first? That should certainly have logs when you run it. If not, there's something else wrong in your environment that we need to understand first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2068119641:139,simpl,simple,139,,https://github.com/google/deepvariant/issues/810#issuecomment-2068119641,1,['simpl'],['simple']
Usability,"Hi @guandailu . We have investigated the use of the MarkDuplicates step. In WGS and exome samples we looked at, we observe no measurable difference at coverages 30x and higher. For coverages lower than 30x, we observe a very slight advantage for MarkDuplicates. For this reason in our [best practices recommendation](https://github.com/google/deepvariant/blob/r1.4/docs/trio-merge-case-study.md), we indicate duplicate marking as optional. . In terms of other preprocessing steps, we observe that running Base Quality Score Recalibrator (BQSR) consistently results in a slight reduction of accuracy (likely because DeepVariant has already learned how to use the underlying concept to recalibrate qualities as it considers calling). We have a strong recommendation NOT to run Variant Score Quality Recalibration (VQSR). We observe large reductions in accuracy, particularly in recall of rare variants. If you would like to filter for higher precision than the default calls, we recommend using the Genotype Quality (GQ) field, setting a higher threshold based on your desire for specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/569#issuecomment-1264952905:639,learn,learned,639,,https://github.com/google/deepvariant/issues/569#issuecomment-1264952905,1,['learn'],['learned']
Usability,"Hi @husamia . I am curious, are you working with human sequence data, or do these come from non-human species?. I will take a look at the ratio of these events in our sample. If we could provide a simple filtering script to postprocess a callset, would this be something that you might use?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450#issuecomment-830343312:197,simpl,simple,197,,https://github.com/google/deepvariant/issues/450#issuecomment-830343312,1,['simpl'],['simple']
Usability,"Hi @husamia . Thank you for the extra information. If there is any feedback that you can provide, I would be very grateful for examples. If you would like to follow up by email, you can reach me at awcarroll@google.com. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450#issuecomment-831068064:67,feedback,feedback,67,,https://github.com/google/deepvariant/issues/450#issuecomment-831068064,1,['feedback'],['feedback']
Usability,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:303,simpl,simplifying,303,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624,1,['simpl'],['simplifying']
Usability,"Hi @internalsensor , please see my answer below.; (And, can you give us some feedback on why you don't use the Docker version, but decided to use the prebuilt binaries instead? Thank you!). ===; Hi @internalsensor , ; I think it's likely what @pgrosu said about multiple versions installed. It'll be useful to figure out what actually got used. I was not able to reproduce the error you found.; In case it's useful, here is what I did to try to reproduce. I used a modified version of the WES script, in which I made sure I pip install a pinned version of intervaltree, and I downloaded the prebuilt binaries instead of rebuilding on my own:. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; wget https://gist.githubusercontent.com/pichuan/5a1216e080ccaf286810af062d6cb7a2/raw/20f6b3e9473b8f2e7d520edace1f2bb7a7231f06/run_wes_case_study_prebuilt_binaries.sh; bash -x run_wes_case_study_prebuilt_binaries.sh; ```. This worked with the prebuilt binaries v0.7.2. So I wasn't able to reproduce your error.; You can also use `pip show intervaltree` to double check what pip package you have.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155#issuecomment-464534790:77,feedback,feedback,77,,https://github.com/google/deepvariant/issues/155#issuecomment-464534790,1,['feedback'],['feedback']
Usability,"Hi @japhill ,; Just to make sure I understand this - are you saying that the Docker image has /mnt in it, and as result was causing problem with Singularity?. I do see a /mnt directory:; ```; $ sudo docker run google/deepvariant:1.3.0 ls -lh /; total 48K; lrwxrwxrwx 1 root root 7 Oct 6 16:47 bin -> usr/bin; drwxr-xr-x 2 root root 4.0K Apr 15 2020 boot; drwxr-xr-x 5 root root 340 Mar 23 23:34 dev; drwxr-xr-x 1 root root 4.0K Mar 23 23:34 etc; drwxr-xr-x 2 root root 4.0K Apr 15 2020 home; lrwxrwxrwx 1 root root 7 Oct 6 16:47 lib -> usr/lib; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib32 -> usr/lib32; lrwxrwxrwx 1 root root 9 Oct 6 16:47 lib64 -> usr/lib64; lrwxrwxrwx 1 root root 10 Oct 6 16:47 libx32 -> usr/libx32; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 media; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 mnt; drwxr-xr-x 1 root root 4.0K Dec 6 23:17 opt; dr-xr-xr-x 525 root root 0 Mar 23 23:34 proc; drwx------ 1 root root 4.0K Dec 6 23:15 root; drwxr-xr-x 5 root root 4.0K Oct 6 16:58 run; lrwxrwxrwx 1 root root 8 Oct 6 16:47 sbin -> usr/sbin; drwxr-xr-x 2 root root 4.0K Oct 6 16:47 srv; dr-xr-xr-x 13 root root 0 Mar 23 23:34 sys; drwxrwxrwt 1 root root 4.0K Dec 6 23:19 tmp; drwxr-xr-x 1 root root 4.0K Oct 6 16:47 usr; drwxr-xr-x 1 root root 4.0K Oct 6 16:58 var; ```; which is empty, so I think your suggestion of something like ""RUN rm -rf /mnt/"" makes sense. I'll also do a quick search to see if there are better approaches here. Thanks for the feedback. I'll track internally and make sure this is updated in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454:1457,feedback,feedback,1457,,https://github.com/google/deepvariant/issues/530#issuecomment-1076920454,1,['feedback'],['feedback']
Usability,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/347#issuecomment-693053180:618,Feedback,Feedback,618,,https://github.com/google/deepvariant/issues/347#issuecomment-693053180,1,['Feedback'],['Feedback']
Usability,"Hi @jguhlin ,; thanks for the feedback, and for letting us know that our users are still interested in Singularity images.; I have been a bit hesitated to make Singularity images part of our formal release process, mostly because the additional quality control burden. ; But, in the future we'll consider building these *.simg files and just distribute them. The steps below I used were documented here:; https://github.com/google/deepvariant/issues/132#issuecomment-482430728. I have the detailed commands that I used for my conversion, and I copied the output *.simg files here:. ```; gs://deepvariant/singularity_images/deepvariant-0.9.0-gpu.simg; gs://deepvariant/singularity_images/deepvariant-0.9.0.simg; ```; Or you can find them in the browser here:; https://console.cloud.google.com/storage/browser/deepvariant/singularity_images/. I was able to test both CPU and GPU version on the Quick Start data (see below). Can you see if if my `deepvariant-0.9.0-gpu.simg` file works for you?. ------. # @pichuan 's notes on building DeepVariant Singularity images for CPU and GPU (v0.9.0). If you don't have singularity on your computer, install it first:; https://sylabs.io/docs/. Once you do, you can pull the DeepVariant Docker image and convert it to a Singularity image. ## CPU image. ```; VERSION=0.9.0; sudo apt -y update && sudo apt-get install -y docker.io; sudo docker pull google/deepvariant:${VERSION}; sudo docker tag google/deepvariant:${VERSION} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant-${VERSION}.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant-${VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-561996442:30,feedback,feedback,30,,https://github.com/google/deepvariant/issues/243#issuecomment-561996442,1,['feedback'],['feedback']
Usability,"Hi @jkalleberg ,; please see See: https://gist.github.com/pichuan/7ad09bf1fa8f519facf6806eca835ea6. I'll close this issue for now. Feel free to open more issues if you have any questions or feedback for us.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1256836258:190,feedback,feedback,190,,https://github.com/google/deepvariant/issues/568#issuecomment-1256836258,1,['feedback'],['feedback']
Usability,"Hi @jordimaggi ; For anything that is `RefCall`, that means: even though a candidate variant was proposed, our machine learning classifier decided the most likely class is 0 (which means reference). ; You can read this section to get a bit more background on this: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. You could potentially take the finer-grained information (like `PL`) and try to adjust your own threshold. This could increase the sensitivity, but will likely hurt the specificity of DeepVariant's results. I'll ask @AndrewCarroll to add his thoughts here as well. Hi @splaisan , for the existing fields we have in our VCF file, we follow the standard definitions you can find on https://en.wikipedia.org/wiki/Variant_Call_Format#Common_FORMAT_fields (and we only fill in a subset of them). Let us know if there's anything specific that is not clear to you. Happy to explain more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531#issuecomment-1082175599:119,learn,learning,119,,https://github.com/google/deepvariant/issues/531#issuecomment-1082175599,2,"['clear', 'learn']","['clear', 'learning']"
Usability,"Hi @jumpyknight . **tl;dr - VCF_caller represents experimental functionality not ready for production use**. DeepVariant has three stages **make_examples** identifies candidate positions that may be variants using relatively simple human-written heuristics. **call_variants** applies the trained neural net model to identify which of these candidates are real variants and at what probability. **postprocess_variants** converts these probability to a VCF output. From the first versions of DeepVariant, very_sensitive_caller has been the logic used to generate candidates for make_examples. VCF_caller is an experimental feature we have been developing that would allow the generation of candidates directly from an input VCF, so that different (or third-party) logic could be applied to generate candidates. However, this feature is not ready for production use. Its inclusion here occurs because this code is in our main branch at the release time and reflects our internal use and experiments with it. DeepVariant v0.9 still uses very_sensitive_caller to generate candidates and VCF_caller is not used for any released model. We attempt to fully document and mention in release notes features that are ready for use. Your eyes to the released code are astute, I was not expecting to field questions for VCF_caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256#issuecomment-568660419:225,simpl,simple,225,,https://github.com/google/deepvariant/issues/256#issuecomment-568660419,1,['simpl'],['simple']
Usability,"Hi @kishwarshafin,. Thanks for the feedback, I actually forgot to get back to you about this but I have indeed filtered out the `NoCall` for the small cohort I have after your early feedback and it indeed solved this issue. Good luck with solving this!. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/521#issuecomment-1102489422:35,feedback,feedback,35,,https://github.com/google/deepvariant/issues/521#issuecomment-1102489422,2,['feedback'],['feedback']
Usability,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465225098:220,learn,learn,220,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098,1,['learn'],['learn']
Usability,"Hi @kyleaoconnell22 . Can I ask a few other questions - first, have you already attempted to use the human model, and, if so, do you have any indication of issues?. Second, do you know some of the rough properties of the genome (does it have a high repeat content? Do you know the approximate variant density and heterozygosity)?. We have been doing some experimentation with silver standard training data. We don't have any conclusive recommendations. We have thought about ising GATK for the silver lablels, but we're worried that this might carry the sort of artifacts that GATK makes into the deep learning model. Another idea we are looking at is to subset the Genome in a Bottle labels to regions which are more similar to the properties of the species to train a model for. I would suggest that this might be more promising as an approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459#issuecomment-858073154:602,learn,learning,602,,https://github.com/google/deepvariant/issues/459#issuecomment-858073154,1,['learn'],['learning']
Usability,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357#issuecomment-698486292:519,clear,clear,519,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292,1,['clear'],['clear']
Usability,"Hi @li1ba . For the question about why does the model have a difficult time calling this 1/1 versus 0/1, we have done further investigation. First, it doesn't look like this is a bug in pre-processing or how the data is represented. It seems to be a property of the model. . Second, the property of the model seems to reflect something learned about exome sequencing as opposed to WGS. To determine that, we ran your snippet through both the WGS and WES models. The WGS model is able to confidently call this site as 1/1:. ```; chr2 24146804 . C T 34.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:34:162:0,162:1:34,45,0; ```. When we run the WES model, we replicate your finding (this is with the most recent DeepVariant v1.4, so there is a small difference in the GQ values. ; ```; chr2 24146804 . C T 29.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:162:0,162:1:26,0,1; ```. This suggests that there is some aspect of exome sequencing that the DeepVariant WES model has learned makes this variant difficult to genotype, possibly because there is some signal that indicates only one allele is present. The reason for this might be some factor which isn't understood (at least by me). This is an interesting observation, but really understanding the reason for a 0/1 call at this position would probably need more investigation (for example, by going into the GIAB training labels for exome sequencing and seeing how often positions that look like this are 0/1 versus 1/1 and trying to understand why). With respect to why your collaborator has different results from you, it's very likely that the difference in mappers has a small effect on which reads are present and how many map discordantly. This small difference pushes the output for the variant on the edge of probabilities between 0/1 an 1/1 to the other side.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592#issuecomment-1335960453:336,learn,learned,336,,https://github.com/google/deepvariant/issues/592#issuecomment-1335960453,2,['learn'],['learned']
Usability,"Hi @linlin-coder ,; Thank you for bringing up this issue. I noticed that you're working on PacBio data. The reason why this is happening is:. In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy. Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence. > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase. So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be:; Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag. I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release. @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817:1166,clear,clear,1166,,https://github.com/google/deepvariant/issues/689#issuecomment-1660748817,1,['clear'],['clear']
Usability,"Hi @loipf , ; You can use `--help` with the different binaries, for example:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/run_deepvariant --help; ```. To see the various binaries, you can use:; ```; docker run google/deepvariant:1.0.0 ls /opt/deepvariant/bin/; ```; to list all the binaries. Then, for example, you can run:. ```; docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/show_examples --help; ```. or any other binaries. I understand your point though. It might be easier if we have a more clear help page without knowing the structure. I'll think about this and see if we can improve in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/362#issuecomment-709713660:524,clear,clear,524,,https://github.com/google/deepvariant/issues/362#issuecomment-709713660,1,['clear'],['clear']
Usability,"Hi @loipf ; I've made changes in internal code. It'll come out in the next release.; After the next release, feel free to let us know if have more feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/362#issuecomment-737653058:147,feedback,feedback,147,,https://github.com/google/deepvariant/issues/362#issuecomment-737653058,1,['feedback'],['feedback']
Usability,"Hi @maca8e . You are correct that DeepVariant_unfiltered is a preferable preset for DeepTrio calling. We had meant to update that in the case study documentation for the most recent release, and failing to do so is an oversight that we will correct. The DeepTrio paper does describe the unfiltered preset as preferable, and it should be reflected here. . With respect to a reference for DeepTrio's workings, have you seen the [DeepTrio preprint](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1)? If so, is there an item you would like described in greater detail. With respect to why NA12891/NA12892 are not used for training the parent model, this is simply because NIST does not have a truth set for these samples, while a truthset from NIST is available for HG001.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475#issuecomment-892390206:664,simpl,simply,664,,https://github.com/google/deepvariant/issues/475#issuecomment-892390206,1,['simpl'],['simply']
Usability,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/464#issuecomment-867256204:644,simpl,simple,644,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204,1,['simpl'],['simple']
Usability,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430736386:114,learn,learning,114,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386,1,['learn'],['learning']
Usability,"Hi @meghanasp21 . Yes - ""PASS"" variants are the ones that DeepVariant has made a call and indicate that DeepVariant thinks there is a germline variant at the position.; You can also refer to : https://samtools.github.io/hts-specs/VCFv4.2.pdf; ```; FILTER - filter status: PASS if this position has passed all filters, i.e., a call is made at this position. ; ```; Other than the conventional ""PASS"" to indicate that the tool has made a call, you might also see many other values being filled into this field. Especially if you have run somatic callers, you might notice people use it for various reasons why they filter out a variant. Currently, in DeepVariant, you might see another value ""RefCall"" - this means that DeepVariant identifies a position as a potential candidate early on, but the machine learning model decided that it is actually Ref (0/0). . So yes, for DeepVariant, you should just look at the ones that has ""PASS"".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/300#issuecomment-617897260:803,learn,learning,803,,https://github.com/google/deepvariant/issues/300#issuecomment-617897260,1,['learn'],['learning']
Usability,"Hi @melkerdawy , thanks for reporting this issue.; In general it is difficult for us (as the developers of DeepVariant) to give advice on the training behavior of other people's data. I can, however, from a perspective of a fellow ML researcher, ask a few more questions here to see if we can help you make progress:. (1) You said ""Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero."" --> The wording you have is confusing. I believe what you see here means that the `model_eval` code takes the checkpoints generated by `model_train`, and evaluated on the tuning data you've generated. And, based on the labeled tuning data, if the model thinks there are either no TNs(True Negatives) or FNs (False Negatives) --> this seems to indicate that it's likely at this point, the model might just don't call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203#issuecomment-518462111:365,learn,learning,365,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111,1,['learn'],['learning']
Usability,"Hi @melkerdawy ,; the DeepVariant codebase is currently designed to for DNA data only. The underlying tool and principle (of converting genomic data into a machine learning problem) could be generalized. But the existing tool as is isn't designed or used for RNA-seq data. In another word - it could work, but it will be open-ended research. I'd recommend you looking into how DeepVariant is done, and look into the [Nucleus](https://github.com/google/nucleus) library as well. We just recently announced a pip package for Nucleus. . Feel free to share your experimental results and discuss any issues you've encountered. We'll try our best to answer and discuss with you here. (Closing for now. Feel free to re-open)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-436861158:164,learn,learning,164,,https://github.com/google/deepvariant/issues/115#issuecomment-436861158,1,['learn'],['learning']
Usability,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241#issuecomment-559228691:218,feedback,feedback,218,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691,1,['feedback'],['feedback']
Usability,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488147736:857,intuit,intuition,857,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736,1,['intuit'],['intuition']
Usability,"Hi @one-matrix ,. From your original post, you mentioned you ran `python deepvariant/call_variants.py`. That won't work in DeepVariant setup. For DeepVariant, all binaries needs to be built with bazel. Unlike other pure Python setup, simply `python` a .py file won't execute it correctly. This is documented in the https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md page that @danielecook mentioned before. But, for extra clarity, let me run through it again, and write it down below for your reference. Here is an example of how I build and execute DeepVariant binaries:. # First, get a machine to run. In my example, I used a machine from GCP, using a command like this: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ```bash; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. # ssh into the machine, and get the latest DeepVariant repo. Then, I ssh into the machine:. ```bash; gcloud compute ssh ${USER}-cpu --zone us-west1-b; ```. Then I get the repo:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant; ```. Following in the instructions in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. I first run:. ```bash; sudo su; ```. and then:. ```bash; ./build-prereq.sh; ```. This step does a lot of stuff, including checking out other repos such as clif and tensorflow, and using that as part of the build environment. On my machine that I tested with just now, it took me 10m56.021s. and then run:. ```bash; ./build_and_test.sh; ```. This step took about 7min on my machine. . The [build_and_test.sh](https://github.com/google/deepvariant/blob/r1.6/build_and_test.sh) script is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872:234,simpl,simply,234,,https://github.com/google/deepvariant/issues/756#issuecomment-1865388872,1,['simpl'],['simply']
Usability,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-490179510:420,feedback,feedback,420,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510,2,"['feedback', 'intuit']","['feedback', 'intuition']"
Usability,"Hi @pgrosu , I filed an internal issue to track your suggestions in https://github.com/google/deepvariant/issues/87#issuecomment-413745335; I'll need to digest it a bit more, and probably talk to to a few users in more detail to design this experience better. On a high level, I think I'll need to at least think about overhauling the GitHub page to make it super clear how to run - even just with our docker image, it's apparently still not obvious for users to find (because it's hidden in the docs). And, the suggestion of making things as simple as possible is great one - both in terms of the directory structure, and also the output that our programs output. I think I have stared DeepVariant for too long that I'm losing empathy for people who look at it for the first time. I'll close this issue on GitHub, but will plan to think about this in more detail and hope to get back with at least some more plan later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-416830583:364,clear,clear,364,,https://github.com/google/deepvariant/issues/87#issuecomment-416830583,2,"['clear', 'simpl']","['clear', 'simple']"
Usability,"Hi @pgrosu , thanks for your feedback!; Thanks to @nmousavi 's work, the Cloud runner page is now updated:; https://cloud.google.com/genomics/docs/tutorials/deepvariant with 0.7.0, and 0.7.0 deepvariant_runner image is now tagged as latest. In terms of our GitHub page --; I fixed a few small thing such as the typo (and ran spell checking and fixed a few more!) Also fixed the `0.4.1` issue. I'll also address the contributing document at some point soon. These changes are right now still just in our internal codebase. I'll get it out when I have a chance to push out some documentation fixes. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-415643215:29,feedback,feedback,29,,https://github.com/google/deepvariant/issues/87#issuecomment-415643215,1,['feedback'],['feedback']
Usability,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didnâ€™t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 â€”> 0.982728; - SNP F1: 0.998913 â€”> 0.998867. Based on feedback, we decided itâ€™s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338#issuecomment-681126260:960,feedback,feedback,960,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260,1,['feedback'],['feedback']
Usability,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680:173,clear,clearly,173,,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680,2,"['clear', 'feedback']","['clearly', 'feedback']"
Usability,"Hi @pichuan @gunjanbaid . I apologize for overcomplicating this PR. I should have taken the rest of the discussion elsewhere. Thanks for your patient responses and advise so far. Even though I made many commits here, the only PR here is for `shuffle_tfrecords_beam.py`, and the changes are trivial as @gunjanbaid has reviewed already. The purpose of the PR is to enable `shuffle_tfrecords_beam.py` to work with DirectRunner with multiple workers (`--direct_num_workers=2, --direct_running_mode=""multi_processing""` for example), as well as with SparkRunner, FlinkRunner etc. To answer @gunjanbaid 's question on using beam.DoFn instead of a callable, my personal opinion is that that may not be necessary. The issue I saw is that `lambda` functions cannot be invoked through ParDo for these runners/modes. I attribute it to the same issue that we see with python multiprocessing which uses pickle to dispatch functions across processes and lambda functions are not picklable. So in this context, I think a callable meets the minimum requirements to enable this. I have tested on my end that the original deepvariant script and the modified script give the same output for a testcase. I am not trying to push you into accepting this PR, but just trying to clear any confusions that may have been caused by my multiple commits to the same branch, so that if and when you do revisit this matter, there is a summary about what's going on. I understand that the changes may still be far outside the priority of the team, so please take this at the appropriate pace. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-723468092:1254,clear,clear,1254,,https://github.com/google/deepvariant/pull/365#issuecomment-723468092,1,['clear'],['clear']
Usability,"Hi @pichuan, thanks for clearing this up! When you get the chance, please let me know what to look for in the call_variants -output. Also, I'm not sure I understand the format, using zcat I get many very short lines. I see AD, DP and VAF but not sure how to read variant positions / probabilities. . Many thanks! ; -Karoliina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2227870805:24,clear,clearing,24,,https://github.com/google/deepvariant/issues/849#issuecomment-2227870805,1,['clear'],['clearing']
Usability,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454#issuecomment-835842617:691,learn,learn,691,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617,1,['learn'],['learn']
Usability,"Hi @pichuan,. Thank you very much. I got to understand your excellent codes of deepvariant by learning tensorflow slim and Estimator API.; Although I still think that this topic is very useful for many users (deep learning beginners), please close this issue if your prioritization in list of things is low. Best regards,. Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-449729322:94,learn,learning,94,,https://github.com/google/deepvariant/issues/127#issuecomment-449729322,2,['learn'],['learning']
Usability,"Hi @pichuan,; I did come across various bits in different documents on how to train custom checkpoint but i don't have a confident on handle on it before I embark. - Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5); - Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?; - Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5); Thank you for guidance!; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765:649,guid,guidance,649,,https://github.com/google/deepvariant/issues/765,1,['guid'],['guidance']
Usability,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/child_trio_1.g.vcf.gz\; /output/parent1_trio_1.g.vcf.gz \; /output/parent2_trio_1.g.vcf.gz \; /output/child_trio_2g.vcf.gz\; /output/parent1_trio_2.g.vcf.gz \; /output/parent2_trio_2.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/trio_cohort_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308:556,clear,clear,556,,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308,1,['clear'],['clear']
Usability,"Hi @qili93 ; Thank you for looking into this! We have an internal issue to track Python 3 upgrade. I don't have a timeline for this now, but will let you know when we have more clear plans. ; Closing this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464963634:177,clear,clear,177,,https://github.com/google/deepvariant/issues/154#issuecomment-464963634,1,['clear'],['clear']
Usability,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-899967692:563,clear,clearly,563,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692,1,['clear'],['clearly']
Usability,"Hi @raphaelbetschart . In addition to what @pgrosu said. We do (rarely) observe genotype calls that differ substantially from the standard observations about HET/HOM/REF. We have made some prior observations about this for Germline data as a [section in our FAQ ](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data) and you can see a [poster](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096) describing this observation as well and relating it to situations with a segmental duplication. . A few other questions/observations -. Have you looked at the site in IGV (as @pgrosu mentions)?. What are the MAPQ values for reads? DeepVariant with default parameters won't see reads with MAPQ<5. If there is a clear bias in MAPQ where the REF allele has a much lower MAPQ, DeepVariant may think the REF alleles are mismapped or might not see them in the pileup. The coverage of this region is pretty high (e.g. as an abundant transcript or as a gene family with many isoforms). Do you know which of these two is the case?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1696419700:793,clear,clear,793,,https://github.com/google/deepvariant/issues/701#issuecomment-1696419700,1,['clear'],['clear']
Usability,"Hi @raphaelbetschart,. Just wondering on a few things:. 1) What are your QUAL and GQ scores for that call site?; 2) Is that SNP at an exon boundary?; 3) What tissue is this from? (Different tissue types can have an impact with DeepVariant RNA-Seq.); 4) Is this site in the [REDIportal](http://srv00.recas.ba.infn.it/atlas/search.html)? . Based on this curve, at 5x coverage you should still get a good majority of the CDS label variants that you saw at 3x coverage (this figure is from the Supplementary data available in [the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false) listed in the Reference section):. ![image](https://github.com/google/deepvariant/assets/6555937/2f6f8745-89ee-455e-8d20-950ef983875b). Thanks,; Paul. #### References; [1] [A deep-learning-based RNA-seq germline variant caller](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031?login=false)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1695811605:803,learn,learning-based,803,,https://github.com/google/deepvariant/issues/701#issuecomment-1695811605,1,['learn'],['learning-based']
Usability,"Hi @rickymagner ,; If you want to learn more about channels, you can take look at this blog post:; https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:; 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way.; 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013:34,learn,learn,34,,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013,1,['learn'],['learn']
Usability,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446#issuecomment-826519447:247,clear,clear,247,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447,1,['clear'],['clear']
Usability,"Hi @sclan ; to give you an update, I made an internal fix to make sure [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py) can output more clear errors when they occur.; The new code is not on GitHub yet, but will come out in the next release. Specifically, I changed the main function to be: ; ```; def main(_):; check_or_create_intermediate_results_dir(FLAGS.intermediate_results_dir); check_flags(). commands = create_all_commands(); for command in commands:; print('\n***** Running the command:*****\n{}\n'.format(command)); try:; subprocess.check_call(command, shell=True, executable='/bin/bash'); except subprocess.CalledProcessError as e:; logging.info(e.output); raise; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-596040813:184,clear,clear,184,,https://github.com/google/deepvariant/issues/232#issuecomment-596040813,1,['clear'],['clear']
Usability,"Hi @segoerge . Thank you for your question. It is a good observation that better support for MNP would be helpful. We have discussed this internally to a limited extent, but have not yet mapped out what would be involved to make the change. I couldn't give a timeframe for this yet (or give an indication as to whether this could be something in the next release). Your feedback is appreciated, as it helps us understand the needs of the user community. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-557229093:370,feedback,feedback,370,,https://github.com/google/deepvariant/issues/238#issuecomment-557229093,1,['feedback'],['feedback']
Usability,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690806128:149,simpl,simpler,149,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128,1,['simpl'],['simpler']
Usability,"Hi @shadrinams . From your description, it looks like you are running the DeepTrio merge component in the way that I would recommend. . Can I ask a few questions:. 1. Are you taking the values from the QUAL field of the multisample glnexus VCF; 2. How have you determined the TP sites? Are these Genome in a Bottle, or do they come from some other source. Your QUAL values for all calls match expectations for DeepTrio. However, I would not expect that ""true"" variants would have a distribution which departs from all calls. I can think of one reason that this might be the case:. Are these true variants de novos? DeepTrio's quality distribution for de novo variants is very different from its general quality distribution. This occurs because DeepTrio has learned that de novo events are quite rare, and so requires a higher standard of evidence to make a call which is a de novo. In these cases, DeepTrio is not extremely confident in the call, which results in a lower quality value. . The other things it might be good to look at is the genotype quality (GQ) field. This is the most direct measure of DeepTrio's confidence in a call, and maps directly from the probability for the called class. The QUAL value of a multisample VCF comes from GLnexus, and is a bit less direct measure of call confidence (still, I don't expect the distributions to be off in the manner that you see). I don't think I can immediately answer this puzzle, but hopefully with a little more information we can figure it out. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-820157988:758,learn,learned,758,,https://github.com/google/deepvariant/issues/440#issuecomment-820157988,1,['learn'],['learned']
Usability,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-821020591:132,learn,learned,132,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591,2,['learn'],['learned']
Usability,"Hi @shadrinams . Your observations about some apparent de novos originating from undercalling of a parent is interesting. I think that we'll look into whether some harmonization or filtering after calls could make the resulting calls more uniform. With respect to your examples. It is often difficult to be definitive about why DeepVariant does not make certain calls. I cannot give you a reason, but for some cases I can share observations. In all cases, I will list the call followed by observation. ```; chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:5:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:..; ```. This looks clean. DeepTrio's GQ is low probably because it is a clear de novo and it has learned such events are rare. ```; chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:27,1,0:18:0,18,45,990,990,990:II; ```; One thing I note - it looks to me like there are 3 alleles represented in the reads for the top parent: 1) there is an insertion event in-phase with a downstream HET SNP. 2) There is a reference allele in-phase with REF at that later position. 3) There is evidence for a T SNP that is also in-phase with the downstream HET variant. For the reads that are HET T, it could be interesting to see if they overlap any other variants that would suggest that they come from a copy number variant elsewhere in the genome. It may be the case that DeepTrio does not call a variant in the parent because some of the variant reads may be coming from elsewhere. ```; chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:28,0:50:0,90,899:..; ```. This is interesting, since the evidence reported by DeepTrio do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-823860823:758,clear,clear,758,,https://github.com/google/deepvariant/issues/440#issuecomment-823860823,2,"['clear', 'learn']","['clear', 'learned']"
Usability,"Hi @situssog ; Is there a specific reason why you have a fastA file and not a fastQ?; PacBio's tool (https://github.com/PacificBiosciences/bam2fastx) can generate either, so we would recommend simply generating the fastQ file to preserve the base quality scores, so then BWA-MEM, DeepVariant, and any other tools can use them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270#issuecomment-593729219:193,simpl,simply,193,,https://github.com/google/deepvariant/issues/270#issuecomment-593729219,1,['simpl'],['simply']
Usability,"Hi @tedyun,. 1. In this use case, we have phased and accurate data from the same cohort **X** that we use for the imputation.; 2. I was actually thinking about simply deleting the GQ=0 sites from my GVCFs which seem to be the simpler solution. As you said, they don't provide any useful information. I just wanted to point out here that having those records in output might confuse downstream applications (i.e. imputation).; 3. Unfortunately not. The problem is that our imputation system is exclusively based on the PL values and doesn't even read GT or GQ. Thank you for your questions and suggestions. Guillaume",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403#issuecomment-760775624:160,simpl,simply,160,,https://github.com/google/deepvariant/issues/403#issuecomment-760775624,2,['simpl'],"['simpler', 'simply']"
Usability,"Hi @tetsuro90 [this documentation](https://github.com/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs) has more information about the representation and the ""half-calls"" specifically. It involves some gnarly issues with overlapping variants in VCF for which there isn't a lot of standardization across tools unfortunately. [This issue](https://github.com/dnanexus-rnd/GLnexus/issues/210) also discusses some potential future developments. Any feedback there is welcome. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/302#issuecomment-620808470:443,feedback,feedback,443,,https://github.com/google/deepvariant/issues/302#issuecomment-620808470,1,['feedback'],['feedback']
Usability,"Hi @themkdemiiir ,; @akolesnikov already mentioned this in his previous answer -- it's totally possible to split by chromosomes first. Especially if that works better for your workflow.; (I know for many users, their workflow already split by chromosomes, so it makes sense to go from these BAMs that are per chromosome.). @akolesnikov 's answer above further explains that DeepVariant does its own sharding for any BAM inputs. You can run DeepVariant on a BAM that includes all chromosomes, but you can also do that on a BAM that include one chromosome (and then combine the VCFs later if you like). Please feel free to follow up if it's not clear. I'll close this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744#issuecomment-1839946718:643,clear,clear,643,,https://github.com/google/deepvariant/issues/744#issuecomment-1839946718,1,['clear'],['clear']
Usability,"Hi @themkdemiiir,. Although, it is absolutely possible to split the bam into chromosomes and run them separably there are better way to make a pipeline. There are 3 binaries that DeepVariant executes: make_examples, call_variants, and postprocess_variants. You may run those binaries from the docker. * make_examples is highly parallelized already. You may shard it to as many shards as needed by simply specifying the output of make_examples as sharded by adding @<num of shards> to the file name and add `--task` flag that specifies the task number for each shard. ; * call_variants will be run with the same number of shards.; * postprocess_variants has to be run in a single process. Here is an example of running shard 11 of make_examples and call_variants broken into 200 shards:. ```; bin/make_examples \; --examples /tmn/your_examples.tfrecord@200.gz \; --mode calling \; --reads /tmp/your_input_bam.bam \; --realign_reads \; --ref=/tmp/your_reference.fna \; --task=11. # Input for each instance of call_variants is the output of one instance of make_examples:; bin/call_variants.par \; --batch_size=32 \; --checkpoint <Path to the model checkpoint or saved model>.ckpt \; --examples /tmp/your_examples.tfrecord-00011-of-00200.gz \; --outfile /tmp/your_call_variants_output.cvo.tfrecord-00011-of-00200.gz. # Input for for postprocess would be the output of all instances of call_variants:; /tmp/your_call_variants_output.cvo.tfrecord@200.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525:397,simpl,simply,397,,https://github.com/google/deepvariant/issues/744#issuecomment-1836586525,1,['simpl'],['simply']
Usability,"Hi @ydLiu-HIT . Thank you for your question. The answer to this is complicated. It looks like the region that these elements is in is a LINE element - long regions with multiple copies through the genome that have high sequence similarity to each other. Because of the high sequence similarity, reads to line elements can map to other parts of the genome, and they are generally very difficult regions to call correctly. We've seen the behavior in DeepVariant not calling variants that are near other variants and in regions with two (or more) variant-rich haplotypes. We think that one of the reasons for this is that DeepVariant has learned that these regions represent uncaptured segmental duplication and LINE elements, which are often labelled as not variant in the more comprehensive genome in a bottle truth set. . Whether these positions represent true variants at that position, or sequences from a similar LINE element elsewhere is difficult to say. Since this is HG002, if this is within the confident regions, you can see whether Genome in a Bottle indicates them to be true variants. However, Genome in a Bottle has some more recent corrections to variants in/near LINE elements, so it may be better to check the updated (though still beta) [truth set](ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4beta_SmallVariantDraftBenchmark_07192019/). DeepVariant will output every candidate considered, so if you want to find positions that are called in this way, looking for 0/0 or ./. calls with more than 35% ALT support and within 100bp of 2 or more candidate variants may be able to pull out many of these examples. . The other option to pull out examples like this would be to intersect with a LINE element annotation track from UCSC. Please let us know if there is something unclear about this answer. This is a rather complicated concept and explanation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/233#issuecomment-550436768:635,learn,learned,635,,https://github.com/google/deepvariant/issues/233#issuecomment-550436768,1,['learn'],['learned']
Usability,"Hi @yonatansc97 . In the default WGS and exome models, DeepVariant does not expand the content of insertion sequences. Instead, it represents them with a character marker that indicates that an insertion is present at the position. To a human, this visually looks like a SNP in the pileup, but the machine learning model can easily distinguish this from a SNP event. This representation is necessary in order to keep the pileup at a consistent width. The information of the content of the insertion is also implicitly present in the 5th channel (supports variant) which only gets a value of 255 with the read when the insertion sequence matches the event to be called. The representation is in some ways sub-optimal, because in some cases information is lost in the presentation to the network. We have developed an additional process, which aligns reads to the ALT allele as well, which expands insertion sequences and makes the full sequence of ALT insertion events visible. This improves accuracy for calling variants in PacBio HiFi data and Oxford Nanopore data, and is defaulted to on in those models. . The flag for this option is: --alt_aligned_pileup, which can have the values: --alt_aligned_pileup=diff_channels and --alt_aligned_pileup=rows. . From our benchmarks, the --alt_aligned_pileup flag did increase accuracy in Illumina WGS and exome models, but only marginally. Since this also somewhat increases the compute cost, we do not use this option in Illumina WGS and exome models. We discuss this feature in more detail in our [blog on DeepVariant v1.0](https://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379#issuecomment-724880533:306,learn,learning,306,,https://github.com/google/deepvariant/issues/379#issuecomment-724880533,1,['learn'],['learning']
Usability,"Hi @zstephens, thanks for reaching out! The goal of `run_deepvariant` was to provide users with a single command they could use to run DeepVariant using Docker. For the sake of simplicity, this approach only accepts [a few different flags](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py#L53). We still do support the `sample_name` flag for `make_examples`, but it is not possible to use with `run_deepvariant`. To use this and other flags, I would suggest using separate commands for each step of DeepVariant (`make_examples`, `call_variants`, or `postprocess_variants`). Each of these binaries is included in the Docker image and can be run using a command such as below (with any additional desired flags). ```; sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0\ ; /opt/deepvariant/bin/make_examples; ```. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/173#issuecomment-483410868:177,simpl,simplicity,177,,https://github.com/google/deepvariant/issues/173#issuecomment-483410868,1,['simpl'],['simplicity']
Usability,"Hi Again,. Iâ€™ve had a chance to do some testing with my own samples, so I thought I should report back:. Going back to _point 4_, I noticed that you could choose to output a gVCF file in DeepVariant. However, that is not what is done by default (and itâ€™s not what I did), and I can also tell thatâ€™s probably not what they did either. Namely, there are over 3 _billion_ base pairs in the human genome: if you add up the passed filters and failed filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldnâ€™t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out â€œcomplex variantsâ€ (with more than one variant at a position), but .vcf files containing those variants werenâ€™t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:756,feedback,feedback,756,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['feedback'],['feedback']
Usability,"Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnâ€™t necessary for the Exome dataset (for the provided alignment from Genos). Iâ€™ve re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:958,learn,learning,958,,https://github.com/google/deepvariant/issues/171,1,['learn'],['learning']
Usability,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-656000987:553,guid,guide,553,,https://github.com/google/deepvariant/issues/321#issuecomment-656000987,1,['guid'],['guide']
Usability,"Hi All,. I appreciate that all of you are contributing scientific insights and taking your time to add thoughts on the DeepVariant GitHub issues. as @pichuan said, please remember that we are all scientists trying to work with each other and be respectful of each other. @Axze-rgb I haven't had a chance to look into the data (with what I have I would be looking at the effect in human data just to get a feel for how unusual it is). However, from your results, I'm not sure that this approach is promising and I am sorry I may have led you down a bad path. I was hoping to see sites that look clearly like subclonal SNPs (so look clear on other sources of error). From your pileups posted, it looks like many of the GQ30 VAF 0.15 population are mapping complexities. . It might be possible to further separate out mapping events, for example by increasing the MapQ filter, or excluding sites that are within a short distance of other variants, but I am not sure the approach will be worthwhile. My apologies, I think that in principle this is an interesting concept, but in practice, I am not sure your results indicate a clear win with the approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277:594,clear,clearly,594,,https://github.com/google/deepvariant/issues/682#issuecomment-1658889277,3,['clear'],"['clear', 'clearly']"
Usability,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676:1392,simpl,simple,1392,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676,1,['simpl'],['simple']
Usability,"Hi Andrea, I am getting the same error while I am trying to run the deepvariant code through the conda installation. Did you find a solution for it?. > Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; > ; > ```; > conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; > ```; > ; > Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; > I'm not sure I've been able to find it.; > ; > Thank you again for your support,; > Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-567477006:459,guid,guide,459,,https://github.com/google/deepvariant/issues/252#issuecomment-567477006,1,['guid'],['guide']
Usability,"Hi Andrew, sorry for the delay,. I was aiming to get per nucleotide values of read depth, base and mapping qualities. I just come to check that this information can in fact be obtained from `samtools depth` and `samtools mpileup`, so I guess that will work for us. Thank you for the feedback!; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/447#issuecomment-830259038:283,feedback,feedback,283,,https://github.com/google/deepvariant/issues/447#issuecomment-830259038,1,['feedback'],['feedback']
Usability,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815783472:39,feedback,feedback,39,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472,2,"['feedback', 'learn']","['feedback', 'learnt']"
Usability,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:793,simpl,simplified,793,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,1,['simpl'],['simplified']
Usability,"Hi Andrew,. Thank you very much for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that pap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:185,simpl,simpler,185,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,2,"['learn', 'simpl']","['learning', 'simpler']"
Usability,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361132599:291,simpl,simplify,291,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599,1,['simpl'],['simplify']
Usability,"Hi Brent I will check in on this and get back to you. Just to be clear, are using `FROM google/deepvariant` and then installing bioconda and using that to install bcftools / samtools or are you using bioconda to install deepvariant, samtools, and bcftools?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822571186:65,clear,clear,65,,https://github.com/google/deepvariant/issues/445#issuecomment-822571186,1,['clear'],['clear']
Usability,"Hi Charles,. In addition to what Pi-Chuan said. We run a case study for exome on 128G instance (https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md). postprocess_variants simply converts variants from internal format to VCF format. This error is very generic and from the information you provided there is no way to say it is related to TensorFlow.; It would be helpful if you could provide logs for postprocess_variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479584377:204,simpl,simply,204,,https://github.com/google/deepvariant/issues/167#issuecomment-479584377,1,['simpl'],['simply']
Usability,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483500506:1186,clear,clearly,1186,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506,1,['clear'],['clearly']
Usability,"Hi Guillaume,. Thank you for filing the issue and for the detailed explanation of your use case. I'd like to ask you some more questions to better understand the situation and to find a potential solution. 1. If we call the cohort you're processing with DeepVariant+GLnexus ""cohort **X**"", are you (a) using cohort **X** as a reference panel to impute another cohort **Y**, (b) using another cohort **Y** as a reference panel to impute your cohort **X**, or (c) using imputation software (e.g. Beagle) to re-genotype cohort **X** using the PL values in cohort **X** itself?; 2. In the example case you mentioned (""some samples have an actual variant but most of the other samples have a no call""), the no calls in the other samples would imply we don't have enough evidence (in terms of coverage, read quality, mapping quality, etc.) to call the other samples either reference or variant. Would keeping that cohort-level variant desirable for your downstream application? If it is, is there any other type of filters you can use (other than the imputation score) to keep those records (e.g. maximum of GQs in all samples)?; 3. Changing the no-call genotypes `./.` to the reference calls `0/0` is a relatively simple transformation (e.g. `bcftools +missing2ref`) that we use for some specific downstream applications. Would that help in your situation?. Thank you,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403#issuecomment-760500649:1209,simpl,simple,1209,,https://github.com/google/deepvariant/issues/403#issuecomment-760500649,1,['simpl'],['simple']
Usability,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325#issuecomment-659193187:248,clear,clear,248,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187,1,['clear'],['clear']
Usability,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339#issuecomment-681204545:227,guid,guidance,227,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545,1,['guid'],['guidance']
Usability,"Hi Mark (@depristo),. There is a theme of elegance with the current implementation that I tend to appreciate, though I agree that streamlining it for support/growth is rich with opportunities to explore. Let me know if you would like to work on it together - or just bounce off ideas - as some could be low-hanging fruit with simple remedies. Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361455478:326,simpl,simple,326,,https://github.com/google/deepvariant/issues/41#issuecomment-361455478,1,['simpl'],['simple']
Usability,"Hi Masaru,; I've filed an internal issue to track - we'll keep usability for beginners in mind for future API change.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-450001402:63,usab,usability,63,,https://github.com/google/deepvariant/issues/127#issuecomment-450001402,1,['usab'],['usability']
Usability,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366745899:56,simpl,simplest,56,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899,1,['simpl'],['simplest']
Usability,"Hi Paul, . I got an error when running on my HPC so I'm just waiting for the IT people to get back to me, I assume its a simple fix since I have managed to get the deepvariant container to work a few months back: . ```; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; ```. I'll let you know when they reply!; Amy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894:121,simpl,simple,121,,https://github.com/google/deepvariant/issues/691#issuecomment-1667418894,1,['simpl'],['simple']
Usability,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353701703:753,clear,clear,753,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703,1,['clear'],['clear']
Usability,"Hi Paul,. I simplified my setup a bit and have removed protobuf in my PATHs. I was able to complete the build, but most of the tests seem to be failing with the below error. TypeError: __new__() got an unexpected keyword argument 'serialized_options'. Attached is the log with verbose failures. Please let me know if you have any clue.; [log2.txt](https://github.com/google/deepvariant/files/2397845/log2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422863842:12,simpl,simplified,12,,https://github.com/google/deepvariant/issues/94#issuecomment-422863842,1,['simpl'],['simplified']
Usability,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So Iâ€™m administering this system but am not really a system administrator (Iâ€™m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just donâ€™t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) Iâ€™ve run the following, but confess I donâ€™t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:279,undo,undone,279,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973,1,['undo'],['undone']
Usability,"Hi Paul; Thanks for all the informations. As learnt from above, I should use the ""DeepVariant -> WhatsHap -> DeepTrio"" pipeline for better accuracy of the Deeptrio, as it do not support the functionality of reading haplotagging. But the DeepVariant can do it since 1.4.0, which means in case of trio analysis, ""DeepVariant + GLnexus"" will already be most accurate way currently, am I get it right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486:45,learn,learnt,45,,https://github.com/google/deepvariant/issues/704#issuecomment-1705891486,1,['learn'],['learnt']
Usability,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191#issuecomment-504481029:661,clear,clear,661,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029,2,['clear'],['clear']
Usability,"Hi Pichuan,. I understand the nature of Cloud Build, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:542,simpl,simple,542,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,1,['simpl'],['simple']
Usability,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016:99,learn,learning,99,,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016,1,['learn'],['learning']
Usability,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?. The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com); > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!; > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not; > ; > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > . region: oregen; m5dn.8xlarge; 32 CPU; 2 x 600GB SSD; Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573841969:1249,Learn,Learning,1249,,https://github.com/google/deepvariant/issues/260#issuecomment-573841969,1,['Learn'],['Learning']
Usability,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422619015:429,simpl,simplify,429,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015,1,['simpl'],['simplify']
Usability,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245:178,simpl,simply,178,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245,1,['simpl'],['simply']
Usability,"Hi Sebastian, . That output doesn't look right. Did you follow [DeepVariant quick start guide](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md#notes-on-gpu-image) ?. Could you please provide the exact commands that you run?. Thank you; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-655676506:88,guid,guide,88,,https://github.com/google/deepvariant/issues/321#issuecomment-655676506,1,['guid'],['guide']
Usability,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936:1749,learn,learning,1749,,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936,1,['learn'],['learning']
Usability,"Hi Sophie,. DeepVariant calls variants based on a trained model for different technologies. I'm not sure it has a model for Horizon Tru-Q 1, and only has models for the following technologies (`WGS`, `WES`, `PACBIO`, `ONT_R104`, `HYBRID_PACBIO_ILLUMINA`):. * NGS (Illumina) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:606,Simpl,Simplex,606,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629,3,"['Simpl', 'simpl']","['Simplex', 'simplex-case-study']"
Usability,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219:922,simpl,simple,922,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219,1,['simpl'],['simple']
Usability,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:913,guid,guide,913,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['guid'],['guide']
Usability,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051:694,guid,guide,694,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051,1,['guid'],['guide']
Usability,"Hi Ted,; Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing?; I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected?. Many thanks!; Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-694394538:25,feedback,feedback,25,,https://github.com/google/deepvariant/issues/346#issuecomment-694394538,1,['feedback'],['feedback']
Usability,"Hi again,; I didn't read carefully so I missed that you said you want to __train__ a model.; If you want to get `make_examples` to create more candidates, the other flags you need to consider are: `vsc_min_count_snps`, `vsc_min_count_indels`, `vsc_min_fraction_snps`, `vsc_min_fraction_indels`. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379110341:371,simpl,simply,371,,https://github.com/google/deepvariant/issues/62#issuecomment-379110341,1,['simpl'],['simply']
Usability,"Hi all,. This is not strictly a DeepVariant issue, but it is an issue I ran into while following the Giraffe case study using exome FASTQs from [here](https://www.internationalgenome.org/data-portal/sample/NA12878) (specifically, `SRR1518158_*.fastq.gz`). I'm running this on a DNAnexus cloud workstation (`mem1_ssd1_v2_x8`) and using the same version of KMC used in the case study. I am trying to run KMC using the following command:. ```; TMPDIR=$(mktemp -d); time ./kmc -k29 -okff -t8 sra.fq.paths ./sra.fq $TMPDIR; ```. where `sra.fq.paths` is the result of `ls SRR1518158_*.fastq.gz > sra.fq.paths`. The error message is simply `Error: unknown exception`. Have you seen this before? I thought I'd ask here before filing an issue with the KMC repo. Thanks,; Samantha",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/767:626,simpl,simply,626,,https://github.com/google/deepvariant/issues/767,1,['simpl'],['simply']
Usability,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308:544,guid,guideline,544,,https://github.com/google/deepvariant/issues/308,1,['guid'],['guideline']
Usability,"Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; AndrÃ© Santos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83:491,simpl,simple,491,,https://github.com/google/deepvariant/issues/83,1,['simpl'],['simple']
Usability,"Hi guys, . I'm running DeepVariantâ€”pretty much exactly as outlined at https://cloud.google.com/genomics/deepvariant â€”but I can't seem to get it to work. . My `deepvariant_configuration.yaml` (unmodified from guide) is ; ```; name: deepvariant_pipeline; inputParameters:; - name: PROJECT_ID; - name: OUTPUT_BUCKET; - name: MODEL; - name: DOCKER_IMAGE; - name: DOCKER_IMAGE_GPU; - name: STAGING_FOLDER_NAME; - name: OUTPUT_FILE_NAME; docker:; imageName: gcr.io/deepvariant-docker/deepvariant_runner; cmd: |; ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones 'us-*' \; --docker_image ""${DOCKER_IMAGE}"" \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --bam gs://deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --ref gs://deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta.gz \; --regions ""chr20:10,000,000-10,010,000""; ```. My `runner.sh` (changed `PROJECT_ID`,`OUTPUT_BUCKET`, and `STAGING_FOLDER_NAME`) is; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJEC",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:208,guid,guide,208,,https://github.com/google/deepvariant/issues/60,1,['guid'],['guide']
Usability,"Hi sorry for the delay,. I reran my sequences with a new copy of the human reference genome from https://www.ncbi.nlm.nih.gov/genome/guide/human/ up until running deepvariant. . I get the same headers as above, it doesn't seem like the bam, sam or even the reference start with the word 'chr1':. sam file header: @SQ	SN:NC_000001.11	LN:248956422; bam file (sorted picard): @HD	VN:1.6	SO:coordinate; @SQ	SN:NC_000001.11	LN:248956422; Original fasta reference file header: >NC_000001.11 Homo sapiens chromosome 1, GRCh38.p13 Primary Assembly; Bed file: chr1	12080	12251. All the code and steps I've done before are on my page under Exome_Pipeline/PE read analysis. I'm not sure! Ah, thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/527#issuecomment-1073019775:133,guid,guide,133,,https://github.com/google/deepvariant/issues/527#issuecomment-1073019775,1,['guid'],['guide']
Usability,"Hi there,. I've been trying to figure out how to actually run deepvariant in a cluster environment but thus far, the instructions seems a little cryptic to me. ; Is there perhaps a step-by-step guide to running deepvariant on a cluster with a PBS scheduler for instance?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474:194,guid,guide,194,,https://github.com/google/deepvariant/issues/474,1,['guid'],['guide']
Usability,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container ðŸ¤·â€â™‚ï¸. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:133,simpl,simple,133,,https://github.com/google/deepvariant/issues/199,1,['simpl'],['simple']
Usability,"Hi!. DeepVariant is not made to call variants in super high depth datasets, so this is not something we recommend doing.; There are several reasons for this, but one is that the pileup images can't be much taller than around 400 reads total due to limitations in the type of machine learning model we use. The cutoff of 1.5k reads is how many reads are considered when creating candidates, but a random selection of 95 of those are actually shown to the model. Increasing the pileup image height above 95 would require retraining, and we don't find that this helps significantly enough to justify the longer runtime. For high-depth applications we generally recommend that you use a specialized variant caller that is meant to support and make full use of all those reads. . To answer your question of The flags that can be set with call_variants are all the ones listed with ""flags.DEFINE""... in https://github.com/google/deepvariant/blob/r1.2/deepvariant/call_variants.py. The cutoff of 1.5k reads is `max_reads_per_partition` in https://github.com/google/deepvariant/blob/r1.2/deepvariant/make_examples_options.py. This is not a flag we recommend users to change though. Just for my curiosity though, can you share what your application is and why the depth is so high?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/496#issuecomment-990103583:283,learn,learning,283,,https://github.com/google/deepvariant/issues/496#issuecomment-990103583,1,['learn'],['learning']
Usability,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328:65,learn,learning,65,,https://github.com/google/deepvariant/issues/328,1,['learn'],['learning']
Usability,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916:143,simpl,simple,143,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916,1,['simpl'],['simple']
Usability,"Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose?; 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features.; 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed?. Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790:834,learn,learn,834,,https://github.com/google/deepvariant/issues/790,1,['learn'],['learn']
Usability,"Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792:401,guid,guide,401,,https://github.com/google/deepvariant/issues/792,2,['guid'],['guide']
Usability,"Hi, i am new to deepvariant and kind of learning some programming languages too as a beginner, i try to run these commands on my ubuntu 16.04 and had the following errors, both with Build_and_test.sh, Build-prereq.sh. please i need help on how to fix these errors and get deepvariant running. thank you. [sudo] password for solokopi: ; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ [[ 0 = \1 ]]; ++ export DV_CPP_TENSORFLOW_TAG=r1.9; ++ DV_CPP_TENSORFLOW_TAG=r1.9; ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:40,learn,learning,40,,https://github.com/google/deepvariant/issues/89,1,['learn'],['learning']
Usability,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462:260,clear,clear,260,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462,1,['clear'],['clear']
Usability,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445:214,guid,guidance,214,,https://github.com/google/deepvariant/issues/445,1,['guid'],['guidance']
Usability,"Hi, we recently updated the quick start and case studies to use docker.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md. It seems like I should rearrange and simplify our documentation to make it more clear.; Can you tell me where is the place you first read? Is it the main github page, or did you clone the codebase and directly start from there?; I will try to make some improvement next week.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415984031:271,simpl,simplify,271,,https://github.com/google/deepvariant/issues/89#issuecomment-415984031,2,"['clear', 'simpl']","['clear', 'simplify']"
Usability,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/198#issuecomment-512031369:142,simpl,simple,142,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369,1,['simpl'],['simple']
Usability,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/364:674,simpl,simply,674,,https://github.com/google/deepvariant/issues/364,3,['simpl'],['simply']
Usability,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291:127,guid,guide,127,,https://github.com/google/deepvariant/issues/291,3,"['guid', 'simpl']","['guide', 'simply']"
Usability,"Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:; ```; time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=0 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512 \; --debug 'true'; ```. I received an error regarding about the checkpoint: ```No checkpoint found.```; I also attached my log for training step here: ; [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:20,guid,guide,20,,https://github.com/google/deepvariant/issues/802,2,"['clear', 'guid']","['clear', 'guide']"
Usability,"Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity.; Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531:318,simpl,simply,318,,https://github.com/google/deepvariant/issues/531,1,['simpl'],['simply']
Usability,"Hi,. I have some thoughts on the **creating-pileup-image** part and really want to try it out. Since I don't have the GPU resources, I followed the **Getting Started with GCP** guide. Q1: How could I retrain my modified forked version of deep variant using gcloud? . Should I change the last parameter in this command?; `gcloud beta compute instances create ""${USER}-deepvariant-quickstart""`. Q2: How could I prepare the trianing data? . You mentioned running make_examples in training mode to get the data in the **Training DeepVariant models** Guide. How do I ""providing the --confident_regions and --truth_variants arguments""? These two terms are too biological for me to understand completely. . Really appreciated if you can provide some help. . Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46:177,guid,guide,177,,https://github.com/google/deepvariant/issues/46,2,"['Guid', 'guid']","['Guide', 'guide']"
Usability,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383347052:72,clear,clear,72,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052,1,['clear'],['clear']
Usability,"Hi,. I'm sorry for the late reply. I didn't have work yesterday and couldn't conduct the experiment. Thank you very much for the detailed suggestions you provided. I have observed that Paul provided very detailed suggestions regarding the steps he mentioned. I will try them out today and provide feedback here. My research top is about short tandem repeat in DNA, for example, in human reference gene hg 19 chr 2:47641559-47641586,; it has following base sequence:. chr 2:47641559-47641586 CAGGT AAAAAAAAAAAAAAAAAAAAAAAAAAA GGGTT. After search articles about next-generation sequencing, I found that their have so many factors that influence sequencing outout,such as during library construction step, PCR process will cause different repeat times because of polymerase slip. Besides that, during sequencing step, taking the Illumina sequencer as an example, during each round of cleaning, the base may not cleaned thoroughly or successfully combined with the next round of base; or other wise, a continuous series of repeated bases will influence illumination recognition signal in sequencer, and the weakened light intensity will affect the efficiency of the sequencer in identifying each binding base. So the above is the background of my use of Deepvariant. In my research topic, remove noise is very important process, because in somatic cells or cancer cells it's have different repeat times in tandem repeat regions, and I want to call every indel from human cells, but their are so many influence factor that change NGS output data so I can't find truth data in human's gene. Thank you for reading the questions I encountered in my research, I would greatly appreciate it if you could help me,; and wish you a pleasant work and life. Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021:297,feedback,feedback,297,,https://github.com/google/deepvariant/issues/697#issuecomment-1683209021,1,['feedback'],['feedback']
Usability,"Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696:116,guid,guide,116,,https://github.com/google/deepvariant/issues/696,2,['guid'],"['guide', 'guideline']"
Usability,"Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/148:1165,simpl,simply,1165,,https://github.com/google/deepvariant/issues/148,1,['simpl'],['simply']
Usability,"Hi,. It is actually possible to train DeepVariant on multiple GPUs, using the [MirroredStrategy](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train.py#L112). You can find the tensorflow documentation here: [link](https://www.tensorflow.org/guide/distributed_training). . It looks like we need to update the [FAQ](https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#can-model_train-be-run-on-multiple-gpus) to reflect thatâ€”thanks for brining it to our attention! The [training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#start-train) is up-to-date, so feel to continue to reference that. It looks like it already applies the `mirrored ` strategy.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240:257,guid,guide,257,,https://github.com/google/deepvariant/issues/885#issuecomment-2362016240,1,['guid'],['guide']
Usability,"Hi,. So I am trying a little bit of a peculiar setup in which I am using ONT corrected reads (with [Ratatosk](https://github.com/DecodeGenetics/Ratatosk)) to call variants with DeepVariant 1.1 in difficult to map regions. For that, I am using the default PACBIO model which works pretty well in the non-difficult to map regions from what I've seen so far.; Now in the difficult to map regions, it is a little bit of a mixed bag which I am trying to understand. More specifically, I get variants in my GVCFs such as:; ```; chr1	26740	.	C	<*>	0	.	END=26740	GT:GQ:MIN_DP:PL	./.:0:7:38,0,128; ```; What I do not understand is that the PL values seems to clearly indicate 0/1, yet the genotype is undefined. Also, my understanding of GQ is that it is derived from the PL values as the difference between most likely and second most likely genotypes. Yet, the GQ is 0 in this example. . I would really appreciate if you could shed some light on this. Thank you very much.; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403:650,clear,clearly,650,,https://github.com/google/deepvariant/issues/403,1,['clear'],['clearly']
Usability,"Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly.; I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus?. Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/712:571,guid,guidance,571,,https://github.com/google/deepvariant/issues/712,1,['guid'],['guidance']
Usability,"Hi,. Thank you for this great work!; I just wanted to learn about the license under which the Deepvariant model is shared?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/891:54,learn,learn,54,,https://github.com/google/deepvariant/issues/891,1,['learn'],['learn']
Usability,"Hi,. Thanks for the reply, but it doesn't address my question.; My doubt is about how to generate the bam file for doing the variant calling. For the checking/validation approach, I would like to ask a few questions. > Ok, then this would probably be my approach:; > ; > 1. Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". I did medium coverage sequencing based on some literature, but since my species might have high genetic diversity I used depth higher than recommended (1x), about ~10X per individual.; https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079667; https://onlinelibrary.wiley.com/doi/10.1111/mec.12105; https://pubmed.ncbi.nlm.nih.gov/34250668/. Then, I can't use the minimum depth you suggest of 15; my data should be good quality, hence GQ of 20 is fine, but depth of 15 is not achievable. > 2. Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs.; I am new in Bioinformatics, could you suggest a program to use to find those ""confident blocks"". I can't think of any program for doing it.; I have short-reads not long-reads, hence finding SV is not very reliable. Any suggestion on how to check if a variant falls within a SV?. > 3. Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks.; > ; > Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment. As mentioned before, I do not have trios data. @pichuan , Is it possible to train a model without trios data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771:1797,simpl,simplest,1797,,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771,1,['simpl'],['simplest']
Usability,"Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```; # works ; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; # fails; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624:100,simpl,simply,100,,https://github.com/google/deepvariant/issues/624,1,['simpl'],['simply']
Usability,"Hi,. Yes, my main question has not been addressed, it is about how to make the bam files for DeepVariant. ""DeepVariant is a deep learning-based variant caller that takes aligned reads (in BAM or CRAM format)"". The documentation doesn't say anything about how to make it/them.; For instance, one can filter reads based on mapping quality- which is not mentioned in the workflow of GATK, or mark PCR duplicates and also remove them.; For example, GATK needs read group information like sample ID, sequencing technology, etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195:129,learn,learning-based,129,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195,1,['learn'],['learning-based']
Usability,"Hi,; As you can see from our training case study, for larger datasets, we shuffled it on a distributed runner (dataflow). I think Beam also supports other things like Spark, but I have not tried it myself.; This shuffling code achieves two things: 1. Shuffle the examples, 2. Count the number of examples. Shuffling is important for training a model. Internally our tensorflow code does some extra shuffling in each batch, but that might not be enough on a global scale. I think a global shuffling is a better practice from a machine learning perspective. But empirically it might also be ok to not shuffle globally.; The other thing that you'll need is the total number of training examples, which I think you can find in the log when you made the examples (but you'll need to sum them up across the shards). The number of examples is used in the calculation of learning rate. You can certainly try without shuffling, but it's not the best practice I'd recommend.; Let me know if it works for you. If you find a good way to shuffle on Spark or other distributed system that you'd like to share, that will be great! Please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91#issuecomment-418172888:534,learn,learning,534,,https://github.com/google/deepvariant/issues/91#issuecomment-418172888,2,['learn'],['learning']
Usability,"Hi,; I am running into an error when I try to use make_examples with a training VCF. The error does not occur when I run make_examples in training mode, without a truth VCF, so I believe the error occurs there. I call make examples with the following:; `/opt/deepvariant/bin/make_examples --mode training --ref refs/${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions refs/confidence.bed `. I am using a simple test VCF attached here and I get the following error:; `; ValueError: Invalid argument: Invalid interval: reference_name: ""NC_000962.3"" start: -1 end: 22; `; Could you point me towards how to debug this issue? Thank you! ; [test.vcf.gz](https://github.com/google/deepvariant/files/1985941/test.vcf.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71:452,simpl,simple,452,,https://github.com/google/deepvariant/issues/71,1,['simpl'],['simple']
Usability,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379:1491,clear,clear,1491,,https://github.com/google/deepvariant/issues/379,1,['clear'],['clear']
Usability,"Hi,; I have a question regarding the use of the _**realign_reads**_ parameter for make_examples.py script. I am interested in running DeepVariant for PACBIO long reads data, and I don't want to realign reads before variant calling. It is stated in the official description: _--[no]realign_reads: If True, locally realign reads before calling variants. Reads longer than 500 bp are never realigned.(default: 'true')_, but, in the description for running PACBIO data it is stated that **_Please note, that if you create your own script make_examples must be called with --norealign_reads flag for PacBio long reads._**. I am having trouble understanding the meaning of realign reads parameter, does the TRUE value realign or doesn't realign reads (basically is the parameter NOrealign _reads, or realign reads)? Just want to clear things up so I don't waste time on long runs. Thank you in advance! :)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/437:823,clear,clear,823,,https://github.com/google/deepvariant/issues/437,1,['clear'],['clear']
Usability,"Hi,; I understand that it's a common use case to be compatible with GATK. We'll consider potentially adding a flag for that conversion. But since we're following the spec (using the VCF v4.3 spec: page 25 of this doc https://samtools.github.io/hts-specs/VCFv4.3.pdf), this won't be of high priority. For now the simple substitution you're doing is correct. I filed a bug internally to track. I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83#issuecomment-403616978:312,simpl,simple,312,,https://github.com/google/deepvariant/issues/83#issuecomment-403616978,1,['simpl'],['simple']
Usability,"Hi,; In the latest release we added this page:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md; As well as a training tutorial:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md. If you decide to train a model, we would love to hear your feedback as detailed as you are willing to provide us. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/80#issuecomment-415562625:324,feedback,feedback,324,,https://github.com/google/deepvariant/issues/80#issuecomment-415562625,1,['feedback'],['feedback']
Usability,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. Weâ€™re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently weâ€™re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204#issuecomment-518518311:457,learn,learns,457,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311,1,['learn'],['learns']
Usability,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12493,resume,resume,12493,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['resume'],['resume']
Usability,I am confusion about your deep learning architecture.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801:31,learn,learning,31,,https://github.com/google/deepvariant/issues/801,1,['learn'],['learning']
Usability,I am exploring sharding the `call_variants` step and I came across this comment that I can simply concatenate TF records together with the `cat` Unix utility:. https://github.com/google/deepvariant/issues/49#issuecomment-366848143. I have a couple of questions if you could please answer. *Q1:* Can I also use the `cat` utility to concatenate GVCF output shards?. Suppose I name the `call_variants` outputs: callvariants-00056-of-00064.gz. *Q2:* Could I pass along `callvariants@64.gz` to the `--infile` option of `postprocess_variants`?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/113:91,simpl,simply,91,,https://github.com/google/deepvariant/issues/113,1,['simpl'],['simply']
Usability,"I am guessing it could be related to the use of the `--nv` flag ([docs](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html)), but I am not familiar with using GPUs with singularity containers. This [guide](https://modinst.lu.lv/wp-content/uploads/2021/03/Singularity_seminars_Aleksandrs_Gutcaits.pdf) suggests unsetting the `LD_LIBRARY_PATH`. Which version of singularity are you using?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870:95,guid,guides,95,,https://github.com/google/deepvariant/issues/580#issuecomment-1304648870,3,['guid'],"['guide', 'guides']"
Usability,"I can open a separate issue if it's helpful, but just a couple more things related to this... ; First, while the haplotype stuff like in the images above is mostly gone with `ws_use_window_selector_model=false`, I still see the problem in some false positive calls. Another thing that happens with things that DV calls de novos but obviously are not is that the kid will just meet some threshold and have a number of MQ ~40 reads with the de novo, where as the parent will have a number of reads with the allele that are MQ ~18 or lower. But the VCF reports AD[1] == 0 for many of these in the parent. If the count of low-quality alleles were reported in the sample fields in the VCF, it would be simpler to filter to make sure the allele was absent from the parent.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-592194090:697,simpl,simpler,697,,https://github.com/google/deepvariant/issues/272#issuecomment-592194090,1,['simpl'],['simpler']
Usability,I download the source code and then simply ran:. `./build-prereq.sh`. I got the error: '/tmp/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl'. ![screen shot 2017-12-07 at 11 24 47 am](https://user-images.githubusercontent.com/7627987/33692414-469c85d2-db41-11e7-909e-1a5b53356481.png),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5:36,simpl,simply,36,,https://github.com/google/deepvariant/issues/5,1,['simpl'],['simply']
Usability,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-698670947:563,feedback,feedback,563,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947,1,['feedback'],['feedback']
Usability,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/247#issuecomment-563285549:1018,learn,learn,1018,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549,1,['learn'],['learn']
Usability,"I guess the naming pattern is related to second question.; Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x.; Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188:458,clear,clears,458,,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188,1,['clear'],['clears']
Usability,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338:281,learn,learning,281,,https://github.com/google/deepvariant/issues/338,2,['learn'],"['learn', 'learning']"
Usability,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175#issuecomment-560625427:864,user experience,user experience,864,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427,1,['user experience'],['user experience']
Usability,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378:101,simpl,simply,101,,https://github.com/google/deepvariant/issues/378,1,['simpl'],['simply']
Usability,"I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801:2,learn,learned,2,,https://github.com/google/deepvariant/issues/801,1,['learn'],['learned']
Usability,"I noticed a small typo in the file `docs/deepvariant-details.md` on the first line. The current text is:. ```; f# DeepVariant usage guide; ```. It would be better to remove the leading ""f"":. ```; # DeepVariant usage guide; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/897:132,guid,guide,132,,https://github.com/google/deepvariant/issues/897,2,['guid'],['guide']
Usability,"I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704:71,guid,guidance,71,,https://github.com/google/deepvariant/issues/704,1,['guid'],['guidance']
Usability,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:561,clear,clearly,561,,https://github.com/google/deepvariant/issues/530,1,['clear'],['clearly']
Usability,"I reviewed the anotated vcf.gz files (just utilized a simple search in a text editor for counts and I identified the following counts:. Checking: /home/username/deepvariant-run/output/RBA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2115432]; 17278/323711 (5.34%) records had indeterminate consistency status due to incomplete calls; 64534 of the values were ./. Checking: /home/username/deepvariant-run/output/RBN2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 59210 of the values were ./. Checking: /home/username/deepvariant-run/output/RBNA2s.cohort.vcf.gz; Family: [2114337 + 2114302] -> [2009617, 2115432]; 25958/358380 (7.24%) records had indeterminate consistency status due to incomplete calls; 118840 of the values were ./.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311#issuecomment-637256372:54,simpl,simple,54,,https://github.com/google/deepvariant/issues/311#issuecomment-637256372,1,['simpl'],['simple']
Usability,"I see! I am very grateful for the support. Not only the problem is solved, it is everything much more clear now, and I have learned a lot from your feedback. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-816108659:102,clear,clear,102,,https://github.com/google/deepvariant/issues/434#issuecomment-816108659,3,"['clear', 'feedback', 'learn']","['clear', 'feedback', 'learned']"
Usability,"I strongly concur. Singularity images would be nice. Using Docker has already given me multiple diseases. It's such a hassle and from what other colleagues tell me, I am not the only one. ; Note that there is nothing wrong with the deepvariant image, it's the Docker process that can cause many problems running, pulling images, restarting, generating errors, etc ... ; Like this issue, unresolved since 2017: ; https://github.com/docker/for-win/issues/813. Dark Souls bosses are easier to take down than pulling a docker image on some systems. ; So to be clear again, nothing wrong at all with deepvariant, which I love more and more btw, but having something else than Docker would be indeed really great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-562108502:556,clear,clear,556,,https://github.com/google/deepvariant/issues/243#issuecomment-562108502,1,['clear'],['clear']
Usability,I think it actually does work with simply reads.bam and reads.bai. Yep!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461539447:35,simpl,simply,35,,https://github.com/google/deepvariant/issues/149#issuecomment-461539447,1,['simpl'],['simply']
Usability,"I think the error is simpler than that. If you grep for these specific flags (i.e. `noparse_sam_aux_fields`, `norealign_reads`, and `nosort_by_haplotypes`) they do not exist in `make_examples` -- though it uses them in the command-line -- and thus it does not know how to process them, which is probably why you are seeing the 252 exit status:. ```Bash; $ grep -E 'noparse_sam_aux_fields|norealign_reads|nosort_by_haplotypes' deepvariant/make_examples.py; $; ```; Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774693583:21,simpl,simpler,21,,https://github.com/google/deepvariant/issues/419#issuecomment-774693583,1,['simpl'],['simpler']
Usability,"I was not able to get this working @pichuan , if you could provide any guidance that'd still be helpful for me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-866335254:71,guid,guidance,71,,https://github.com/google/deepvariant/issues/462#issuecomment-866335254,1,['guid'],['guidance']
Usability,"I was suspicious something else might be the issue. So I did a simple test to see if there is an issue with Luisa's BAM file, and noticed that I cannot even create an index - which would naturally make even the prerequisite `make_examples` not complete properly:. ```; paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; --2018-03-07 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; HTTP request sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: Ã¢ENCFF528VXT.bamÃ¢. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - Ã¢ENCFF528VXT.bamÃ¢ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:63,simpl,simple,63,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,1,['simpl'],['simple']
Usability,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:39,guid,guide,39,,https://github.com/google/deepvariant/issues/44,1,['guid'],['guide']
Usability,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/329:71,simpl,simply,71,,https://github.com/google/deepvariant/issues/329,1,['simpl'],['simply']
Usability,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355389148:191,learn,learning,191,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148,1,['learn'],['learning']
Usability,"I'm not completely sure about your setting. Our installation guide currently is done on Ubuntu 16. ; In your case, it seems like you're unable to install python-wheel? I think that's outside the scope of DeepVariant support. A few possible ways to get unstuck : maybe you can see whether you can skip installing python-wheel, and see what you actually need to install to proceed to the next step. I don't think we can be of more help on this issue. I'm closing this issue for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66#issuecomment-403636246:61,guid,guide,61,,https://github.com/google/deepvariant/issues/66#issuecomment-403636246,1,['guid'],['guide']
Usability,I'm not sure I see how the learning rate or batch_size would affect those metrics -- could you provide more information on your setup? And the two cases you are seeing -- are those randomly occurring or have you pinpointed why it flips one way or the other?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203#issuecomment-517389773:27,learn,learning,27,,https://github.com/google/deepvariant/issues/203#issuecomment-517389773,1,['learn'],['learning']
Usability,"I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49:494,simpl,simple-SG,494,,https://github.com/google/deepvariant/issues/49,1,['simpl'],['simple-SG']
Usability,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/193:647,guid,guidance,647,,https://github.com/google/deepvariant/issues/193,1,['guid'],['guidance']
Usability,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:52,simpl,simple,52,,https://github.com/google/deepvariant/issues/194,1,['simpl'],['simple']
Usability,"I've filed an internal issue to track. The update should come out in the next release. I'll close this comment for now, but will post an update once it's out.; Thanks for the feedback!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/414#issuecomment-771075195:175,feedback,feedback,175,,https://github.com/google/deepvariant/issues/414#issuecomment-771075195,1,['feedback'],['feedback']
Usability,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:49,guid,guide,49,,https://github.com/google/deepvariant/issues/292,1,['guid'],['guide']
Usability,"If I remember correctly, wildcards like * and ? should work. We can probably improve the comment there.; But concatenating everything together works too. You can directly cat all `*tfrecord.gz` into another big all.tfrecord.gz file. I would suggest trying wildcard first though. In terms of how to set num_examples: for now if you know roughly how many examples you have (for example, I can't remember if make_examples print out that information), you can just set a rough number. It's only being used here: ; https://github.com/google/deepvariant/blob/r0.4/deepvariant/model_train.py#L211; It does affect the learning rate decay, but it doesn't have to be exact.; I'll see if I can come back with a better example to count examples later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-351130156:610,learn,learning,610,,https://github.com/google/deepvariant/issues/10#issuecomment-351130156,1,['learn'],['learning']
Usability,"If you're looking for simply merging the tfrecord files (without having to touch Python code) to one, you can actually just concatenate tfrecord files together.; Something like:; `cat shard.*.tfrecord > merged.tfrecord`. or you can also concatenate zipped tfrecord files:; `cat shard.*.tfrecord.gz > merged.tfrecord.gz`; will work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366848143:22,simpl,simply,22,,https://github.com/google/deepvariant/issues/49#issuecomment-366848143,1,['simpl'],['simply']
Usability,"Indeed the file was truncated, sorry about that. I am still testing locally; with other even smaller files ( like : wget; http://dv-testfiles.s3.amazonaws.com/wgEncodeUwRepliSeqGm12878G1bAlnRep1.bam; which is public and smaller and not truncated ) and I get the same exact; error again. 2018-03-07 22:36 GMT+01:00 Paul Grosu <notifications@github.com>:. > I was suspicious something else might be the issue. So I did a simple test; > to see if there is an issue with Luisa's BAM file, and noticed that I; > cannot even create an index - which would naturally make even the; > prerequisite make_examples not complete properly:; >; > paul@gubuntu:~/data/luisa$ wget http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > --2018-03-07 <http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam--2018-03-07> 16:20:42-- http://dv-testfiles.s3.amazonaws.com/ENCFF528VXT.bam; > Resolving dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)... 52.218.52.113; > Connecting to dv-testfiles.s3.amazonaws.com (dv-testfiles.s3.amazonaws.com)|52.218.52.113|:80... connected.; > HTTP request sent, awaiting response... 200 OK; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: Ã¢ENCFF528VXT.bamÃ¢; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - Ã¢ENCFF528VXT.bamÃ¢ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:419,simpl,simple,419,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,1,['simpl'],['simple']
Usability,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366748047:188,simpl,simplest,188,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047,1,['simpl'],['simplest']
Usability,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13#issuecomment-351172185:36,simpl,simplest,36,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185,3,"['simpl', 'usab']","['simplest', 'simplistic', 'usable']"
Usability,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353393763:202,simpl,simple,202,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763,1,['simpl'],['simple']
Usability,"It was great to see haploid support added with version 1.6. However, it took me some time to understand that I had it working correctly as my expectation was that the genotypes would be represented as hemizygous (0 or 1) rather than homozygous (0/0 or 1/1) in the specified regions. Would it be useful to add a clear statement in the documentation regarding the current representation in ""haploid"" regions, and to possibly consider adding an option in postprocess_variants that allowed an alternative output with hemizygous genotypes? I understand that some tools used in downstream applications may have problems with hemizygous genotypes, and therefore the desirability of representing them as homozygous. However there are also downstream applications where true hemizygote representation has significant value. For us this would be the ability to generate accurate AF, AC and AN values after aggregating into multisample vcfs, but I am sure there are others that would benefit from a more accurate representation of genotypes on the true X and Y chromosomes.; Many thanks for considering this and for providing this very useful tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/751:311,clear,clear,311,,https://github.com/google/deepvariant/issues/751,1,['clear'],['clear']
Usability,"It's all the variants that have been called RefCall, actually, see the command:. ```; bcftools query -f ""%FILTER\t[ %GQ]\n"" OUTPUT_VCF|awk '$1==""RefCall""' > RefCall_all_GQ; ```. So 1/1 and 0/1 should be excluded, unless Deepvariant consider 1/1 as a RefCall. . I am asking around about if I am allowed to share the data, they don't ""belong' to me (though we could talk days about what it means to own scientific data). I am doing my best to get a ""yes"" answer. cheers. PS: precisely, at the moment I wanted to make a kind of hexbin plot of VAF vs GQ, however I am encountering a problem with sites like those: . Chrom_3 7095984 . A ACGAACGTTTTGCGATAGTATTTAACAAAATATGTTATATGTTTTTCATTGAAATCAATCTTTAATAGAATTTCTTTTAGTATTTGTGTATACATAAATGTTGGATCTAAATGATCGATATTTTCGACGGATTTTTTGTTACTTCGAGGAACGAAACTAATCGAAATCGAATCTTGATCACAATTTTCTGCATCTTGTTTCAATTTTTGACTTAAAGAT,ATGAACGTTTTGCGATAGTATTTCACAAAATATGTTATATGTTTTTCATTGAAATCAATCTTTAATAGAATTTCTTTTAGTATTTGTGTATACATAAATGTTGGATCTAAATGATCGATATTTTCGACGGATTTTTTGTTACTTGGAGGAACGAAACTAATTGAAATCGAATCTTGATCACAATTTTGTGCATCTTGTTTCAATTTTTCACTTAAAGAT 4.3 RefCall . GT:GQ:DP:AD:VAF:PL ./.:2:217:3,47,64:0.21659,0.294931:0,12,3,12,2,3. Clearly DeepVariant thinks there are 3 potential alleles, but that in the end it can't determine ... ; While most of the time I have 35 (GQ) against whatever, but a single value. ; This is causing my code to plot complete nonsense with negative GQ showing up in the plot. At the moment I don't know if I should find a way to replace the 2 values by a single one, or find a way to deal with the complexity in my plot. I would appreciate if you have an idea ^^'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1648455718:1155,Clear,Clearly,1155,,https://github.com/google/deepvariant/issues/682#issuecomment-1648455718,1,['Clear'],['Clearly']
Usability,"It's normal for the loss curve to start flattening out, although yours does change pretty abruptly. What exactly did you change (you mentioned using a `very simple topology`, not sure if that means you changed the architecture or something)? Also the commands you used would be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194#issuecomment-509337028:157,simpl,simple,157,,https://github.com/google/deepvariant/issues/194#issuecomment-509337028,1,['simpl'],['simple']
Usability,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251#issuecomment-563508582:84,learn,learning,84,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582,2,['learn'],"['learn', 'learning']"
Usability,"Machine has 64 cores, 2TB RAM, Centos is OS. Deep variant docker code works well when input bam file size is less than; 20 Gb file size, but when I increase the file size / coverage, I get the; error. On Wed, Sep 22, 2021 at 9:08 PM Pi-Chuan Chang ***@***.***>; wrote:. > @kirti141 <https://github.com/kirti141> from the log, I agree that it; > isn't quite clear.; > Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/482#issuecomment-925047566>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ANQXTKYZH6MM2EGS7CBSZPTUDHZ7FANCNFSM5DR4DILA>; > .; > Triage notifications on the go with GitHub Mobile for iOS; > <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>; > or Android; > <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.; >; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925379521:357,clear,clear,357,,https://github.com/google/deepvariant/issues/482#issuecomment-925379521,1,['clear'],['clear']
Usability,"My goal is to use DeepVariant in a transfer learning application. Is it possible to get the last layer as an embedding output?. If yes:; Is it possible to use it as a Python module to get these embeddings?. Also, is it possible to run DeepVariant in with a VCF input?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371:44,learn,learning,44,,https://github.com/google/deepvariant/issues/371,1,['learn'],['learning']
Usability,"Nice catch! Thank you! It does seem that there are different number of outputs, though I've (to my knowledge) only used the commands from the full run with 19 cpus and --dry_run=true. I've cleared the output directories and am running the full command from the beginning. Which nvidia-driver - cuda combination do you run deepvariant with? I'm looking into the gpu -problem, I'm thinking I need to install an older nvidia-driver and cuda. Currently the driver is 555.42.02, but looking at this https://docs.nvidia.com/deploy/cuda-compatibility/#id1 , it's not compatible with cuda 11.3.1 that the deepvariant:1.6.1-gpu is using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485:189,clear,cleared,189,,https://github.com/google/deepvariant/issues/849#issuecomment-2230253485,1,['clear'],['cleared']
Usability,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:93,simpl,simple,93,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773,1,['simpl'],['simple']
Usability,"Oh I thought I was using a consistent model and codebase â€” thanks for the links. *Edit*: I was using the correct model, it was simply that it was not localized in the correct folder apparently. > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461399048:127,simpl,simply,127,,https://github.com/google/deepvariant/issues/151#issuecomment-461399048,1,['simpl'],['simply']
Usability,"Ok, then this would probably be my approach:. 1) Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". 2) Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs. 3) Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks. Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2361910524:777,simpl,simplest,777,,https://github.com/google/deepvariant/issues/878#issuecomment-2361910524,1,['simpl'],['simplest']
Usability,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430171385:69,simpl,simple,69,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385,2,['simpl'],"['simple', 'simplified']"
Usability,"RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/714:245,intuit,intuitive,245,,https://github.com/google/deepvariant/issues/714,2,"['clear', 'intuit']","['clear', 'intuitive']"
Usability,"Running several DeepVariant jobs (v0.9.0) on the same server, e.g., by splitting a dataset via the `--regions` option, results in corrupted files (DataLossError) because of non-unique file names for the temp files generated under `/tmp` on the server (confirmed, see below). > I just checked the code, and you're right that the temp file names will be the same:; > https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266; > For now, please pass in different `intermediate_results_dir` for each run.; > For example:; > `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. > I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. > 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; > 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. > For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/175#issuecomment-560625427_. @pichuan; I think as an immediate step, updating the docs and making this explicit (also the option of using the `intermediate_results_dir`) would be reasonable. Concerning a proper solution, creating a randomly named temp dir for all temp files of the job (which then could have non-random names) seems like a very straightforward way; ideally, this should also take care of the clean-up, e.g.,as it is provided by Python's `tempfile` module. I assume basic functionality like this exists in all relevant programming languages. +Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/242:1158,user experience,user experience,1158,,https://github.com/google/deepvariant/issues/242,1,['user experience'],['user experience']
Usability,Simple suggestion,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/398:0,Simpl,Simple,0,,https://github.com/google/deepvariant/issues/398,1,['Simpl'],['Simple']
Usability,"Sorry but simple sequence repeats are mutable in prokaryotes too.... On Mon, 31 Jul 2023, 17:45 Axze-rgb, ***@***.***> wrote:. > that's not a feature of this genome we are not in humans.; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658755258>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2V75T2P2ATODR6Z6QDXS7OLHANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046:10,simpl,simple,10,,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046,1,['simpl'],['simple']
Usability,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844185505:738,feedback,feedback,738,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505,1,['feedback'],['feedback']
Usability,"Sorry for the confusion here. We're phasing the reads internally to help with DeepVariant accuracy. (""Direct"" is referring to that we're doing this now directly in DeepVariant instead of relying on external HP tags from other tools like WhatsHap). However, DeepVariant currently still doesn't generate phased variant calls. Therefore you won't see ""|"" in the VCF files. Hopefully this is more clear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1547305204:393,clear,clear,393,,https://github.com/google/deepvariant/issues/649#issuecomment-1547305204,1,['clear'],['clear']
Usability,"Sorry for the delay, I appreciate your feedback in the matter. Closing the issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403#issuecomment-768232068:39,feedback,feedback,39,,https://github.com/google/deepvariant/issues/403#issuecomment-768232068,1,['feedback'],['feedback']
Usability,"Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7106,undo,undo,7106,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['undo'],['undo']
Usability,"Successfully run, once again sincerely thank you for your guidanceï¼",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/700#issuecomment-1687514662:58,guid,guidance,58,,https://github.com/google/deepvariant/issues/700#issuecomment-1687514662,1,['guid'],['guidance']
Usability,"Sure! You can find the checkpoints for each sequencing technology at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/`. For example, the model for Illumina data can be found at `gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt`. These models are mounted in our [Dockerfile](https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L156-L200). Take a look at our [custom training case study](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#advanced-case-study-train-a-customized-snp-and-small-indel-variant-caller-for-bgiseq-500-data) if you want to learn more.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801#issuecomment-2040306496:632,learn,learn,632,,https://github.com/google/deepvariant/issues/801#issuecomment-2040306496,1,['learn'],['learn']
Usability,"Sure! please find attached the logs in the terminal.; In the meantime, I will run the simple case study.; Thank you!; [output.log](https://github.com/google/deepvariant/files/15052710/output.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270:86,simpl,simple,86,,https://github.com/google/deepvariant/issues/810#issuecomment-2068128270,1,['simpl'],['simple']
Usability,Thank you - I think these commands will be helpful as I learn about using Google Cloud. Please don't rush the AWS test - I am guessing I will have to wait until at least Sunday to be able to set up an account and confirm that using Google Cloud can resolve this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480271688:56,learn,learn,56,,https://github.com/google/deepvariant/issues/167#issuecomment-480271688,1,['learn'],['learn']
Usability,Thank you - I'm attaching my BED file which simply defines the entire length of the chromosome (I am working on a bacterium):; NC_000962.3 0	4411531; Is this an issue with a non-human genome?. (Saved with a .txt so that I could upload.); [confidence.bed.txt](https://github.com/google/deepvariant/files/1986525/confidence.bed.txt),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71#issuecomment-387627131:44,simpl,simply,44,,https://github.com/google/deepvariant/issues/71#issuecomment-387627131,1,['simpl'],['simply']
Usability,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:142,simpl,simple,142,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828,1,['simpl'],['simple']
Usability,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:132,feedback,feedback,132,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,1,['feedback'],['feedback']
Usability,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445:14,clear,clearing,14,,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445,1,['clear'],['clearing']
Usability,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-489377484:892,guid,guide-instruction-set-specific-dispatching-on-intel-architectures,892,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484,1,['guid'],['guide-instruction-set-specific-dispatching-on-intel-architectures']
Usability,"Thank you for the feedback! @danielecook I can confirm that the files in the tmp directory do look to be normal as you described above. @kishwarshafin, I tested the docker that you suggested and now it seems that Deepvariant did not run at all (vcfs and gvcfs are empty); [deepvarrun_b37_MND_G33.1kei.log](https://github.com/google/deepvariant/files/14458499/deepvarrun_b37_MND_G33.1kei.log); ; I have attached a log file for one of the samples, so you can see what happened.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674:18,feedback,feedback,18,,https://github.com/google/deepvariant/issues/776#issuecomment-1972749674,1,['feedback'],['feedback']
Usability,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479627680:700,clear,clear,700,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680,2,"['clear', 'usab']","['clear', 'usability']"
Usability,"Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>; from . import multiarray; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/795:970,simpl,simply,970,,https://github.com/google/deepvariant/issues/795,1,['simpl'],['simply']
Usability,"Thank you for your prompt and professional response. I have reviewed my files and reconfigured my IGV. As @AndrewCarroll mentioned, this anomaly is that two different representations of a varaint. While both representations are equivalent, I believe that people might prefer a simpler representation here (e.g., homozygous SNV instead of three heterozygous varaints). Do you have any recommendations for post-processing methods to normalize these types of varaints into a single representation? Alternatively, can DeepVariant introduce more advanced models or encoding methods to handle this situation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590364272:277,simpl,simpler,277,,https://github.com/google/deepvariant/issues/660#issuecomment-1590364272,1,['simpl'],['simpler']
Usability,"Thank you for your quick answer. I had to make a couple of changes to the command (see below), but now it seems to be working:; ```; conda create -y -n deepvariant -c bioconda -c conda-forge python=2.7 deepvariant google-cloud-sdk=239.0.0; ```; Everything is installed correctly. Is there a guide to follow for locally installed variant caller?; I'm not sure I've been able to find it. . Thank you again for your support,; Andrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566566093:291,guid,guide,291,,https://github.com/google/deepvariant/issues/252#issuecomment-566566093,1,['guid'],['guide']
Usability,"Thank you for your reply!; For the following code, I don't have permission to modify the path `/usr/bin/`, and I didn't find `/usr/bin/python3` from `subprocess.py`, so I didn't fix the error either.; As a study student, I am working hard to learn to solve these mistakes. It hasn't been resolved yet.; ```; File ""/path/Mambaforge-4.14.0-1/envs/dpv/lib/python3.6/subprocess.py"", line 1364, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/python3': '/usr/bin/python3'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359:242,learn,learn,242,,https://github.com/google/deepvariant/issues/598#issuecomment-1356305359,1,['learn'],['learn']
Usability,"Thank you so much.; In my current understanding, if I use the pre-trained parameters for DeepVariant in Keras or other deep learning frameworks, building the equivalent model in each library is required.; Anyway, I'll try which approach is the easiest for my aim. Please let another confirmation.; For saving the checkpoints ""DeepVariant-inception_v3-0.7.0+data-wgs_standard"", did you used the function in this?:; https://github.com/google/deepvariant/blob/3c43de4541c45673e30d14daef742fca68fdf69b/deepvariant/testing/tf_test_utils.py#L48. Best.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-446044386:124,learn,learning,124,,https://github.com/google/deepvariant/issues/127#issuecomment-446044386,1,['learn'],['learning']
Usability,"Thank you very much for your prompt response, particularly on the weekend!. I can see that I overlooked the line `MODEL=""${HOME}/${MODEL_NAME}/model.ckpt""` in this [quick-start](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-quick-start.md) guide. I apologize about that. So, if I use `/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard/model.ckpt`, then that works!. If I only put the prefix with ""model,"" I get another error mentioning a checkpoint, but I am assuming that is what the .ckpt extension stands for. Thank you again. **Update (4/1)**: I just remembered that I am supposed to close the issues on GitHub, which is what I did :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478414504:259,guid,guide,259,,https://github.com/google/deepvariant/issues/166#issuecomment-478414504,1,['guid'],['guide']
Usability,"Thank you very much for your reply. However I have 2 questions regarding the example visualization. . 1. As I am using multiple cores, the example files are splitted in to ; examples.tfrecord-00000-of-00008.gz to examples.tfrecord-00007-of-00008.gz. can I simply use ""examples.tfrecord-00000-of-00008.gz"" as the source path? or do I have to combine the 8 examples file together first?. 2. I cannot run the program as stated in the notebook as the `label` is not one of the features in the example for deepvariant. Is the label in the notebook the same as `alt_allele_indices/encoded` in Deepvariant? If so, how can I extract the information from the feature? I have tried 'alt_allele_indices/encoded': tf.FixedLenFeature([], tf.string), but it just gives my random symbols.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/229#issuecomment-545946241:256,simpl,simply,256,,https://github.com/google/deepvariant/issues/229#issuecomment-545946241,1,['simpl'],['simply']
Usability,"Thank you! ; My experiment was designed for the reference to be as close as possible to the studied populations so that shouldn't be an issue. Thanks for the always fast feedback.; I read in the blog post. > Of the 94,554 Mendelian violations where the child is HOM_REF, **only 17,475 (18%) of those have the HOM_REF call based just upon reference and non-reference read counts, the remaining 82% had the HOM_REF call produced by the CNN**. This seemed suspicious, so we investigated the allele depth fractions for each of HOM_REF, HETEROZYGOUS, and HOM_ALT calls in all three individuals. That's interesting because I also get quite a non trivial number of HOM_REF calls which, just based on the biology and specifics of my experiment (I prefer to remain vague about that publicly) is highly suspicious. In fact, all new homozygous variants are suspicious in my experiment. . I have highlighted a sentence in boldface, simply: how did you do that? How do you know that deepvariant made the call based on non-reference/reference read ratio or that it made the call based on its CNN interpretation? I thought the CNN was used for all calls? Or I am missing something obvious here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-569126095:170,feedback,feedback,170,,https://github.com/google/deepvariant/issues/257#issuecomment-569126095,2,"['feedback', 'simpl']","['feedback', 'simply']"
Usability,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877:388,simpl,simply,388,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877,1,['simpl'],['simply']
Usability,"Thank you!; You are right.The ""RNC"" comes from GLnexus. ; https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md ""If a candidate is made, but is called as reference (either 0/0 or ./.) it means that the neural network processed the genomic region, but based on all of its learned experience from training data, it decided the highest probability for the position was as non-variant. Some of the reasons that DeepVariant may suspect a false positive are: strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage.""; So,is it possible for different RNC to correspond to the above reasons(strand-bias in reads, low mapping quality in reads, low base quality in reads, and overall low coverage)?; When I met './.' ,I have reason to believe that it is 0/0 with greater probability than 0/1.; However,when the ""RNC"" is II,it means""gVCF input site is non-called"",for example: ./.:137:2,129:5:0,13,4:II ./.:137:86,50:6:0,4,39:II.; In this situation,why doesn't DeepVariant call the mutation(DP=137,alt reads=50)?. ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; https://github-wiki-see.page/m/dnanexus-rnd/GLnexus/wiki/Reading-GLnexus-pVCFs; One of GLnexus' main functions is to generate a population-wide ""project"" VCF (pVCF) based on the input gVCFs for each individual sample. ; <html>; <body>; <!--StartFragment--><h3 style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-tran",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:277,learn,learned,277,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954,1,['learn'],['learned']
Usability,"Thank you, we're taking a look at the example and expect to have feedback relatively soon (with some delay for holidays in the US).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/388#issuecomment-734139746:65,feedback,feedback,65,,https://github.com/google/deepvariant/issues/388#issuecomment-734139746,1,['feedback'],['feedback']
Usability,"Thanks @jaqueytw . I don't believe we're currently encouraging our users to use GATK tools to process our files. But if you find any documentation that mentioned/encouraged that, please do let me know and I'd like to fix it. We're actively working on coming up with our own recommendation for best practice. Your analysis and feedback is very valuable. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170#issuecomment-482328482:326,feedback,feedback,326,,https://github.com/google/deepvariant/issues/170#issuecomment-482328482,1,['feedback'],['feedback']
Usability,"Thanks @lvclark for the context of why you're doing this. That's very helpful to know. In that case, you'll want to run the whole pipeline separately as well (meaning, make_examples -> call_variants -> postprocess_variants separately), one using `gvcf1_parent1.tfrecord@32.gz` , the other one with `gvcf3_parent1.tfrecord@32.gz`. And not to combine them mid-way. Our team is working on makeing chrX/Y calling a bit better for DeepVariant and DeepTrio. I'll make sure to pass this feedback as well so we'll think about the use case here a bit more later on.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130:480,feedback,feedback,480,,https://github.com/google/deepvariant/issues/413#issuecomment-1480069130,1,['feedback'],['feedback']
Usability,"Thanks @machomachopadre for your report. Just so I'm 100% clear, you've got python 2.7 and 3.5 on the machine, and our build-prereqs.sh script is installing some packages into python 3.5 and some into 2.7? I don't think we've been clear before about this, but DeepVariant is intended for python 2.7 only, as we've never tested it using python3. . Can you confirm that you can install DeepVariant on a clean Ubuntu 16 instance in the cloud?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/30#issuecomment-355031587:58,clear,clear,58,,https://github.com/google/deepvariant/issues/30#issuecomment-355031587,2,['clear'],['clear']
Usability,"Thanks @nmousavi @chrisfleisch @melkerdawy ; For now, please see if the workaround that @nmousavi suggested can be used. @chrisfleisch I'm actually already planning to take a closer look at the way we build the docker image. Thanks for your feedback. I will take this into account when I do that. I'll add the information in your last comment to our internal tracking bug. Note that this might be a little low on my priority list. But I'll try to get to it soon, and I might check back with you directly to make sure things work for you.; I'll leave this GitHub issue open!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458349387:241,feedback,feedback,241,,https://github.com/google/deepvariant/issues/132#issuecomment-458349387,1,['feedback'],['feedback']
Usability,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274:533,guid,guide,533,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274,1,['guid'],['guide']
Usability,"Thanks @pgrosu. Knowing these would be very helpful. Besides compute, this can also be an issue with the input data. @Taghrid-M ,. Can you please tell a little more about the data in `HG004-hg38.ont.mm2.bam`:. 1) What chemistry is this data R9 or R10?; 2) What is the basecaller version you used for basecalling this data?; 3) What is the average read length of the reads?. Please note, DeepVariant currently supports R10.4 simplex and duplex variant calling for nanopore. If your data is from previous chemistry or basecaller version, please use [PEPPER](https://github.com/kishwarshafin/pepper) to call variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641030077:424,simpl,simplex,424,,https://github.com/google/deepvariant/issues/681#issuecomment-1641030077,1,['simpl'],['simplex']
Usability,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052:37,clear,clear,37,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052,1,['clear'],['clear']
Usability,Thanks for all the feedback and discussion above! I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1559918325:19,feedback,feedback,19,,https://github.com/google/deepvariant/issues/650#issuecomment-1559918325,1,['feedback'],['feedback']
Usability,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909:11,clear,clearing,11,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909,1,['clear'],['clearing']
Usability,"Thanks for the comment, @jumpyknight . What you suggest is interesting, but do you think it is plausible even if the insertion is of length 5? If that was the case, I would have expected to have more pixels 'lit up' as snps, or more pixeld 'darkened' as insertions, right after the position at which the insertion took place. However, no such behaviour takes place (referring again to the 6th channel). I'm not sure what infomation is relevant here so I'll post a bunch of stuff:. 1. The variant: as I said it is at chr20-10001435, it is labeled to be a simple SNP, hom-alt 1/1.; 2. The bam-file read I mentioned: . - Starts at: 10001358; - Cigar: 78M, 5I, 18M. That means that we have; 10001358 ... 10001435 X X X X X 10001436 ... 10001453; M ... M I I I I I M ... M; Where M indicated Match and I indicates Insertion. - It is the forward read, with mapping quality 60, ; - Has the following tags: [(RG, NA12878), (XT, U), (NM, 5), (SM, 37), (AM, 37), (X0, 1), (X1, 0), (XM, 0), (XO, 1), (XG, 5), (MD, 96)]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379#issuecomment-723807366:554,simpl,simple,554,,https://github.com/google/deepvariant/issues/379#issuecomment-723807366,1,['simpl'],['simple']
Usability,"Thanks for the comments. I tried to train a simple 10-cnn-layer architecture and got the above results. Although the final loss still seems to be rather high, the accuracy is acceptable for the simple cnn network. I will close this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194#issuecomment-513082707:44,simpl,simple,44,,https://github.com/google/deepvariant/issues/194#issuecomment-513082707,2,['simpl'],['simple']
Usability,Thanks for the feedback and the good news about the model. I plan to get some WES data from Element for testing.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330:15,feedback,feedback,15,,https://github.com/google/deepvariant/issues/703#issuecomment-1708558330,1,['feedback'],['feedback']
Usability,"Thanks for the feedback, Iâ€™m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:ï»¿; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. â€”Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:15,feedback,feedback,15,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545,2,"['clear', 'feedback']","['clear', 'feedback']"
Usability,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-462075143:15,feedback,feedback,15,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143,1,['feedback'],['feedback']
Usability,"Thanks for the update, @Stikus . In the code I'm working on, I currently changed that block to:; ```; # Because of an issue with pypi's numpy on Ubuntu 14.04. we need to compile from; # source.; # See https://github.com/tensorflow/tensorflow/issues/6968#issuecomment-279061085; if [[ ""$(lsb_release -d)"" == *Ubuntu*14.04.* ]]; then; echo ""Installing numpy with -no-binary=:all:. This will take a bit longer.""; pip3 install ""${PIP_ARGS[@]}"" --no-binary=:all: ""numpy==${DV_TF_NUMPY_VERSION}""; else; pip3 install ""${PIP_ARGS[@]}"" ""numpy==${DV_TF_NUMPY_VERSION}""; fi; ```. But I'm also wondering if I should just remove if/else statement for different Ubuntu versions if we're not internally testing it to make sure everything still runs. So I might end up simplifying this further in the next release. Glad to hear that the fix you mentioned above worked for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742700347:753,simpl,simplifying,753,,https://github.com/google/deepvariant/issues/394#issuecomment-742700347,1,['simpl'],['simplifying']
Usability,Thanks for the update.The differences could be related to a change in the Tensorflow version. We welcome any additional feedback you have.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669:120,feedback,feedback,120,,https://github.com/google/deepvariant/issues/624#issuecomment-1499324669,1,['feedback'],['feedback']
Usability,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-720871901:534,simpl,simply,534,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901,1,['simpl'],['simply']
Usability,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730:416,simpl,simple,416,,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730,1,['simpl'],['simple']
Usability,"Thanks so much! I think I understand the issue now, but Deepvariant is still giving a wrong answer.; You can see that this particular location in the genome has a tandem repeat of 6 copies of a 47 bp sequence:; https://genome.ucsc.edu/cgi-bin/hgc?hgsid=960400993_qiSzxsvvUkaPrDYeYJ2KhGLAK2ay&c=chr3&l=76220845&r=76221817&o=76221234&t=76221509&g=simpleRepeat&i=trf. But I actually have long PacBio reads for this sample, with the zmw consensus sequence at the same location of:; TGAGCTTAATCATAGAACATGGTAATACTAGGAGACATCATGAAGGATCCCTGTGTTGTAGATATACTCTTCTTTACTTCCATTGAGAAGTAGTAGTTCAATTTCCCCAGGTAGTCTGAATCAATAACCCCAGGCAATATTGACTGTTTCTGTGGTGAAAGCATTCCTCCATCTAGAACTAAGTCCTCTTGCCCAACAGAAGATAAAGTCATGAGCATGGGAAGCAAAAATTTTGCTAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGTAGTGCCACTCTCATTTTACAAGTGGGTAACTCAGGGTGATGGTGAGCAGTGCCCCTCTCATTTTACAAGTGGGTAACTCAGGGTGACGGTGAGCAGTGCCACTCTCATTTTACAAGTAACTCAGGGTGATGGTGAGCAGTGCCACTCTCATTTTCCACGCTTTGATTCCTGAACCCATTAATTGTGGCTGTTGATGAAACTACTATATGTTGGAAACTGCTTCAGAGAATATACAACCTTCTGCAGAACCTTGGCCCAGCTGTGTAAGGTATTGCGATCTAGCTGGTACTGTAACTGAATTCAAAAGACCCTTTTATCATTTTTATCAAGTTAGCTGCTTCTGGATGATGGGGAACATGGTAAGACCGATGGACTTCATGACCATGAGCCCATTGCCACACTTTTTTGTCTTTGAGGTGAGTTCCTTGATCAGAAGCAATGCTGTATTTAATACTGTGCCTGTGGATAAGACATTTTATAAGTCCACGGATGGTAGTTTCGGTGGAAGCATTGCACCCACGGAAGACAAATCCATAACCTGAGAAGGGTCTATTCCAATAAGCACAAAATGCTGCCACTTCCATAGTGGAAGCAGTCTAATGTAGATAAACTGCCACTAGGTAGCTGGCTGATCACCCTGGGGAATAATGCCAAATGGGATCACAATGTGGTCTCTACTGCTGGCAGATTGTATAATCTGCCAGTGGTGGCCATAGCTAGGTCAGCCTTGGTGAGTGGAAACCTATGTTGCTGAGTGCATGCATAACCTTCATCCCTGCCACCATGTCCACCTGTTACTGGTGGAATGTATCTGAGCCACGTGGCACCAAAACACGTTACCAGTGGCAAATTGGTATGGGTTTGCAGCAACTTCAGTTCTTGCCTCCTCAGAAGAAAGAATCTGACTGAGAGGCATAAGGTAGAAGGAGGGGCTGAGGCAAGTTTTAGAGCAGGAGTGAATGTTTATTTAAAAAGCCTTAGAGCAGGAATGAAAGGAAGGAAAGAAAGTATACTTGGAAGAGGGCCAAGTGGGTGACTTGAAAGACAAGTGTACATGTTGACCTTGTGACTAGGCTTATACGTTGGCATAATTCCAGGGTCTTGTGTCACTTCTCCCAACCCGCCCAACCCTTGAGATCTTATTGGGAAGCTGCTGATAACCAGT",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/388#issuecomment-735306929:345,simpl,simpleRepeat,345,,https://github.com/google/deepvariant/issues/388#issuecomment-735306929,1,['simpl'],['simpleRepeat']
Usability,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-637523488:51,clear,clearer,51,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488,1,['clear'],['clearer']
Usability,"That's great news Ram, and glad to hear it all worked out. The logs were basically guiding me through rules of implication, and it looked like we were getting close. Let us know if you run into any other issues, as we would be happy to help you out. @depristo Thank you Mark for the nice compliments, that means quite a lot. I always enjoy helping out folks and the team. I find exploring complex and challenging problems interesting and fun.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423408772:83,guid,guiding,83,,https://github.com/google/deepvariant/issues/94#issuecomment-423408772,1,['guid'],['guiding']
Usability,The [blog post on deep variant](https://research.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html) mentions:. > a deep learning technology to reconstruct the true genome sequence from HTS sequencer data with significantly greater accuracy than previous classical methods. How could I reconstruct the true genome sequence? Would an example of this be using something like gatk with the generated VCF file to correct the assembly?. E.g. https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_fasta_FastaAlternateReferenceMaker.php,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13:134,learn,learning,134,,https://github.com/google/deepvariant/issues/13,1,['learn'],['learning']
Usability,"The code in [Download binaries, models, and test data](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#download-binaries-models-and-test-data) the of the DV quick start guide ran successfully. However, running [make_examples](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#make_examples) using the quickstart-testdata failed with the following error:; ```; Traceback (most recent call last):; attila-ThinkS:~/tools/deepvariant$ python bin/make_examples.zip \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/make_examples.py"", line 45, in <module>; from deepvariant import variant_caller; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/variant_caller.py"", line 50, in <module>; from deepvariant.python import variant_calling; ImportError: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41:201,guid,guide,201,,https://github.com/google/deepvariant/issues/41,1,['guid'],['guide']
Usability,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777:88,simpl,simple,88,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777,1,['simpl'],['simple']
Usability,"The images all have to be the same size for the model, so they are standardized to a height of 100 (5 rows for reference + 95 reads). Standardization like this is common in machine learning. Coverage does actually get that high for many of our training datasets when we use the full coverage without downsampling.; By the way, where did you find that link? It's a pretty old version of the notebook. We have since updated it here: https://github.com/google/deepvariant/blob/r1.3/docs/visualizing_examples.ipynb",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811:181,learn,learning,181,,https://github.com/google/deepvariant/issues/536#issuecomment-1106635811,1,['learn'],['learning']
Usability,The number is referring to the number of steps in training when this checkpoint is saved.; You can see https://www.tensorflow.org/guide/checkpoints for more information.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/192#issuecomment-506979241:130,guid,guide,130,,https://github.com/google/deepvariant/issues/192#issuecomment-506979241,1,['guid'],['guide']
Usability,"There is a simple typo in the `rtg vcfmerge` step above, it should `=` after `Sex`. `--add-header ""##SAMPLE<ID=HG002,Sex=MALE>"" \`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/779#issuecomment-2118639400:11,simpl,simple,11,,https://github.com/google/deepvariant/issues/779#issuecomment-2118639400,1,['simpl'],['simple']
Usability,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416#issuecomment-889821634:527,clear,clearer,527,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634,1,['clear'],['clearer']
Usability,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:11,simpl,simple,11,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104,1,['simpl'],['simple']
Usability,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1469,feedback,feedback,1469,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612,1,['feedback'],['feedback']
Usability,"This is very good! . #### For Singularity . You can take a look at the following two links:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:335,simpl,simplify,335,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,1,['simpl'],['simplify']
Usability,"This might be an obvious question but i cannot work it out, what does the -B mean?. For example from your singularity guide: . ```; singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```; wget https://containers/deepvariant_1.3.0.sif ; module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna; --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam; --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz ; --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz; --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/; ```. Sorry, still figuring it out! Thanks, Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/506:118,guid,guide,118,,https://github.com/google/deepvariant/issues/506,1,['guid'],['guide']
Usability,This seems like a genuine bug. Is there any way you can share the genome reference and reads along with a command line that reproduces the issue? It's not clear to me what the actual issue is and that would help a lot debugging the actual problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71#issuecomment-387816424:155,clear,clear,155,,https://github.com/google/deepvariant/issues/71#issuecomment-387816424,1,['clear'],['clear']
Usability,"To be 100% clear, are you saying you booted a clean ubuntu 16 instance and it failed to build there? Or is this on an already customized machine?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355348946:11,clear,clear,11,,https://github.com/google/deepvariant/issues/32#issuecomment-355348946,1,['clear'],['clear']
Usability,"To make it clearer, I put the path structure here.; ```; /deepvariant/core/; cloud_utils_test.py; math.py; ...; ```; And in `cloud_utils_test.py`:; ```; """"""Tests for deepvariant .core.cloud_utils."""""". from __future__ import absolute_import; from __future__ import division; from __future__ import print_function. import httplib; ...; ```; Through `httplib`, it imports `mimetools`, which imports `tempfile`, which imports `ramdom`, which imports `math`. ; But since there is a `math.py` in the same path, it shadows the `math` module in python's standard library, causing an error. To test the hypothesis, simply importing `httplib` in the same path caused the following error:; ```; >>> import httplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355522771:11,clear,clearer,11,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771,2,"['clear', 'simpl']","['clearer', 'simply']"
Usability,"To whom it may concern,. I have created an openstack instance to run deepvariant 0.9.0 as detailed below. I had no problem running deepvariant in another openstack instance, but I cannot figure out why it will not work in the instance that I newly created. I was wondering if deepvariant developers could provide some guidance towards running deepvariant again. Regards,; Sangjin. ```; ***** Running the command:*****; time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}. real 0m1.608s; user 0m25.676s; sys 0m23.860s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 55; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315:318,guid,guidance,318,,https://github.com/google/deepvariant/issues/315,1,['guid'],['guidance']
Usability,Unfortunately it's not clear from your post what might be going wrong here. Is this on a clean install of Ubuntu 16? We'd recommend starting there first to make sure everything is working and then moving to whatever environment you are running on.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5#issuecomment-349829021:23,clear,clear,23,,https://github.com/google/deepvariant/issues/5#issuecomment-349829021,1,['clear'],['clear']
Usability,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:132,simpl,simplify,132,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407,1,['simpl'],['simplify']
Usability,"Update: Internally our team has been making other improvements to postprocess_variants, so I've not been actively looking into this issue. I'll plan to resume in the next few weeks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2235560333:152,resume,resume,152,,https://github.com/google/deepvariant/issues/811#issuecomment-2235560333,1,['resume'],['resume']
Usability,We added a note about needing the `gsutil` from Google Cloud SDK to our [Build and Test guide](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md). Let us know if you are still having issues.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5#issuecomment-350478903:88,guid,guide,88,,https://github.com/google/deepvariant/issues/5#issuecomment-350478903,1,['guid'],['guide']
Usability,We are interested in training DeepVariant and are wondering the source of the training data for v0.6? Could you direct us towards the BAM files for the replicates listed under WGS models in the [usage guide](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md)?. Thank you!,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/80:201,guid,guide,201,,https://github.com/google/deepvariant/issues/80,1,['guid'],['guide']
Usability,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203:111,learn,learning,111,,https://github.com/google/deepvariant/issues/203,2,['learn'],['learning']
Usability,"We have updated the documentation to mention that AVX instructions are needed. These changes will come out with the next release. Thank you for the feedback!. Edit: we specifically mention this requirement again in the quickstart documentation, in addition to the page linked below by @pgrosu. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248#issuecomment-566700041:148,feedback,feedback,148,,https://github.com/google/deepvariant/issues/248#issuecomment-566700041,1,['feedback'],['feedback']
Usability,"When you say ""_when we use another tool to process the gvcf created by deepvariant, some information are changed_,"" it might worth figuring out what exactly is changed. Without that info, the feedback isn't actionable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170#issuecomment-482327182:192,feedback,feedback,192,,https://github.com/google/deepvariant/issues/170#issuecomment-482327182,1,['feedback'],['feedback']
Usability,With a brand new t2.medium instance per the quickstart guide and your command I get:. ```; docker: invalid reference format.; See 'docker run --help'. real	0m0.046s; user	0m0.023s; sys	0m0.028s; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-867185369:55,guid,guide,55,,https://github.com/google/deepvariant/issues/462#issuecomment-867185369,1,['guid'],['guide']
Usability,"Yes i do have a faidx index file too, actually i am doing it with; mitochondrial genome for hg38 version. Again i modified the command kept both input sorted bam and fasta file; along with faidx index in the same directory, By following the given format; https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"". => Here is my recent command used for docker run. It would be great help if; I am able to resolve this issue. Thank you in advance. sudo docker run -v; */media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:{BIN_VERSION=""1.6.1""} python; /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py; *--reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162176427:331,simpl,simplex-case-study,331,,https://github.com/google/deepvariant/issues/829#issuecomment-2162176427,1,['simpl'],['simplex-case-study']
Usability,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725:325,simpl,simply,325,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725,1,['simpl'],['simply']
Usability,"Yes, it works simply updating singularity. Thanks for figuring this out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822637152:14,simpl,simply,14,,https://github.com/google/deepvariant/issues/445#issuecomment-822637152,1,['simpl'],['simply']
Usability,"Yup that's right. Sorry, I meant `model.ckpt`, not just `model`.; Feel free to open another issue if you have other questions. In the next release, I'm hoping to make it easier to use. One thing I'm looking into is to not have to specify a super long model path, which is error-prone. When the next release comes out, it'll be good to have your feedback again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478851260:345,feedback,feedback,345,,https://github.com/google/deepvariant/issues/166#issuecomment-478851260,1,['feedback'],['feedback']
Usability,"ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; W0415 07:34:19.583559 140368878327552 deprecation.py:323] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:7399,clear,clear,7399,,https://github.com/google/deepvariant/issues/172,1,['clear'],['clear']
Usability,"al daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio); * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)); * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html); * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:2318,guid,guides,2318,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,1,['guid'],['guides']
Usability,"bject ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; | | | ; | | --0.57%--google::protobuf::Message::MergeFrom; | | | ; | | --0.54%--g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2322,learn,learning,2322,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"bject ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::cmessage::DeepCopy; | | ; | --0.56%--google::protobuf::python::cmessage::MergeFro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7848,learn,learning,7848,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ces), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in off-target regions was noticeably lower). That said, I think comparisons with my own data are in the ballpark of what they were describing in what I quoted above, although I would have expected the indel accuracy to be in-between (perhaps something more like 90-95%). Nevertheless, even though I am trying to learn more about the DeepVariant, I apologize that I should have taken mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:1376,intuit,intuition,1376,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,1,['intuit'],['intuition']
Usability,"ck'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7770,learn,learning,7770,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ck'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2244,learn,learning,2244,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; |",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6162,learn,learning,6162,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_dqd3ut4s/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '32']"".; E0430 18:57:45.247818 140240365713216 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8s9w7qaa/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '25']"".; E0430 18:57:45.247906 139736525375296 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_8kqng5_c/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '1']"".; E0430 18:57:45.354531 139703252227904 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_47rk8xc1/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '22']"".; E0430 18:57:45.318170 140515109386048 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_4p5rc3ja/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '35']"".; E0430 18:57:45.306068 140062873229120 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_xavizfpc/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '42']"".; E0430 18:57:45.268234 140590012761920 errors.py:61] Command line parsing failure: make_examples does not accept positional arguments but some are present on the command line: ""['/tmp/Bazel.runfiles_ovmu_l59/runfiles/com_google_deepvariant/deepvariant/make_examples.py', '8']"".; parallel: This job failed:. could you provide any guidance on how to run the make_example.zip working on parallel (like create a 48 jobs)? Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-830444850:13854,guid,guidance,13854,,https://github.com/google/deepvariant/issues/441#issuecomment-830444850,1,['guid'],['guidance']
Usability,"e ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1445,guid,guide,1445,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['guid'],['guide']
Usability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isnâ€™t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7999,guid,guidance,7999,,https://github.com/google/deepvariant/issues/171,1,['guid'],['guidance']
Usability,"e under some circumstances. . So let's try the `mount` approach. The reason we used `--mount` instead of `-v` is because it is more granular, telling us specifically which directory it is having an issue with. In this case it is getting stuck on the `variant_calling` directory, and somehow Docker is not able to recognize it. So let's try to determine what type of directory, and what permissions it has (including above and below it). $`1)`$ So if you could please type the following commands, then we can inspect the output to determine what might be the issue:. ```; stat /tiger/home/ajp1/analysis/demography/tasmanian_devil. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling. stat /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs. ls -l /tiger/home/ajp1/analysis/demography | grep tasmanian_devil. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/ | grep variant_calling. ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling | grep inputs; ```. $`2)`$ Now I have some minor questions about your system and Docker, in order to determine if there could be other underlying causes. For the following questions, it's okay if you don't remember everything: . * What operating system and version are your running? ; * How long ago did you install Docker?; * What commands did you use to install Docker? ; * What version of Docker are you running? (The command is in the commands below.). You previously mention that `ajp1` is part of the docker group, so we can use that as guidance when we inspect the output from the commands above. Below is a set of commands if you could please run, to answer some of the questions above (except for the installation ones). For Linux usually the commands are as follows (some will provide you information, as I included them for multiple operating systems):. ```; uname -a. cat /etc/lsb-release. cat /etc/redhat-release. cat /etc/os-release. docker --version; ```. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107:1925,guid,guidance,1925,,https://github.com/google/deepvariant/issues/184#issuecomment-1702017107,1,['guid'],['guidance']
Usability,"e_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singular",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:13776,clear,clear,13776,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['clear'],['clear']
Usability,"eate failed for thread 63 of 64: Resource temporarily unavailable; > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > Traceback (most recent call last):; > File ""/tmp/Bazel.runfiles_XqQaQr/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; > import numpy as np; > File ""/usr/local/lib/python2.7/dist-packages/numpy/__init__.py"", line 142, in <module>; > from . import core; > File ""/usr/local/lib/python2.7/dist-packages/numpy/core/__init__.py"", line 47, in <module>; > raise ImportError(msg); > ImportError:; > ; > IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!; > ; > Importing the multiarray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1250,simpl,simply,1250,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,1,['simpl'],['simply']
Usability,"el.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.0, TPs/Indels = 40.0, TPs/SNPs = 210.0, global_step = 0, loss = 1.3173751; I1209 06:57:08.677582 46912496317632 estimator.py:2039] Savin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:2293,simpl,simply,2293,,https://github.com/google/deepvariant/issues/251,1,['simpl'],['simply']
Usability,"elow cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/inp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:1087,clear,clear,1087,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,1,['clear'],['clear']
Usability,"ely affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2074,learn,learning,2074,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:3725,learn,learning,3725,,https://github.com/google/deepvariant/issues/199,1,['learn'],['learning']
Usability,"er, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descript",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7524,learn,learning,7524,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,erenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14083,simpl,simplify,14083,,https://github.com/google/deepvariant/issues/19,1,['simpl'],['simplify']
Usability,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1425,learn,learn,1425,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665,4,['learn'],"['learn', 'learning']"
Usability,"following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1367,learn,learning-container-license,1367,,https://github.com/google/deepvariant/issues/761,1,['learn'],['learning-container-license']
Usability,"g into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHO",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:2845,undo,undo,2845,,https://github.com/google/deepvariant/issues/739,1,['undo'],['undo']
Usability,"g the union of the Truth Set with any MNP variant called in either GATK or DeepVariant, here is the Hap.py that I see:. <google-sheets-html-origin><style type=""text/css""><!--td {border: 1px solid #cccccc;}br {mso-data-placement:same-cell;}--></style>. Â  | Filter | TRUTH.TOTAL | TRUTH.TP | TRUTH.FN | QUERY.FP | FP.gt | FP.al | METRIC.Recall | METRIC.Precision | METRIC.F1_Score; -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --; HG001-DV | PASS | 27307 | 26199 | 1108 | 510 | 95 | 10 | 0.9594 | 0.9809 | 0.9700; HG001-GATK | PASS | 27307 | 26258 | 1049 | 1746 | 153 | 17 | 0.9616 | 0.9377 | 0.9495; HG002-DV | PASS | 28662 | 27505 | 1157 | 613 | 77 | 3 | 0.9596 | 0.9782 | 0.9688; HG002-GATK | PASS | 28662 | 27539 | 1123 | 2073 | 133 | 7 | 0.9608 | 0.9300 | 0.9452; HG003-DV | PASS | 28274 | 27234 | 1040 | 588 | 73 | 7 | 0.9632 | 0.9789 | 0.9710; HG003-GATK | PASS | 28274 | 27188 | 1086 | 2060 | 156 | 8 | 0.9616 | 0.9296 | 0.9453. A few observations from these metrics:. 1. For both GATK and DeepVariant, MNPs are notably harder to call. F1 is 0.945-0.97, while we expect F1 of around 0.9945 for DeepVariant at 30x with Illumina.; 2. DeepVariant and GATK4 have fairly similar recall, with DeepVariant being higher in recall in 1 sample and GATK in the other 2.; 3. DeepVariant has noticeably higher precision (0.98 vs 0.93). From this, I suspect that there is at least a few factors that make MNPs more difficult to call. I suspect that DeepVariant has learned some of those factors and is more conservative to call MNPs, even when they might look like real calls. There are several other possible explanations (e.g. that hap.py has trouble in the comparison process correctly annotating sites). Following this current set of metrics, I plan to inspect several of the correctly and incorrectly called variants in IGV with their support and see if I can better understand what factors are making MNPs more difficult to call, and if it seems like there are any specifically addressable issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1558724808:1862,learn,learned,1862,,https://github.com/google/deepvariant/issues/520#issuecomment-1558724808,1,['learn'],['learned']
Usability,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638521636:1779,intuit,intuition,1779,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636,1,['intuit'],['intuition']
Usability,"have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arno",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1906,clear,clearly,1906,,https://github.com/google/deepvariant/issues/793,1,['clear'],['clearly']
Usability,"hello @akolesnikov,. I was wondering if you might have some additional guidance towards running deepvariant. I have started running deepvariant successfully in another server, but I would like to be run the process in parallel in multiple servers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315#issuecomment-640852141:71,guid,guidance,71,,https://github.com/google/deepvariant/issues/315#issuecomment-640852141,1,['guid'],['guidance']
Usability,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2252,learn,learning,2252,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,1,['learn'],['learning']
Usability,"ibssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | |",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6232,learn,learning,6232,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ing ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1228,learn,learning-container-license,1228,,https://github.com/google/deepvariant/issues/844,1,['learn'],['learning-container-license']
Usability,"isfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (4.1.2); Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.4.4); Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.2.2); Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (1.11.0); Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.11.3); Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (3.4.2); Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python2.7/dist-packages (1.7); Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:16203,learn,learn,16203,,https://github.com/google/deepvariant/issues/89,1,['learn'],['learn']
Usability,"isfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (4.1.2); Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.4.4); Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.2.2); Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (1.11.0); Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.11.3); Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (3.4.2); Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python2.7/dist-packages (1.7); Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:12932,learn,learn,12932,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['learn'],['learn']
Usability,"lem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The specific implementation differs from GATK (and from the linked description, the GATK logic sounds more complex). Benchmarks reassembling all regions with DeepVariant consistently show the same accuracy to the region selection version. For the second step, conceptually, the methods are very similar. Both construct a de Bruijn graph of reference and alternate contigs. The same authors of these GATK methods are authors of DeepVariant, so apart from writing in C++ for speed, I expect these two to be conceptually similar. For the third step the methods are entirely different. This is where DeepVariant applies a trained convolutional neural network, looking directly at the raw information across the reads, whereas GATK applies a PairHMM to calulate the likelihood of candidate full haplotypes based on their support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488147736:1925,simpl,simple,1925,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736,1,['simpl'],['simple']
Usability,"les with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. â€”; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1396,simpl,simply,1396,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183,1,['simpl'],['simply']
Usability,"lieve the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). You donâ€™t have to re-open the ticket, but I would certainly welcome any feedback / thoughts that you might have. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:8634,feedback,feedback,8634,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,1,['feedback'],['feedback']
Usability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4271,guid,guide,4271,,https://github.com/google/deepvariant/issues/252,2,['guid'],"['guide', 'guidelines']"
Usability,"low/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3557, in _create_op_internal; ret = Operation(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op); ```. Is there something simple I am missing here? Thanks for the support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:22457,simpl,simple,22457,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['simpl'],['simple']
Usability,"markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:3615,learn,learn,3615,,https://github.com/google/deepvariant/issues/21,1,['learn'],['learn']
Usability,"me chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:2926,clear,clearly,2926,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,1,['clear'],['clearly']
Usability,"my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in off-target regions was noticeably lower). That said, I think comparisons with my own data are in the ballpark of what they were descri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:1131,learn,learning,1131,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,1,['learn'],['learning']
Usability,"nd estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4216,learn,learn,4216,,https://github.com/google/deepvariant/issues/171,1,['learn'],['learn']
Usability,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the motherâ€™s file provided as parent for the non-PAR regions of chromosomeX, and; > only the fatherâ€™s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:3066,feedback,feedback,3066,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,1,['feedback'],['feedback']
Usability,"nt please send them our way. There are a few separate issues here; let me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1238,simpl,simple,1238,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665,1,['simpl'],['simple']
Usability,"o,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1060,guid,guidance,1060,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,1,['guid'],['guidance']
Usability,"oad . This is a good question and the best answer I can give to it is both complicated and not conclusive. TL;DR - for long reads, retraining will (probably) not change things much, but there might be other future opportunities to use T2T truth in training strategies regardless of the reference used. . Generally, I think DeepVariant will give good results on T2T without retraining specifically for it, and given the better completeness of the T2T reference, probably just using this will give generally better results. In the past, we have trained with both GRCh37 and GRCh38, and we don't see the model behaving very differently with either reference. It's possible that re-training with the T2T could lead to marginally better accuracy in some areas, especially in segmental duplications, which are better resolved in T2T. At present, GRCh38 will have some segmental duplications that are collapsed, or where a copy is missing. DeepVariant seems to have learned some of the patterns for this, and is can sometimes reject variants in regions that look like segdups. This feature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578:972,learn,learned,972,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578,1,['learn'],['learned']
Usability,"om-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from a GCP VM instance.; 6. I have a standard setup with increased CPUs to 1025. . It is not clear to me what the issues are and what to make of the errors? ; I am sorry If this is not the correct forum to post in - please let me know where to seek help if not! ; Cheers, C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:9448,clear,clear,9448,,https://github.com/google/deepvariant/issues/129,1,['clear'],['clear']
Usability,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2543,feedback,feedback,2543,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,1,['feedback'],['feedback']
Usability,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880); [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1); [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_; [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:5925,Learn,Learning,5925,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,1,['Learn'],['Learning']
Usability,"orted from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4023,Simpl,SimpleNamespace,4023,,https://github.com/google/deepvariant/issues/469,1,['Simpl'],['SimpleNamespace']
Usability,"ot), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:1575,undo,undo,1575,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,1,['undo'],['undo']
Usability,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:2956,learn,learning,2956,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['learn'],['learning']
Usability,"p2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03â€¯PM Pi-Chuan Chang ***@***.***>; wrote:. > And, just in case the documentation isn't clear:; >; > This part:; >; > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > ...; >; > The variable BIN_VERSION was specified in earlier in the steps:; >; > BIN_VERSION=""1.6.1""; >; > So, in Unix command it's equivalent to:; >; > google/deepvariant:""1.6.1"" \; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749:2685,clear,clear,2685,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749,1,['clear'],['clear']
Usability,"pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1429,guid,guides,1429,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,1,['guid'],['guides']
Usability,"python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1203,Learn,Learning,1203,,https://github.com/google/deepvariant/issues/761,1,['Learn'],['Learning']
Usability,"r _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in off-target regions was noticeably lower). That said, I think comparisons with my own data are in the ballpark of what they were describing in what I quoted above, although I would have expected the indel accuracy to be in-between (perhaps something more like 90-95%). Nevertheless, even though I am trying to learn more about the DeepVariant, I apologize that I should have taken more time to consider my phrasing. For _point 4_, I think I understand your response, but that will probably be more clear as I do some testing with DeepVariant. Thank you again for your time and detailed response. So, please feel free to close this ticket. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:2342,learn,learn,2342,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,2,"['clear', 'learn']","['clear', 'learn']"
Usability,"r_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with Ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:2796,learn,learning-container-license,2796,,https://github.com/google/deepvariant/issues/774,1,['learn'],['learning-container-license']
Usability,"rain/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2290,learn,learning,2290,,https://github.com/google/deepvariant/issues/876,1,['learn'],['learning']
Usability,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help?. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196:1600,Simpl,Simplifying,1600,,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196,1,['Simpl'],['Simplifying']
Usability,"re collapsed, or where a copy is missing. DeepVariant seems to have learned some of the patterns for this, and is can sometimes reject variants in regions that look like segdups. This feature is not necessarily bad, but both using and training with the T2T reference might help it adjust its priors for how likely this is in the T2T reference (we had a [poster](https://twitter.com/acarroll_ATG/status/1231655024213856256) discussing this phenomenon and how DeepVariant reacts to it). . However, I think the effects of retraining here will be fairly minimal and restricted to either segmental duplication regions or structural variants. In addition, it's unclear to me whether this would occur only for short-reads. Right now, the quality of the truth sets is limited by long-read mappability to the reference. With good coverage of HiFi reads, we expect SNP F1 of more than 0.999. It seems likely that the model already knows enough to accurately call variants if the reference can be resolved, and though T2T may help the mapping resolve the remainder, it's unclear whether there is more to learn in further training. This highlights one key point where T2T may help with training - that the current training is limited by the truth set. Training with v4.2.1 truth sets is still constrained by the confident regions of the genome. If we can get fully complete, 100% accurate truth sets covering the genome, this will provide more training examples of difficult regions in the training process, and I think this could further improve a model (whether it's on the T2T reference or on GRCh38). I think there will be an opportunity for this as complete T2T assemblies become available for more samples. Finally, from a practical perspective, the current v4.2.1 truth sets are relative to GRCh38, so in order to train we'd need to first be able to generate truth variants and confident regions for some sample on T2T. That's certainly doable, but it is tricky to do correctly. Hopefully this has answere",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578:1997,learn,learn,1997,,https://github.com/google/deepvariant/issues/534#issuecomment-1102014578,1,['learn'],['learn']
Usability,"rity, but I got the same error. I've tried to find a solution through various online resources, but nothing has helped so far. `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 121, in train; tune_dataset_config = data_providers.read_dataset_config(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/data_providers.py"", line 634, in read_dataset_config; dataset_config = text_format.Parse(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 648, in Parse; return ParseLines(text.split(b'\n' if isinstance(text, bytes) else u'\n'),; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 722, in ParseLines; return parser.ParseLines(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 776, in ParseLines; self._ParseOrMerge(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 804, in _ParseOrMerge; self._MergeField(tokenizer, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 894, in _MergeField; raise tokenizer.ParseErrorPreviousToken(; google.protobuf.text_format.ParseError: 13:1 : Message type ""learning.genomics.deepvariant.DeepVariantDatasetConfig"" has no field named ""s2"".`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837:2515,learn,learning,2515,,https://github.com/google/deepvariant/issues/837,1,['learn'],['learning']
Usability,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:2556,feedback,feedback,2556,,https://github.com/google/deepvariant/issues/225,1,['feedback'],['feedback']
Usability,"rolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8	75144983	.	T	TG,<*>	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17,0:0.485714,0:49,0,64,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:31:0,105,1049; ```; and kid:; ```; chr8	75144981	.	T	A,<*>	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:71,0,67,990,990,990; chr8	75144982	.	A	T,<*>	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16,0:0.592593,0:63,0,63,990,990,990; chr8	75144983	.	T	G,<*>	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:67,0,71,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:25:0,75,749; ```; I am attaching a small sam for kid and dad aligned to hg38; [kid.sam.gz](https://github.com/google/deepvariant/files/4206576/kid.sam.gz); [dad.sam.gz](https://github.com/google/deepvariant/files/4206577/dad.sam.gz). I have other scenarios, but this one is one that seems clearly a deep variant issue and not a problem with glnexus.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272:2866,clear,clearly,2866,,https://github.com/google/deepvariant/issues/272,1,['clear'],['clearly']
Usability,"rself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:2558,learn,learning,2558,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['learn'],['learning']
Usability,"running call variants on ONT data using the command . `run_pepper_margin_deepvariant call_variant -b ./Sample/alignments/GRCh38/41195.minimap2.bam -f /data/Homo_sapiens_assembly38.fasta -o Sample -t 4 -s Sample --phased_output --ont_r9_guppy5_sup; `. no errors but it's stuck at Starting Candidate Finding and the CPU is at 0% for hours. Can I stop it and resume? why there is no error? will it ever finish?. ....; [12-24-2023 05:47:47] INFO: SUMMARY PROCESSED 7280/7280.; [12-24-2023 05:47:47] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: FINISHED PREDICTION; [12-24-2023 05:54:33] INFO: ELAPSED TIME: 867 Min 41 Sec; [12-24-2023 05:54:33] INFO: PREDICTION FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: TOTAL ELAPSED TIME FOR INFERENCE: 867 Min 44 Sec; [12-24-2023 05:54:33] INFO: STEP 3/3 FINDING CANDIDATES; [12-24-2023 05:54:33] INFO: OUTPUT: 41195/pepper/; [12-24-2023 05:55:25] INFO: STARTING CANDIDATE FINDING. - Operating system: Linux; - DeepVariant version: kishwars/pepper_deepvariant r0.8; - Installation method (Docker, built from source, etc.): Docker; ONT long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/758:356,resume,resume,356,,https://github.com/google/deepvariant/issues/758,1,['resume'],['resume']
Usability,"s one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:; So, then I ran:. ```bash; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```. The command above gave me:; ```. ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with Ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:2830,learn,learning-container-license,2830,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,1,['learn'],['learning-container-license']
Usability,"ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hope it helps and have a great weekend!; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:11522,learn,learning,11522,,https://github.com/google/deepvariant/issues/50,4,['learn'],['learning']
Usability,"stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1686,learn,learning,1686,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,1,['learn'],['learning']
Usability,"suecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1201,learn,learning-,1201,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['learn'],['learning-']
Usability,"t might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1175,learn,learn,1175,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['learn'],"['learn', 'learning']"
Usability,"t(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21574,learn,learn,21574,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['learn'],['learn']
Usability,"t_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. â€”; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1064,simpl,simply,1064,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500,1,['simpl'],['simply']
Usability,"tart to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:2022,clear,clear,2022,,https://github.com/google/deepvariant/issues/716,1,['clear'],['clear']
Usability,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-902138384:505,simpl,simple,505,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384,1,['simpl'],['simple']
Usability,"thanks for the great piece of software!; I agree with Jordi and would add that some general guidelines on how to use the numerous INFO fields would be nice, do they live somewhere?; it is a pity to have so many metrics and ignore how to put them to good use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531#issuecomment-1081798812:92,guid,guidelines,92,,https://github.com/google/deepvariant/issues/531#issuecomment-1081798812,1,['guid'],['guidelines']
Usability,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1022,learn,learning,1022,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,5,['learn'],"['learning', 'learns']"
Usability,"tially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1030,guid,guide-to-hyperparameters-search-for-deep-learning-models,1030,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['guid'],['guide-to-hyperparameters-search-for-deep-learning-models']
Usability,"tion by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2158,learn,learning,2158,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; ```. I am new to Deep Learning and am struggling to decide whether something is wrong with my training approach/scripts or whether the model just needs more time / different hyperparams. Given the number of examples, I can only run 1 epoch at a time before I hit the 24hr cluster wall-time limit. So I have only trained for around 30,000 steps in total across 2 epochs so far (starting from last checkpoint after 1st epoch). . All advice much appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:14051,Learn,Learning,14051,,https://github.com/google/deepvariant/issues/876,1,['Learn'],['Learning']
Usability,"ut you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RAM machine, it usually only uses about 500% CPU (instead of all of the 64 cores). This step took about 2m. I can confirm that I successfully run this step without the errors you saw. However, if I remove the files first by `rm -f HG002.examples.tfrecord*.gz` and then repeat the call_variants command above, I can see the errors you reported above:; ```; ValueError: Cannot find matching files with the pattern ""HG002.examples.tfrecord@64.gz""; ```. So, please make sure you that your make_examples step completed successfully before you proceed. (4) As you noticed so far, creating a workflow can be complicated (and mostly has nothing to do with variant caller itself). If you end up trying out the Cloud runner and has more questions, please feel free to get in touch with @nmousavi and the Cloud team. I'm sure the team will love to get your feedback if you decide to try it out. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:5734,feedback,feedback,5734,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,1,['feedback'],['feedback']
Usability,"value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; # internvaltree v3 has some API changes with v2; ##########################################################################; pip install 'in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21941,learn,learn-,21941,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['learn'],['learn-']
Usability,"ve a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount it whenever I create a new instance, but I don't have to re-upload the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:1589,feedback,feedback,1589,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,1,['feedback'],['feedback']
Usability,"ve_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; parallel: This job failed:; sudo docker run -v /home/root:/home/root gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/make_examples --mode training --ref /home/chenyangwang600/training-case-study/input/data/ucsc_hg19.fa --reads /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam --examples /home/chenyangwang600/training-case-study/output/validation_set.with_label.tfrecord@8.gz --truth_variants /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz --confident_regions /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed --task 1 --regions 'chr21 chr22'. real 0m4.444s; user 0m0.318s; sys 0m0.216s`. I thought I followed the instructions in the guide(Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data) except that I used a 8vCPUs with ; `gcloud beta compute instances create ""cpu-eight"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-8"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""`. and set variables; N_SHARDS=""8"". I tried to use another VM but also failed. How can I solve this issue?. Thanks,; Yang",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:3057,guid,guide,3057,,https://github.com/google/deepvariant/issues/184,1,['guid'],['guide']
Usability,"via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1455,guid,guide-to-hyperparameters-search-for-deep-learning-models,1455,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,2,"['guid', 'simpl']","['guide-to-hyperparameters-search-for-deep-learning-models', 'simple']"
Usability,"what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:2119,learn,learning,2119,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['learn'],['learning']
Usability,"with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different lea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1959,learn,learn,1959,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,1,['learn'],['learn']
Usability,"y - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descript",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:1998,learn,learning,1998,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"y use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2199,simpl,simple,2199,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,1,['simpl'],['simple']
Usability,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1333,simpl,simpler,1333,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,1,['simpl'],['simpler']
Usability,"ze:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docker file. If so, please remake your .sif file with `1.6.0`. ). Because @alanlamsiu used the .sif file, I'll also do something similar:; So, then I ran:. ```bash; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```. The command above gave me:; ```. ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:2666,Learn,Learning,2666,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,1,['Learn'],['Learning']
Usability,"| ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6084,learn,learning,6084,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
