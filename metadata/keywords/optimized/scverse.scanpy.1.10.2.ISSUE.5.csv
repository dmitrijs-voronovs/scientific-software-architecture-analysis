quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Modifiability,"Fix finding variables with sc.pl.scatter(..., use_raw=True)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:12,variab,variables,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,1,['variab'],['variables']
Modifiability,Fix graph metrics when some variables are constant,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,1,['variab'],['variables']
Modifiability,Fix malformed flake8 config file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1789:21,config,config,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1789,1,['config'],['config']
Modifiability,Fix scatter plots for sparse layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/713:29,layers,layers,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/713,1,['layers'],['layers']
Modifiability,"Fixed in https://github.com/theislab/scanpy/commit/57161ec444eef7815e159037c6944ddcc75572d9. However, the version1 branch is not stable yet... another day or two... What made me believe that seaborn is still doing strange things, is this... one call to `seaborn.set_style` messes up the whole configuration... That's a bug, isn't it?; <img width=""207"" alt=""image"" src=""https://user-images.githubusercontent.com/16916678/37690247-05c2686e-2caa-11e8-8dc2-7365a90f8748.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935:293,config,configuration,293,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108#issuecomment-374805935,1,['config'],['configuration']
Modifiability,Fixes #1646 . Now supports coloring by boolean variables such as `True` and `False`. **Tasks to complete:** . - [x] Add test,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2460:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2460,1,['variab'],['variables']
Modifiability,"Fixes #1806. New behaviour for Moran's I and Geary's C. If one of the variables passed has constant values, the score for that variable is `nan` and the function warns the user about this. Previously, the presence of this variable would silently fail, corrupting the other outputs as well. Adds a new utility `is_constant` to check if values in an array are constant. * Could have less code repetition, since now there's some logic that is applied to any case which is 2d, but the conditional isn't structured this way.; * Performance hit pretty minor, as computing the metric itself is expensive.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:70,variab,variables,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,3,['variab'],"['variable', 'variables']"
Modifiability,"Fixes #1892. Scipy now returns `np.nan` for Mann-Whitney U tests where there it used to error. Namely, variables for which all values are the same.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1893:103,variab,variables,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1893,1,['variab'],['variables']
Modifiability,"Fixes #2211. I’ll first try with all removed. I expect the min_tests run to fail. Seeing what fails,. - If it’s not much, I’ll just refactor the fixtures a bit and so on; - Else I’ll move algorithms to the `test` extra. Then we can merge this PR and over time refactor our tests so more and more extras go from `tests` to `tests-full`. @ivirshup do you like the collection extras’ names (`io`, `speedups`, `algorithms`)? Should we add an extra named `all` that installs all the `-full` extras?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2222:132,refactor,refactor,132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2222,2,['refactor'],['refactor']
Modifiability,"Fixes #767. This is a work in progress PR adding ICA as a dimensionality reduction method. Some points:. This is faster and works with larger data than the sklearn version – entirely due to the whitening step. sklearn uses `np.linalg.svd` for whitening, which causes errors about using 32 bit lapack for large datasets since we use 32 bit floats and is slow (but exact). I've swapped that with the arpack svd. I may try and upstream this in the future, but there are a number of open PRs about ICA that I'd like to wait for a bit on: https://github.com/scikit-learn/scikit-learn/pull/11860, https://github.com/scikit-learn/scikit-learn/issues/13056. As a benchmark, I was able to compute 40 dimensions of an ICA on 50k cells (tabula muris) and 7.5k highly variable genes in about a minute (59.3s) on my laptop. As a comparison (for a smaller dataset – 10k PBMCs) here are two pair grid plots showing cell embeddings on ten components compared with the top ten components of a PCA. <details>; <summary> PCA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899041-0c9f5b80-13b5-11ea-973f-81d4c27fe3b1.png). </details>. <details>; <summary> ICA </summary>. ![image](https://user-images.githubusercontent.com/8238804/69899077-7cade180-13b5-11ea-9a0b-023868553181.png). </details>. Things left to do:. - [ ] Look into numerical stability; - [ ] Figure out if I should be be scaling the whitening matrix differently; - [ ] More in depth comparison of results with sklearn based ICA; - [ ] Documentation; - [ ] Share `_choose_obs_rep` with `sc.metrics` PR. Once this is done, I'd like to also add sklearns NMF.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941:756,variab,variable,756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941,1,['variab'],['variable']
Modifiability,"Fixes for deprecation warnings from anndata`v0.6.22`. This is mostly by replacing access to `.X` with access via `{obs,var}_vector` or `sc.get.obs_df`. Supercedes #713, fixes #700 and #690. Functions changed:. * `sc.pl.violin`; * Dataframe for plotting now constructed with `obs_df`, access to `adata.X` no longer used.; * `sc.pl.scatter`; * Changed default `layer` from `""X""` to `""None""`. `""X""` is still supported, but should throw a deprecation warning if it's explicitly used.; * Replace usage of `._get_obs_array` with `.obs_vector`; * `sc.pl._tools.scatterplots.plot_scatter`; * Normalized access to layers, now sparse and dense should similarly.; * `sc.get.obs_df`; * Added support for `use_raw`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/730:605,layers,layers,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/730,1,['layers'],['layers']
Modifiability,"Fixes the doctest setup that was broken in https://github.com/scverse/scanpy/pull/2874. The goal here is to make `_modify_doctests` work again without breaking coverage. That means; 1. the plugin can’t be imported in `scanpy/tests` but has to be imported globally; 2. the plugin has to live outside of `scanpy` since importing scanpy while importing it breaks coverage; 1. The plugin can’t import scanpy at the top level (neither `import testing.scanpy` nor `import testing.scanpy._pytest` is allowed to transitively `import scanpy`). having `testing.scanpy` in `src` and `scanpy` in the root of the repo is a bit gross, but better than adding even more top level stuff",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3057:189,plugin,plugin,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3057,3,['plugin'],['plugin']
Modifiability,"Fixes; ```; Traceback (most recent call last):; File ""/tmp/tmptq4o33we/job_working_directory/000/50/configs/tmpe21tizb1"", line 29, in <module>; scale='width'); File ""/usr/local/lib/python3.7/site-packages/scanpy/plotting/_tools/__init__.py"", line 564, in rank_genes_groups_violin; df[g] = X_col; File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3487, in __setitem__; self._set_item(key, value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3563, in _set_item; self._ensure_valid_index(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 3540, in _ensure_valid_index; value = Series(value); File ""/usr/local/lib/python3.7/site-packages/pandas/core/series.py"", line 314, in __init__; data = sanitize_array(data, index, dtype, copy, raise_cast_failure=True); File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/construction.py"", line 729, in sanitize_array; raise Exception(""Data must be 1-dimensional""); Exception: Data must be 1-dimensional; ```. <!-- ; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1669:100,config,configs,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1669,1,['config'],['configs']
Modifiability,Following discussions with @giovp I've extended the `scanpy.datasets.visium_sge` function to optionally return a path to the high-resolution tissue image also available in the visium Spatial Transcriptomics datasets.; This makes it easy to leverage `scanpy.datasets` to fully explore visium datasets.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1506:39,extend,extended,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1506,1,['extend'],['extended']
Modifiability,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358:903,variab,variables,903,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358,1,['variab'],['variables']
Modifiability,"For continuous values I don't think we need to add anything to the color bar. If a dot color is not part of the colorbar then is assumed that is a NaN. I searched in matplotlib for a similar case in which a colorbar includes NaN values but could not find any example. If this feature is wanted, what we can do is to use the option for colorbar extension and use it for NaNs but we need to find a way to set the label for NaN. ```PYTHON; import numpy as np; import matplotlib.pyplot as plt. adata = sc.datasets.pbmc68k_reduced(); adata.obs['n_genes'].iloc[::4] = np.nan; cmap = plt.get_cmap('viridis'); cmap.set_under('lightgray'); cmap.set_bad('lightgray'). fig, ax = plt.subplots(); cax = ax.scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1], ; c=adata.obs['n_genes'], s=20, ; cmap=cmap, ; vmin=1000, ; vmax=2000, plotnonfinite=True); fig.colorbar(cax, extend='min', extendrect=True, extendfrac=0.1). plt.show(); ```; ![image](https://user-images.githubusercontent.com/4964309/90750699-7b22a180-e2d5-11ea-9a67-1ad7feb8a6a4.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507:865,extend,extend,865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-677477507,3,['extend'],"['extend', 'extendfrac', 'extendrect']"
Modifiability,"For me it's good, I would just merge so we have a prototype version for the vignette, then we can extend for other data type/modify it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419:98,extend,extend,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1013#issuecomment-580245419,1,['extend'],['extend']
Modifiability,"For some context, this has come up in discussion with cellxgene before: (https://github.com/chanzuckerberg/cellxgene/issues/1152#issuecomment-604286306). I think I still feel the same way about this. Basically, a continuous colormap is defined by more than just the name of the colorspace. There are parameters like maximum value, minimum value, middle value (for divergent colormaps), scale, and binning. I'm not sure how useful it is to keep just the color scheme without any of these other values. Why this parameter, and not others?. I'm not sure it's the right solution for the use case. I think that use case would be better fit by being able to generate all the plots individually, then collect them into a figure. This way you would have complete control over how the colormaps were applied to each of the continuous variables separately. Unfortunately, this isn't particularly ergonomic to do with matplotlib since individuals plots have to know about the `Figure` when constructed. Side issue: We probably don't want to save separate color palettes for each gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302:825,variab,variables,825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729531302,1,['variab'],['variables']
Modifiability,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT.; ```python; import anndata2ri; from rpy2.robjects.packages import importr; from rpy2.robjects import r, pandas2ri; import numpy as np. anndata2ri.activate(); pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):; if layer:; mat = adata.layers[layer]; else:; mat = adata.X. # Set names for the input matrix; cell_names = adata.obs_names; gene_names = adata.var_names; r.assign('mat', mat.T); r.assign('cell_names', cell_names); r.assign('gene_names', gene_names); r('colnames(mat) <- cell_names'); r('rownames(mat) <- gene_names'). seurat = importr('Seurat'); r('seurat_obj <- CreateSeuratObject(mat)'). # Run; for k, v in kwargs.items():; r.assign(k, v); kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]); r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))); adata.layers['SCT_data'] = sct_data.T; sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))); adata.layers['SCT_counts'] = sct_data.T; return adata; ```; ```python; adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA; R[write to console]: Place corrected count matrix in counts slot; R[write to console]: Set default assay to SCT. adata; layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```; Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200:333,layers,layers,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1564578200,5,['layers'],['layers']
Modifiability,"From my side it's ready to be merged. I have left a note in the comments about extending this to enrichment scores, and that this would be difficult. I though it might be good to leave that in there in case anyone wants to extend it later. Thoughts on that?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/549#issuecomment-478145349:79,extend,extending,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/549#issuecomment-478145349,2,['extend'],"['extend', 'extending']"
Modifiability,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:281,variab,variable,281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707,2,['variab'],['variable']
Modifiability,"From what I can gather, one goal here is to refactor the dotplot function and give it a complex heatmap layout, with a central heatmap using circle patches with a color and size aesthetic (= the dotplot) and one or more annotation heatmaps for rows and columns, which could be categorical or quantitative each. Potentially relevant features in codaplot are. - co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:44,refactor,refactor,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactor']
Modifiability,Getting Error Variable names are not unique when using .read_10x_h5 function,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/534:14,Variab,Variable,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534,1,['Variab'],['Variable']
Modifiability,"Good Evening,. My goal here is to get either a Gene barcode or dense matrix from a .h5 file from 10x. I'm currently trying to use the .read_10x_h5() function to help me achieve this. To my understanding I just need to input the file name into the function. When I run my code (See below), I get an error stating that ""Variable names are not unique. To make them unique, call `.var_names_make_unique`."" From the documentation I don't see a way that I can call .var_names_make_unique(). Is there some preprocessing that I'm missing?. ```python; user_input = input(""Enter the path of your file: ""); def convert_h5_to_adata(filename):; filename = str(filename); if os.access(filename, os.R_OK):; sc.read_10x_h5(filename); return; convert_h5_to_adata(user_input); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/534:318,Variab,Variable,318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/534,1,['Variab'],['Variable']
Modifiability,"Good point that spike-ins may not always behave like endogenous transcripts. However, since spike-ins can account for the later biases in the workflow (e.g. variable sequencing depth, capture efficiencies, amplification etc.), I do think that they are still an important part of the solution and a step closer to absolute quantification- would you disagree?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966:157,variab,variable,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-679132966,1,['variab'],['variable']
Modifiability,"Good! So, I'd really like to jump in and work on ann_matrix as well, if you think this is efficient. Of course, I don't want to mess up what you had in mind.; 1. yes, that's important - can i help?; 2. that's easy, simply put it in smp as a multicolumn object; 3. should be very easy as well, maybe recarray can directly be written with a single key, if not, one has to make the separation between str and float columns -> shall I attack that? see [this](https://github.com/theislab/scanpy/commit/ac79f8991953bf7f4ae33f243b384560c131a8f9#L650-L669) for how it was done with the ddata using its 'rowcat' attribute. should be straightforwardly adapted, right?*; ---; *sorry, I simply forgot to add readwrite.py on thursday night, which caused master to be non-working since then, of course. with readwrite.py added, master now works just fine. I guess the only change you made to utils.py was adding the AnnData.from_dict(...) in the function read()? so one could use readwrite.py from master within ann_matrix. or just create readwrite.py again by cutting out everything related to reading/writing from utils and pasting it into the new module readwrite.py.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990:642,adapt,adapted,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1#issuecomment-277506990,1,['adapt'],['adapted']
Modifiability,"Hah, I've gotten much better at numba since I wrote this function. I figured out I can just get the core part to work on floats and don't have to worry about casting between types. Makes this a much easier decision. Now floats aren't converted to integers in the first place. > We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64. I think we can be a little flexible on this, and just generally follow numpy promotion rules (except for when they're bad).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713:451,flexible,flexible,451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558449713,1,['flexible'],['flexible']
Modifiability,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:; - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient.; - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced).; - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient); while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. ; Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802:556,layers,layers,556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-553948802,1,['layers'],['layers']
Modifiability,"Have the same issue. Windows, Ubuntu for WSL, miniconda:. > conda install -c bioconda/label/cf201901 scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. > UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. > Specifications:. > - scanpy -> python[version='>=3.6,<3.7.0a0']. > Your python: python=3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368:237,flexible,flexible,237,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-582183368,2,['flexible'],['flexible']
Modifiability,"Have you subsetted your AnnData object to highly variable genes, while keeping the full dataset in `.raw`? In that case it could be that genes that are found as markers via `rank_genes_groups`, are not in `adata.var_names`, but only in `adata.raw.var_names` and therefore cannot be found by the plotting function. I've previously encountered issues with this, but I thought it had been solved now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456769781:49,variab,variable,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456769781,1,['variab'],['variable']
Modifiability,"Having the exact same problem. Windows machine, win10, 64 bit. Trying to install from miniconda. FWIW, I have installed scanpy successfully on two other windows machines (my home computer and my work computer) in the last three weeks. Now following identical steps on my laptop and having this tissue. . ```; conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pytables conflicts for:; scanpy -> pytables; Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package h5py conflicts for:; scanpy -> h5py!=2.10.0; Package patsy conflicts for:; scanpy -> patsy; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package seaborn conflicts for:; scanpy -> seaborn; Package setuptools conflicts for:; scanpy -> setuptools; Package python conflicts for:; scanpy -> python[version='>=3.6']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package networkx conflicts for:; scanpy -> networkx; Package python-igraph conflicts for:; scanpy -> python-igraph; Package louvain conflicts for:; scanpy -> louvain;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824:471,flexible,flexible,471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575769824,2,['flexible'],['flexible']
Modifiability,"Hej,. I have been trying to plot the ranked gene groups with a dotplot in this way. ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, layer='imputed') ; ```. or from non-raw data; ```; sc.plotting.tools.rank_genes_groups_dotplot(all_data_flt_clst, n_genes=5, save='.pdf', groupby='batch', dendrogram=False, use_raw=False) ; ```. I get an error regarding the index names of the genes. Other commands for plotting gene rank groups (such as heatmap) show the same problem. The error goes as follows:; ```; KeyError: 'Indices ""[\'PRM2\', \'ACAP1\', \'SPEM1\', \'SPATA3\', \'C10orf62\', \'TNP1\', \'MIR193BHG\', \'PRM1\', \'CCDC179\', \'AC007557.1\', \'SPACA1\', \'ERICH2\', \'RP11-360D2.1\', \'TIPARP-AS1\', \'GS1-124K5.4\', \'DCN\', \'C1S\', \'SERPING1\', \'C1R\', \'SERPINF1\', \'GAGE2A\', \'PTMA\', \'HMGB1\', \'VCX2\', \'ERP29\', \'ZCWPW1\', \'SMC1B\', \'DPH7\', \'SCML1\', \'CLSPN\', \'CSAD\', \'C1QBP\', \'DNAJB6\', \'TCF3\', \'RRBP1\', \'HSP90AA1\', \'TMED10\', \'ART3\', \'BUB1\', \'KRBOX1\', \'B2M\', \'IFITM3\', \'GNG11\', \'IFITM2\', \'IFI27\', \'TYROBP\', \'S100A4\', \'FCER1G\', \'CD163\', \'CYBA\', \'CALD1\', \'IGFBP7\', \'TIMP3\', \'PTGDS\', \'TSHZ2\', \'MT-ND3\', \'MT-ND1\', \'MT-ND2\', \'MT-ATP6\', \'MT-ND4\', \'MT-ND1\', \'HMGN5\', \'MT-ND3\', \'ALDH1A1\', \'MT-ATP6\']"" contain invalid observation/variables names/indices.'; ```. I checked the var_names and obs_names of my object, but they seem totally fine. Do you have any ideas about the origin of the problem? I can post the backtracking of the error if needed :). Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438:1393,variab,variables,1393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438,1,['variab'],['variables']
Modifiability,"Hello all,; For these 2 functions,; `sc.pp.filter_cells(adata, min_genes=200)`; `sc.pp.filter_genes(adata, min_cells=3)`; the authors make `inplace=True` as default. Because I want to tranfer the output into an variable, I change these functions to; ```python; a=sc.pp.filter_cells(adata, min_genes=200, inplace=False); sc.pp.filter_genes(a, min_cells=3, inplace=False); ```; but it creates errors and the output of a is NoType:; ```python; aceback (most recent call last):; File “C:\Users\Yuanjian\AppData\Local\Programs\Python\Python36\lib\site-packages\IPython\core\interactiveshell.py”, line 3343, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File “”, line 2, in; sc.pp.filter_genes(a, min_cells=3, inplace=False) # exclude genes only expressed in <3 cells; File “C:\Users\Yuanjian\AppData\Local\Programs\Python\Python36\lib\site-packages\scanpy\preprocessing_simple.py”, line 259, in filter_genes; X if min_cells is None and max_cells is None else X > 0, axis=0; ```. Does anybody know why inplace=False doesn’t work?; Thanks!; Best,; YJ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2030:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2030,1,['variab'],['variable']
Modifiability,"Hello scanpy team,. this is my first pull request, I hope I at least vaguely adhered to your contribution guidelines. I did not find an issue tackling this exact situation, but if I overlooked it, feel free to point that out. This PR changes few lines of code involved in getting dimensionality reductions like X_pca. The current code skips subsetting the respective array to the n_pcs requested, if the representation name is not X_pca. I do not think this is optimal. This change allows to only take the dimensions needed also for non X_pca representations. This can be useful when using harmony for example or storing different PCA embeddings in the same object. Then it is not needed to first store them as X_pca to be able to properly use the n_pcs parameter in e.g. pp.neighbors. It might make sense then to adapt the documentation respectively as well. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. ----------. Fixes #1846",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2179:814,adapt,adapt,814,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2179,1,['adapt'],['adapt']
Modifiability,"Hello! I'm trying to recolor some categorical variables in the scanpy.api.pl.tsne function but am having some trouble. Specifically, with continuous data, I'm fine using the `color_map` key word to change between scales like ""viridis"" and ""Purples"" but when trying to pass the `palette` key word for categorical data (sample labels, louvain lables), it doesn't seem to update the colors in the plot. Perhaps I'm specifying the color palette incorrectly? Here are a few versions I've tried:. ```; sc.pl.tsne(adata, ; color=['louvain'], ; #palette=['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9'], ; #palette=['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'],; #palette=""Set3"",; palette=sns.color_palette(""hls"", 15),; legend_fontsize=""20""); ```; ![image](https://user-images.githubusercontent.com/33738960/40148279-0b344502-5922-11e8-998c-4d5ece963253.png). But all of these attempts result in the same default cluster color scheme. Any suggestions?. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156:46,variab,variables,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156,1,['variab'],['variables']
Modifiability,"Hello, I am trying to calculate highly variable genes from my data sets using the above code from the scanpy script published on github. I am facing this error. could someone please help me with this? Thank you very much.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1560:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1560,1,['variab'],['variable']
Modifiability,"Hello,. In some cases, I need to visualize clustering results (categorical) on UMAP for each batch. . I know `sc.pl.umap(adata[adata.obs['batch] == 'batch1], color = 'louvain')` is a solution. However, other cells are missing. I think the other cells colored by grey as background should be a better way. . I notice that `sc.pl.umap(adata, color = 'batch', groups = ['batch1'] )` can retain other cells as grey, though sometimes cells were submerged in the bottom layer (I used reoder_categories to bypass this issue). But, `color` and `groups` must be correspondence! . Is there any way to fulfill my needs in `Scanpy` if I missed something. ; Or, could the authors add a parameters, such as `restrict_to` in `sc.tl.louvain`, to implement this function: ① liberate strong associations between `color` and `groups`, and ② add support for ordering categorical variable",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/759:859,variab,variable,859,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/759,1,['variab'],['variable']
Modifiability,"Hello,; I have the same doubt. I think with the use of sc.pp.scale, the distribution of genes(equal to different variables) is normal distribution which mean is 0 and variance is 1. And this is an ideal data moduel for PCA. So I wanna know whether sc.pp.scale is the import step before sc.tl.pca for the reason I guess above.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103660346:113,variab,variables,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103660346,1,['variab'],['variables']
Modifiability,"Hello,; I've gotten scanpy working on my local computer, but for memory reasons I need to move to our server (linux). I am running into the same errors as above - any advice is appreciated!. Skipping optional fixer: buffer; Skipping optional fixer: idioms; Skipping optional fixer: set_literal; Skipping optional fixer: ws_comma; running build_ext; Cannot find the C core of igraph on this system using pkg-config.; We will now try to download and compile the C core from scratch.; Version number of the C core: 0.7.1.post6; We will also try: 0.7.1; ; Version 0.7.1.post6 of the C core of igraph is not found among the nightly builds.; Use the --c-core-version switch to try a different version.; ; Could not download and compile the C core of igraph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318:407,config,config,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-518220318,1,['config'],['config']
Modifiability,"Hello,; When I call 'dpt_scatter' with the groups parameter I get the following error:; NameError: name 'names' is not defined. It looks like this is from line 230 in scanpy/plotting/ann_data.py and the 'names' variable just doesn't exist.; I'm assuming it should just be 'groups'?. Thanks,; Sarah",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/32:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/32,1,['variab'],['variable']
Modifiability,Here is the way I extracted the most variable genes in scanpy: ; ![Screen Shot 2023-12-22 at 11 59 13 AM](https://github.com/scverse/scanpy/assets/65792233/97be237d-eff8-4910-b858-163d301c2bf6),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897471:37,variab,variable,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897471,1,['variab'],['variable']
Modifiability,"Hey @buesra-oezmen! I don't think this is a UMAP issue, but instead an issue of the MultiVI parameterization or the data. But as I know the data quite well in this case, I'm pretty sure it's maybe just a MultiVI model that is not sufficiently trained. Maybe try for a few more epochs? Or otherwise maybe the network architecture needs to be adapted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471:92,parameteriz,parameterization,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2148#issuecomment-1047005471,2,"['adapt', 'parameteriz']","['adapted', 'parameterization']"
Modifiability,"Hey @giovp !. Thanks for your review and sorry for the delay, but I think I addressed all requests now:; - code moved to experimental; - fixed broken column ordering when batch argument was used with HVG selection; - tests adapted to the new code location. I was not sure how the `highly_variable_genes()` should look like in its experimental version. For now, I removed everything that is not related Pearson residuals, including input arguments and docstring. I also left a note in non-experimental `highly_variable_genes()`'s docstring that mentions the experimental version with the additional Pearson flavor. Feel free to remove again if you don't like it. Regarding the tutorial: Sure, that would be nice! I can prepare a short demo notebook. Do you think we could start with a rather concise notebook now to package it with the initial release in `experimental` (basically demonstrating how to use it on some example data, and some theory/background info how it works / why it makes sense), and then prepare a longer later on? Then I'd just open a pull request (?) in your tutorial-github for that?. Let me know if there is more to do here :). Cheers, Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-879988467,1,['adapt'],['adapted']
Modifiability,"Hey @ivirshup, . `hasattr(__builtins__, ""__IPYTHON__"")` now seems to always return False, even if it's True when run from within the notebook. That causes figures to end up very blurry due to the missing png2x config. Am I missing anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495:210,config,config,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1477#issuecomment-834394495,1,['config'],['config']
Modifiability,"Hey Phil!. Nice, I played around with it! :smile:. I think it's really cool and I only have some minor stylistic remarks. - Why do we want the `**`? The layout and the indenting highlights the variable names enough already. Also, Jupyter notebooks don't even interpret them.; - Why don't we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:193,variab,variable,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['variab'],['variable']
Modifiability,"Hey everyone, thanks for the discussion so far! I don't have much to add to what @dkobak said earlier, so let me summarize a bit from my perspective:. I am motivated to contribute the method here because people were interested to use it with scanpy after seeing the preprint, and scanpy devs reached out to us to implement it here. For that it does not matter if it ends up in `external` or `core`, but as @giovp mentioned, the code is easy to integrate into the existing normalize/hvg-selection workflow and the method itself is well connected to established workflows. @adamgayoso raised the question if new preprint methods should be allowed in `core` at all, had several suggestions how this PR could be handled (halt until peer review publication/put in `external` for now/extend method to support also e.g. deviance residuals and others), and some open questions about the exact workflow integration. I would like to clarify with everyone how to proceed now. @ivirshup @LuckyMD, could you help us a bit to decide how to move forward?. In terms of development, I answered all of your code review comments @giovp, so maybe you can briefly check & resolve those you are happy with..?! I am also ready to finally write tests once we are decided on where this PR is going.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490:778,extend,extend,778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-801883490,1,['extend'],['extend']
Modifiability,"Hey! So you used `restrict_to = ('louvain', ['1'])` if you wanted to recluster '1'? You'll have a new clustering variable `louvain_R` in this case, which you can rename anything you like... checkout the documentation: https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.api.tl.louvain.html. I guess that this is what you want.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/347#issuecomment-436379004:113,variab,variable,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/347#issuecomment-436379004,1,['variab'],['variable']
Modifiability,"Hey!. Logistic regression currently doesn’t output p-values i believe. Either way, it also treats genes as independent variables, so no need for subsetting here either. As for your second question, you may have misunderstood my answer. `sc.tl.rank_genes_groups()` gives you marker genes just as MAST or diffxpy do (but with more complex models that can incorporate covariates). I was just commenting on the interpretation of marker genes. They tell you which genes characterize a cluster, but don’t necessary tell you which genes contributed most to the global split of clusters that was generated (which i thought you were asking about). That type of question would require a feature importance metric on a multiclass classification problem. For example training a random forest to predict the clusters and then using gini importance to rank the features. That is not a common question asked of single-cell data though, so there’s no tool i’m familiar with that does this. I hope that is clearer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347:119,variab,variables,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515168347,1,['variab'],['variables']
Modifiability,"Hey, just wanted to comment here on why it's taken so long for a review. I'm personally not comfortable with having significant code in the package that we cannot test on CI. We're looking into this, but it's been slow going since it looks like we have to set this up and manage it on our own. As far as I can tell this process is:. * Put money into the azure account; * Set up containers; * Configure pipelines to use these containers (not sure if we can use the standard Tasks on ""self hosted"" containers) . @Zethson, since you're actually at the institute with the money you may have better luck moving the first step forward than I've had. Do you think you'd be able to look into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859:392,Config,Configure,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-815455859,1,['Config'],['Configure']
Modifiability,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:796,adapt,adapt,796,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,1,['adapt'],['adapt']
Modifiability,"Hey, thanks a lot for spotting & the nice reproducible example! Big help. From my first look into it, it seems this is a bug indeed;. **Bug appearing when**; - `batch_key` is not `None` and; - `flavor` is `“seurat”` or `“cell_ranger”` and; - then using `subset=True`. Other cases are not suffering from this it seems (e.g. `flavor=""seurat_v3""`, or when `batch_key=None`). **Issue**; It appears in the cases describe above, `subset=True` will cause the first `n_top_genes` many genes of `adata.var` to be used as selection: not the actual `n_top_genes` highly variable genes. Fix is on the way: I'll follow up here. **Your Example**; Reveals that `sc.pp.highly_variable_genes(ad_sub, n_top_genes = 1000, batch_key = ""Age"", subset = True)` suffers from this. . **Circumvent bug**; For now, I recommend not using `subset=True` if the cases above hold for you:; Rather, use . `adata_subset = adata[:, adata.var[""highly_variable""]]`. when subsetting: which is basically the ""subsetting afterwards"" strategy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3027#issuecomment-2090618325:559,variab,variable,559,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3027#issuecomment-2090618325,1,['variab'],['variable']
Modifiability,"Hey, this has been something that's been confusing me a bit when annotating my arguments. Since python is pretty polymorphic (until its not), I find it hard to capture the traits an object should have using types I'm familiar with. Some examples:. * If you need to provide a list of genes, this could be a finite (ordered?) iterable whose elements are coercible to the same type as `obs_names`. ; * An integer. Could be a numpy integer, could be a python integer. What's are the correct typings for these? Do I do a Union of everything I can think of that matches this? Is there a way to say: ""should behave right if I call `np.array` on it"" (limiting possible arguments types to pd.Series, list, tuple, np.array, dask array, and probably some others)?. I guess I'd like to so some information on best practices and common idioms in the contribution guide. I haven't seen too many scientific python packages use type annotations, so I'm not sure how set conventions are. If anyone has seen some good writing on type annotations for the scientific python stack, I'd love to take a look.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790:113,polymorphi,polymorphic,113,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441140790,1,['polymorphi'],['polymorphic']
Modifiability,"Hi !; To answer about how it is useful, we are using ICA in our lab to a dataset of more than 100k cells, with a lot of complexity, and the main advantage of ICA against PCA is that it helps us detecting small populations of cells. As these small populations are not accounting for a lot of variance within the dataset, using a treshold on PCs, we discarded the PCs that would allow the separate them.; Another advantage is that we do not make use of a selection of ""highly variable genes"" anymore, and use all genes expressed in more than 100 cells for the whole analysis... Doing the same and applying PCA gave us quite poor results.. . We made use of Seurat implementation.. and I tried fastICA from sklearn once but I couldn't obtain similar results... I have not looked thoroughly into seurat's code tough... . Hope it helps ! ; Best",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834:474,variab,variable,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-519089834,1,['variab'],['variable']
Modifiability,"Hi ,; In my case, the seurat object using the sceasy algorithm to transfer into anndata object for trajectory inference analysis.; The code lying below:; import numpy as np; import pandas as pd; import matplotlib.pyplot as pl; from matplotlib import rcParams; import scanpy as sc; sc.pp.recipe_zheng17(adata); sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20); sc.tl.draw_graph(adata); sc.pl.draw_graph(adata, color='paul15_clusters', legend_loc='on data'); The picture showing confused result posted below:; ![Uploading image.png…](). The object information:; >>> adata; AnnData object with n_obs × n_vars = 17885 × 999; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'RNA_snn_res.0.5', 'seurat_clusters', 'pANN_0.25_0.02_752', 'DF.classifications_0.25_0.02_752', 'percent.rp', 'pANN_0.25_0.02_826', 'DF.classifications_0.25_0.02_826', 'group', 'celltype', 'n_counts_all'; var: 'vst.mean', 'vst.variance', 'vst.variance.expected', 'vst.variance.standardized', 'vst.variable', 'n_counts', 'mean', 'std'; uns: 'seurat_clusters_colors', 'log1p', 'pca', 'neighbors', 'draw_graph'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'; varm: 'PCs'; obsp: 'distances', 'connectivities'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1880:1019,variab,variable,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1880,1,['variab'],['variable']
Modifiability,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:75,variab,variable,75,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:118,variab,variable,118,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD, I'm trying out embedding_density() using the latest scanpy version. . First of all, this is a wonderful feature - thank you!. Second, I was wondering if it would be possible to extend this and create a 'differential density' to visualize differences between two conditions? (possibly on a lower resolution grid?). btw. there is a typo on https://icb-scanpy.readthedocs-hosted.com/en/latest/index.html under Master: the name is switched to 'density_embedding()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575:190,extend,extend,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575,1,['extend'],['extend']
Modifiability,"Hi @LuckyMD, thanks for the recommendations we (@SharkieJones) will try lowering the number of variable genes (we ranked based on variance) and will look through the tutorials.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458246040:95,variab,variable,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458246040,1,['variab'],['variable']
Modifiability,"Hi @LuckyMD,. Many thanks for your comments. . The route via PCA followed by clustering & embedding (UMAP/tSNE) works perfectly fine for me. I have also got some interesting results from the analysis. Now, I want to try clustering cells with specific gene sets instead of the conventional dimensional reduction. Yes, I tried the following lines before:. ```; adata.obsm['X_geneset1`] = adata[:, ['gene1', 'gene2', 'gene3', 'gene4']].X; ```; It still says, KeyError: 'Indices ""[\'Ada\', \'Mustn1\', \'Mlc1\', \'Gfra\', \'Gm765\', \'Csrp2\', \'Socs2\', \'Dnajb9\']"" contain invalid observation/variables names/indices.'. All of these genes are present in my dataset. I am still trying to figure out why this is happening :/ ; Maybe, I will paste the short code snippet later. . P.S: Sorry for getting off the subject. Is there an alternative normalization step included apart from the log-normalization method? For example, TMM in edgeR & SCnorm- that uses quantile regression to calculate the dependence of read counts on sequencing depth for each gene (when count-depth relationship varies among genes).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552:592,variab,variables,592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-488001552,1,['variab'],['variables']
Modifiability,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:815,extend,extended,815,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['extend'],['extended']
Modifiability,"Hi @a-munoz-rojas,. I don't think there is a way to speed-up `scipy.stats.mannwhitney`, as it expects 1d vectors; not a matrix. Regarding ties, this is a simple multiplier. So should be easy to implement or use from `scipy.stats`. I have a matrix version of `scipy.stats.mannwhitney` and `scipy.stats.tiecorrect` which is almost a 1-to-1 rewrite. I can share it in case you are interested.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211:338,rewrite,rewrite,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528788211,1,['rewrite'],['rewrite']
Modifiability,"Hi @aditisk,. You can always make a dummy `.obs` variable for cluster membership. Something like this:; `adata.obs['cluster_dummy'] = adata.obs['louvain'] == adata.obs['louvain'].cat.categories[0]`. By iterating over the last index (currently at 0), you can create dummy variables to visualize via `sc.pl.umap(adata, 'cluster_dummy')`. Hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/513#issuecomment-469174695:49,variab,variable,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/513#issuecomment-469174695,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi @cornhundred,. While I can't speak for the intention of the authors of MNN, typically one would use 3000-6000 highly variable genes in scRNA-seq data analysis. That tends to cover the most important sources of variation. If you have a very deeply sequenced dataset from a sensitive scRNA-seq protocol (Smart-seq2/mcSCRB-seq?) with a lot of heterogeneity, you could make an argument for using more. Generally, 10,000 is a lot though. I would probably use fewer genes. The new `highly_variable_genes()` function does not subset the genes anymore, but instead creates a `.var['highly_variable']` column which stores a boolean variable indicating which genes are highly variable and which are not. You should be able to use this column to subset adata.var_names as an input to `sce.mnn_correct()` via the `var_index` and `var_subset` parameters. Using these inputs should not subset your `AnnData` object. Batch correction can create negative gene expression levels. People tend to deal with this differently. Some people force pre-batch-correction zeros to remain zero, others cast negative values to zero, and others again ignore it. I don't think there's a best approach to this. In the end you will probably get similar results in terms of embedding and trajectory inference. You just have to be careful how you interpret the gene expression values themselves. I have so far ignored it. By the way, I've written a bit about these topics in my best practices tutorial. The case study that goes with the manuscript (currently under review) is publicly available [here](https://github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946:120,variab,variable,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449#issuecomment-458072946,3,['variab'],['variable']
Modifiability,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```; ...; and (color is None or color in adata.obs.keys() or color in adata.var.index)):; File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__; hash(key); TypeError: unhashable type: 'list'; ```. - For components: the command was . ```; sc.pl.scatter(; adata=adata,; x='EKLF',; y='Cebpa',; color='EgrNab',; layers=('X', 'X', 'X'),; use_raw=False,; sort_order=True,; components='all',; projection='2d',; legend_loc='right margin',; legend_fontsize=1,; legend_fontweight='normal',; palette='viridis',; frameon=True,; right_margin=1.0,; size=1.0,; show=False,; save='.png'); ```; and the error:. ```; components = np.array(components).astype(int) - 1; ValueError: invalid literal for int() with base 10: 'all'; ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136:648,layers,layers,648,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311#issuecomment-431284136,1,['layers'],['layers']
Modifiability,"Hi @geovp!. Yes, I mean the original image that was supplied to SpaceRanger pipeline.; It doesn't have to be a TIFF image - in my experience slide scanners save; JPEG images internally, so there is no value in converting that to TIFF.; Also, it would be cool to use sc.pl.spatial for other technologies - say to; overlay single cell spatial over the microscopy image image. Nice, I was using this hacky way before (if I remember correctly I also; changed spot size in the respective slot) - so it does work. I am wondering if you could add support for a fullres slot with size factor; 1 and explain which variables need to be set for it to work in the tutorial. On Thu, Oct 1, 2020 at 8:32 PM giovp <notifications@github.com> wrote:. > Hi @vitkl <https://github.com/vitkl> ,; > by fullres you mean the tiff image yes? This is not supported for now; > unfortunately, but we are working toward some extensions to make this; > possible (cc @hspitzer <https://github.com/hspitzer> ).; > One hacky way to go about this for now could be to:; >; > - assign the tiff to the hires slot in; > adata.uns['spatial]['library_id']['images']['hires']; > - change the hires scalefactor value to 1 in the respective slot; > This should work. also for plotting the spots in the right size. Of; > course, this is also possible if you replace the ""lowres"" instead.; > Let me know what you think about it and if it works.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1436#issuecomment-702351783>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AFMFTV5FBT2DB4GKUIZUVNTSITKMBANCNFSM4R5XDYSQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732:605,variab,variables,605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-702607732,1,['variab'],['variables']
Modifiability,"Hi @gheimberg,. In your example you are not using a deepcopy to assign `adata.X` to `adata.layers['other']`. So when you log transform the data in the layer, it automatically log transforms the data in `adata.X` as well, as you just passed the reference. That being said, this is still a bug as even with a `adata.X.copy()` the warning is given.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535:91,layers,layers,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333#issuecomment-664944535,1,['layers'],['layers']
Modifiability,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:92,variab,variable,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938,2,['variab'],['variable']
Modifiability,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748:364,layers,layers,364,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650#issuecomment-779132748,2,['layers'],['layers']
Modifiability,"Hi @ivirshup ,; Thanks for your help.; Versions:; ```; In [1]: import numba; In [2]: numba.__version__; Out[2]: '0.45.0'; ```; I had to downgrade the original numba version in order to MNN_correct to work according to a Stackoverflow post.; Now I updated anndata through conda:; ```conda update anndata```; And ran this code (minus highly variable gene calculation):; ```; adataCombat = sc.read_h5ad(results_file); #Run combat:; # sc.pp.highly_variable_genes(adataCombat); sc.pp.pca(adataCombat, svd_solver='arpack'); sc.pp.combat(adataCombat, key='sample'); sc.pp.neighbors(adataCombat, n_pcs =50); ```; with even worse output:; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""_it_sol"" failed type inference due to: Cannot unify array(float64, 2d, C) and array(float64, 1d, C) for 'sum2', defined at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (311). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 311:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; g_new = (t2*n*g_hat + d_old*g_bar) / (t2*n + d_old); sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones((1, s_data.shape[1])); ^. [1] During: typing of assignment at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py (313). File ""anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py"", line 313:; def _it_sol(s_data, g_hat, d_hat, g_bar, t2, a, b, conv=0.0001) -> Tuple[float, float]:; <source elided>; sum2 = sum2 ** 2; sum2 = sum2.sum(axis=1); ^. @numba.jit; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:269: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled be",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656:339,variab,variable,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1164#issuecomment-614594656,1,['variab'],['variable']
Modifiability,"Hi @ivirshup ; I made some updates to PR #2055 . The column grouping argument was changed to a string/list argument 'col_groups'.; A few examples:; ```; pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); pbmc.obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:377,variab,variable,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,3,['variab'],['variable']
Modifiability,"Hi @marcellp,. The point of the tutorial is to easily become familiarized with Scanpy-based analysis of scRNA-seq data rather than to allow the exploration of a comprehensive dataset. Thus, the pbmc3k object is a reduced version of the one that can be downloaded from the 10X website to make everything run much faster. If you want to take a look at the full object, you would have to download the object from 10X and run the tutorial with that dataset (I believe it's called 2.7k PBMCs there). I'm not 100% sure how this object was generated, but I assume the number of genes were reduced to leave only the most highly variable genes in the dataset with sufficient levels of expression. This is often done in scRNA-seq analysis to reduce the number of features to calculate e.g., PCA-based embeddings for downstream analysis. Thus, it is not uncommon for some genes to not be taken into account when generating an embedding. If you want more background on scRNA-seq analysis in general, I would recommend [this introductory paper](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817:620,variab,variable,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665549817,1,['variab'],['variable']
Modifiability,"Hi Alex!; Before, I filtered gene with `min_mean` and `min_disp`, and left about 1300 genes for downstream analysis. Maybe the dataset is highly similar, so I reduce the gene number and choose the top 200 highly variable genes and it run without error. ; Thanks a lot,; Jiping",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221:212,variab,variable,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33#issuecomment-324831221,1,['variab'],['variable']
Modifiability,"Hi Alex, . Here is an interesting bug with scanpy. For developers, it is useful to be able to reload a previously imported module within the environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468:171,variab,variables,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468,1,['variab'],['variables']
Modifiability,"Hi Alma,. thanks for raising your thoughts here!. I’ll try to clarify the output a bit and tag @ivirshup here. `sc.pp.neighbors` produces two main results, which it indeed stores in the `ad.obsp`:. 1. A distance matrix in `adata.obsp['distances']`. This matrix has shape (n_obs, n_obs): for each observation, only `n_neighbors-1 `entries will be non-zero. The nearest neighbor of an observation, itself with distance 0, is discarded, hence the `-1`. It is probably what you have been thinking of in your description. 2. A connectivity graph in `adata.obsp['connectivity']`. This graph has shape (n_obs, flexible), where the flexible number of connections for each observation are determined during the UMAP algorithm. Hence if you’re interested in the distance matrix, `adata.obsp['distances']` would be what you’re looking for! Coming back to your code example, here the test should be a pass:; ```py; # Import packages. import scanpy as sc; import anndata as ad; import numpy as np. # set random seed; np.random.seed(42). # create dummy data; adata = ad.AnnData(shape=(1000,1)); adata.obsm['rep'] = np.random.random(size = (1000,2)). # get spatial connectivities; k = 10; sc.pp.neighbors(adata, n_neighbors=k, use_rep = 'rep', knn = True). # get and count connectivities for each cell; gr = adata.obsp['distances']; nn = (np.array(gr.todense()) > 0).sum(axis=1).flatten(). # check if neighbors are equal to k-1; np.testing.assert_equal(nn, k-1); ```. Might actually try to clarify this in documentation, small PR addressing this will follow soon. How does that sound to you? Please persist if you think I miss the point!. That being said, I think that the computation of the distance matrix and the connectivity graph are both correct.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2587#issuecomment-1691673182:603,flexible,flexible,603,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2587#issuecomment-1691673182,2,['flexible'],['flexible']
Modifiability,"Hi Dan, about 1.: I’m asking you what the semantic meaning is :smile: . about 2.: there are two functions called `dendrogram`, and they have compatible signatures. Each computed dendrogram can be plotted. So what I’m saying is that the plotting version hasn’t been adapted. Also an important question: in `tl.dendrogram`, we call `_choose_representation`, which will compute a PCA for the .obs axis. When specifying `axis='var'`, should it compute a PCA for the `var` axis instead?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869:265,adapt,adapted,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2771#issuecomment-1947927869,1,['adapt'],['adapted']
Modifiability,"Hi Dustin!. Thank you for providing a template. You can easily read that template in with two additional lines of code:; ```; import scanpy.api as sc; import pandas as pd; adata = sc.read_excel('./data/base_template.xlsx', sheet='expression').T; adata.obs = pd.read_excel('./data/base_template.xlsx', sheet_name='observations', index_col='observations'); adata.var = pd.read_excel('./data/base_template.xlsx', sheet_name='genes', index_col='gene_symbol'); ```; I'm a bit hesitant to put all of this into `sc.read_excel`... it would add 4 new parameters and many people might not have organized their excel files this way? Or am I wrong with the last assumption? If it's something very common todo, I can add the two other lines and extend `read_excel`. What do you think? Are the two further lines too much additional code?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/103#issuecomment-373161702:732,extend,extend,732,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/103#issuecomment-373161702,1,['extend'],['extend']
Modifiability,"Hi Fidel, ; Here is the pull request for vmin vmax in dotplot...; I am guessing that there could be similar issues with other plotting functions and other plotting keywords.; In general, it would be best to check plotting methods for plotting keywords and use what is provided in kwds by default rather than setting the variable without checking whether it was provided. ; Thanks for your groups great work!; Tim",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/390:320,variab,variable,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/390,1,['variab'],['variable']
Modifiability,Hi Gökcen: makes sense!. Hi Sidney: if I'm not completely mistaken: I don't think that the Jaccard metric makes sense at all for continuous ordinal variables. It would make sense if one had boolean gene expression or something like this... I guess this is the reason why you get a meaningless graph with it. I always only use euclidean distance. All other desired aspects of the metric are engineered in the preprocessing already.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207:148,variab,variables,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-398688207,1,['variab'],['variables']
Modifiability,"Hi I got this warning when I used sc.pl.scatter:; ```; .../scanpy/plotting/_anndata.py:311: DeprecationWarning: Use obs_vector instead of _get_obs_array, _get_obs_array will be removed in the future.; x_arr = adata._get_obs_array(x, use_raw=use_raw, layer=layers[0]); .../anndata/base.py:1618: FutureWarning: In a future version of AnnData, access to `.X` by passing `layer='X'` will be removed. Instead pass `layer=None`.; FutureWarning; ```. I would like to know if the plots generated with this warning are correct.; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/690:256,layers,layers,256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/690,1,['layers'],['layers']
Modifiability,"Hi Isaac, I've updated to v1.4.4 but I'm still getting this problem. I've finally produced a minimal test case:. ```; import scanpy as sc; sc.logging.print_versions(); #adata = sc.datasets.pbmc3k(); adata = sc.read(""orig/transpose_rsem_cell_by_gene.tsv.gz""); print(adata); adata = adata.T; print(adata); adata.raw = adata; print(adata); sc.pp.filter_cells(adata, min_genes=200); print(adata); adata = adata[adata.obs['n_genes'] < 5000, :]; print(adata); adata = adata[adata.obs['n_genes'] > 100, :]; print(adata); ```. output is:; ```. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 ; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; AnnData object with n_obs × n_vars = 60498 × 466 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; AnnData object with n_obs × n_vars = 466 × 60498 ; obs: 'n_genes'; View of AnnData object with n_obs × n_vars = 311 × 60498 ; obs: 'n_genes'; Traceback (most recent call last):; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/series.py"", line 977, in _get_values; return self._constructor(self._data.get_slice(indexer),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/managers.py"", line 1510, in get_slice; return self.__class__(self._block._slice(slobj),; File ""/cluster/home/max/miniconda3/envs/py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py"", line 268, in _slice; return self.values[slicer]; IndexError: boolean index did not match indexed array along dimension 0; dimension is 466 but corresponding boolean di",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235:783,Variab,Variable,783,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-516194235,3,['Variab'],['Variable']
Modifiability,"Hi Quentin,. When you plot a categorical variable for the first time, scanpy stores the colors for each category in adata.uns, that's why it is modifying your adata. For continuous variables (like your adata.X), it does not do that, hence there is no warning there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2315#issuecomment-1256967526,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi Samuele,. `covariates` argument refers to the additional covariates (biological or technical) that are used in the model fit. It's the `mod` parameter in the R function combat (https://www.rdocumentation.org/packages/sva/versions/3.20.0/topics/ComBat) and `X` in equation 2.1 in Johnson et al. 2007, https://academic.oup.com/biostatistics/article/8/1/118/252073. Since only the batch variable is ""regressed out"" from the gene expression, adding extra covariates changes the way batch effect coefficient is estimated. By the way, https://scanpy.discourse.group is a better place to ask questions and start such discussions :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/780#issuecomment-521669600:387,variab,variable,387,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521669600,1,['variab'],['variable']
Modifiability,"Hi Scanpy devs. Sorry, this isn't an enhancement request, just wan't sure where this fitted. . Just a quick one- when's the next Scanpy release (1.6.0?) scheduled for?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1365:37,enhance,enhancement,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1365,1,['enhance'],['enhancement']
Modifiability,"Hi Scott,. sure, I remember! :smile: For some reason, I forgot to mention you personally in the [release notes](http://scanpy.readthedocs.io/en/latest/#version-1-1-may-31-2018), is now fixed. Sorry about that! . You could add MAGIC as a preprocessing similar to DCA in the imputation section: http://scanpy.readthedocs.io/en/latest/api/index.html#preprocessing-pp. In terms of code, I would also adapt the conventions of DCA: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/dca.py. We had some discussions on how to do this best: https://github.com/theislab/scanpy/issues/142 and https://github.com/theislab/scanpy/pull/186. If you think you have better conventions, happy to adopt these. DCA is also not yet released... Best,; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798:396,adapt,adapt,396,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/187#issuecomment-402263798,1,['adapt'],['adapt']
Modifiability,"Hi Shamini,. Have you tried running the highly variable genes function on the non-log-transformed, non-normalised counts? You want to use raw counts, see the documentation:; `Expects logarithmized data, except when flavor='seurat_v3', in which count data is expected.`; The numbers in your count matrix are too large at some point in the hvg calculation, might be solved by passing it the data in the correct format!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2242#issuecomment-1256969218:47,variab,variable,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2242#issuecomment-1256969218,1,['variab'],['variable']
Modifiability,"Hi ShuhuaGao,. thanks for your input! Monocle 2 has many more options for preprocessing, that's right. I believe though that you should get along with the limited options of Scanpy for a robust pseudotime and branching inference using DPT; simply because DPT is very robust. Nonetheless I have to admit that I've not worked with an extensive number of data types. From this experience, my understanding is the following. * for RNA-Seq data, you should normalize and extract highly-variable genes. this is most simply done by using the procedure of cell ranger [`sc.pp.recipe_zheng17`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/recipes.py#L59-L78) (example [here](https://github.com/theislab/scanpy_usage/tree/master/170503_zheng17)) or, if you want more control, the Seurat workflow (example [here](https://github.com/theislab/scanpy_usage/tree/master/170505_seurat)); * for qPCR, a simple log-normalization ([sc.pp.log1p](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L280-L298)) should suffice (see example [here](https://github.com/theislab/scanpy_usage/tree/master/170501_moignard15)); you might though consider ""normalizing per cell / UMI correction"", one of the steps done in RNA-seq part ([`sc.pp.normalize_per_cell`](https://github.com/theislab/scanpy/blob/373dc325bdc24754dd658bc06b818987de6d568c/scanpy/preprocessing/simple.py#L405-L452)). Ask if you have further questions. 😄",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579:481,variab,variable,481,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312623579,1,['variab'],['variable']
Modifiability,"Hi Thank you for getting back to me. I updated it and now when I try to import scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:597,config,config,597,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,7,"['Config', 'config']","['Configuration', 'config']"
Modifiability,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:1017,flexible,flexible,1017,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,2,"['flexible', 'variab']","['flexible', 'variables']"
Modifiability,"Hi all,. Right now we have two layers in the scanpy API. The top layer consists of the major modules like `pp,pl,tl` as well as the smaller ones like `queries,get,datasets`. In addition, we have some useful functions directly under the scanpy package like `read/read_text/read_mtx` etc. It is obvious that the field is advancing and alternative/better ways to perform fundamental tasks in downstream analysis (e.g. normalization, DE tests, gene selection) are emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:31,layers,layers,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,2,"['extend', 'layers']","['extend', 'layers']"
Modifiability,"Hi all,. this is a re-implementation of the ComBat function in python for batch effect removal by Brent Pedersen at the University of Utah which I slightly modified to work with AnnData objects. I asked Brent for permission and he would be happy with us using this. Originally, the code was written in R for the SVA package:; Jeffrey T. Leek, W. Evan Johnson, Hilary S. Parker, Andrew E. Jaffe; and John D. Storey (). sva: Surrogate Variable Analysis. R package; version 3.4.0. The idea is taken from this paper:; Johnson WE, Rabinovic A, Li C (2007). Adjusting batch effects in microarray; expression data using Empirical Bayes methods. Biostatistics 8:118-127. . Originally, the method was developed to adjust for batch effects in microarray data, however, it is commonly applied to scRNA-seq data nowadays. The method fits linear models to the genes and pools statistical power by means of EB to estimate per gene correction factors. . I understand that @mbuttner also has an implementation of this - maybe we can combine and get the best of both approaches?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398:433,Variab,Variable,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398,1,['Variab'],['Variable']
Modifiability,"Hi authors,. First off, love scanpy. Big fan. . I was just wondering if you have considered including an option in `scanpy.tl.rank_genes_groups` to specify which variables to select for testing, allowing users to select a subset of variables which would or would not be considered in the statistical test. For context, I'm trying to test between groups of cells while ignoring ribosomal / mitochondrial genes, but retain them in the `.var` and `.X` objects for downstream analysis/visualisation. Making a temp object with these variables removed solely for stats testing partially works, but it's confounded by having to further apply the boolean slice to the `.raw` object as well. Thanks, K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1744:162,variab,variables,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1744,3,['variab'],['variables']
Modifiability,"Hi everyone, here is the way I extracted the top 500 most variable gees in seurat: ; ![Screen Shot 2023-12-22 at 11 58 39 AM](https://github.com/scverse/scanpy/assets/65792233/5b59b6d1-696e-4695-a7df-ad8f74d6412f)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897023:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1867897023,1,['variab'],['variable']
Modifiability,"Hi everyone,. Seeing how many new single cell and spatial tools are being developed in Python, and how we are increasingly using it in general and scanpy in particular, at saezlab we decided to re-implement our tools to estimate pathways and Transcription factor (TF) activity ([Dorothea](https://saezlab.github.io/dorothea/) and [Progeny](https://saezlab.github.io/progeny/)) in it. Here's a first draft in Python of our tools:; https://github.com/saezlab/dorothea-py; https://github.com/saezlab/progeny-py. Our tools take gene expression as input and generate matrices of TF and pathway activities. They can be understood as: ; 1) Prior-knowledge dimensionality reduction methods (`obsm`). Examples of usage:; 	* Used as input for NN; 	* Used as input for integration methods; 2) New data assays (`X`). Examples of usage:; 	* Plot feature activities in projections such as PCA or UMAP; 	* Plot feature activities in heat-maps, clustermaps, violin plots, etc; 	* Differences between groups can be modeled to find significant differences. Because of this duality, the integration of our tools into scanpy is not straightforward. If we store the activities in `obsm` they can be used as a dimensonality reduction embedding but then we lose acces to all the fantastic plotting functions based on `X`. Then if we add add our activities to `X`, they have a very different distribution than gene expression plus there would be an overlap of names between genes and TFs. A solution to this would be to have a separate `.layer` to store this matrices but layers must contain the same dimensions as `X`. Another workaround would be to store it in `.raw` but then we force the user to use remove its previous contents, plus it is used in some methods as default which could cause problems. . What would be a smart solution to integrate our tools in your universe?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1724:1548,layers,layers,1548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1724,1,['layers'],['layers']
Modifiability,"Hi everyone,. a while back, @giovp asked me to create a pull request that integrates analytical Pearson residuals in scanpy. We already discussed a bit with @LuckyMD and @ivirshup over at berenslab/umi-normalization#1 how to structure it, and now I made a version that should be ready for review. As discussed earlier, this pull request implements two core methods:; - `sc.pp.normalize_pearson_residuals()`, which applies the method to `adata.X`. Overall, the function is very similar in structure to `sc.pp.normalize_total()` (support for layers, inplace operation etc).; - `sc.pp.highly_variable_genes(flavor='pearson_residuals')`, which selects genes based on Pearson residual variance. The ""inner"" function `_highly_variable_pearson_residuals()` is structured similarly to `_highly_variable_seurat_v3()` (support for multiple batches, median ranks for tie breaking). It includes the `chunksize` argument to allow for memory-efficient computation of the residual variance. We discussed quite a lot how to implement a third function that would bundle gene selection, normalization by analytical residuals and PCA. This PR includes the two options that emerged at the end of that discussion, so now we have to choose ;). - `sc.pp.recipe_pearson_residuals()` which does HVG selection and normalization both via Pearson residuals prior to PCA; - `sc.pp.normalize_pearson_residuals_pca()` which applies any HVG selection if the user previously added one to the `adata` object, and then normalizes via Pearson residuals and does PCA. Both functions retain the raw input counts as `adata.X` and add fields for PCA/Normalization/HVG selection results (or return them) as applicable, most importantly the `X_pca` in `adata.obsm['pearson_residuals_X_pca']`. I hope this addresses some of the issues we discussed over at the other repo in a scanpy-y way. Let me know what you think and where you think improvements are needed!. Cheers, Jan.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715:540,layers,layers,540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715,1,['layers'],['layers']
Modifiability,"Hi guys,; I am trying to open a .loom file from : http://scope.aertslab.org/#/53d2bb24-9335-48d4-b874-eab05dd8c690/Aerts_Fly_AdultBrain_Filtered_57k.loom/gene. I can open the .loom file by:. ```py; loom_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. However i would like to open it with scanpy by:. ```py; loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. and i get the following error:. ```pytb; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\L",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:921,layers,layers,921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,2,['layers'],['layers']
Modifiability,"Hi there!. I am having a similar issue when trying to install Scanpy using conda in Ubuntu. I have uninstalled and installed Anaconda so it is the newest version and still amb getting the same error. Pip install though works well. I was wondering if you could help me with the issue as it is vry interesting for me to install it with conda. The output when installing is the following one:. > Collecting package metadata (current_repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; > Collecting package metadata (repodata.json): done; > Solving environment: failed with initial frozen solve. Retrying with flexible solve.; > Solving environment: - ; > Found conflicts! Looking for incompatible packages.; > This can take several minutes. Press CTRL-C to abort.; > failed ; > ; > UnsatisfiableError: The following specifications were found to be incompatible with each other:; > ; > Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:; > ; > - feature:/linux-64::__glibc==2.31=0; > - feature:|@/linux-64::__glibc==2.31=0; > ; > Your installed version is: 2.31",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859:523,flexible,flexible,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1298#issuecomment-1008789859,2,['flexible'],['flexible']
Modifiability,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:1638,variab,variable,1638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['variab'],['variable']
Modifiability,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file; ; Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:480,variab,variable,480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,4,['variab'],"['variable', 'variables-axis']"
Modifiability,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:270,adapt,adapt,270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,1,['adapt'],['adapt']
Modifiability,"Hi! We didn't look at the method yet, but superficially:. - uppercase letters are only allowed in CONSTANTS and ClassNames, not module names, parameter names or function names; - preprocessing goes into sce.pp. This way there's way less keyword arguments and no need to prefix them. If the preprocessed data isn't useful for other applications, you can put it into .layers; - Scanpy functions should either mutate or return something, depending on the value of the `inplace` parameter",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337:366,layers,layers,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903#issuecomment-555603337,1,['layers'],['layers']
Modifiability,"Hi!. It looks like you have too many 0 count genes in your dataset. I would filter genes and cells before calculating highly variable genes. In case you're interested, I've been working on a tutorial for single-cell RNA-seq analysis. It's available [here](www.github.com/theislab/single-cell-tutorial)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/509#issuecomment-468852316:125,variab,variable,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/509#issuecomment-468852316,1,['variab'],['variable']
Modifiability,"Hi!. The colors of a categorical variable `'louvain'` are stored in `.uns['louvain_colors']`. You can directly modify that. The `palette` keyword can be used to initialize the field in `.uns`. For instance using `sc.pl.tsne(adata, color='louvain', palette=sc.pl.palletes.vega_20)`. But you're right, it should not only be used for initialization but also overwrite an existing color annotation. I'll fix this in version 1.1. Several other plotting flaws will be fixed in there, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-389749721:33,variab,variable,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-389749721,1,['variab'],['variable']
Modifiability,"Hi, . I've been getting the same error when trying to use `sc.pp.normalize_total` after `sc.pp.downsample_counts.` Normalize total returns a CSR sparse matrix of type `<class 'numpy.int64'>`, which then causes `sc.pp.normalize_total` to error. Not sure where the correct `dtype` should take place.; 	; ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc.X = pbmc.raw.X; sc.pp.downsample_counts(pbmc, counts_per_cell=500); sc.pp.normalize_total(pbmc, target_sum=1e4); ```. Here's the traceback:. ```pytb; Normalizing counts per cell. ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-136-3305b6c650f4> in <module>; 2 pbmc.X = pbmc.raw.X; 3 sc.pp.downsample_counts(pbmc, counts_per_cell=500); ----> 4 sc.pp.normalize_total(pbmc, target_sum=1e4). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layers, layer_norm, inplace); 166 adata.obs[key_added] = counts_per_cell; 167 if hasattr(adata.X, '__itruediv__'):; --> 168 _normalize_data(adata.X, counts_per_cell, target_sum); 169 else:; 170 adata.X = _normalize_data(adata.X, counts_per_cell, target_sum, copy=True). ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/scanpy/preprocessing/_normalization.py in _normalize_data(X, counts, after, copy); 14 after = np.median(counts[counts>0]) if after is None else after; 15 counts += (counts == 0); ---> 16 counts /= after; 17 if issparse(X):; 18 sparsefuncs.inplace_row_scale(X, 1/counts). TypeError: ufunc 'true_divide' output (typecode 'd') could not be coerced to provided output parameter (typecode 'l') according to the casting rule ''same_kind''; ```. ```; >>> pbmc.X; <700x765 sparse matrix of type '<class 'numpy.int64'>'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-538776417:1013,layers,layers,1013,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-538776417,1,['layers'],['layers']
Modifiability,"Hi, . Thanks for developing a good tool for analyzing scRNAseq data. In the process of learning scRNAseq analysis with scanpy I have come across a few places in the documentation that left me a bit confused. . The most confusing is that I could not find a description for the variable `n_genes_by_counts` calculated by the function scanpy.pp.calculate_qc_metrics and mentioned in the tutorial https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html. . I asked a question about this in [the discourse](https://scanpy.discourse.group/t/clarification-of-qc-metrics/295) and was asked to open a github issue. . I guess an explanation for the docs could be something like `n_genes_by_counts: The number of genes with at least 1 count in a cell. Calculated for all cells.` . Since I am writing this I might add that I did not understand exactly how the normalization function [scanpy.pp.normalize_total](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_total.html) works before I stumbled upon the description of the deprecated function with the same purpose: [scanpy.pp.normalize_per_cell](https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.normalize_per_cell.html) . The deprecated function has a helpful line saying: `Normalize each cell by total counts over all genes, so that every cell has the same total count after normalization.` which at least helped me clear things up. Maybe this line should be added to the new version of the function?. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434:276,variab,variable,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434,1,['variab'],['variable']
Modifiability,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:264,variab,variable,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695,2,['variab'],['variable']
Modifiability,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:505,variab,variable,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"Hi, @andrea-tango ; About the second issue - you dont need pca; you can do something like. ```; # project reference adata to latent dimensions with your autoencoder; adata_ref.obsm['X_latent'] = autoencoder.to_latent(adata_ref.X); # use your latent variables to calculate neighbors; sc.pp.neighbors(adata_ref, use_rep='X_latent'); sc.tl.umap(adata_ref); # project your new adata to latent dimensions with your autoencoder; adata_new.obsm['X_latent'] = autoencoder.to_latent(adata_new.X); sc.tl.ingest(adata_new, adata_ref, embedding_method='umap'); ```. About the first, yes, ingest needs vars in the same order. The ordering thing you describe is definitely not the issue with ingest.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540:249,variab,variables,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1128#issuecomment-603848540,1,['variab'],['variables']
Modifiability,"Hi, @falexwolf ; Do you have any specific things in mind for `rank_genes_groups` refactoring? What should be done?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723:81,refactor,refactoring,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723,1,['refactor'],['refactoring']
Modifiability,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:203,variab,variable,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646,2,['variab'],['variable']
Modifiability,"Hi, I have a Seruat processed dataset, of which I wanted to use scVI for integration. I stored the raw count and cell information then assembled them in scanpy as anndata via method mentioned: `https://smorabit.github.io/tutorials/8_velocyto/`. . So I could get batch HVG function to work without specifying flavor, however, I couldn't get it to work with specifying `flavor=""seurat_v3""`, I wonder how important it is to set this parameter and why does it not work for me?. Thanks a lot!. ```; adata; AnnData object with n_obs × n_vars = 73998 × 13639; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'barcode', 'Celltype2', 'Clusters', 'Sample'; uns: 'log1p'; layers: 'counts'. for i in adatas:; i.layers['counts'] = i.X; adata = ad.concat(adatas); adata.obs_names_make_unique; sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; flavor=""seurat_v3"",; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In [197], line 1; ----> 1 sc.pp.highly_variable_genes(; 2 adata_new,; 3 flavor=""seurat_v3"",; 4 layer=""counts"",; 5 batch_key=""Sample"",; 6 subset=True; 7 ); 8 adata_new. File ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py:422, in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 416 raise ValueError(; 417 '`pp.highly_variable_genes` expects an `AnnData` argument, '; 418 'pass `inplace=False` if you want to return a `pd.DataFrame`.'; 419 ); 421 if flavor == 'seurat_v3':; --> 422 return _highly_variable_genes_seurat_v3(; 423 adata,; 424 layer=layer,; 425 n_top_genes=n_top_genes,; 426 batch_key=batch_key,; 427 check_values=check_values,; 428 span=span,; 429 subset=subset,; 430 inplace=inplace,; 431 ); 433 if batch_key is None:; 434 df = _highly_variable_genes_single_batch(; 435 adata,; 436 layer=layer,; (...); 443",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2427:662,layers,layers,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2427,2,['layers'],['layers']
Modifiability,"Hi, I was doing a dataset integration on quite some datasets. . ```py; adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7] #each is an adata object from scRNA-seq. for i in adatas:; i.layers['counts'] = i.X. adatas = [AT1,AT2,AT3,AT4,AT5,AT6,AT7]. adata = ad.concat(adatas); adata.obs_names_make_unique. sc.pp.log1p(adata); sc.pp.highly_variable_genes(; adata,; layer=""logcounts"",; batch_key=""Sample"",; subset=True; ). scvi.model.SCVI.setup_anndata(adata, layer=""counts"", batch_key=""Sample""). vae = scvi.model.SCVI(adata, n_hidden=256); vae.train(); adata.obsm[""X_scVI""] = vae.get_latent_representation(); sc.pp.neighbors(adata, use_rep=""X_scVI""); from scvi.model.utils import mde; import pymde; adata.obsm[""X_mde""] = mde(adata.obsm[""X_scVI""]); adata.obsm[""X_normalized_scVI""] = vae.get_normalized_expression(); adata.write_h5ad('Integrated.h5ad'); ```. So I did use make obs names unique after ad.concat(adatas). However, after I was finished with the integration and moving on to write_h5ad, it returns the following errors and tells me they can't write my h5ad cuz I have duplicated rows:. ```pytb; Feb 26 11:20:39 PM: Your dataset appears to contain duplicated items (rows); when embedding, you should typically have unique items.; Feb 26 11:20:39 PM: The following items have duplicates [60449 60452 60455 ... 70783 70784 70785]; Traceback (most recent call last):; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/utils.py"", line 214, in func_wrapper; return func(elem, key, val, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 171, in write_elem; _REGISTRY.get_writer(dest_type, (t, elem.dtype.kind), modifiers)(; File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/registry.py"", line 24, in wrapper; result = func(g, k, *args, **kwargs); File ""/home/joyzheng/.local/lib/python3.8/site-packages/anndata/_io/specs/methods.py"", line 346, in write_vlen_string_array; f.create_dataset(k, data=elem.astype(st",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2432:171,layers,layers,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2432,1,['layers'],['layers']
Modifiability,"Hi, everything is OK with the benchmarks, `regress_out` would fail if called with variables that doesn’t exist. The reason these are named differently is here: https://github.com/scverse/scanpy/blob/ad657edfb52e9957b9a93b3a16fc8a87852f3f09/benchmarks/benchmarks/_utils.py#L27-L31. I did that to be able to run benchmarks benchmarks on multiple data sets with the same code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2185859889:82,variab,variables,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2185859889,1,['variab'],['variables']
Modifiability,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False); sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab10"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.obs['louvain'], cmap=""tab20b"", s=0.1); ```; ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```; fig, ax = plt.subplots(2,2,figsize=(12,8)); sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False); sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False); ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1); ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],; c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",; s=0.1, vmin=-0.6, vmax=3.5); ```; ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:; scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889:272,variab,variables,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-430385889,1,['variab'],['variables']
Modifiability,"Hi, thanks again for your interest in GLM-PCA. We welcome its inclusion in scanpy, but some caveats are that it is about 10x slower than PCA and we are still working to improve its numerical stability and ability to handle sparse data matrices. . With that in mind, we have put together an implementation of [Pearson and deviance residuals](https://github.com/kstreet13/scry/blob/master/R/nullResiduals.R) as an approximation to GLM-PCA via the [scry R package](https://github.com/kstreet13/scry). These residuals, based on binomial and poisson approximation to multinomial, can be computed in closed form so they are computationally as fast as log-transforming. The sctransform method uses a negative binomial likelihood which doesn't have a closed form solution and is more complicated to implement (although we do recomment it from a statistical validity standpoint). . In addition to the null residuals, the scry package has an implementation of [feature selection via deviance](https://github.com/kstreet13/scry/blob/master/R/featureSelection.R), which may also be of interest as an alternative to highly variable genes. This is also a closed form computation. Both the feature selection and null residuals functions allow adjusting for categorical batch labels. I do hope to implement both of these in python eventually but it's pretty far down my to-do list. Given the functions are fairly simple, I welcome anyone to go ahead and copy them into python if they find it potentially useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190:1110,variab,variable,1110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-593125190,1,['variab'],['variable']
Modifiability,"Hi, the documentation describes running MNN batch correction on a set of highly variable genes - how many genes are recommended? . * I tried running with the top 10,000 variable genes. Is this a good number. * Also, does selecting for these highly variable genes effectively permanently filter the dataset for only these genes?. * I noticed that the minimum gene expression level for some genes was less than zero after batch correction, is this expected?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/449:80,variab,variable,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/449,3,['variab'],['variable']
Modifiability,"Hi, the expression matrix I exported from adata.write only have the top variable genes. Is there a way to output the raw matrix including all genes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-497746073:72,variab,variable,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-497746073,1,['variab'],['variable']
Modifiability,"Hi, this is a PR with some changes to Hashsolo, mainly the docstring:; - Rewrite some param descriptions for brevity + clarity; - Explicitly list the obs keys being added to adata; - Copy input adata if inplace=False",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2429:73,Rewrite,Rewrite,73,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2429,1,['Rewrite'],['Rewrite']
Modifiability,"Hi, this issue tracker is better suited for bugs or enhancement request. Please ask e.g. here: https://discourse.scverse.org/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3080#issuecomment-2154471120:52,enhance,enhancement,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3080#issuecomment-2154471120,1,['enhance'],['enhancement']
Modifiability,"Hi, you can also do this directly; `adata.obsm[""mylayer_pca""] = sc.tl.pca(adata.layers[""mylayer""])`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567:80,layers,layers,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654741567,1,['layers'],['layers']
Modifiability,"Hi,. I am testing `sc.pl.stacked_violin` and I had an issue with running it on an AnnData variable that is imported from a `h5ad` file. I explain. If I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. No problem, but if I run:. ```; >>> import scanpy.api as sc; >>> adata = sc.datasets.krumsiek11(); >>> adata.write('anndata.h5ad'); >>> adata = sc.read_h5ad('anndata.h5ad'); >>> sc.pl.stacked_violin(adata, adata.var_names, 'cell_type', use_raw=False, color='blue', show=False); ```. then I got the error:. ```; Traceback (most recent call last):; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/anndata.py"", line 896, in stacked_violin; orient='vertical', scale=scale, ax=ax, **kwds); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 2387, in violinplot; color, palette, saturation); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 562, in __init__; self.establish_variables(x, y, hue, data, orient, order, hue_order); File ""/miniconda3/envs/scanpy/lib/python3.6/site-packages/seaborn/categorical.py"", line 155, in establish_variables; raise ValueError(err); ValueError: Could not interpret input 'variable'; ```. I tested it with versions 1.3.1 and 1.3.2, installed with bioconda. Any idea?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/318:90,variab,variable,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318,2,['variab'],['variable']
Modifiability,"Hi,. I got negative values after run score_genes with log1P or counts layer like below:; using layers['counts']:; ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:; ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3297:95,layers,layers,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297,2,['layers'],['layers']
Modifiability,"Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me?. Thanks,; Jphe. ```py; adata_raw = adata.copy(); df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None); genes = list(df[0]); genes = [k for k in genes if k in adata.var.index ]; adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20); sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata; ```; ```; AnnData object with n_obs × n_vars = 53165 × 1080 ; obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'; var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap'; varm: 'PCs'; layers: 'counts'; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/817:1284,layers,layers,1284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817,1,['layers'],['layers']
Modifiability,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667#issuecomment-497027543:185,variab,variable,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667#issuecomment-497027543,2,['variab'],['variable']
Modifiability,"Hi,. Just wanted to start the PR. Passes the tests except one. Also need to deal with solver names since they don't correspond to anything dask uses. Also refactored where the DaskArray mock class is. Pinging @ivirshup",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2563:155,refactor,refactored,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2563,1,['refactor'],['refactored']
Modifiability,"Hi,. Since the update I get this TypeError when running `sc.pp.normalize_total`. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); sc.pp.normalize_total(adata, target_sum=1e4); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Input In [4], in <cell line: 3>(); 1 import scanpy as sc; 2 adata = sc.datasets.pbmc3k(); ----> 3 sc.pp.normalize_total(adata, target_sum=1e4). File ~/my-conda-envs/sc2022-multiomics/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:200, in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 197 if key_added is not None:; 198 adata.obs[key_added] = counts_per_cell; 199 _set_obs_rep(; --> 200 adata, _normalize_data(X, counts_per_cell, target_sum), layer=layer; 201 ); 202 else:; 203 # not recarray because need to support sparse; 204 dat = dict(; 205 X=_normalize_data(X, counts_per_cell, target_sum, copy=True),; 206 norm_factor=counts_per_cell,; 207 ). File ~/my-conda-envs/sc2022-multiomics/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:25, in _normalize_data(X, counts, after, copy); 23 if issubclass(X.dtype.type, (int, np.integer)):; 24 X = X.astype(np.float32) # TODO: Check if float64 should be used; ---> 25 if isinstance(counts, DaskArray):; 26 counts_greater_than_zero = counts[counts > 0].compute_chunk_sizes(); 27 else:. TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union; ```. I've checked that obs_names, var_names, obs columns names are all unique. Any clue how to solve?. Thanks!. #### Versions. <details>. -----; anndata 0.7.8; scanpy 1.9.0; -----; PIL 9.1.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; entrypoints 0.4; executing 0.8.3; google NA; h5py 3.6.0; hypergeom_ufunc NA; ipykernel 6.12.1; jedi 0.18.1; job",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2210:677,layers,layers,677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2210,1,['layers'],['layers']
Modifiability,"Hi,. The issues I was mostly running into were that when saving the anndata; variable as a h5ad file, 'pheno_jaccard_ig' was not compatible with this; action. So, I had to either remove pheno_jaccard_ig from the anndata object; and then save it as h5ad or convert it to a sparse matrix. This also; happened with a few other functions I tried on the anndata object, and I; kept getting the error ""this function is not compatible with COO matrix; format"", always talking about pheno_jaccard_ig. Therefore, since a sparse; matrix object does not have any problems with the functions I was running; on adata, changing pheno_jaccard_ig to a sparse matrix from the start makes; sense to circumvent any of those issues I was getting before. I hope this makes sense.; Thank you,; Deena Shefter. On Wed, Jul 27, 2022 at 6:10 PM Lukas Heumos ***@***.***>; wrote:. > Hi,; >; > could you please provide more details? What issues did you run into?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/scverse/scanpy/pull/2295#issuecomment-1197424392>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AMILUOEK7J64GU3YOR7DB53VWGXULANCNFSM534YT5ZA>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2295#issuecomment-1274299180,1,['variab'],['variable']
Modifiability,"Hi,. This is for #1876 ; I added a new variable `groupby_expand` to dotplot and baseplot to allow the function using two variables from groupby as x and y axis. ; Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi,. To make a pca plot you need the functions `scn.tl.pca(adata_needed)` and `scn.pl.pca(adata_needed)`. If you would like to visualize your CD4+ and CD8+ cells on this plot, you need to store a variable in `adata_needed.obs` which contains the information which cell is CD4+ and which cell is CD8+. For example:; ```; adata_needed.obs['cell_type'] = my_list_with_cd_labels; scn.tl.pca(adata_needed); scn.pl.pac(adata_needed, color='cell_type'); ```. Here the variable `my_list_with_cd_labels` should look someting like this:; `my_list_with_cd_labels = ['CD8+', 'CD4+', 'CD4+', 'CD4+', 'CD4+', 'CD8+', 'CD8+', ..., 'CD8+', 'CD4+']`; Where the length of the list is equal to the number of cells you have in `adata_needed` and the order is the same as well. It may be helpful to consult this tutorial as well: https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb. I hope that helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112:196,variab,variable,196,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383#issuecomment-443276112,2,['variab'],['variable']
Modifiability,"Hi,. Using Seurat, in their variable gene function I've had some success using the `equal_frequency` option, where each bin contains an equal number of genes. Would it possible to implement this option in scanpy? . If you'd like I could submit a PR to implement this feature. I think it could be as simple as using `pd.qcut` instead of `pd.cut` or you could use a similar style as in the `cell_ranger` flavor with `pd.cut(df['mean'], np.r_[-np.inf,; np.percentile(df['mean'], np.arange(10, 105, 5)), np.inf])`. I don't know how useful it would be, but I could also add the option to have more bins in the `cell_ranger` flavor by replacing `np.arange(10,105,5)` with `np.linspace(10, 100, n_bins - 1)`. Best,; David",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/415:28,variab,variable,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/415,1,['variab'],['variable']
Modifiability,"Hi,. We get this error without swapping axes:. The code is ; ```; genes = [""DES"", ""CD34"", ""COL1A1""]; sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1"", ); ```; The error is ; ```; IndexError Traceback (most recent call last); <ipython-input-35-8f09494e5255> in <module>; ----> 1 sc.pl.stacked_violin(adata, genes, groupby = ""leiden_0.1""). ~/anaconda3/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 929 axs_list.append(ax); 930 ax = sns.violinplot('variable', y='value', data=df, inner=None, order=order,; --> 931 orient='vertical', scale=scale, ax=ax, color=row_colors[idx], **kwds); 932 ; 933 if stripplot:. IndexError: list index out of range; ```. However, I would still consider the addition because in many cases where the amount of genes is considerable, if the user wants the genes to be in the rows, `swap_axes = True` should be necessary, and they would not be able to color the violins according to the clusters of cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/465#issuecomment-461450618:710,variab,variable,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465#issuecomment-461450618,1,['variab'],['variable']
Modifiability,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```; adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]; sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'); ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464:40,variab,variable,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464,2,['variab'],['variable']
Modifiability,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:68,variab,variable,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065,1,['variab'],['variable']
Modifiability,"Hi,. thanks for you interest in scanpy!. Does this issue still persist for you?; If yes, is it possible to extend your example so that I can test it too, to see what might cause the computation to fail?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2472#issuecomment-1718993973:107,extend,extend,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2472#issuecomment-1718993973,1,['extend'],['extend']
Modifiability,"Hi,; I couldn't import scanpy due to an error: DLL load failed. I have checked pre-existing issues, but all of them seem to be an h5py issue. My error report seems different from them.; ```; >>> import scanpy as sc; D:\Anaconda\lib\site-packages\dask\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.; data = yaml.load(f.read()) or {}; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\__init__.py"", line 36, in <module>; from . import tools as tl; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\__init__.py"", line 17, in <module>; from ._sim import sim; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\readwrite.py"", line 10, in <module>; import tables; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found. >>> print(sys.version); 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468:251,config,config,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468,1,['config'],['config']
Modifiability,"Hi,; I have two levels in my ```groupby``` variable and was trying to find the differential expression genes between the two levels using the wilcoxon rank sum test. The following two commands will give different pvals and slightly different logfoldchanges; ```; sc.tl.rank_genes_groups(adata, groupby='status', groups=['ALS'], reference='ctrl', n_genes=100000, method='wilcoxon', use_raw=False); sc.tl.rank_genes_groups(adata, groupby='status', groups=['ctrl'], reference='ALS', n_genes=100000, method='wilcoxon', use_raw=False); ```; ```; 	gene	logfoldchanges_ALS_ctrl	pvals_ALS_ctrl	logfoldchanges_ctrl_ALS	pvals_ctrl_ALS; 0	SLC11A1	2.9489155	5.91E-75	-2.9489155	2.08E-73; 1	NEAT1	1.1250153	5.11E-66	-1.1250151	6.82E-64; 2	FKBP5	2.7334108	8.94E-47	-2.7334108	1.78E-45; 3	SPP1	2.1242297	2.27E-42	-2.1242297	2.69E-41; 4	FCGR3A	2.6661332	6.95E-40	-2.6661332	5.37E-39; 5	HAMP	5.394592	1.27E-37	-5.394592	2.27E-36; 6	CD163	3.0886266	9.11E-36	-3.0886264	1.71E-34; 7	RASSF4	2.3211384	2.83E-34	-2.3211384	3.74E-33; 8	DSE	2.8529236	7.43E-33	-2.8529236	7.86E-32; 9	MAFB	2.7013724	3.67E-32	-2.7013724	6.43E-31; 10	DENND3	1.4753485	5.13E-29	-1.4753484	1.19E-27; 11	APOE	1.4111803	1.12E-28	-1.4111804	9.04E-28; 12	C1QB	1.5169998	3.53E-27	-1.5169998	1.68E-25; 13	C3	1.3675922	1.05E-25	-1.3675922	2.62E-25; ```; Am I not doing it right, or because of the tie issue mentioned here?; #698 ; Thanks for your help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/754:43,variab,variable,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/754,1,['variab'],['variable']
Modifiability,"Hi,; I'm attempting to run scvelo on my scanpy processed 10x data.; Initially I provided the .h5ad file from scanpy as the input file for scvelo but ran into error KeyError: 'unspliced', I'm assuming unspliced data can't be found and also that ""adata.layers"" doesn't exist in my scanpy processed .h5ad file.; I also attempted providing the unprocessed data directly into scvelo but again came up with the error KeyError: 'unspliced'.; I've attempted adata.layers.keys() which returns odict_keys([]) and scv.pp.show_proportions(adata) which returns Abundance of []: []. (Currently running version 1.3.2 of scanpy and 0.1.11 of scvelo).; My adata file doesn't seem to contain adata.layers information, is there a way to add this information via scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/342:251,layers,layers,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342,3,['layers'],['layers']
Modifiability,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645:1254,variab,variables,1254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645,1,['variab'],['variables']
Modifiability,"Hi,; Is it necessary to use only high variable genes for the downstream analysis ?; If an examperiment includes many batches, then each batch will give a different set of high variable genes, how to determine the shared high variable genes (intersection or union) when integrating the batches ? Does scany have any fucntion to get the shared genes ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:38,variab,variable,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,3,['variab'],['variable']
Modifiability,"Hi,; Problem solved! Thanks a lot.; I am new to scanpy and now find it flexible and well designed. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/689#issuecomment-502412783:71,flexible,flexible,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689#issuecomment-502412783,1,['flexible'],['flexible']
Modifiability,"Hi,; Scanpy detect high variable genes with normalized (but not logarithmized) data (refer to Clustering 3k PBMCs following a Seurat Tutorial), while Seurat do this by first normalize the raw data, then logarithmize the data and finally detect high variable genes, which one is better ? or both of them works well ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/188:24,variab,variable,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188,2,['variab'],['variable']
Modifiability,"Hi,; Thank you for your amazing package!. I ran ""sc.tl.rank_genes_groups"" on Jupyter Notebook, and when I did: . `pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names', 'pvals_adj', 'logfoldchanges']})`. I was only able to see 0.0 for p-values and adjusted p-values for all of the 2,000 highly variable genes, while logfoldchanges showed 6 decimal places like 1.816276. . The version of Scanpy that I am using is 1.7.2, and I was wondering if there was a way to see more decimal places for p-values and adjusted p-values, like in the form of 3.642456e-222 in your tutorial. If this is not a formatting problem, do you think the Wilcoxon test gave me 0.0 for all the highly variable genes? ; Based on sc.pl.rank_genes_groups, the scores for 25 genes per cluster are pretty high though (most of them are in a range of 100-200). I am pretty new to python and scanpy, so your advice will be greatly helpful.; Thank you!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1869:339,variab,variable,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1869,2,['variab'],['variable']
Modifiability,"Hi. This is unlikely to be a scanpy issue. You probably don’t have enough memory or there’s some problem with your Jupyter configuration. But in any case, we need more information to tell which one it is. Please share the logs that `jupyter lab` created, especially any stack traces around “kernel died, restarting”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889:123,config,configuration,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675#issuecomment-1750301889,1,['config'],['configuration']
Modifiability,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,1,['variab'],['variable']
Modifiability,Highly variable genes for sparse dataset in backed mode,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764,1,['variab'],['variable']
Modifiability,"Hm, I adapted your reproducer to use scanpy 1.10.3’s code and it doesn’t seem to be an issue: https://gist.github.com/flying-sheep/b2ae449ab70a9358e07a82f284de5dca#file-score_genes_diagnostics_tests2-ipynb. I’m going to assume that this is fixed in 1.10.3. If you can reproduce it with 1.10.3, we can reopen it!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983:6,adapt,adapted,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3167#issuecomment-2414177983,1,['adapt'],['adapted']
Modifiability,"Hm, `n_counts` and `total_counts` is of course non-sense. Scanpy tries to adapt the `n_...` convention in scikit-learn and statsmodels for anything that is a number. We'll soon expose the quantile normalization preprocessing function to the users in a proper way. Then we'll have 95%-quantile counts vs. total counts. Then it starts making sense to use the notion `total_`. So, in the light of that, we could think about moving there. Yes, we'd deprecate old names and output a warning, too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327:74,adapt,adapt,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-435731327,1,['adapt'],['adapt']
Modifiability,"Hmm ok, I mean that'd work too, I guess this is more like the difference between programmer's view and practitioner's view 😄 . This is definitely more flexible and cleaner in terms of API design, but if we imagine a notebook with e.g. five sc.pl.dotplot calls, wouldn't it be painful to write this for each call (and memorize)? There are two reasons I suggested `set_figure_params` . 1. It already has all the tricks to make figures more publication-ready e.g. rasterization, Arial font etc. ; 2. Italicized gene names is likely going to be a session-wide (or project-wide) decision, where one would use it in all figures not just in one, which brings us to the importance of conciseness. . Last suggestion is like `theme(axis_text_x=element_text(angle=90, hjust=1))` kind of argument but `rotation=90` also works 😃 Anyway, my two cents, happy with any option that achieves this, the community will appreciate it I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842:151,flexible,flexible,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-873024842,1,['flexible'],['flexible']
Modifiability,"Hmm, doesn’t seem to work with `functools.partial`. It works if I do almost the same, but with a lambda:. ```py; import scanpy as sc; from functools import wraps, partial. pca = wraps(sc.pl.scatter)(lambda *args, **kw: sc.pl.scatter(*args, basis=""pca"", **kw)); ```. But I think the custom solution above is better anyway! `wraps` is if you really want to pose as the wrapped function, while we only want to inherit its signature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934:407,inherit,inherit,407,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/535#issuecomment-474255934,1,['inherit'],['inherit']
Modifiability,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638:498,variab,variables,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588#issuecomment-760249638,1,['variab'],['variables']
Modifiability,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:57,Extend,Extending,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,1,['Extend'],['Extending']
Modifiability,"Hopefully last update on this PR. What I did:; - I noticed a regression on the method `rank_genes_groups_violin`, therefore I reverted back the code to the original one and I added an additional method `genes_groups_violin` which should be used if we want to pass the list of genes directly to the violin plot. The code is just a POC, but maybe it can be integrated; - Within the same method `rank_genes_groups_violin`, I found a bug: the ax variable was overwritten for each group (I don't know if it gave you error before). In my case, all the plots were merged into a single figure, every one on top of the previous ones; - Additionally, the parameters `gene_symbols` and `computed_distribution` were not defined within the method `rank_genes_groups_violin`. I added a default parameter (`None`) for `gene_symbols`, since it was defined in the docstring. With `computed_distribution` I didn't know what you wanted to do so I temporarily commented the line that used it",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636:442,variab,variable,442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/141#issuecomment-387106636,1,['variab'],['variable']
Modifiability,"How about [changing the encoding globally](https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python#17628350)? Would that break anything? Also, there is the same bug with the package you rely on, `louvain`. As for the properly configured system, ubuntu:17.10 is the most generic and recent system I can think of, shouldn't it be properly configured out of the box? If not, is there a way to configure the system so that the encoding is globally set to utf-8?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172:250,config,configured,250,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343491172,3,['config'],"['configure', 'configured']"
Modifiability,How to add adata.layers information for running scvelo?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/342:17,layers,layers,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/342,1,['layers'],['layers']
Modifiability,How to control/normalize for variability among samples?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2155:29,variab,variability,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2155,1,['variab'],['variability']
Modifiability,How to use stacked_violin with variable y-axis limits between rows?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386:31,variab,variable,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386,1,['variab'],['variable']
Modifiability,"I added UMAP support for visualization \o/ Here is how MNIST ""single cells"" look like:. ![image](https://user-images.githubusercontent.com/1140359/36549038-bee9d1c4-17bf-11e8-9383-19a70c9ee018.png). ![image](https://user-images.githubusercontent.com/1140359/36549046-c74cbcb4-17bf-11e8-9d8f-595dc7be3e8c.png). I'm not so familiar with code sytle and variable naming etc. yet, and I haven't fully tested things like additional umap kwargs ~and 3d visuazliation~ etc. but let's keep PR here and resolve things along the way.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/92:350,variab,variable,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/92,1,['variab'],['variable']
Modifiability,"I agree that such an opportunity totally should be, but the example given is not relevant, because to ingest data you have to have all the genes, which were involved in the determination of neighbors, to be present in the query dataset.; The relevant example would be when you somehow managed to count common embeddings (e.g. Harmony-adjusted PCA on the reference's highly variable genes and corresponding Symphony-adjusted PCA on the query), you can then use these embeddings to ingest Umap without the necessity for all the gene_names to be the same.; So maybe the check of equality of gene vars is required only if inside tl.ingest initial embeddings for the query are calculated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993:373,variab,variable,373,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2001#issuecomment-1370720993,1,['variab'],['variable']
Modifiability,"I agree with Phil, but it's not a priority right now. The installation of both igraph and louvain has to be done only once... these packages don't evolve much. So I think it's OK for people to have this little inconvenience as it's only once in the beginning.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980:147,evolve,evolve,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/176#issuecomment-398686980,1,['evolve'],['evolve']
Modifiability,"I also just realised that `pl.scatter` does not take `ncols` as an argument. I am also using scanpy v1.4 from bioconda. The online documentation also does not mention ncols as an argument:; `scanpy.pl.scatter(adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None)` (copy-paste from https://icb-scanpy.readthedocs-hosted.com/en/latest/api/scanpy.pl.scatter.html), but it does mention `ncols` below in the description.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-475639263:258,layers,layers,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-475639263,1,['layers'],['layers']
Modifiability,"I also like the `normalization` and `normalization_axis` suggestion, I use z-score also all the time but it's very painful right now. I also agree with Stephen's concerns, minmax is not perfect and can be misleading, and it's safer to be more flexible, provide more normalization options and let the user be responsible for how the plots look like IMO. As I mentioned in #1913, we have to write in the legend that it's min-max scaled expression. We can even remove the color legend labels (0.0, 0.5, 1.0) if minmax is applied.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527:243,flexible,flexible,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-873078527,1,['flexible'],['flexible']
Modifiability,"I also recently encountered this issue. I've dug into the problem a little bit and for me the cause seems to be that the sc.pp.scale function introduces the NaN values. This occurs for columns which show very little variance and are almost constant. According to the current documentation this should not be the current expected behaviour though and should only (possibly) occur in future versions: . `Variables (genes) that do not display any variation (are constant across all observations) are retained and (for zero_center==True) set to 0 during this operation. In the future, they might be set to NaNs.`. So I'm not sure if this is a bug or if the documentation has not been updated yet. . I've currently circumvented the issue by scaling in sklearn (which retains 0s instead of NaNs) and manually loading the scaled results into my adata object as this is the behaviour I would like for my dataset. In case my example dataset would be helpful let me know then I can share it with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706:402,Variab,Variables,402,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706,1,['Variab'],['Variables']
Modifiability,"I am also unable to install scanpy on mac OS. I tried using python 3.8.x . 3.7.x and 3.6.x. ```; (base) $ conda activate SCA. (SCA) $ conda --version; conda 4.8.2. (SCA) $ python --version; Python 3.6.10 :: Anaconda, Inc. (SCA) $ conda install -c bioconda scanpy; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \ ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112:384,flexible,flexible,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-609514112,1,['flexible'],['flexible']
Modifiability,"I am currently having the same issue as well. As a user-only mostly, I tried to dig into the code and found a workaround to get a dataframe with the logreg scores (so, please forgive any inaccuracy and my naivety). After `sc.tl.rank_genes_groups` with `method='logreg'`:. ```python; colnames = ['names', `'scores']. test = [pd.DataFrame(adata.uns[""logreg""][c])[group] for c in colnames]; test = pd.concat(test, axis=1, names=[None, 'group'], keys=colnames); test = test.stack(level=1).reset_index(); test[""group""] = test[""group""].astype(""int""); test.sort_values('group', inplace=True). test; ```; I guess the code could be adapted to expect the exception of the logistic regression being different, i.e. not having logfoldchange and p-values, and allow the retrieval of a Dataframe with scores nonetheless.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688:623,adapt,adapted,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1530#issuecomment-1236487688,1,['adapt'],['adapted']
Modifiability,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:296,variab,variable,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['variab'],['variable']
Modifiability,"I am getting the same highly variable genes between the two runs. The discrepancy is introduced at the PCA step which generates slightly different results between the two runs. The biological interpretation ends up essentially the same in my case but the clusterings are subtly different, making it hard to automate my annotation. I would like the overall pipeline to be reproducible across platforms if possible. I can dig a bit into the PCA code... it seems like this might be an issue on the scikit-learn end.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096:29,variab,variable,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620866096,1,['variab'],['variable']
Modifiability,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:; - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2; - python 3.8.8; - numpy 1.20.0; - numba 0.51.2; - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:; ```; unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells); ```; When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): ; `zsh: illegal hardware instruction`. Is there anything I could do? ; Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927:436,config,config,436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567#issuecomment-1024104927,2,"['config', 'layers']","['config', 'layers']"
Modifiability,"I am trying to get the highly variable genes for a data set. The data set was normalized by fitting a negative binomial model and using the residuals as expression levels. This gives mean gene expression values that can be negative and are very close to 0. When I use the command:; `disp_filter = sc.pp.filter_genes_dispersion(adata.X, min_mean=0, min_disp=0.5)`; I get very few differentially expressed genes. Looking at the dispersions via `disp_filter['dispersions']` shows that many dispersions appear to be NaN. And superficial inspection shows that the genes with negative means have NaN dispersions. This feels like it shouldn't be the case. It is possible to calculate the variance for the genes that have NaN dispersions. Are all negative dispersion values cast to NaN?. Changing the 'mean_mean' parameter to a negative value changes nothing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172,1,['variab'],['variable']
Modifiability,"I can replicate. This is so weird. Sometimes it returns reasonable results. <details>; <summary> Previous investigation </summary>. This data is a bit strange (is it important for `X` to be all negative?). Interestingly, it also breaks `sc.pp.scale`. Any insights you can provide on the data would be helpful. I was seeing this kind of issue before with parallelization, where we were running into a numba bug and being returned the uninitialized array as a result. Some other things I've been noticing:. * If I calculate morans I on the first 100 variables, the 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:548,variab,variables,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541,5,['variab'],"['variable', 'variables']"
Modifiability,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:58,variab,variable,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,2,['variab'],['variable']
Modifiability,"I created a PR to this branch to add GPU support for :; *`tl.rank_gene_groups` with method='logreg'; *`tl.embedding_density`; *`correlation_matrix`; *`diffmap`; I added `.layers` support for `pp.pca`. This helps with the ""Pearson Residuals"" workflow.; The default pca solver for device GPU is now ""auto""; I also fixed a bug in `tl.rank_gene_groups` with `method='logreg'` with selecting groups (eg. groups = [""2"",""1"",""5""]) that is currently still in scanpy.; ![image](https://user-images.githubusercontent.com/37635888/179788802-6783f87d-19eb-497c-922e-59c18d6015d5.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399:171,layers,layers,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1189986399,1,['layers'],['layers']
Modifiability,"I did figure out what's going on. I worked on a view of an AnnData object, where the original AnnData object did not have the X_pca field and it could not be added only in the view. I updated to the latest scanpy and anndata version; > scanpy==1.4+18.gaabe446 anndata==0.6.18+3.g3e93ed7 numpy==1.15.4 scipy==1.2.1 pandas==0.24.1 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . this is my AnnData object:; ```; adata; print(adata); ```; > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. if I now filter my AnnData object for highly variable genes I only got a ""View"" of my AnnData object; ```; adata2 = adata[:, adata.var['highly_variable']]; print(adata2); print(adata); ```. > View of AnnData object with n_obs × n_vars = 14775 × 1999 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. > AnnData object with n_obs × n_vars = 14775 × 25386 ; > obs: 'sample', 'n_genes', 'percent_mito', 'n_counts'; > var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. then on adata2, I cannot add the X_pca field; `sc.tl.pca(adata2, svd_solver='arpack')`. > ---------------------------------------------------------------------------; > ValueError Traceback (most recent call last); > <ipython-input-25-05be375bfc24> in <module>; > 5 print(adata); > 6 print(adata2); > ----> 7 sc.tl.pca(adata2, svd_solver='arpack'); > 8 print(adata2); > ; > ~/miniconda3/lib/python3.7/site-packages/scanpy-1.4+18.gaabe446-py3.7.egg/scanpy/preprocessing/_simple.py in pca(data, n_comps, zero_center, svd_solver, random_state, return_info, use_highly_variable, dtype, copy, chunked, chunk_size); > 504 ; > 505 if data_is_AnnData:; > --> 506 adata.obsm['X_pca'] = X_pca; > 507 if use_highly_variable:; > 508 adata.varm['PCs'] = np.zeros(shape=(adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094:703,variab,variable,703,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/504#issuecomment-467361094,1,['variab'],['variable']
Modifiability,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:166,variab,variable,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,1,['variab'],['variable']
Modifiability,"I don't think so, not unless you call `sc.set_figure_params()`. But this modifies the global config.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/897#issuecomment-556743231:93,config,config,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897#issuecomment-556743231,1,['config'],['config']
Modifiability,"I don't think we should bother with numba, since it'll likely be a pretty core requirement once we can start transitioning to `pydata/sparse`. For `pyplot`, does `matplotlib` also take a while to import? Management of environment variables is a good reason not to defer that import. If we're already using `h5py`, could we drop `tables` as a requirement?. I think bad import times are only really noticeable for interactive use, since any script using scanpy will likely take longer to run. Do import times change depending on interactive environment? I wouldn't be surprised if different code ran when importing something like matplotlib in a notebook vs in a script.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460:230,variab,variables,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516404460,1,['variab'],['variables']
Modifiability,"I downloaded the github source archive at the 1.8.2 tag. The build process applies a few patches viewable [here](https://salsa.debian.org/med-team/python-scanpy/-/tree/master/debian/patches). One is a small change to some R code, and the other is I marked several more tests as needs internet because the Debian builds in an environment without network access and those ultimately tried to download something. (And it's really unclear if we can legally redistributed the 10x pbmc3k dataset.). The Debian build file is (here)[https://salsa.debian.org/med-team/python-scanpy/-/blob/master/debian/rules] though mostly it lets you see what tests I was skipping because of missing dependencies. Also if I set a color like in_tissue, or array_row the data shows up. I can paste the full build log if you'd like but this is the dependencies installed and the environment variables. . ```; Build-Origin: Debian; Build-Architecture: amd64; Build-Date: Sun, 14 Nov 2021 20:11:26 +0000; Build-Path: /<<PKGBUILDDIR>>; Installed-Build-Depends:; adduser (= 3.118),; adwaita-icon-theme (= 41.0-1),; autoconf (= 2.71-2),; automake (= 1:1.16.5-1),; autopoint (= 0.21-4),; autotools-dev (= 20180224.1+nmu1),; base-files (= 12),; base-passwd (= 3.5.52),; bash (= 5.1-3.1),; binutils (= 2.37-8),; binutils-common (= 2.37-8),; binutils-x86-64-linux-gnu (= 2.37-8),; blt (= 2.5.3+dfsg-4.1),; bsdextrautils (= 2.37.2-4),; bsdutils (= 1:2.37.2-4),; build-essential (= 12.9),; bzip2 (= 1.0.8-4),; ca-certificates (= 20211016),; coreutils (= 8.32-4.1),; cpp (= 4:11.2.0-2),; cpp-11 (= 11.2.0-10),; dash (= 0.5.11+git20210903+057cd650a4ed-3),; dbus (= 1.12.20-3),; dbus-bin (= 1.12.20-3),; dbus-daemon (= 1.12.20-3),; dbus-session-bus-common (= 1.12.20-3),; dbus-system-bus-common (= 1.12.20-3),; dbus-user-session (= 1.12.20-3),; dconf-gsettings-backend (= 0.40.0-2),; dconf-service (= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:864,variab,variables,864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['variab'],['variables']
Modifiability,"I experienced the same issue, but none of fixes proposed here worked.; Eventually I re-installed Anaconda, immediately set up the channels, and made a new environment:. ```; conda config --add channels default; conda config --add channels bioconda; conda config --add channels bioconda. #create a new environment; conda create --name <environment name>; #activate your environment ; conda activate <environment name>; ```. Now that I had a new environment (which is easier to work with if you're working on multiple projects; easy switch between environments!), I tried to install scanpy again. Did not work, but then I tried it again, this time with the version number of Python, and that did the trick for me!. `conda install -c bioconda scanpy python=3.7`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003:180,config,config,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584658003,3,['config'],['config']
Modifiability,"I extended the documentation a bit now, see https://github.com/theislab/scanpy/commit/c7e58e3a8e32b7f395e25267cd3cba684d6d40c4.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/780#issuecomment-521671691:2,extend,extended,2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/780#issuecomment-521671691,1,['extend'],['extended']
Modifiability,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275:629,variab,variables,629,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275,3,['variab'],['variables']
Modifiability,I found that running the function 'tl.rank_genes_groups' gives the error the following error message:; UnboundLocalError: local variable 'adata_comp' referenced before assignment. ![scanpy api tl rank_genes_groups_error](https://user-images.githubusercontent.com/35155633/34642043-0191dce0-f305-11e7-847f-37b1ff34a77d.png),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/63:128,variab,variable,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/63,1,['variab'],['variable']
Modifiability,"I get quite a strange scanpy error, which appears a bit stochastic... This is has happened for the first time in version 1.1. I am trying to get a scatter plot of a subsetted anndata object like this:; `p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac')`. When I do this the first time round, I get this error message about categorical variables from sanitize_anndata (none of which are actually used in the call). ```---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-66-fc1479c238f7> in <module>(); 9 plt.show(); 10 ; ---> 11 p4 = sc.pl.scatter(adata[adata.obs['n_counts']<10000 ,:], 'n_counts', 'n_genes', color='mt_frac'); 12 p5 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac'); 13 . ~/scanpy/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 162 show=show,; 163 save=save,; --> 164 ax=ax); 165 ; 166 elif x in adata.var_keys() and y in adata.var_keys() and color not in adata.obs_keys():. ~/scanpy/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, left_margin, size, title, show, save, ax); 281 ax=None):; 282 """"""See docstring of scatter.""""""; --> 283 sanitize_anndata(adata); 284 if legend_loc not in VALID_LEGENDLOCS:; 285 raise ValueError(. ~/scanpy/scanpy/utils.py in sanitize_anndata(adata); 481 # backwards compat... remove this in the future; 482 def sanitize_anndata(adata):; --> 483 adata._sanitize(); 484 ; 485 . ~/anndata/anndata/base.py in _sanitize(self); 1284 if len(c.categories) < len(c):; 1285 df[key] = c; -> 1286 df[key].cat.categories = df[key].cat.categories.ast",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:383,variab,variables,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variables']
Modifiability,"I get the following error when trying to use sc.pl.scatter to plot gene expression, with use_raw=False. I am using sc.pl.scatter instead of sc.pl.umap, .tsne, etc., because of the need to use custom basis names.; ```; File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 118, in scatter; ax=ax); File ""/opt/Python/3.6.5/lib/python3.6/site-packages/scanpy/plotting/_anndata.py"", line 390, in _scatter_obs; c = adata.raw.obs_vector(key, layer=layers[2]); TypeError: obs_vector() got an unexpected keyword argument 'layer'; ```. The following snippet is copied from `_scatter_obs()` in /scanpy/plotting/_anndata.py; ``` python; # coloring according to gene expression; elif (use_raw; and adata.raw is not None; and key in adata.raw.var_names):; c = adata.raw.obs_vector(key); elif key in adata.var_names:; c = adata.raw.obs_vector(key, layer=layers[2]); ```; Should line 390 be c = adata.obs_vector(key, layer=layers[2]) since it is handling the case when use_raw==False and adata.raw.obs_vector does not take layer as argument.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762:475,layers,layers,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762,3,['layers'],['layers']
Modifiability,"I had the same issue, and it turns out setting up channels solves the problem as follows:; ```; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; ```; Ref: ; https://bioconda.github.io/recipes/scanpy/README.html; https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242:102,config,config,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583508242,3,['config'],['config']
Modifiability,"I have a loom file created from Seurat object by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:623,layers,layers,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,5,['layers'],['layers']
Modifiability,"I have an adata object and multiple CD4+ and CD8+ cells as observations and gene expression as variables. I want to draw a PCA with each dot corresponding to one cell and colored by cell type (e.g. all CD4 cells in blue, CD8 in orange). Each cell have a unique identifier, and I have a list cd4, which contains identifiers of only CD4 cells, and the same for CD8. I tried the following:; `scn.pl.pca(adata_needed, groups={""cd4"":cd4, ""cd8"":cd8}, palette = [""red"", ""blue""])`; But the result is the PCA plot in grey; ![image](https://user-images.githubusercontent.com/45490688/49294731-db036400-f4c4-11e8-9b8f-00b3d20e5c41.png). I also tried to visualize only one sample:; `scn.pl.pca(adata_needed, groups=[""SRR1551000""])`. But got exactly the same result. I decided to use ""color"" parameter in pca plot:; `scn.pl.pca(adata_needed, color=""SRR1551000"")`; But got the following mistake:. > ValueError: key 'SRR1551000' is invalid! pass valid observation annotation, one of [] or a gene name Index(['ENSG....', 'ENSG...', ). The observation names are there (adata_needed.obs_names outputs the observation I try to give to the function). Here's the plot I need, but drawn with pandas. ; ![image](https://user-images.githubusercontent.com/45490688/49296023-3be06b80-f4c8-11e8-8a21-f1aa9e9b4a46.png). How can I draw a PCA plot like this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/383:95,variab,variables,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/383,1,['variab'],['variables']
Modifiability,"I have an anndata object like this . > adata_all:; > AnnData object with n_obs × n_vars = 10000 × 14; > obs: 'sample', 'batch', 'condition'; > var: 'n', 'channel', 'marker', '$PnB', '$PnG', '$PnE', 'signal_type', '$PnR-0', '$PnR-1', '$PnR-2', 'AB'; > uns: 'meta', 'neighbors', 'pca', 'sample_colors', 'umap', 'condition_colors'; > obsm: 'X_pca', 'X_umap', 'X_tsne'; > varm: 'PCs'; > layers: 'original'; > obsp: 'connectivities', 'distances'. The conditions are as follow: conditions = ['a', 'b', 'c'].; How can I draw tSNEs for each marker separated by each condition in a row? As you can see condition is a feature of obstacles and marker is a feature of variables. I want to plot tSNEs for each marker in three different tSNEs based on conditions. Is this possible?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2464:383,layers,layers,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2464,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"I have been unable to get this to look good by default. It can be made to look good by playing around with the parameters, but then we're not really saving the user much effort. A strategy that seemed to work okay was to repel the labels from the points, followed by a second repulsion from other labels. But then I had to redraw the lines manually. Current thoughts are to punt this down the road. Maybe there will be a better solution in the future, or maybe there's a clever parameterization fix I hadn't thought of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935:478,parameteriz,parameterization,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839597935,1,['parameteriz'],['parameterization']
Modifiability,"I have got what I want with the following code adapted from dotplot():. gene_ids = adata.raw.var.index.values; clusters = adata.obs['louvain'].cat.categories; obs = adata.raw[:,gene_ids].X.toarray(); obs = pd.DataFrame(obs,columns=gene_ids,index=adata.obs['louvain']); average_obs = obs.groupby(level=0).mean(); obs_bool = obs.astype(bool); fraction_obs = obs_bool.groupby(level=0).sum()/obs_bool.groupby(level=0).count(); average_obs.T.to_csv(""average.csv""); fraction_obs.T.to_csv(""fraction.csv"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069:47,adapt,adapted,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/336#issuecomment-435754069,1,['adapt'],['adapted']
Modifiability,"I have some issues runnign tSNE with `sc.tsne(adata)`. It seems to work on the `moignard15` data set but running the same code with my data set results in the following error. ```; compute tSNE; preprocess using PCA with 50 PCs; --> avoid this by setting n_pcs = 0; 0:00:02.013 - compute PCA with n_comps = 50; 0:00:00.162 - finished; ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-5-ea03cbb426c5> in <module>(); ----> 1 sc.tsne(adata). /opt/conda/lib/python3.6/site-packages/scanpy/tools/tsne.py in tsne(adata, random_state, n_pcs, perplexity); 59 sett.m(0, 'preprocess using PCA with', n_pcs, 'PCs'); 60 sett.m(0, '--> avoid this by setting n_pcs = 0'); ---> 61 X = pca(adata.X, random_state=random_state, n_comps=n_pcs); 62 adata['X_pca'] = X; 63 else:. /opt/conda/lib/python3.6/site-packages/scanpy/tools/pca.py in pca(adata_or_X, n_comps, zero_center, svd_solver, random_state); 60 zero_center, svd_solver,; 61 random_state=random_state); ---> 62 adata['X_pca'] = X_pca; 63 if isadata:; 64 return adata. UnboundLocalError: local variable 'adata' referenced before assignment; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/10:1132,variab,variable,1132,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/10,1,['variab'],['variable']
Modifiability,"I have the same error on scanpy 1.9.5, seaborn 0.13.0, the error seems to be specific to 'multi_panel = True' and produces 3 empty graphs that all inherit the ""n_genes_by_counts"" x-label instead of the proper one.; The same graph is produced normally with 'multi_panel = False'. `sc.pl.violin(full_adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_MT'], multi_panel=True, stripplot=False)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761837779:147,inherit,inherit,147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680#issuecomment-1761837779,1,['inherit'],['inherit']
Modifiability,"I have the same problem. I am using macOS catalina 10.15.2. $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package natsort conflicts for:; scanpy -> natsort; Package louvain conflicts for:; scanpy -> louvain; Package patsy conflicts for:; scanpy -> patsy; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; Package zlib conflicts for:; python=3.7 -> zlib[version='>=1.2.11,<1.3.0a0']; Package libcxx conflicts for:; python=3.7 -> libcxx[version='>=4.0.1']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package statsmodels conflicts for:; scanpy -> statsmodels[version='>=0.10.0rc2']; Package numba conflicts for:; scanpy -> numba[version='>=0.41.0']; Package readline conflicts for:; python=3.7 -> readline[version='>=7.0,<8.0a0']; Package importlib-metadata conflicts for:; scanpy -> importlib-metadata; Package setuptools conflicts for:; scanpy -> setuptools; Package tqdm conflicts for:; scanpy -> tqdm; Package libffi conflicts for:; python=3.7 -> libffi[version='>=3.2.1,<4.0a0']; Package scipy conflicts for:; scanpy -> scipy[version='<1.3|>=1.3']; Package anndata conflicts for:; scanpy -> anndata[version='>=0.6.10|>=0.6.22rc1']; Package pip conflicts for:; python=3.7 -> pip; Package seaborn conflicts for:; scanpy -> ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241:224,flexible,flexible,224,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-580295241,2,['flexible'],['flexible']
Modifiability,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. ; pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml; precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194:103,config,config,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784244194,2,['config'],['config']
Modifiability,"I just got the same error with a similar situation. . I get umap coordinates from a collaborator, which I store in `adata.obs`. Before the last update this worked:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class')`; Now, this produces a `IndexError: Key ""UMAP1"" is not valid observation/variable name/index.` error. Now I need to run this for the same plot:; `sc.pl.scatter(adata, x='UMAP1', y='UMAP2', color='cell_type_class', use_raw=False)`. These covariates are all in `adata.obs.keys()`. It seems that `use_raw` is taking precendence over `x` and `y` being from `adata.obs`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351:307,variab,variable,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-512184351,1,['variab'],['variable']
Modifiability,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664:211,variab,variable,211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-432836664,2,['variab'],['variable']
Modifiability,"I like @flying-sheep's very last solution. To enable this for truly large-scale data and AnnData's that are backed on disk we need a much more efficient transposition implementation, which will probably need to return a view. That's problematic as it will break backwards compat (`.T` returns a copy these days). But it's good as it will allow adding fields to `.var`. @LuckyMD: At the time, when you mentioned that you wanted to plot over genes in scatter, I was fine with with having the scatter wrapper and assuming no ambiguity in obs and var keys. Now, I'd advocate for @flying-sheep's solution. Of course, we'll maintain the feature in `pl.scatter` when refactoring its code (a lot of it became redundant after fidel introduced the completely rewritten scatter plots).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742:660,refactor,refactoring,660,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441473742,1,['refactor'],['refactoring']
Modifiability,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:219,plugin,plugins,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,2,['plugin'],['plugins']
Modifiability,"I like that this method is fairly simple, and could have a meaningful cutoff, but I think I'd like more evidence of it's usefulness before thinking about including it. I have two main points of concern:. * Are there examples of this method being used outside of the glmPCA paper? I would at least like to know that reasonable results can be found downstream of this.; * In the glmPCA paper, the identified genes are highly correlated (~1) with highly expressed genes, and lowly correlated (~.3 with highly variable gene selection. While I'm not sure which highly variable gene method they compared against, should the low correlation with common practice give us pause?. <img width=""784"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/112927072-2515b680-9160-11eb-967a-373536aad6d1.png"">. @giovp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884:506,variab,variable,506,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1765#issuecomment-809874884,2,['variab'],['variable']
Modifiability,"I like the thought... for exactly the reason you brought up, I recommended storing log-normalized data in `adata.raw` in my best practices workflow. That way DE analysis and plotting is done on that data type rather than raw counts. I have been working with `adata.layers['counts']` for count data and don't keep filtered out cells/genes (easy to recreate anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830:265,layers,layers,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819683830,1,['layers'],['layers']
Modifiability,"I noticed that `sc.pl.paga` function has this piece of code. ```py; # compute positions; if pos is None:; adj_tree = None; if layout in {'rt', 'rt_circular', 'eq_tree'}:; adj_tree = adata.uns['paga']['connectivities_tree']; pos = _compute_pos(; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); ```. and layout is, by default, `None`. This may result in passing an empty `adj_tree` to the `_compute_pos()` function. Actually this is exactly what happened to me:. ```pytb; >>> sc.pl.paga(data, color='leiden'); ---------------------------------------------------------------------------; UnboundLocalError Traceback (most recent call last); <ipython-input-138-ed5614508f4e> in <module>(); ----> 1 sc.pl.paga(data, color='leiden')#, layout='rt'). /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy-1.4+8.g86f189e-py3.6.egg/scanpy/plotting/_tools/paga.py in paga(adata, threshold, color, layout, layout_kwds, init_pos, root, labels, single_component, solid_edges, dashed_edges, transitions, fontsize, fontweight, text_kwds, node_size_scale, node_size_power, edge_width_scale, min_edge_width, max_edge_width, arrowsize, title, left_margin, random_state, pos, normalize_to_color, cmap, cax, colorbar, cb_kwds, frameon, add_pos, export_to_gexf, use_raw, colors, groups, plot, show, save, ax); 443 adj_tree = adata.uns['paga']['connectivities_tree']; 444 pos = _compute_pos(; --> 445 adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root); 446 ; 447 if plot:. UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. is this intended?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/483:1674,variab,variable,1674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/483,1,['variab'],['variable']
Modifiability,"I only just now got the distinction between types and classes in python. So when they talk about “types”, they mean stuff in the typing module, got it. So. - Types (`typing.*`), ABCs, and regular classes can be used for type annotation; - ABCs and regular classes can be used for `isinstance` and `issubclass` checking; - ABCs and mixins can be mixed in to enhance a class you defined. where a mixin is simply a regular class that happens to rely on some properties of the class it can be mixed with, and a regular class being any class that’s not a type or an ABC. - `collections.abc.Mapping` is an ABC and can be mixed in to enhance your basic mapping class with some convenience methods, or used to check if something has the basic mapping protocol (no matter if it was mixed in or not). What’s the basic protocol and what will be mixed in is [nicely documented](https://docs.python.org/3/library/collections.abc.html).; - `typing.Mapping` is a generic type, to be used in annotations only. There’s a few projects implementing type checking using them, e.g. mypy or typecheck-decorator. Check out the [docs for abstract base classes](https://docs.python.org/3/library/abc.html), they explain how ABCs work. (namely by `register`ing virtual subclasses and/or implementing `__subclasshook__`). Mixin example:. ```py; class EnumerableMixin:; """"""silly mixin class for iterables""""""; def enumerate(self, start=0):; yield from enumerate(self, start). class EnumerableList(list, EnumerableMixin):; pass. for i, e in EnumerableList.enumerate(): print(i, e); ```. ABC example:. ```py; class PositiveNumbers(collections.abc.Set):; def __contains__(self, i):; return isinstance(i, int) and i >= 0; def __iter__(self): return itertools.count(); def __len__(self): return float('inf'). # __lt__ is mixed in!; print({0, 1, 10_000} < PositiveNumbers()). # `set` doesn’t inherit from collections.abc.Set, the __subclasshook__ does its magic here; isinstance({}, collections.abc.Set); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839:357,enhance,enhance,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-445181839,3,"['enhance', 'inherit']","['enhance', 'inherit']"
Modifiability,"I ran the newest Scanpy package's ; ```; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.05,; batch_key='batch'); ```. It indeed gave me information about highly_variable_nbatches etc. But all the genes were labelled as not variable ('False'). Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935:251,variab,variable,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935,1,['variab'],['variable']
Modifiability,"I ran your code snippet multiple times on my end and I got the same results each time. Is this true for you as well? If both you and your partner can generate the same results consistently each time, then it is strange that your results disagree with each other... Some followup questions I have:; 1) Are ALL packages the same version? (Packages like Numba, scipy, sklearn, etc. should also be the same version to remove that as a potential source of variability); 2) Are you guys using the same operating system? ; 3) Can you run UMAP directly on the randomly generated matrix to see if your embeddings are the same? If they are, UMAP is likely not at fault.; 4) If you perturb your nearest neighbor matrix by adding noise to the edges such that the total edge weight differs by ~0.001 between perturbations, can you recreate the big differences in the UMAP projection? Small differences in the edge weights of the nearest neighbor graph CAN lead to huge differences in the UMAP projection if the graph has no inherent structure (which should be the case for randomly generated data).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404:451,variab,variability,451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009#issuecomment-578310404,1,['variab'],['variability']
Modifiability,"I really like the `sc.extract` idea (or `sc.cast`/`sc.object`). It would be pretty cool if that went both ways though. For example, if I want to generate differential expression results with `diffxpy`, store everything in my `AnnData` object, and visualize later with `sc.pl.rank_genes_groups_violin()`, this could be done by some kind of `sc.read.diffxpy()` function. Maybe you could just extend `sc.read` and `sc.write` and make them into larger modules? Or rename both to `sc.io`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487563174:390,extend,extend,390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487563174,1,['extend'],['extend']
Modifiability,"I removed all this automatic setting of backends etc. . Currently ""is_interactive"" is only used to choose different progress bars (tqdm behaves very differently on the command line, in jupyter and then, unfortunately again differently in Rodeo) and to decide on whether a `total wall time` should be output when leaving the session. It's now left to the user to choose the matplotlib backend. If she/he logs in via ssh without setting an -X tunnel, the default interactive backend will simply fail. But that's left to the user now, no longer output of, which seemed to annoy you (I can understand that); ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054:643,variab,variable,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16#issuecomment-298663054,1,['variab'],['variable']
Modifiability,I second the suggestion by @falexwolf to rename the function to something simpler but also to keep the previous functionality with a Deprecate message as suggested by @LuckyMD. @Koncopd The changes also requires adapting the corresponding `sc.pl.rank_genes_groups*` functions. I can take over that once the PR is ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020:212,adapt,adapting,212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-627433020,1,['adapt'],['adapting']
Modifiability,"I see, there’s also code to make that exact shape. Seems like you need to override this as well:. https://github.com/scverse/scanpy/blob/ed3b277b2f498e3cab04c9416aaddf97eec8c3e2/scanpy/plotting/_baseplot_class.py#L522-L542. maybe simply. ```py; def _plot_legend(self, legend_ax, return_ax_dict, normalize): ; self._plot_colorbar(legend_ax, normalize) ; return_ax_dict['color_legend_ax'] = color_legend_ax; ```. but as said: we will start working on a more flexible and less fiddle plotting API",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829:456,flexible,flexible,456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609294829,1,['flexible'],['flexible']
Modifiability,"I see. Yes then maybe we should change this annotation:; `groups : {‘all’}, Iterable[str] (default: 'all').`; `Subset of groups, e.g. ['g1', 'g2', 'g3'], to which comparison shall be restricted, or 'all' (default), for all groups.`; to something like:; `Subset of groups, e.g. ['g1', 'g2', 'g3'], for which differentially expressed genes should be calculated, or 'all' (default) for all groups.`; ?; I could also take a look to see how we can make the reference argument more flexible, if you agree that would be a good feature.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985:476,flexible,flexible,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519#issuecomment-744303985,1,['flexible'],['flexible']
Modifiability,I think @falexwolf voiced my thoughts much more eloquently. A non-hidden directory in the root folder makes a sensible default to me. Would anyone be against also having some environmental variables/ a scanpy config (I’m thinking `.cfg` or `.json`) so this (and things like verbosity) don’t have to be set manually each session?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476615093:189,variab,variables,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476615093,2,"['config', 'variab']","['config', 'variables']"
Modifiability,"I think I got the docs right, let me know if there are any issues. I'm noticing some conflict with convention for metric names. I'm copying `scater`, and using labels like `total_counts` and `total_features_by_counts`. `filter_genes`, `filter_cells`, `spring_project`, and a couple of the recipes use `n_counts` or `n_cells`. My preference is for the `scater ` way, since formatting allows it to be bit more flexible. It'd be nice for there to be a consistent default key for these features. For example, while updating the clustering tutorial, I ended up with both `n_counts` and `total_counts` in the same `adata.obs`. As changing the defaults could break some code, what's the right path forward? When `scater` updated their metric names, I think they used both the old and new keys with a deprecation warning. They talk about it a bit under the documentation for `scatter::calcuateQCMetrics`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473:408,flexible,flexible,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-434172473,1,['flexible'],['flexible']
Modifiability,I think I messed something up when trying to rewrite the history @ivirshup :(,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499:45,rewrite,rewrite,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1204#issuecomment-651244499,1,['rewrite'],['rewrite']
Modifiability,I think autoreload does indeed do more than importlib.reload:. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html#caveats. > Functions and classes imported via ‘from xxx import foo’ are upgraded to new versions when ‘xxx’ is reloaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529:104,config,config,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468#issuecomment-462133529,1,['config'],['config']
Modifiability,"I think it's reasonable that the runtime gets the final say. It's pretty standard for precedence to go: `runtime > environment > config file`, right? I don't think it's reasonable for a library to make the decision, as it should be done by the program.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220:129,config,config,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874663220,1,['config'],['config']
Modifiability,I think pynndescent setting the value at import time will take precedence over the environment variable,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048:95,variab,variable,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1933#issuecomment-874659048,1,['variab'],['variable']
Modifiability,"I think that makes sense to allow keeping a measure of magnitude, potentially implemented as an option, like with `sc.pp.scale`s `zero_center`. I'd be interested to see how different highly variable gene selection was on data transformed this way, vs the batched approach we have now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/722#issuecomment-509119409:190,variab,variable,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/722#issuecomment-509119409,1,['variab'],['variable']
Modifiability,"I think that's a good idea. A general solution would be to move the part of rank_genes_groups where some statistics are calculated (e.g. log2fc, fractions, mean expression per group) to a different function with more flexible features. For example, users run regress_out or combat sometimes and then run rank_genes_groups on these corrected values, but they wanna calculate log2fc and other summary stats on the ""raw"" logTP10k values. Having another function with a layer argument would solve this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680:217,flexible,flexible,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1454#issuecomment-707400680,1,['flexible'],['flexible']
Modifiability,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492:1115,adapt,adapt,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-431634492,1,['adapt'],['adapt']
Modifiability,"I think the problem is the option `sort_order` which is True by default for; numerical data. This changes the ordering of the dots and thus it messes; up with your own sizes. Setting `sort_order=False` should fix the problem. On Tue, Feb 12, 2019 at 6:07 AM Andreas <notifications@github.com> wrote:. > I'm trying to use an array for the size argument to my umap/scatterplot; > with the following code; >; > import scanpy.api as sc; > import numpy as np; > sc.settings.figdir = ""testdir""; > sc.settings.file_format_figs = ""png""; > sc.logging.print_versions(); >; > With these libraries; > scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4; > scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1; >; > Running the following code bit. I use some dummy variable for size.; >; > somedata = sc.datasets.paul15(); > sc.pp.pca(somedata); > sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); > sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); > sc.tl.leiden(somedata, resolution=0.5, random_state=42); > z = np.abs(somedata.obsm['X_pca'][:,0])**1; > sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); > sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); >; > I get the following two figure as output; > [image: umapcontinuous_expr]; > <https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png>; > [image: umapgroup_value]; > <https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png>; >; > I would expect to see a similar size allocation/distribution but they are; > very different. I Could not really find a cause for this looking at the; > scatter plot function so it might be somewhere deeper.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/478>, or ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152:795,variab,variable,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478#issuecomment-462722152,1,['variab'],['variable']
Modifiability,"I think the spring export function currently fails because it only checks whether each column in `adata.obs` is a pandas categorical variable (`not is_categorical(adata.obs[obs_name])`) and, if not, assumes it's a continuous variable and then tries to join a str with an integer. . If you look at your file `data.obs` contains a number of categorical variables that are currently numpy objects; ```pytb; data.obs.dtypes; ClusterID int32; ClusterName object; RNA_snn_res_0_5 object; nCount_RNA float32; nFeature_RNA int32; orig_ident object; percent_mt float32; seurat_clusters object; louvain category; dtype: object; ```. As a quick fix, I think you can do something like this:; ```python; adata = data.copy(); obj_cols = adata.obs.columns[adata.obs.dtypes == np.object]; adata.obs[obj_cols] = adata.obs[obj_cols].astype('category') ; sce.exporting.spring_project(adata, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True); ```; Not sure what's the best way to fix it for the future: check for other dtypes or uses f-strings to avoid the str concatenation errors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431:133,variab,variable,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889#issuecomment-590643431,3,['variab'],"['variable', 'variables']"
Modifiability,"I think the two best ways to go forward:. - Either the Seurat people agree their implementation’s order is a bug and we switch to the paper order. Then we don’t _necessarily_ need to add any way to configure it, just to follow suit. But we could add a way to configure it to support different possible orderings; - Or they decide that it isn’t, in which case we should add that way to configure things. I think it makes more sense to encode orthogonal choices into orthogonal options. If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. E.g. if the following makes sense, then it should for sure be multiple options:. ```python; for flavor, order in product(; ('seurat_v3', 'seurat'),; ('rank', 'batches'),; ):; hvg(…, flavor=flavor, order=order); ```. (`order`, `'rank'`, and `'batches'` are ad-hoc names, not necessarily good ones)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1893265335:198,config,configure,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1893265335,3,['config'],['configure']
Modifiability,"I think these plots are well overdue for a refactor. It would make sense to me if the kind of marginal plots were abstracted out into their own classes. Maybe like `MarginalBar`, `MarginalDendrogram`, `MarginalLabels`. But if you have any thoughts about how you think it could be done, or want to talk it over I'd be happy to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1088642888:43,refactor,refactor,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1088642888,1,['refactor'],['refactor']
Modifiability,"I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. I’m less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/828:258,layers,layers,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828,1,['layers'],['layers']
Modifiability,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088:24,enhance,enhancement,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646#issuecomment-777891088,1,['enhance'],['enhancement']
Modifiability,"I think this would be good. I can think of cases where I wouldn't want every gene to have the same quantile normalization (e.g. when plotting markers for populations whose frequencies differ by orders of magnitude). In addition to including a quantile argument, would you also include a ""vectorized"" vmax, vmin argument?. ```python; sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=[2.0, 3.0]); ```. If you want to get even more flexible about how cutoffs can be chosen, we could also allow `vmin` and `vmax` to be callables. That way you could get the behavior of specifying a quantile from:. ```python; from functools import partial. sc.pl.{scatterfunc}(adata, color=[""gene1"", ""gene2""], vmax=partial(np.quantile, q=.99)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775#issuecomment-520202242:438,flexible,flexible,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775#issuecomment-520202242,1,['flexible'],['flexible']
Modifiability,"I think we should allow for categorical colors along either axis, and right now it's becomes ambiguous. A good example of an annotation that can apply to both observations and variables is `species`. I'd like to shift to a nested model to limit the amount of reserved keys in `.uns`. It reduces that chance of unintentional naming collisions. As for the amount of things that would need to change, a lot has to change anyways. Hardly any code that works with the current setup will work with mappings (`len` is all I can think of). If we're already making a breaking change, might as well take advantage and future proof it a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760:176,variab,variables,176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1340#issuecomment-666266760,1,['variab'],['variables']
Modifiability,"I think we should introduce a standardized “mask” argument to scanpy functions. This would be a boolean array (or reference to a boolean array in `obs`/ `var`) which masks out certain data entries. This can be thought of as a generalization of how highly variable genes is handled. As an example:. ```python; sc.pp.pca(adata, use_highly_variable=True); ```. Would be equivalent to:. ```python; sc.pp.pca(adata, mask=""highly_variable""); # or; sc.pp.pca(adata, mask=adata.obs[""highly_variable""]); ```. One of the big advantages of making this more widespread is that tasks which previously required using `.raw` or creating new anndata objects will be much easier. Some uses for this change:. ### Plotting. A big one is plotting. Right now if you want to show gene expression for a subset of cells, you have to manually work with the Matplotlib Axes:. ```python; ax = sc.pl.umap(pbmc, show=False); sc.pl.umap(; pbmc[pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells',])],; color=""LDHB"",; ax=ax,; ); ```. If a user could provide a mask, this could be reduced, and would make plotting more than one value possible:. ```python; sc.pl.umap(; pbmc,; color=['LDHB', 'LYZ', 'CD79A’],; mask=pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells’,]),; ); ```. ### Other uses. This has come up before in a few contexts:. * Performing normalization on just some variables https://github.com/scverse/scanpy/issues/2142#issuecomment-1046729522; * Selecting a subset of variables for DE tests: https://github.com/scverse/scanpy/issues/1744; * See also https://github.com/scverse/scanpy/issues/748; * Changing use_raw https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988. ## Implementation. I think this could fit quite well into the `sc.get` getter/ validation functions (https://github.com/scverse/scanpy/issues/828#issuecomment-560072919).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234:255,variab,variable,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234,3,['variab'],"['variable', 'variables']"
Modifiability,"I think you have some variables which are the same for all samples. This leads to a division by zero error, which numba is not handling gracefully or mentioning. Here's how to find those values:. ```python; np.where((adata.X[[0], :] == adata.X).all(axis=0)); ```. I believe if you filter these out, this should work. I'm not sure if there is a correct value for Morans I or Gearys C in this case. Should we error?. --------------------. Numba bug report: https://github.com/numba/numba/issues/6976",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830:22,variab,variables,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826743830,1,['variab'],['variables']
Modifiability,"I tried to set `var_names` from gene_symbols, and I get a warning message:; `Variable names are not unique. To make them unique, call `.var_names_make_unique`.`. In calling `adata.var_names_make_unique()` I get the error:; `TypeError: unsupported operand type(s) for +: 'float' and 'str'`. I can ignore this and take it through most of the analysis and am able to make the plots and rank the genes by name, however, I am unable to save. Calling `adata.write('./write/adata.h5ad')` gives the following error:. ```; File ""pandas/_libs/src/inference.pyx"", line 1472, in pandas._libs.lib.map_infer. TypeError: object of type 'float' has no len(); ```. Also, the clustering is slightly different, I'm guessing from not having unique gene names. I've looked through the documentation for `sc.pl.rank_genes_groups_*` and cannot figure out how to keep the index as the Ensembl gene ID and just use gene_symbols to call the plots (`sc.pl.violin`, etc.) and use the `sc.tl.rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184:77,Variab,Variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-473778184,1,['Variab'],['Variable']
Modifiability,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing?. I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214:141,adapt,adaptation,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427489214,1,['adapt'],['adaptation']
Modifiability,"I was looking over normalize_total and saw some strange behaviour. Since it's such a common function, I think it's important that it has standard scanpy behaviour. To this end, this PR looks at cleanup up it's code. ### Addition. `layer` argument. A specific layer can now be normalized by itself. ### Deprecations. I've deprecated the `layers` and `layer_norm` argument. Normalizing multiple layers at once seems less useful than normalizing a specific layer. These seem like very specific use cases that are easy for user's to implement themselves, and are not common patterns in scanpy functions. ### TODO:. - [x] Tests for deprecations ; - [x] Scheduling of deprecations (deprecate in 1.8, remove in 1.9)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1667:337,layers,layers,337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1667,2,['layers'],['layers']
Modifiability,"I was trying to get top 1000 variable genes in 1.3M dataset, but every time I ended up with zero genes after the `sc.pp.filter_genes_dispersion(adata, n_top_genes=1000)` call. The reason is that `sc.pp.filter_genes_dispersion(adata, n_top_genes=x)` actually returns `x - num_zero_expression_genes` genes instead of x, where num_zero_expression_genes represents number of genes without any expression. . Here is a small reproducible example:. ![image](https://user-images.githubusercontent.com/1140359/38215015-9f3de66e-36c6-11e8-8c96-9c9a6458741d.png). It's easy to fix with a prior `sc.pp.filter_genes(adata, min_counts=1)` call, but I think filter_genes_dispersion should retrieve n_top_genes, regardless of presence of zero expression genes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/115:29,variab,variable,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/115,1,['variab'],['variable']
Modifiability,"I was trying to reproduce the results in Example 1 on notebook; https://github.com/theislab/scanpy_usage/tree/master/170505_seurat. I'm getting two problems in the filtering steps in cell 9:; 1) although genes seem to be filtered (there are 1838 genes left versus 13714 before), the plot does not show a different colour for 'highly variable' and 'other' genes. Both appear black (see attached figure). I've both tried it in a jupyter notebook and ipython. I'm running python in a conda environment with matplotlib 4.3.2.25.py35_0 and seaborn 0.8_py35. 2) There's also the following warning message, that seems to complain of a divide by zero on the mean:; /anaconda/lib/python3.5/site-packages/scanpy/preprocessing/simple.py:193: RuntimeWarning: invalid value encountered in true_divide; dispersion = var / mean; Is ; ![figure_10](https://user-images.githubusercontent.com/10065683/30990958-f0e3dec6-a457-11e7-9921-f1b6b9f72861.png). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:333,variab,variable,333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['variab'],['variable']
Modifiability,"I was wondering if someone who is familiar with sc.pp.regress_out could confirm the following:; I would like to regress out nonlinear effect, e.g. ~1 + a + a^2 + a^3, where a is non-categorical variable. I have looked at the code of regress_out: https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L677; It seems that the code performs the fitting for all specified variables at once, but I am not sure:; https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L701; If the design passed to GLM is combined of all keys passed to the function then I could just create the necessary columns a, a^2, a^3 and pass this as keys. Can someone confirm if I understand this correctly and passing the polynomial columns will do the fitting of a polynom?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839:194,variab,variable,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839,2,['variab'],"['variable', 'variables']"
Modifiability,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192:499,variab,variables,499,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617#issuecomment-554257192,1,['variab'],['variables']
Modifiability,"I wonder if `dimensions` does too much or too little. Dimensions should always match, no? Flipping just one of the layers while plotting has no use.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2520#issuecomment-1598796691:115,layers,layers,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2520#issuecomment-1598796691,1,['layers'],['layers']
Modifiability,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:187,variab,variable,187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,1,['variab'],['variable']
Modifiability,"I would just like to add that the issue with `normalize_total` can arise not only from the `downsample_counts` function. . In my case, I am working on `.loom` files generated with *velocyto* - I want to be able to estimate RNA velocity in the end. That means I have 'spliced' and 'unspliced' layers in my anndata object. I wanted to use `normalize_total` on all the layers, which should be possible by setting parameter `layers='all'`. However, I was getting a TypeError, as in #435 . The workaround described at the end of that issue solved it for me. My point is just that fixing only `downsample_counts` is not enough and functions that work on layers should accept integer data. I think that my case is not that uncommon and will happen more often as people use scVelo with velocyto pipelines. . Alternatively, you should warn people about it in the tutorials and make them convert everything to float.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823:292,layers,layers,292,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552929823,4,['layers'],['layers']
Modifiability,"I would like to use stacked_violin plot with variable y-axis limits, particularly when swap_axes=True. Examples [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c), particularly code in line 7, show this. How do I do this? When I use it now with my code, it always chooses a uniform y-axis limit for all genes. Which option do I use for variable y-axis limits? Maybe this aspect of scanpy.api.pl.stacked_violin() should be better documented.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386:45,variab,variable,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386,2,['variab'],['variable']
Modifiability,"I would say this is not a scanpy question.; It is not clear what do you mean by correlation of a categorical variable with multiple categories and a continuous variable. ; If you have a binary categorical variable, you can calculate Point Biserial Correlation, but for a multicategorical variable you would have to discretize your continuous variable and calculate Chi-squared test. You can also try ANOVA. If you think you know what variables are dependent and independent you can use logistic regression and look at its coefficients or try ANCOVA.; some additional information with examples; https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984:109,variab,variable,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1845#issuecomment-848101984,7,['variab'],"['variable', 'variable-and-a-categorical-variab', 'variables']"
Modifiability,"I'd like to merge this. I think we could just rename the function, and deprecate `subsample`. I'd like the name `sample` more for this function if we could add a `dim` argument so users can subsample on the variable as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/943#issuecomment-577981681:207,variab,variable,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/943#issuecomment-577981681,1,['variab'],['variable']
Modifiability,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:186,variab,variable,186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018,3,['variab'],"['variable', 'variables']"
Modifiability,"I'm getting the same error from RStudio with reticulate:. From the console:. ```; py_install('scanpy'); Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... done. # All requested packages already installed. Collecting package metadata (current_repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve. PackagesNotFoundError: The following packages are not available from current channels:. - scanpy. Current channels:. - https://conda.anaconda.org/conda-forge/linux-64; - https://conda.anaconda.org/conda-forge/noarch; - https://repo.anaconda.com/pkgs/main/linux-64; - https://repo.anaconda.com/pkgs/main/noarch; - https://repo.anaconda.com/pkgs/r/linux-64; - https://repo.anaconda.com/pkgs/r/noarch. To search for alternate channels that may provide the conda package you're; looking for, navigate to. https://anaconda.org. and use the search bar at the top of the page. Error: one or more Python packages failed to install [error code 1]; ```. If I switch to the terminal and try `pip` or `conda` I get:. ```; pip install scanpy; ```. ```; Requirement already satisfied: scanpy in /home/tsundoku/anaconda3/lib/python3.7/site-packages (1.4.5.post2); Requirement already satisfied: setuptools-scm in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (3.3.3); Requirement already satisfied: scipy>=1.3 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (1.3.2); Requirement already satisfied: pandas>=0.21 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (0.25.3); Requirement already satisfied: packaging in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from scanpy) (19.2); Requirement already satisfied: natsort in /home/tsundoku/anacond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:418,flexible,flexible,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,2,['flexible'],['flexible']
Modifiability,"I'm glad you all are considering adding this. I updated the implementation to work with sparse counts. . ```python; def seurat_v3_highly_variable_genes(; adata, n_top_genes: int = 4000, batch_key: str = ""batch""; ):; """""" An adapted implementation of the ""vst"" feature selection in Seurat v3. The major differences are that we use lowess insted of loess. For further details of the sparse arithmetic see https://www.overleaf.com/read/ckptrbgzzzpg. :param n_top_genes: How many variable genes to return; :param batch_key: key in adata.obs that contains batch info. If None, do not use batch info. """""". from scanpy.preprocessing._utils import _get_mean_var; from scanpy.preprocessing._distributed import materialize_as_ndarray. lowess = sm.nonparametric.lowess. if batch_key is None:; batch_correction = False; batch_key = ""batch""; adata.obs[batch_key] = pd.Categorical(np.zeros((adata.X.shape[0])).astype(int)); else:; batch_correction = True. norm_gene_vars = []; for b in np.unique(adata.obs[batch_key]):. mean, var = materialize_as_ndarray(; _get_mean_var(adata[adata.obs[batch_key] == b].X); ); not_const = var > 0; estimat_var = np.zeros((adata.X.shape[1])). y = np.log10(var[not_const]); x = np.log10(mean[not_const]); # output is sorted by x; v = lowess(y, x, frac=0.15); estimat_var[not_const][np.argsort(x)] = v[:, 1]. # get normalized variance; reg_std = np.sqrt(10 ** estimat_var); batch_counts = adata[adata.obs[batch_key] == b].X.copy(); # clip large values as in Seurat; N = np.sum(adata.obs[""batch""] == b); vmax = np.sqrt(N); clip_val = reg_std * vmax + mean; # could be something faster here; for g in range(batch_counts.shape[1]):; batch_counts[:, g][batch_counts[:, g] > vmax] = clip_val[g]. if sp_sparse.issparse(batch_counts):; squared_batch_counts_sum = np.array(batch_counts.power(2).sum(axis=0)); batch_counts_sum = np.array(batch_counts.sum(axis=0)); else:; squared_batch_counts_sum = np.square(batch_counts).sum(axis=0); batch_counts_sum = batch_counts.sum(axis=0). norm_gene_var",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-615304326,2,"['adapt', 'variab']","['adapted', 'variable']"
Modifiability,"I'm going through it right now :P). R packages just tend to come with very thorough vignettes, documenting how the package is supposed to be used end-to-end, highlighting special use-cases, etc. I think [Seurat](http://satijalab.org/seurat/get_started.html), [Monocle](http://cole-trapnell-lab.github.io/monocle-release/), or really any Bioconductor package (eg. [Scater](https://bioconductor.org/packages/release/bioc/vignettes/scater/inst/doc/vignette-intro.html)) are great examples. Jupyter notebooks are fantastic for helping get an idea of how to use a package, but sometimes the documentation within them is lacking, making it hard to understand what's going on at each step without having to dive into source code. I find Scanpy's straight forward to follow though. I really appreciate the modular design of Scanpy. At least if you're familiar with single-cell analysis, you can recognize the steps quite easily and just string together the appropriate functions for your analysis pipeline. Some other packages are bit trickier. For example, I found Velocyto (which I love btw) incredibly hard to navigate. There are functions that are used in some analysis notebooks but not all of them, so it becomes unclear what the standard analysis pipeline with it should be. Obviously you can dive into the paper, understand the statistical guts of the method, go through all the source code and see what's there, but then it starts becoming a barrier to newer users adapting it. . Anyway, this is just a general thing I also noticed moving from R to Python. Not saying that it's something that necessarily needs changing, but it does create a little bit of friction to newer users, so it may be worth thinking about as a community. I suspect this will improve as the genomics user-base of Python increases, and as all these packages have more time to develop. Also, I understand that this is more of a ""community"" chat and may not belong in the scanpy/issues page anymore, so feel free to close it ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657:1718,adapt,adapting,1718,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/74#issuecomment-363820657,1,['adapt'],['adapting']
Modifiability,I'm improving on this whole organization... Right now you can set the global variable: `sc.settings.figdir = path_to_some_dir/prefix_` or you can use `save='_mysuffix.pdf'` in any plotting function. Does this help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/145#issuecomment-386717637:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/145#issuecomment-386717637,1,['variab'],['variable']
Modifiability,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716:204,adapt,adapt,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270#issuecomment-427480716,2,['adapt'],"['adapt', 'adaption']"
Modifiability,"I'm new to scanpy, and I want to plot umap with some genes. In this tutorial, it's written below. > sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP']). But, I could show only highly variable genes, because other genes were discarded by the code below. > adata = adata[:, adata.var.highly_variable]. So, how can I plot umap with genes without highly variable? How can I leave all genes in anndata?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2098:181,variab,variable,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2098,2,['variab'],['variable']
Modifiability,"I'm not sure if `sc.write` is forgotten or left out on purpose. Leaving it out make sense as it only offers `sc.write('file.csv', adata)` over `adata.write()`. Alternatively, extending `sc.write` functionality to loom and zarr and keeping it in the new API might make it more useful. I was using it just to make the code more symmetric :) i.e. `sc.read` and `sc.write`, but I don't mind if it's removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/423:175,extend,extending,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/423,1,['extend'],['extending']
Modifiability,"I'm not that familiar with loom files, but it definitely sounds good ;). So I guess every tool and plotting function would need a layers argument to determine which layer it should work on? At least if layers is implemented in a sufficiently general way to be used for data processing layers as well. Any idea on the timeline for this? Then I could adapt the tutorial to work with this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358:130,layers,layers,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414606358,4,"['adapt', 'layers']","['adapt', 'layers']"
Modifiability,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:125,config,config,125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,4,"['Config', 'config']","['Config', 'config']"
Modifiability,"I'm trying to use an array for the size argument to my umap/scatterplot with the following code; ```; import scanpy.api as sc; import numpy as np; sc.settings.figdir = ""testdir""; sc.settings.file_format_figs = ""png""; sc.logging.print_versions(); ```; With these libraries; `scanpy==1.3.7 anndata==0.6.16 numpy==1.16.1 scipy==1.2.0 pandas==0.23.4 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `. Running the following code bit. I use some dummy variable for size.; ```; somedata = sc.datasets.paul15(); sc.pp.pca(somedata); sc.pp.neighbors(somedata, n_neighbors=4, n_pcs=20); sc.tl.umap(somedata, spread=1, min_dist=0.1, random_state=42); sc.tl.leiden(somedata, resolution=0.5, random_state=42); z = np.abs(somedata.obsm['X_pca'][:,0])**1; sc.pl.umap(somedata, color=['1110007C09Rik'], size=z, cmap='viridis', save='continuous_expr.png'); sc.pl.umap(somedata, color=['leiden'], size=z, cmap='viridis', save='group_value.png'); ```; I get the following two figure as output; ![umapcontinuous_expr](https://user-images.githubusercontent.com/715716/52612879-951a3300-2e59-11e9-9dad-a8afc60a4b54.png); ![umapgroup_value](https://user-images.githubusercontent.com/715716/52612880-95b2c980-2e59-11e9-9a44-81dd84e3274d.png). I would expect to see a similar size allocation/distribution but they are very different. I Could not really find a cause for this looking at the scatter plot function so it might be somewhere deeper. . I'm need help with getting some grasp on how to interpret this issue and if possible how to map the size argument to the same data points over different plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/478:474,variab,variable,474,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/478,1,['variab'],['variable']
Modifiability,"I'm using scanpy 1.8.2, anndata 0.8.0 and h5py 3.1.0. I got this error while reading an h5ad file:. ```; adata=sc.read_h5ad('XXXX.h5ad'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 if zarr and isinstance(elem, (zarr.Group, zarr.Array)):; --> 156 parent = elem.store # Not sure how to always get a name out of this; 157 elif isinstance(elem, SparseDataset):. /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group). /opt/conda/lib/python3.8/enum.py in __getitem__(cls, name); 386 def __getitem__(cls, name):; --> 387 return cls._member_map_[name]; 388 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-15-a2632df74a34> in <module>; ----> 1 adata=sc.read_h5ad('XXXX.h5ad'). /opt/conda/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size). /opt/conda/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 160 parent = elem.file.name; 161 return parent; --> 162 ; 163 ; 164 def report_read_key_on_error(func):. AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:1606,layers,layers,1606,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['layers'],['layers']
Modifiability,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:823,adapt,adapted,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,1,['adapt'],['adapted']
Modifiability,I've also initialized the `experimental` module. I think it should be straightforward to copy over the functions currently in core.; e.g.; ```; sc.pp.normalize_pearson_residuals() -> sc.experimental.pp.normalize_pearson_residuals(); ```. For highly variable genes it might be a bit ugly because it essentially only supports one modality. We really need to start thinking about #1739 ...; Let me know if it makes sense and if there is something unclear.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014:249,variab,variable,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874227014,1,['variab'],['variable']
Modifiability,"I've been working on scRNA from different patients (diseased and healthy) control and was wondering what is the best approach to concatenate these data? It seems like inner join is usually recommended, but given significant individual variability (especially disease versus healthy states) there would be a large proportion of genes lost. Clustering also does not have good result. So I was wondering in this situation, shall I use outer join and fill value to zero? . Another issue is memory use, I'm running this on google colab, and even using TPU, either using combat for batch correction after concatenation or concatetating two subsets of data after batch correction, would take much RAM that it just crashes (there're about 38K cells). Are there any way to limit memory use in this kind of situation? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431:235,variab,variability,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431,1,['variab'],['variability']
Modifiability,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825:262,config,configuration,262,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-784875825,1,['config'],['configuration']
Modifiability,"I've punted on this issue for getting the expression atlas downloader added. I think it'd be worth changing the default data directory at the same time as dealing with configuration more generally, so related breaking changes can happen together. I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. [Everett](https://everett.readthedocs.io/en/latest/index.html) seems nice, but maybe a little immature. I like the ability to use context managers (making testing easier) and the auto documentation features. Generally, I think there should be a longer planning discussion about how configuration works. But that could be multiple issues. For example:. * Could we not change global state for plotting? We could shift over to using the `pyplot.rc_context` manager internally.; * What's the appropriate way to set logging level? It seems to keep changing and breaking things; * What's the appropriate precedence for config setting? I'd think `set in session > environment variable > config file > defaults`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932:168,config,configuration,168,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478214932,6,"['config', 'variab']","['config', 'configuration', 'variable']"
Modifiability,"I've run into the same issue. In my case `adata.layers[""analytic_pearson_residuals""].sum(1)` gives an array of nans because there are nans in `analytic_pearson[""X""]`, as indicated by RuntimeWarning. . I am still only investigating this, but if treating nans as 0 is OK there is numpy.nansum function that could be used instead of sum.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496#issuecomment-1778940190:48,layers,layers,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496#issuecomment-1778940190,1,['layers'],['layers']
Modifiability,"IIRC, you can limit the number of CPUs used through blas. This works on my machine:. ```; export OMP_NUM_THREADS=1; ```. Different blas libraries use different environment variables for this, so I'd check to make sure it's actually restricting the number of threads used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499:172,variab,variables,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621660499,1,['variab'],['variables']
Modifiability,"If a linter flexible enough to enforce this existed, it would be great. The test should definitely exist, something in our code requires the docstrings to have that format, I just forgot which part. (But in any case it guarantees consistent formatting so that’s nice). #1492 should fix that test to ignore blank lines for the time being. Also isn’t it cool that it points exactly to the problematic line?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155:12,flexible,flexible,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1484#issuecomment-725978155,1,['flexible'],['flexible']
Modifiability,"If plots fail on CI, this should let us see the expected, actual, and diff through azure. ~~Hopefully~~ It works!. I think this could make debugging plotting issues much easier. Here's an example of what the results look like:. <img width=""1090"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/104561575-b5814680-569b-11eb-8d1e-a9971affc645.png"">. Current issue, if tests are run in parallel, this does not work (https://github.com/pytest-dev/pytest-nunit/issues/40), which is not an immediate problem for CI, but limits applicability for local usage. I believe this is an issue with this particular pytest plugin, not necessarily this strategy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1587:626,plugin,plugin,626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587,1,['plugin'],['plugin']
Modifiability,"If you try to color a scatter plot by gene expression in a layer, but the layer is sparse, an error is thrown. This occurs on both current release and master. Code to reproduce:. ```python; import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(); pbmc.layers[""sparse""] = pbmc.raw.X; sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse"") ; ```. <details>; <summary> Traceback: </summary>. ```python; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-1-89244dc07987> in <module>; 3 pbmc = sc.datasets.pbmc68k_reduced(); 4 pbmc.layers[""sparse""] = pbmc.raw.X; ----> 5 sc.pl.pca(pbmc, color=[""HES4""], layer=""sparse""). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in pca(adata, **kwargs); 410 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.; 411 """"""; --> 412 return plot_scatter(adata, 'pca', **kwargs); 413 ; 414 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/scanpy/plotting/_tools/scatterplots.py in plot_scatter(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 199 _data_points[:, 0], _data_points[:, 1],; 200 marker=""."", c=color_vector, rasterized=settings._vector_friendly,; --> 201 **kwargs,; 202 ); 203 . ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs); 1587 def inner(ax, *args, data=None, **kwargs):; 1588 if data is None:; -> 1589 return func(ax, *map(sanitize_sequence, args), **kwargs); 1590 ; 1591 bound = new_sig.bind(ax, *args, **kwargs). ~/miniconda3/envs/scanpy-conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidth",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/700:254,layers,layers,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700,2,['layers'],['layers']
Modifiability,"In #458 @fidelram suggested that this would be the way to go. If I put the text into another variable, this variable will only be used once. Does this still make sense? Anyways, I think this is just temporary until `pl.scatter` is in a better shape if I follow @falexwolf correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476509763:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476509763,2,['variab'],['variable']
Modifiability,"In https://github.com/scverse/scanpy/pull/2220, DocSearch was removed from the `latest` docs. Our current theme would probably support it, so we could re-introduce it (https://github.com/pydata/pydata-sphinx-theme/issues/795). @ivirshup how do I get access to our DocSearch account?. PS: There’s more discussion about search plugins supported by our theme here: https://github.com/pydata/pydata-sphinx-theme/issues/202",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2763#issuecomment-1825412324:325,plugin,plugins,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2763#issuecomment-1825412324,1,['plugin'],['plugins']
Modifiability,"In lines 345 - 348, a list of dtypes was getting appended instead of extended. Fixed in PR #1070 ; ```; dtypes.append([; ('highly_variable_nbatches', int),; ('highly_variable_intersection', np.bool_),; ]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726:69,extend,extended,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1033#issuecomment-589868726,1,['extend'],['extended']
Modifiability,"In my preprocessing pipeline, I filtered for the most variable genes and also regress out n_counts and cell cycle. However, it seems like every time I restart my Jupyter notebook and rerun the preprocessing pipeline, I get a different neighborhood graph and UMAP, and therefore different clustering. If I use the same preprocessed data, I can reproduce the same PCA (using svd_solver='arpack'), UMAP, and clusters. So I was wondering if there is randomness introduced during filtering process or regress_out methods? What should I do to ensure reproducibility?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/313:54,variab,variable,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313,1,['variab'],['variable']
Modifiability,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178:106,flexible,flexible,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178,1,['flexible'],['flexible']
Modifiability,"In the help documentation of sc.pp.scale, it is said ""zero_center If `False`, omit zero-centering variables, which allows to handle sparse input efficiently. ; I am still confused about zero_center. If zero_center=False, what will sc.pp.scale do ? Could you give a simple example ? For example, [1,2,3] would be [-1.22,0,1.22] after scaling, but what if zero_center=False ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815:98,variab,variables,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1293207815,1,['variab'],['variables']
Modifiability,"In the scanpy documentation for sc.pl.dotplot, it indicates that it returns a list of matplotlib.axes.Axes. However, this is not true, it returns a gridspec object instead. I noticed this because I wanted to make some subtle edits to the results to enhance the figure such as changing axis labels, adding overlapping lines to delineate the marker genes for each cell type, etc. But I am struggling to do so. I am wondering how the API intends for us to interact with the gridspec object returned to modify the figure. If I edit the code to return the figure object as well as the gridspec, I can access the axis like so. ```; ax = fig.add_subplot(gs[1,0]); ```. but I can't seem to overwrite the default axis labels or add new lines as commands like; ```; ax.set_ylabel('new label'); ```. Don't change the figure at all. This is all Scanpy 1.4.5.post1 but it was the same for 1.4.4.post1. Thanks!. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->. scanpy==1.4.5.post1 anndata==0.6.22.post1 umap==0.4.0 numpy==1.17.2 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. but the same things happened with scanpy==1.4.4.post1; > ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/979:249,enhance,enhance,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/979,1,['enhance'],['enhance']
Modifiability,"In theory Combat knows how to take care of zero variance genes according to the; [code](https://github.com/theislab/scanpy/blob/4156314407c5368fa0b66ac18470d80f3748a71f/scanpy/preprocessing/_combat.py#L124).; Well, post-Combat apparently NaNs are everywhere:; ```; np.sum(np.isnan(adata_Combat.X)); Out[2]: 8089368. np.sum(~np.isnan(adata_Combat.X)); Out[3]: 0; ```; This is really weird if only 3 genes have zero variance, right? Could it have anything to do with this warnings?:; ```; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854:556,variab,variables,556,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616458854,1,['variab'],['variables']
Modifiability,"In theory I think we can do most of that. In practice, I got some errors. I think it would be worth formalizing what the supported interface for doing multimodal analysis is. I'd really like it to be uniform. I could see it being based on keys in `.var`:. ```python; adata.var[""gex""] = adata.var[""expression_type""] == ""Gene Expression""; sc.pl.pca(adata, var_key=""gex""); sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); # This also has the nice feature that it could abstract out the current `use_highly_variable` argument; ```. View based:. ```python; gex_view = adata[:, adata.var[""expression_type""] == ""Gene Expression""]; sc.pp.pca(gex_view) # Calculate pca on gene expression; sc.pl.pca(adata, color=[""Protein1"", ""Protein2""]); ```. Different expression types could be put under `.obsm` (probably the closest ""analogy"" to `SingleCellExperiment`'s `assays()`). But this raises questions of what counts as a variable, and I think would take more work to implement. Of course, there are many other ways this could be done as well. As it could impact APIs throughout `scanpy`, I think input from @falexwolf and @flying-sheep is important here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618:909,variab,variable,909,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/479#issuecomment-464417618,1,['variab'],['variable']
Modifiability,Inherit layer argument in _highly_variable_genes_single_batch when using sc.pp.highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2183:0,Inherit,Inherit,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2183,1,['Inherit'],['Inherit']
Modifiability,Inherit main requirements,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/898:0,Inherit,Inherit,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898,1,['Inherit'],['Inherit']
Modifiability,Insufficient explanation in docs about n_genes_by_counts variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1434:57,variab,variable,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1434,1,['variab'],['variable']
Modifiability,Interesting paper... the question is which is worse false signals from experiment or from the imputation.; Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/653#issuecomment-494737289:511,variab,variability,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653#issuecomment-494737289,1,['variab'],['variability']
Modifiability,"Interestingly, I can't seem to reproduce this even with `pip` on-top of a conda install:. <details>; <summary> me trying </summary>. ```python; isaac@Mimir:~/tmp/genomic-features-docs; $ mamba create -n test-2978 ""anndata==0.9.0"" ipython scanpy; [ ... ]; isaac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:584,enhance,enhanced,584,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,1,['enhance'],['enhanced']
Modifiability,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:1080,flexible,flexible,1080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['flexible'],['flexible']
Modifiability,"Is HVG function designed to be done after regress out?; ```; # Regress out process of interest from expression data; sc.pp.regress_out(adata_b_rn_sub2, keys='LogReg_decision') ; # Find HVGs (across samples, not per sample as samples are very different in terms of beta bio); sc.pp.highly_variable_genes(adata_b_rn_sub2, flavor='cell_ranger', ; batch_key=None, n_top_genes =2000); print('\n','Number of highly variable genes: {:d}'.format(; np.sum(adata_b_rn_sub2.var['highly_variable']))); rcParams['figure.figsize']=(10,5); sc.pl.highly_variable_genes(adata_b_rn_sub2); ```; ![image](https://user-images.githubusercontent.com/47607471/118307048-739dcb00-b4ea-11eb-86fa-904c6b2bf37f.png); If I do scale before HVG; ![image](https://user-images.githubusercontent.com/47607471/118307211-ab0c7780-b4ea-11eb-8c86-44ab1457cb40.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-841394437:409,variab,variable,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-841394437,1,['variab'],['variable']
Modifiability,"Is there anybody meeting the same error with me?; I used de.test.wald() to do differentially expressed genes analysis with totally 47K cells. ### Minimal code sample (that we can copy&paste without having any data). ```python; test_sf = de.test.wald(; data=adata.layers['counts'],; formula_loc=""~ 1 + disease + size_factors"",; factor_loc_totest=""disease"",; as_numeric=['size_factors'],; gene_names=adata.var_names,; sample_description=adata.obs; ); ```. ```pytb; error: 'i' format requires -2147483648 <= number <= 2147483647; ```. #### Versions. <scanpy==1.7.1 anndata==0.7.5 umap==0.4.6 numpy==1.19.2 scipy==1.5.2 pandas==1.1.3 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3>. #### It seems this error happens when cell amount is over 10K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1874:263,layers,layers,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1874,1,['layers'],['layers']
Modifiability,"Isaac says that lobpcg seems much less precise and is probably not worth it, so we should probably just adapt the warning to mention the imprecision and call it a day",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465:104,adapt,adapt,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2419927465,1,['adapt'],['adapt']
Modifiability,"Isaac,. this is great, thank you so much!. Regarding the default for the dataset directory. I like this solution!. Very small edits in addition to what I commented in the code:; * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`?; * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272; * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`?. Notes:; * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread?; * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose.; * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file...; * `pyplot.rc_context` sounds awesome.; * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822:967,config,config,967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822,4,['config'],['config']
Modifiability,Issue with regress_out() and tagging rather than removing highly variable genes?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667:65,variab,variable,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667,1,['variab'],['variable']
Modifiability,Issue with repeated variables in rank_genes_groups_stacked_violin,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/252:20,variab,variables,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/252,1,['variab'],['variables']
Modifiability,"It is common to store raw counts (=unnormalized) of all measured genes under `adata.raw`, while having normalized and unnormalized expression of a subset of genes (might be only protein coding genes, or all genes except ribosomal and mitochondrial etc) at `adata.X` and `adata.layers['counts']` respectively. This however gives rise to a lot of trouble in plotting since visualizing raw counts is not a great idea due to the dynamic range. It is super annoying to pass `use_raw=False` to a lot of functions. Furthermore, weird `rank_genes_groups` outputs as a result of raw counts might go unnoticed because of this. (happened to me many times). I think there was a discussion somewhere about switching to `use_raw=False` by default in all functions, but this may potentially break things since it's a significant behavior change. This might be reasonable for Scanpy 2.0, but not in 1.x I assume. My suggestion is to have a `use_raw` option under `sc.settings` (i.e. the global `ScanpyConfig` instance) which is `None` by default, and can be set to `False` (e.g. `sc.settings.use_raw=False`) which would then affect all the functions with `use_raw` argument. This way we don't break the behavior but still have a reasonable way to turn this thing off :) . Let me know what you think.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798:277,layers,layers,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798,1,['layers'],['layers']
Modifiability,"It is, too bad numpy has no good variable-length string array type. When would bytes make sense? Bytes just mean “data, but I don’t know its structure or am about to write it to disk”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-545906897:33,variab,variable-length,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-545906897,1,['variab'],['variable-length']
Modifiability,"It looks like the issue here is that you have a different number of variables. Here's a small example of what's happening:. ```python; import pandas as pd. a = pd.DataFrame({""bool"": [True, False]}, index=[0, 1]); b = pd.DataFrame(index=[0,1,2]). b[""bool""] = a[""bool""]; b; ```. ```; bool; 0 True; 1 False; 2 NaN; ```. So when you try to subset by `adata.var.highly_variable` you have a bunch of null values in that index, which `AnnData` does not allow (it's not super obvious what the right thing to do here is anyways). What you might want to do is:. ```python; adata = adata[:, adata.var.highly_variable & adata.var.highly_variable.notna()].copy(); ```. or. ```python; adata = adata[:, ACT_sub2.var_names[ACT_sub2.var['highly_variable']]].copy(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095#issuecomment-1015376680:68,variab,variables,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095#issuecomment-1015376680,1,['variab'],['variables']
Modifiability,"It looks like we might not be handling non-expressed genes in all of the highly variable genes implementations. For me this was solved by filtering out genes that were not expressed in any cell!; `sc.pp.filter_genes(adata, min_cells=1)`; If I include a batch_key in the hvg function, I still get the error. I guess in that case you have to ensure that every gene is expressed in every batch? Seems like a bug to fix. _Originally posted by @LisaSikkema in https://github.com/theislab/scanpy/issues/391#issuecomment-870384617_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1910:80,variab,variable,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1910,1,['variab'],['variable']
Modifiability,It seems like `_read_legacy_10x_h5()` invokes ` _collect_datasets()` without taking `genome` into account? Perhaps this was lost during a refactor?; https://github.com/scverse/scanpy/blob/bd06cc3d1e0bd990f6994e54414512fa0b25fea0/scanpy/readwrite.py#L222,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1112677198:138,refactor,refactor,138,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1112677198,1,['refactor'],['refactor']
Modifiability,"It sounds like you have a better idea than me. But I'll give it a shot. How does this look?. ```; color_map : `matplotlib.colors.Colormap` or `str`, optional (default: None); Color map to use for continous variables. Anything that works for `cmap`; argument of `pyplot.scatter` should work here (e.g. `""magma""`, `""viridis""`,; `mpl.cm.cividis`). If `None` value of `mpl.rcParams[""image.cmap""]` is used.; palette : `str`, list of `str`, or `Cycler` optional (default: `None`); Colors to use for plotting categorical annotation groups. The palette can be; a valid `matplotlib.pyplot.colormap` name like `'Set2'` or `'tab20'`, a list; of colors like `['red', '#ccdd11', (0.1, 0.2, 1)]` or a Cycler object.; If `None`, `mpl.rcParams[""axes.prop_cycle""]` is used unless categorical; variable already has colors stored in `adata.uns[""{var}_colors""]`.; ```. I could maybe also mention that passing an argument for palette overwrites the values in `adata.uns[""{var}_colors""]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/477#issuecomment-462730256:206,variab,variables,206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/477#issuecomment-462730256,2,['variab'],"['variable', 'variables']"
Modifiability,"It was configurable with the default `k=10` before, now it uses n_neighbors from `sc.tl.neighbors`.; As discussed with @falexwolf .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1110:7,config,configurable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1110,2,['config'],['configurable']
Modifiability,"It would help to tell *where* in the code the error is thrown. Please provide a traceback. > Does using adata.var_names_make_unique() also makes the variable names of adata.X unique?. If X is a DataFrame, yes. Otherwise X doesn’t have any names stored inside (`var_names` are stored as `.var.index`.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763974778:149,variab,variable,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685#issuecomment-1763974778,1,['variab'],['variable']
Modifiability,"It's `'./write/'`, so it's not a hidden directory - i guess it wouldn't be a good idea to save large files in a hidden fashion; whereas the config was hidden in `'.scanpy/'` - but the latter is not really needed anymore and I could simply remove it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453:140,config,config,140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346321453,1,['config'],['config']
Modifiability,It's convenient to be able to specify multiple variables in combat in the R package. So I added the support for extra covariates (categorical or numeric) and converted some methods to private. There are tests for the new covariate option and also the private _design_matrix function now.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/618:47,variab,variables,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618,1,['variab'],['variables']
Modifiability,"It's definitely a problem that you are seeing all of these version restrictions at once. This may be related to having too many entries in your PYTHONPATH environment variable. `PYTHONPATH` should probably just be empty, since python already knows to look where pip installs packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532:167,variab,variable,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-654682532,1,['variab'],['variable']
Modifiability,"It's not just that the length is different, is that `sc.get.obs_df(adata, [""col""], use_raw=x)[""col""]` is the same regardless of the value of `x`, but it's different for `var_df`. I think it's easier to build code around functions with more orthogonal arguments. > However, I consider that since this option is everywhere it should be here as well. . Could we add an example of `sc.get.var_df(adata.raw, ...)`, leave out `use_raw` for now, and see if anyone complains?. I've been trying to leave out `use_raw` on functions where variable length matters anyways. For example: `adata.var_vector`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337:528,variab,variable,528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1499#issuecomment-731030337,1,['variab'],['variable']
Modifiability,"It's only on master, but I don't think that would change anything here. The fix was for cases where `sc.pp.highly_variable_genes()` outputs an error due to bin boundaries being duplicated as genes were unexpressed. . I reckon this is not actually a bug. It's a possible scenario that no genes are highly variable in all batches, no? Is `highly_variable_intersection` `False` everywhere, or is `highly_variable_nbatches` somehow false? The former can be `False` if your batches are heterogeneous.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108:304,variab,variable,304,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559392108,1,['variab'],['variable']
Modifiability,"It's the right function, but those docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:904,variab,variables,904,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['variab'],['variables']
Modifiability,"I’m not 100% up to date, but if you want more fancy differential expression analysis than what `rank_genes_groups` provides, you should give https://github.com/theislab/diffxpy a shot!. @davidsebfischer it’s maintained, right?. I’m going to close this unless I’m wrong and we want to enhance `rank_genes_groups` after all",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640143156:284,enhance,enhance,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2550#issuecomment-1640143156,1,['enhance'],['enhance']
Modifiability,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```; In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-385-66af52bcd3f3> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace); 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(; 71 obs_metrics[""total_{expr_type}""]); ---> 72 proportions = top_segment_proportions(X, percent_top); 73 # Since there are local loop variables, formatting must occur in their scope; 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns); 182 if not isspmatrix_csr(mtx):; 183 mtx = csr_matrix(mtx); --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns); 185 else:; 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0; ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450:819,variab,variables,819,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450,1,['variab'],['variables']
Modifiability,Just made a pull request that fixes this issue. @brianpenghe . I did some debugging and the variable `num_rows` was incorrectly calculated only when `swap_axes==False` on line 880 of `_anndata.py`. Instead of `num_rows = len(categories)` it should be `num_rows = len(var_names)` . If you make that small change in your _anndata.py in `~/anaconda3/lib/site-packages/scanpy/plotting/_anndata.py` then recompile the packages using `python -m compileall .` and restart python it should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/405#issuecomment-471238684:92,variab,variable,92,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/405#issuecomment-471238684,1,['variab'],['variable']
Modifiability,"Just push things like this on the master branch. :wink: And, consider using `'a string with a variable: {}'.format(variable)` for formatting strings. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/83#issuecomment-364916493,2,['variab'],['variable']
Modifiability,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>; <summary>Script </summary>. ```python; import scanpy as sc; from scanpy.tools._ingest import ingest; import numpy as np; import pandas as pd; from functools import reduce. def simplify_annot(annot):; names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False); unique_names, idxs = np.unique(names, return_index=True); new_annot = annot.iloc[:, idxs].copy(); new_annot.columns = unique_names; return new_annot. def process(dset):; dset.layers[""counts""] = dset.X.copy(); sc.pp.normalize_total(dset); sc.pp.log1p(dset); sc.pp.highly_variable_genes(dset); sc.pp.pca(dset); sc.pp.neighbors(dset, n_neighbors=30); sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) ; dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True); # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]; dsets = [dset1, dset2]; for dset in dsets:; dset.obs = simplify_annot(dset.obs); sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]); dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:; process(dset). # dset1, dset2, dset3 = dsets; dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True); ```. Traceback:. ```python; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest; return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(); File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint; adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))); File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in _",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063:548,layers,layers,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651#issuecomment-519508063,1,['layers'],['layers']
Modifiability,"Just to follow-up because I also had a similar question to the OP. Here's one way to plot several marker genes in different colors on the same UMAP plot. The trick is to make different colormaps that have an alpha gradient so that cells with NA expression appear transparent. Then just use matplot axes to merge the images. The only issue is that Scanpy doesn't yet allow you to remove colorbars for continuous variables, so the multiple colorbars can throw off the scaling, which you can work around by changing the figure parameters. ```; #make red colormap; colors2 = plt.cm.Reds(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap = mymap(np.arange(mymap.N)); my_cmap[:,-1] = np.linspace(0, 1, mymap.N); my_cmap = colors.ListedColormap(my_cmap). sc.pl.umap(adata, color=['AIF1'], use_raw=True, color_map=my_cmap, show=False, frameon=False); ```; ![image](https://user-images.githubusercontent.com/56206488/126086651-df0d46c9-5f1d-4b64-8109-f82cd1feb9cb.png). ```; #make blue colormap; colors2 = plt.cm.Blues(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap2 = mymap(np.arange(mymap.N)); my_cmap2[:,-1] = np.linspace(0, 1, mymap.N); my_cmap2 = colors.ListedColormap(my_cmap2). sc.pl.umap(adata, color=['CD3E'], use_raw=True, color_map=my_cmap2, show=False, frameon=False, vmax=3); ```; ![image](https://user-images.githubusercontent.com/56206488/126086666-a0828d86-d943-47b8-8207-eb42aeb32e4b.png). ```; #make green colormap; colors2 = plt.cm.Greens(np.linspace(0, 1, 128)); colorsComb = np.vstack([colors2]); mymap = colors.LinearSegmentedColormap.from_list('my_colormap', colorsComb); my_cmap3 = mymap(np.arange(mymap.N)); my_cmap3[:,-1] = np.linspace(0, 1, mymap.N); my_cmap3 = colors.ListedColormap(my_cmap3). sc.pl.umap(adata, color=['CD79A'], use_raw=True, color_map=my_cmap3, show=False, frameon=False)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601:411,variab,variables,411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/532#issuecomment-882140601,1,['variab'],['variables']
Modifiability,KeyError: 'dict' … Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297:58,layers,layers,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297,1,['layers'],['layers']
Modifiability,Layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236,1,['Layers'],['Layers']
Modifiability,Layers support for PCA and regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2588:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2588,1,['Layers'],['Layers']
Modifiability,Layers to scatter,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/555:0,Layers,Layers,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/555,1,['Layers'],['Layers']
Modifiability,"Let’s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424995243:46,plugin,plugin,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424995243,1,['plugin'],['plugin']
Modifiability,"Look at the documentation before you ask questions. The object returned from the function you called doesn’t return a matplotlib object, it returns a dictionary, assuming that the ‘show’ parameter is off. You can’t loop through a dictionary like an array, you need to retrieve the keys access individual values and then use the ‘ylim’ property. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: ZxyChopcat ***@***.***>; Sent: Thursday, September 16, 2021 1:24:05 PM; To: theislab/scanpy ***@***.***>; Cc: Vekeria, Jai Patel ***@***.***>; Comment ***@***.***>; Subject: Re: [theislab/scanpy] How to use stacked_violin with variable y-axis limits between rows? (#386). Hi,; I tried to set the y-axis limit, but failed with the error:; `>>> axes = sc.pl.stacked_violin(adata, marker_genes, groupby='cell_types', rotation=90,swap_axes=True,row_palette='muted',yticklabels=True,show=False). for ax in axes:; ... ax.set_ylim(0, 5); ...; Traceback (most recent call last):; File """", line 2, in; AttributeError: 'str' object has no attribute 'set_ylim'; `; I use scanpy 1.8.1.; Do you have any idea? Thanks!. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftheislab%2Fscanpy%2Fissues%2F386%23issuecomment-921089934&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=t3jhsNr2Q3IlftHnubs6%2FWZyy%2FAijC2BWJ18Ih41Py0%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAL6KD25HVPRX7SK4DD5UPE3UCIR3LANCNFSM4GH7A7BA&data=04%7C01%7Cjai.vekeria%40pitt.edu%7C4da79e06909d45b4b4e508d97936c8d4%7C9ef9f489e0a04eeb87cc3a526112fd0d%7C1%7C0%7C637674098542578553%7CUnknown%7CTWF",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209:656,variab,variable,656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-921104209,1,['variab'],['variable']
Modifiability,"Looking at this again, now that I have gone through everything, I think we actually need to check types directly and shouldn't rely on `isbacked` because it is possible to do something like `adata.layers['foo'] = sparse_dataset(g_layer)` and this should also error our with a helpful message.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455:197,layers,layers,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3048#issuecomment-2107583455,1,['layers'],['layers']
Modifiability,"Looks good! Remaining questions:. - The plan is to add the Visium reading function to `anndata`, right?; - You’re repeating yourself with the docs: `doc_scatter_basic` (and therefore `doc_scatter_embedding`) and the docstring of `pl.spatial` both contain similar text for the same parameters. If you want to reorder them, you could do something fancy (like slicing doc_scatter_embedding) or just mention the parameter names in the free text, something like:. ```restructuredtext; Scatter plot in spatial coordinates. Use the parameter `img_key` to see the microscopy image in the background.; Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed,; and `scale_spot` to control the size of the Visium spots plotted on top.; ```. - Is it possible to derive the amount of cropping? Then we could extend the `crop_coord` parameter to this:. ```py; Union[; Iterable[Literal['left', 'l', 'right', 'r', 'top', 't', 'bottom', 'b']],; Tuple[int, int, int, int], # l, r, t, b; ]; ```. - Maybe it makes sense to add some test data and a test plot? (very low res of course)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703:810,extend,extend,810,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1012#issuecomment-578688703,1,['extend'],['extend']
Modifiability,"Looks like a really cool idea, and undoubtedly inspired by some existing class-based plotting solution. Scanpy's plots are of course more domain-specific than the generic APIs in Plotnine, Plotly or Altair. Nevertheless, there seem to be quite some generic APIs in your classes like `add_legend` and so on. Before I review this proper: Did you think about using and extending a generic class-based plotting solution? If not: Could you investigate if any of them can be extended so we don't have to maintain the non-domain-specific parts of the API? If yes: What makes them unsuited for our needs?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085:366,extend,extending,366,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127#issuecomment-607813085,2,['extend'],"['extended', 'extending']"
Modifiability,"Looks very good to me, thank you very much!. Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'?. In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105:221,extend,extend,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429445105,1,['extend'],['extend']
Modifiability,"Louvain is being difficult to build since a new setuptools release dropped any python2 compatibility https://github.com/vtraag/louvain-igraph/issues/57. We've largely worked around this in #2063, by making louvain dependent tests optional. However, the paul15 PAGA test is difficult to extract louvian from. It checks hardcoded values based on the results of a louvain clustering. To adapt this test to use leiden, we would have to redo the tutorial and create new results. Or louvain building could be fixed, but the package is deprecated anyways.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2065:384,adapt,adapt,384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2065,1,['adapt'],['adapt']
Modifiability,"Make sure you are searching the `conda-forge` channel, too. ; Either `conda install -c conda-forge -c bioconda scanpy` or [configure the default channels](https://bioconda.github.io/user/install.html?highlight=conda%20forge#set-up-channels).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004:123,config,configure,123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142#issuecomment-613475004,1,['config'],['configure']
Modifiability,Making `scores` parameterized,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1152:16,parameteriz,parameterized,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152,1,['parameteriz'],['parameterized']
Modifiability,"Matplotlib takes a while but less time. Can you please point me to what you mean with the environment variables?. No idea about tables, @falexwolf wrote the sim module I think and it’s not commonly used …. I don’t think import times change noticably, but I didn’t measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-522595568:102,variab,variables,102,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-522595568,1,['variab'],['variables']
Modifiability,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:308,variab,variable,308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577,1,['variab'],['variable']
Modifiability,More flexible umap,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1132:5,flexible,flexible,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1132,1,['flexible'],['flexible']
Modifiability,"My opinion would be that you need to write `adata.raw = adata.copy()` if you want a copy to be made, since almost all assignments do not create a copy of the assigned object in anndata. But we should look into whether this is a change that was made deliberately or not. If we don't change it, we could maybe warn if we're mutating `adata.X` and `adata.raw.X` also refers to the same thing?. Overall, I would recommend that you use `adata.layers[""counts""] = adata.X.copy()` instead of using `.raw` at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2150583605:438,layers,layers,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2150583605,1,['layers'],['layers']
Modifiability,"My primary wish here is very simple. I'd like the following sequence of commands:; ```; sc.pp.neighbors(adata); sc.tl.tsne(adata); ```; to produce a reasonable t-SNE result that could be called ""t-SNE"" in publications. What you suggest @ivirshup (t-SNE on normalized UMAP affinities) could maybe achieve that, but we would need to check. As I said, I don't think anybody ever has tried that. I could imagine that it would roughly correspond to t-SNE with perplexity less than 30, perhaps 20 or so, but this is just a wild guess. . I am worried that it may be a bit weird to refer to this as ""t-SNE"" in publications, because it's really t-SNE on normalized UMAP affinities which is an odd-sounding hybrid. But if the result is similar enough to t-SNE, then maybe it's okay to call it simply ""t-SNE (as implemented in Scanpy)""... A *separate* question is how a user would be able to achieve t-SNE *proper*, and here I could live with either; ```; sc.pp.neighbors(adata, method='tsne') # this would use perplexity=30 by default; sc.tl.tsne(adata); ```; or; ```; sc.pp.neighbors_tsne(adata); sc.tl.tsne(adata); ```; This is just a question of API, and is less important for me personally. I agree that it could be better to have `neighbors()` compute kNN adjacency matrix without computing any weights, but this is refactoring beyond the scope of this PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738:1311,refactor,refactoring,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-762282738,1,['refactor'],['refactoring']
Modifiability,New plot function to draw the relations between 2 categorical adata.obs variables?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2797:72,variab,variables,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2797,1,['variab'],['variables']
Modifiability,"New version of `uns` as discussed in https://github.com/theislab/anndata/issues/295#issuecomment-596164456. The reason for setting a dummy `library_id` (as ""0"" for instance) is:; * The `library_id` information is in the molecule_info file [explained here](https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/molecule_info). Recover it would require to input an additional file path; * All the 10x data I had a look at, both from the same tissue (so 2 slides from same tissue slice) and from different tissues, have `library_id` entry set to ""0"". So I am not really sure how space ranger set that value but it does not appear to be unique (and therefore not a natural `batch_key` value).; * I think it would be more useful if this value is set according to user choice. Only in the context of `adata.concatenate` it should be modified according to `batch_key`. This is also the only point in the analysis where the `library_id` entry matter.; * I agree with respect to maintaining the tree structure before and after concatenation, so the reason for keeping the `library_id` entry and setting it to a dummy variable by default. Looking forward to hear what you think @ivirshup and if agree I'll go on with PRs for `anndata.concatenate`. . Also, let's keep this `spatial` branch open until we really have (almost) everything up and running for spatial analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1105:1143,variab,variable,1143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1105,1,['variab'],['variable']
Modifiability,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:58,variab,variables,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,4,['variab'],['variables']
Modifiability,No worries and thank you for usually very prompt suggestions.; The idea that scanpy can handle many cells efficiently is great and therefore I have been trying it in a computing cluster (and not my local machine) for the future usage. This in turn makes configuration just a bit more difficult. ; Looking forward to a more stable version with more added function.; Thank you; Hashem,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466:254,config,configuration,254,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324641466,1,['config'],['configuration']
Modifiability,"No, I meant an argument like `over='cells'` that would default to `.obs` covariates. So explicitly telling the function. Implicit would be nice, but you pointed out that it would require guessing, which has its own issues. I'm on the fence about your solution of doing neither and requiring the user to use `adata.T`. I do see your rationale behind 'variables' and 'observations' though. I'm just not entirely sure that is clear to the user in the same way it is clear to the developer. As a user I see cells and genes in my dataset and may not be aware that one of them are treated as the variables that describe the other. Then the question is: do you want to be as user-friendly as possible (I'll call it 'the R way') or stick with consistent conventions that may not be clear to everyone ('the numpy way'?). Both can cause frustrations and both have benefits.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483:350,variab,variables,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441253483,2,['variab'],['variables']
Modifiability,"No, it currently doesn't. Instead it uses the `scores`... usually some ""differential z-score"" that goes into the t-test. We will extend differential testing in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402:129,extend,extend,129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/159#issuecomment-390656402,1,['extend'],['extend']
Modifiability,"No, not at all. It depends how you calculate highly variable genes. If you don't use the `batch` parameter, then it always works fine. If you use the `batch` parameter, it outputs `adata.var['highly_variable_genes_intersection']` and `adata.var['highly_variable_genes_nbatches']` which is information on how many batches a particular HVG is shared by. In the intersection field the genes are labelled as `True` that are shared by all batches. If no HVG is shared by all batches, this will be `False` for all genes. In that case you can define `adata.var['highly_variable']` as you like... usually you would do that depending on the nbatches output.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621:52,variab,variable,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935#issuecomment-559558621,1,['variab'],['variable']
Modifiability,"No, there is no way within Scanpy. I'll talk to Philipp about the find_sigmas function... and get back to you. My personal opinion is that in a wide range of values, the qualitative (significant) results should be independent of the value of k. The default value of k=30, meaning that we construct a k-nearest neighbor graph in which each cell is connected with 30 neighbors, yields good results on all data sets (>10) that I worked with so far. If you have very little noise, for example, by selecting only very few highly variable genes in the preprocessing, you might obtain a more ""pronounced structure"" by reducing k (I'd recommend at least 3, though). Also with very noisy data, k=30 should be high enough to average out noise effects. To summarize, k=30 is a conservative choice that in my experience does the job for everything. In some cases, it pays off to reduce the value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879:524,variab,variable,524,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-309980879,1,['variab'],['variable']
Modifiability,"No, there should not be any reason that is associated with a small number of genes per se. In the moignard15 example, everything works for 40 genes; in the toggleswitch, everything works for 2 genes. Does your PCA look meaningful? Try supplying a very small number of PCs to DPT (`n_pcs=3` or so). If you do not find significant genes with `filter_genes_dispersion`, you have to adapt the parameters [e.g. set `min_disp` to a lower value](https://github.com/theislab/scanpy/blob/2cea8341e28eb8d0658f62d010631f77465e16d7/scanpy/preprocessing/simple.py#L132-L177). See the example [here](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Alternatively, you can simply select the `n_top_genes` highest variabale genes by setting `flavor` to `'cell_ranger'`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910:379,adapt,adapt,379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/25#issuecomment-313320910,2,"['adapt', 'variab']","['adapt', 'variabale']"
Modifiability,"Noglob turns off all globbing though. Would be great if one could turn off just Extended globbing for a command. After all, `pip install *.whl` could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008:80,Extend,Extended,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1441#issuecomment-703437008,1,['Extend'],['Extended']
Modifiability,"Notes from discussion with @Intron7, @flying-sheep, @gtca, and @grst at hackathon:. * Updated idea from @gtca, based on: https://github.com/scverse/anndata/issues/706; * Use `layer_to`, `layer_from`as argument. Has possibility to still do operations inplace on arrays if you only pass `layer_from`; * Could be useful to have semantically meaningful default arguments e.g. `layers_from=""counts""``layers_to=""normalized""`; * Returning a new `AnnData` object with only new arrays could be a flexible base, as discussed in https://github.com/scverse/anndata/issues/658; * `inplace=False` returning function specific types (sometimes an array, sometimes a dict of arrays) is bad. This would be an alternative. * Being able to update arrays inplace is still important for memory usage. * Lots of discussion of when/ how we want to modify the AnnData",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852:487,flexible,flexible,487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2583#issuecomment-1664211852,1,['flexible'],['flexible']
Modifiability,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583:82,flexible,flexible,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583,1,['flexible'],['flexible']
Modifiability,"OK, got the formatting issues. Let's discuss at the office. Let's stick with `None`, this just requires to rewrite a very small number of strings... will not be a problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404268015:107,rewrite,rewrite,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404268015,1,['rewrite'],['rewrite']
Modifiability,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:290,enhance,enhancement,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['enhance'],['enhancement']
Modifiability,"Of course! would be wild if the plotting would internally transpose the anndata object in case one of the provided `keys` exists in `.var`. `sc.pl.violin(adata.T, 'key')` is 100% the right thing to do. I think the docs are a bit improvable though:. > *keys* : str or list of str; > &emsp;Keys for accessing variables of .var_names or fields of .obs. The mention of `var_names` here means that you can select one or more genes to plot. How can we phrase that better? Maybe we should also add an example that uses transposing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129:307,variab,variables,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375#issuecomment-441056129,1,['variab'],['variables']
Modifiability,Oh! I keep forgetting to ask: Why did the original scanpy config file get removed?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479745339:58,config,config,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479745339,1,['config'],['config']
Modifiability,"Ok so you are probably using this: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html. you can check the definition in the docs:; ```; For the dispersion-based methods ([Satija15] and [Zheng17]), the normalized dispersion is obtained by scaling with the mean and standard deviation of the dispersions for genes falling into a given bin for mean expression of genes. This means that for each bin of mean expression, highly variable genes are selected.; ```. this slightly changes according to the `flavour`, all citations are also mentioned in the docs (link above). does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521:452,variab,variable,452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1803#issuecomment-827063521,1,['variab'],['variable']
Modifiability,"Ok, makes sense.; What if I implement that `cmap `in `embedding` also accepts a `dict ` of `{variable: colormap}`?. `{'n_counts_all': 'copper', 'n_genes_cmap': matplotlib.colors.Colormap}`. It maps variable names to `str ` or `Colormap`. Therefore, the colormap can be processed before and is not stored in AnnData.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081:93,variab,variable,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489#issuecomment-729656081,2,['variab'],['variable']
Modifiability,Okay... scvelo uses `adata.uns['velocity_settings']['embeddings'].extend()` which only works on lists. @VolkerBergen shall i report this again in the `scvelo` repo or is this sufficient for you?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/887#issuecomment-545948736:66,extend,extend,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887#issuecomment-545948736,1,['extend'],['extend']
Modifiability,"One benefit of the newer scanpy versions is that calling highly_variable_genes() marks them as 'highly_variable' rather than removes them by default. Later steps (like PCA) use these tags and leave the rest of the data intact. This is great. Following the [pbmc3k workflow](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html) though still has a step which requires you to remove all non-highly-variable genes before continuing:. `adata = adata[:, adata.var['highly_variable']]`. If I do this, things work fine, but if I skip it then the next regress_out step fails with:. `ValueError: The first guess on the deviance function returned a nan. This could be a boundary problem and should be reported.`. I found discussions like [this one](https://github.com/theislab/scanpy/issues/230) where it is suggested a column of 0s might be the issue, but I followed this workflow directly which included these steps:. ```; sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); ```. Is there a best practice (perhaps something along the line of @LuckyMD 's suggestion in issue #492 ?) to handle this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/667:405,variab,variable,405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667,1,['variab'],['variable']
Modifiability,"One of the aims of scanpy is to be self-contained and easy-to-install for users and also to be easy to maintain by the developers. Heavy dependencies like louvain and python-igraph are already troublesome, expecting users to have rpy2 + proper R installation + Bioconductor + scran would risk smooth user experience and easy maintainability. I was wondering whether it makes sense to have a community-maintained `scanpy-contrib` or `scanpy-extensions` repository (and python package) similar to https://github.com/keras-team/keras-contrib ? There are also couple of things I have in mind like `sc.pl.netsne(adata, anotheradata)` for embedding unseen samples via parametric tSNE, or `sc.tl.simlr` and `sc.pl.simlr` for [SIMLR](https://github.com/BatzoglouLabSU/SIMLR) via RPy2 bridge... . These are popular requests for Scanpy and people expect the same convenient API and an easy integration with AnnData objects. However, they will probably not be included in the mainstream Scanpy because of the reasons I mentioned above. What do you think @falexwolf and @flying-sheep ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880:325,maintainab,maintainability,325,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-381980880,1,['maintainab'],['maintainability']
Modifiability,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:297,config,configure,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136,1,['config'],['configure']
Modifiability,"One thing to note:; if the categories of the groupby variable (clusters in this case) don't match up with the categories in the marker_genes_dict, there will be no matching if colors between rows and columns. This is what also happens in the tutorial, where; marker_genes_dict = {'NK': ['GNLY', 'NKG7'],; 'T-cell': ['CD3D'],; 'B-cell': ['CD79A', 'MS4A1'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; while the clusters are numbers (""1"", ""2"" etc.).; We could consider throwing a warning in that case, but I don't think it's necessary.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826:53,variab,variable,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734837826,1,['variab'],['variable']
Modifiability,PCA fails with batch highly-variable gene correction,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:28,variab,variable,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,1,['variab'],['variable']
Modifiability,"PYTHON_VERSION variable is empty, so we actually pass `python=` in `conda create` so Travis always tests scanpy with latest Python in Conda distribution. Therefore Python 3.5 is actually never tested. Furthermore, conda switched to python 3.7, so now all test are run on Python 3.7. This is also the reason of weird HDF error message we get in tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/201:15,variab,variable,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/201,1,['variab'],['variable']
Modifiability,"Pass(state.func_ir,; 297 state.typemap,; 298 state.calltypes,; (...); 304 state.metadata,; 305 state.parfor_diagnostics); --> 306 parfor_pass.run(); 308 # check the parfor pass worked and warn if it didn't; 309 has_parfor = False. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:2926, in ParforPass.run(self); 2924 # Validate reduction in parfors.; 2925 for p in parfors:; -> 2926 get_parfor_reductions(self.func_ir, p, p.params, self.calltypes); 2928 # Validate parameters:; 2929 for p in parfors:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3549, in get_parfor_reductions(func_ir, parfor, parfor_params, calltypes, reductions, reduce_varnames, param_uses, param_nodes, var_to_param); 3547 if param_name in used_vars and param_name not in reduce_varnames:; 3548 param_nodes[param].reverse(); -> 3549 reduce_nodes = get_reduce_nodes(param, param_nodes[param], func_ir); 3550 # Certain kinds of ill-formed Python (like potentially undefined; 3551 # variables) in combination with SSA can make things look like; 3552 # reductions except that they don't have reduction operators.; 3553 # If we get to this point but don't find a reduction operator; 3554 # then assume it is this situation and just don't treat this; 3555 # variable as a reduction.; 3556 if reduce_nodes is not None:. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3637, in get_reduce_nodes(reduction_node, nodes, func_ir); 3635 defs[lhs.name] = rhs; 3636 if isinstance(rhs, ir.Var) and rhs.name in defs:; -> 3637 rhs = lookup(rhs); 3638 if isinstance(rhs, ir.Expr):; 3639 in_vars = set(lookup(v, True).name for v in rhs.list_vars()). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/parfors/parfor.py:3627, in get_reduce_nodes.<locals>.lookup(var, varonly); 3625 val = defs.get(var.name, None); 3626 if isinstance(val, ir.Var):; -> 3627 return lookup(val); 3628 else:; 3629 return var if (varonly or val is None) else",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:9848,variab,variables,9848,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['variab'],['variables']
Modifiability,"Please adapt the corresponding test to:. ```; @pytest.mark.parametrize(""flavor"", [""default"", ""use_fastpp""]); def test_scale(flavor):; adata = pbmc68k_reduced(); adata.X = adata.raw.X; v = adata[:, 0 : adata.shape[1] // 2]; # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965; assert v.is_view; with pytest.warns(Warning, match=""view""):; sc.pp.scale(v, flavor=flavor); assert not v.is_view; assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01); assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001); ```. It fails for me with `FAILED scanpy/tests/test_preprocessing.py::test_scale[use_fastpp] - numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267:7,adapt,adapt,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540014267,1,['adapt'],['adapt']
Modifiability,"Please add the relevant part of `jupyter lab`’s log. If it’s a SEGFAULT, please reproduce with the [`PYTHONFAULTHANDLER`](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONFAULTHANDLER) env variable set to a non-empty string to get a traceback",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840#issuecomment-1929143068:198,variab,variable,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840#issuecomment-1929143068,1,['variab'],['variable']
Modifiability,"Please ask questions on https://discourse.scverse.org/. See the Note in the 2017 tutorial:. > [!NOTE]; > ; > If you don’t proceed below with correcting the data with `sc.pp.regress_out` and scaling it via `sc.pp.scale`, you can also get away without using `.raw` at all.; > ; > The result of the previous highly-variable-genes detection is stored as an annotation in `.var.highly_variable` and auto-detected by PCA and hence, `sc.pp.neighbors` and subsequent manifold/graph tools. In that case, the step actually do the filtering below is unnecessary, too. Since data sizes these days are big enough that a sparse .X is all but necessary (`pp.scale` densifies data), and methods exist that work with unscaled expression values and therefore don‘t need scaling, people tend to not do it these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3095#issuecomment-2154540758:312,variab,variable-genes,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3095#issuecomment-2154540758,1,['variab'],['variable-genes']
Modifiability,Plot of ranked genes groups with non-raw data or layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438:49,layers,layers,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438,1,['layers'],['layers']
Modifiability,Plotting config files,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:9,config,config,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,1,['config'],['config']
Modifiability,"Poetry is great! But i remember two problems:. 1. no good way to editably install into some env: python-poetry/poetry#34; 2. doesn’t support plugins yet so only hardcoded versions in static metadata: python-poetry/poetry#140. Maybe @ivirshup knows more. We talked about it way back when. The things I’m missing from flit are better dynamic version support (currently a bit hacky, but a PR exists) and sth. like `poetry install --no-root`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746:141,plugin,plugins,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-764971746,1,['plugin'],['plugins']
Modifiability,Possible enhancement: multithreaded (via numba) mann-whitney tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060:9,enhance,enhancement,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060,1,['enhance'],['enhancement']
Modifiability,"Pytest supports two test layouts, in-package and outside-of-package. I prefer the outside-of-package mode outlined here: https://docs.pytest.org/en/6.2.x/goodpractices.html. Scanpy currently mixes test utils with tests, but pytest’s test files (`test_*.py` and `conftest.py`) aren’t Python modules one is supposed to import from. To clean things up, we can refactor scanpy to a in-package structure:. - `pyproject.toml`: add `addopts = ['--import-mode=importlib']` to `[tool.pytest.ini_options]`; - `scanpy/tests/__init__.py` during implementation, make it throw an error on import so we can make sure nobody imports things from there, then delete; - `scanpy/tests/**/__init__.py` delete; - `scanpy/test_utils/` or `scanpy/testing/`; - `__init__.py`: leave empty for now, later add public, documented test utils; - `_private.py` add private test utils that can be imported in our tests, such as the `@needs_somepackage` decorators. Later we can decide if we want to keep the in-package layout or switch to the outside-of-package layout",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2225:357,refactor,refactor,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2225,1,['refactor'],['refactor']
Modifiability,Quite a simple addition meant to fix a bug when one works with specific layers.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2183:72,layers,layers,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2183,1,['layers'],['layers']
Modifiability,"Recently I need to run PCA on multiple layers of AnnData. If no one is working on this, I can work on a pull request for this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960:39,layers,layers,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-2458452960,1,['layers'],['layers']
Modifiability,"Ref https://github.com/theislab/scanpy/issues/1698#issuecomment-824634639, cut down in https://github.com/theislab/scanpy/issues/1698#issuecomment-826712541. Basically, `gearys_c`, `morans_i` can run into silent numba parallelization errors when all values for a variable are the same. We should specifically address these cases. As suggested by @Hrovatin, `nan` and a warning is probably sufficient for this case. That said, it may take some work to consistently throw a warning. * Depending on the threading backend, this may error (solving the problem of incorrect values); * We may be able to control this, but how the parallel backend interacts with the `error_model` seems unclear; * I don't think we can throw a warning from numba code, let alone parallel numba code; * It would be nice if we could guarantee a `tbb` or `omp` threading backend, since they seem more consistent, but I personally have not been able to `pip install` these for a bit over a year now. This may be macOS specific.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1806:263,variab,variable,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1806,1,['variab'],['variable']
Modifiability,Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316:0,Refactor,Refactor,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316,1,['Refactor'],['Refactor']
Modifiability,Refactor score_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3170:0,Refactor,Refactor,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3170,1,['Refactor'],['Refactor']
Modifiability,Refactoring t-test default warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2798:0,Refactor,Refactoring,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2798,1,['Refactor'],['Refactoring']
Modifiability,"Regarding your previous question: Scanpy currently has its own UMAP version as we needed a few more things and wanted to freeze a version. In Scanpy 2, we might change this and use the then more evolved UMAP package instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398682339:195,evolve,evolved,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398682339,1,['evolve'],['evolved']
Modifiability,Related question - is it necessary to do. `adata.layers['counts']=adata.X.copy()`. or is:. `adata.layers['counts']=adata.X` sufficient?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413237366:49,layers,layers,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1413237366,2,['layers'],['layers']
Modifiability,"Removes the need to specify a genome string in 10x h5 files by default. This removes a personal annoyance of mine, where I had to figure out what the reference was called when there's often only one reference used per file. For cellranger `v3.0.0+` files, specifying a genome acts as a filter on input, as it did already. However, it doesn't only act if `gex_only` is `True`. Additionally, the behavior of `gex_only` has been changed to fit the documentation, i.e. it just filters for gene expression variables. For legacy files:. * If the file only has one genome group, that one is used by default; * If multiple genomes are found and the user did not specify one, an error will be thrown. This is because there are no structural assurances the genomes will match to the same samples. As the behavior of the function has meaningfully changed, this is a breaking change (though I'd be surprised if it affected many people). Personally, I haven't seen many 10x files which contain multiple genomes, so I'd appreciate feedback or examples from people who have.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/442:501,variab,variables,501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/442,1,['variab'],['variables']
Modifiability,"Reproducible example:. ```python; import scanpy as sc; import scanpy.external as ice; from itertools import cycle. pbmc = sc.datasets.pbmc68k_reduced(); sce.pp.mnn_correct(pbmc, batch_key=""phase""); ```. It looks like `mnn_correct` is only returning one variable, through its documentation looks like it should return three. @chriscainx, could you offer some guidance here?. As a workaround for now, you could just call `mnnpy.mnn_correct` with the same signature you've been using. It'll return a one-tuple with a modified anndata object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637:253,variab,variable,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/757#issuecomment-516793637,1,['variab'],['variable']
Modifiability,"Reviewed harmonization paying attention to some more details. What do you think of using the `|` separator to describe `adata.X | adata.layers[layer]` e.g. [here](https://icb-scanpy--2742.com.readthedocs.build/en/2742/generated/scanpy.pp.regress_out.html)?. Some things causing some sort of heterogeneity and are NOT taken care of here:; - the inconsistent and mixed use of `inplace` and `copy` (effort: lot of work); - some inconsistent use of `key_added` & flavours thereof, which affect the return section (effort: medium amount of work). What is also not taken care of here:; - Other small things, such as [ingest](https://scanpy.readthedocs.io/en/latest/generated/scanpy.tl.ingest.html) not having a `return_joint` argument although this is mentioned in its doc. Might raise smaller issues in the future for these specific things rather than bloating this purpose-driven PR up?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2742#issuecomment-1812943870:136,layers,layers,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2742#issuecomment-1812943870,1,['layers'],['layers']
Modifiability,"Right now, if you specify `groupby` to `sc.tl.dendrogram`, but not `dendrogram_key`, storage (and subsequent retrieval) from `adata.uns` is messed up because `groupby` is converted from `str` --> `list` during computation. To give a more concrete example, if my `groupby` variable was ""cell_subtype"", I would expect it to be stored in `adata.uns` as ""dendrogram_cell_subtype"". However, because of the list conversion it's stored as ""dendrogram_['cell_subtype']"" (shown below). ![image](https://user-images.githubusercontent.com/4998310/96769236-d5f77880-13ac-11eb-947f-3dbcf7069d82.png). This PR attempts to address that. I'm not sure if this is the way you want to go about fixing it, but it's one option. Thanks for the great package!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1465:272,variab,variable,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1465,1,['variab'],['variable']
Modifiability,"Running `sc.pl.paga(adata)` in v1.4 returns an error:; ```; Traceback (most recent call last):. File ""<ipython-input-412-3baa85828ec9>"", line 1, in <module>; sc.pl.paga(adata). File ""/path/to/scanpy/scanpy/plotting/_tools/paga.py"", line 445, in paga; adjacency_solid, layout=layout, random_state=random_state, init_pos=init_pos, layout_kwds=layout_kwds, adj_tree=adj_tree, root=root). UnboundLocalError: local variable 'adj_tree' referenced before assignment; ```. There is a conditional before the referenced line, which assigns value to `adj_tree`, and indeed, running these works fine:; ```; sc.pl.paga(adata, layout='rt'); sc.pl.paga(adata, layout='rt_circular'); sc.pl.paga(adata, layout='eq_tree'); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/487:410,variab,variable,410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/487,1,['variab'],['variable']
Modifiability,Running and saving PCA on different anndata layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301:44,layers,layers,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301,1,['layers'],['layers']
Modifiability,"Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; (0:01:39); running recipe zheng17; filtered out 3983 genes that are detectedin less than 1 counts; Killed; ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/811:233,Variab,Variable,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811,1,['Variab'],['Variable']
Modifiability,"Scanpy does have logging implemented (examples: [neighbors](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/neighbors/__init__.py#L84), [highly variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:184,variab,variable,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,1,['variab'],['variable']
Modifiability,"Scanpy has enhanced sc.pl.umap function last year. For example, now sc.pl.umap(adata,color=[""louvain""],groups=""1"") can highligt cluster 1 while displaying other clusters in gray color. I think they are very similar, excepting that gene expressing values are continuous variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721:11,enhance,enhanced,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1550#issuecomment-748025721,2,"['enhance', 'variab']","['enhanced', 'variables']"
Modifiability,"See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3239:52,extend,extending,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239,1,['extend'],['extending']
Modifiability,Set up Azure Pipelines with initial configuration,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516:36,config,configuration,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516,1,['config'],['configuration']
Modifiability,Seurat v3 VST highly variable gene method,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993:21,variab,variable,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993,1,['variab'],['variable']
Modifiability,Should be fixed by: https://github.com/algolia/docsearch-configs/pull/4840,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336:57,config,configs,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2047#issuecomment-969076336,1,['config'],['configs']
Modifiability,"Simple test case; ```; data = sc.read(""pbmc3k.h5ad""); logical_ar = data.var[""name""] == ""RER1""; df = data[:, logical_ar]; df.uns = data.uns # this causes an error ; ```. Causes this error; ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-16-8b2cadedfe9b> in <module>(); 1 l = data.var[""name""] == ""RER1""; 2 df = data[:, l]; ----> 3 df.uns = data.uns. /usr/local/lib/python3.6/site-packages/anndata/base.py in uns(self, value); 987 # here, we directly generate the copy; 988 adata = self._adata_ref._getitem_copy((self._oidx, self._vidx)); --> 989 self._init_as_actual(adata); 990 self._uns = value; 991 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, shape, filename, filemode); 816 self._varm = BoundRecArr(varm, self, 'varm'); 817 ; --> 818 self._check_dimensions(); 819 self._check_uniqueness(); 820 . /usr/local/lib/python3.6/site-packages/anndata/base.py in _check_dimensions(self, key); 1692 raise ValueError('Observations annot. `obs` must have number of '; 1693 'rows of `X` ({}), but has {} rows.'; -> 1694 .format(self._n_obs, self._obs.shape[0])); 1695 if 'var' in key and len(self._var) != self._n_vars:; 1696 raise ValueError('Variables annot. `var` must have number of '. ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323:1308,Variab,Variables,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323,1,['Variab'],['Variables']
Modifiability,"So admittedly this is mostly a philosophical objection, it's probably unlikely to cause any computational problems. The main practical issue I have is converting between Python and R where this causes the type of columns to change. I realise this is kinda a niche problem though and doesn't practically make a lot of difference. My main two philosophically objects are:. 1. Users should have control over what type things are (unless this is required for computational reasons); 2. Functions shouldn't have side effects (i.e. I expect the `highly_variable_genes()` function to calculate the highly variable genes, not do that AND modify a bunch of unrelated columns in `obs`/`var`). I can see the potential computational benefits in some cases but I wonder if that is actually true for everywhere it is used. It seems mostly like a case of ""this might potentially make a difference so let's do it just in case"". Where it does matter it's probably possible to get the same benefit without making permanent changes to the object. (Also I realise this is a pretty minor thing so not trying to make a big deal out of it. It annoys me but maybe not worth making major changes for 😝.)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479:598,variab,variable,598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-800904479,1,['variab'],['variable']
Modifiability,"So far as I can tell, any further downstream operations also acts on layers... so it is not useful to store raw counts there since they will just be modified with counts normalization, log normalization, etc. Storing things in layers sequentially, I just end up with a bunch of layers that all are identically fully processed rather than preserving the raw-er aspect of the counts matrix. Not sure if this is new behavior but it is super frustrating",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-2070663668:69,layers,layers,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-2070663668,3,['layers'],['layers']
Modifiability,"So it seems that in every case, no matter what array type is given to `andata.X`, the `counts_per_cell` variable generated in `normalize_total()` is always being created as a numpy array. So I'm not sure why there was a note next to the line in `_normalize_data()` about not being able to use dask, because the input counts here are always numpy (because they've been created already in `normalize_total()`). Presumably this is not intended?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308:104,variab,variable,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1663#issuecomment-784836308,1,['variab'],['variable']
Modifiability,So one should always use .copy() when creating new layers from .X?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389#issuecomment-1413251794:51,layers,layers,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389#issuecomment-1413251794,1,['layers'],['layers']
Modifiability,"So one small update here -- it works like a charm for categorical variables, but not for continuous variables.; e.g.; > sc.pl.umap(testData, save = fileName, color='CCL5',s=50,frameon=False,legend_loc = None). Still gives something like a legend:; ![image](https://user-images.githubusercontent.com/10536275/99786010-40234a80-2b1e-11eb-83ab-77c9341dab05.png). Presumably this is because the color strip on the right is not actually a legend in the underlying matplotlib?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768:66,variab,variables,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1502#issuecomment-731065768,2,['variab'],['variables']
Modifiability,"So quick! thanks. I have been using the dendrograms for a while so; hopefully not so many bugs appear. Something that I wanted to have for; discussion is on some parameters relevant for the dendrogram, like the; genes used, the correlation method and the linkage method. All this can be; modified but currently is hard coded as I didn't want to add 3 more; parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I; thought about setting some variables like the rcParams for matplotlib but I; think is not justified for just 3 parameters and can be very confusing. Or; we can have a function to compute a dendrogram with all parameters; required, and save this in .uns like rank_genes_groups. Then if other; functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430667617:494,variab,variables,494,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430667617,1,['variab'],['variables']
Modifiability,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:159,config,configure,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,3,['config'],"['configure', 'configured']"
Modifiability,"So this is possibly related to #1136 (pure speculation 😅 ). Basically, on a Vm with ubuntu 18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:357,flexible,flexible,357,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,2,['flexible'],['flexible']
Modifiability,So what is now the recommended way of calculating and plotting PCA on layers? Is it still the same as suggested in https://github.com/theislab/scanpy/issues/1301? With this I am still unable to use sc.pl.pca_variance_ratio.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999:70,layers,layers,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1308#issuecomment-665610999,1,['layers'],['layers']
Modifiability,"So, does that mean that every time we apply some kind of filtration (adata = adata[ condition]) we should use .copy()? ; For instance, when filtering the highly variable genes (see the image extracted from the scanpy legacy workflow)? ; ![image](https://github.com/scverse/scanpy/assets/64482157/e929e440-c093-4571-b0c3-43febd052128)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073#issuecomment-2192467124:161,variab,variable,161,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073#issuecomment-2192467124,1,['variab'],['variable']
Modifiability,"So, it look like it does fit all elements at once if it's a continuous variable (I'm not completley sure why this doesn't seem to be the case for categorical). . I think your solution would work, but it may be worthwhile to spot check. It would probably also be nice to have a nice API for this on our end, like being able to just provide a patsy formula. I did a quick check comparing your suggestion to the results of adding features with the function below, and it seems fine. ```python; import statsmodels.formula.api as smf. def regress_out_poly(y, x, degree=2):; poly = "" + "".join(f""np.power(x, {i})"" for i in range(1, degree + 1)); mod = smf.glm(f""y ~ {poly}"", {""y"": y, ""x"": x}, family=sm.families.Gaussian()); return mod.fit().resid_response; ```. @LuckyMD may have more to say on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974:71,variab,variable,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839#issuecomment-841958974,1,['variab'],['variable']
Modifiability,Some refactoring ahead of key_added,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3182:5,refactor,refactoring,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3182,1,['refactor'],['refactoring']
Modifiability,"Some refactoring:. - single-source AggType in aggregate tests; - fix warnings in aggregate; - fix logs in pca, umap, and tsne",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3186:5,refactor,refactoring,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3186,1,['refactor'],['refactoring']
Modifiability,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289:185,adapt,adapted,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-432779289,1,['adapt'],['adapted']
Modifiability,"Sorry but the question is not clear. The plotting functions underwent a refactoring recently but that one should still work no? I'll close this for the moment, feel free to reopen it but please do so with a reproducible example, thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813:72,refactor,refactoring,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1268#issuecomment-702370813,1,['refactor'],['refactoring']
Modifiability,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame.; - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved.; - I think that would be pretty straightforward. What outputs are you referring to, specifically?; - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077:133,extend,extend,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-786183077,2,"['extend', 'variab']","['extend', 'variables']"
Modifiability,"Sorry for the late response! This seems to have come just after I went through the issues last weekend...; ; It looks great! :smile:. Some small notes:; * `sc.pl.correlation` should be `sc.pl.correlation_matrix` (there will be other ""correlation plots"", just think of the typical bivariate scatter plot...); * `sc.tl.dendrogram` suggests it is a function that can be generically applied to any hierarchical clustering of observations. We could even have dendrograms of variables, right? I'm fine with putting it into the API with just that generic name, but it would be good to have a `.. note::` in the docstring, which states that this does a very specific thing: computing hierarchical clustering on predefined groups using Pearson correlation as a distance metric; I know that this is super standard in the field, but we should nonetheless be very clear about it. In particular as Scanpy grows and we extend its functionality to other methods for grouping observations, structuring their relations (e.g. hierarchical clustering with another distance metric or so, or something that we don't think of at this stage), I fear that people might start to get confused. Even now, they don't know what, for instance, the relation of `tl.dendrogram` to PAGA is: instead of correlating cluster mediod vectors, PAGA computes the connectivity between clusters in the underlying graph. Also, it is not restricted to a tree. It would be great to have a note like that (I can also put it; also, I wanted to rewrite the PAGA docstring anyways and I'll make a link to `tl.dendrogram`...). Thanks again!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916:469,variab,variables,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/425#issuecomment-456024916,3,"['extend', 'rewrite', 'variab']","['extend', 'rewrite', 'variables']"
Modifiability,"Sorry for the late response, Joshua! Could it be that your dataset has less than 50 cells or variables or something like this?. I believe that you're stating this. Computing a 50 dimensional PCA with less than 50 observations is probably not possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/331#issuecomment-435733122:93,variab,variables,93,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/331#issuecomment-435733122,1,['variab'],['variables']
Modifiability,"Sort of. I believe weights are between 0 and 1, where the edge to the nearest neighbor has weight=1, and the k-th+ neighbor has weight=0. I'm not quite sure how the weights are scaled within that, but I'm pretty sure it's not rank based. Leland Mcinnes has explained it much better than I can in his explanations of UMAP. It's discussed [in the docs](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html#adapting-to-real-world-data) starting with the part on Riemannian geometry, but is also covered in his talks or the UMAP paper.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177:415,adapt,adapting-to-real-world-data,415,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484016177,1,['adapt'],['adapting-to-real-world-data']
Modifiability,"Sorta!. ![image](https://user-images.githubusercontent.com/8238804/108616034-ce7cd480-745d-11eb-93e4-996a912c5041.png). Not sure if it's not working because something is wrong with the configuration, because it doesn't work with PRs, or that it takes a bit for search results to be available. One downside of using this over algolia's search is that we get search analytics through algolia, while we'd have to upgrade our readthedocs subscription to have access to that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773:185,config,configuration,185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1672#issuecomment-782797773,1,['config'],['configuration']
Modifiability,Sounds good - added an entry to the release note and updated the tests to not use the parameterization.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767:86,parameteriz,parameterization,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2859#issuecomment-1947513767,1,['parameteriz'],['parameterization']
Modifiability,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478:534,layers,layers,534,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281#issuecomment-437031478,1,['layers'],['layers']
Modifiability,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool; > tl.dendogram in the clustering section that has a parameter to select the; > clustering for which one wants a dendogram, typically defaulting to; > louvain (unfortunately, we still don't have a good consistent naming; > convention across all tools; groupby predominates but is not ideal in; > this setting. grouping or cluster_key would also be possible).; >; > You can still have the plotting functions call that function with default; > parameters. But the user wants more control, he or she can run the; > dendogram tool.; >; > Of course, we also want dendograms for genes. I think the most elegant; > (but maybe confusing solution) is to do it as in pl.scatter, where; > annotation of observations is selected if the key is in .obs and; > variables annotations are selected if the key is in .var. What do you; > think?; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430880800:956,variab,variables,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430880800,1,['variab'],['variables']
Modifiability,Support coloring by boolean variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1646:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646,1,['variab'],['variables']
Modifiability,"Sure! @ivirshup figured out independently within 2 hours of me that `is_string_dtype` now works differently: theislab/anndata#107. The fix needed three parts:. 1. I fixed the tests to actually work (they were broken since forever because they used a hardcoded file name instead of `tmp_path`, and therefore reused the same file); 2. I pulled his changes, which covered the writing portion of the needed fixes; 3. I fixed the reading portion in theislab/anndata@4c8163129302391419c7ee4943e7fb766599e2a2; 4. I fixed the highly variable genes function that relied on a slightly different behavior of series in 0.23",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460184736:525,variab,variable,525,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460184736,1,['variab'],['variable']
Modifiability,"Sure, I can have a look at the docs. I've been using it because of the `layers` argument since `pl.umap` does not seem to have it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/458#issuecomment-476005968:72,layers,layers,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/458#issuecomment-476005968,1,['layers'],['layers']
Modifiability,"Sure, and I'm not against supporting special cases! Could you please explain the setup?. Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. Some systems are strange. We should be nice and support those systems while still doing the correct thing by default. We shouldn't do the wrong thing by default to accommodate strange cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606:248,variab,variable,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606,1,['variab'],['variable']
Modifiability,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python; #[...]; r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition; r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'); r('diffDash <- gsub(""-"", ""_"", diffDash)'); r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'); filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')); filtout_indicator = np.in1d(adata.var_names, filtout_genes); adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object; #[...]; ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901:275,variab,variables,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-1594300901,2,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"Thank you for this. I took a dive into the data set to figure out what filtering step is causing this problem, and it seems to be the conditions set for `sc.pp.highly_variable_genes`. To carry on with `LYZ`, it does not show up because its `mean_counts` is too high, above the *maximum* of 3 set in the analysis:. ```; >>> adata.raw.to_adata().var.loc['LYZ']; gene_ids ENSG00000090382; n_cells 1631; mt False; n_cells_by_counts 1631; mean_counts 10.2467; pct_dropout_by_counts 39.5926; total_counts 27666; highly_variable False; means 3.68714; dispersions 5.12101; dispersions_norm 3.65908; Name: LYZ, dtype: object; ```. Is this filtering on `max_mean` as described in the tutorial a reasonable thing to do? That said, even if I were to not filter the matrix to restrict it to genes detected as highly variable, by default `scanpy.tl.pca` would not even use `LYZ` as a potential contributor to a principal component, because it is not highly variable. Again, the equivalent Seurat tutorial does have LYZ in it, but I assume that is because they are now using a different way to classify which genes are variable. Would you say my interpretation is correct? If so, would a better implementation of `sc.pp.highly_variable_genes` solve the problem? Would be happy to contribute if that is something that would be needed and there's no good Python-based alternative around.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276:803,variab,variable,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665648276,3,['variab'],['variable']
Modifiability,"Thank you for your thoughts!. 1. `normalize_per_cell` needs to remove zero-expression cells as these can't be normalized, the alternative would be to require it as a preprocessing step; but you're right, wflynny, I'll frame it as a fall-back for `normalize_per_cell` in the next version and output a warning... which will make things backwards compatible...; 2. Any filtering operation on the cells/observations should also affect `.raw`. I'll look into this today. ; 3. Any filtering operation on the variables should **not** affect `.raw`. I didn't know that this gives problems in `rank_genes_groups`? Of course, you don't find everything in `.X` that you find in `.raw.X` and you'll get a key error if you try to; but is there a fundamental problem, @LuckyMD?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976:502,variab,variables,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/210#issuecomment-407038976,1,['variab'],['variables']
Modifiability,"Thank you very much! I merged this via the command line after adapting to the private module design. I still get a to me cryptic AttributeError from patsy on my Mac, but the tests are fine and on the Linux server it also runs fine:; ```; preprocessing/_combat.py:150: in combat; s_data, design, var_pooled, stand_mean = stand_data(model, data); preprocessing/_combat.py:78: in stand_data; design = design_mat(model, batch_levels); preprocessing/_combat.py:32: in design_mat; model, return_type=""dataframe""); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:291: in dmatrix; NA_action, return_type); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:165: in _do_highlevel_design; NA_action); ../../../miniconda3/lib/python3.6/site-packages/patsy/highlevel.py:62: in _try_incr_builders; formula_like = ModelDesc.from_formula(formula_like); ../../../miniconda3/lib/python3.6/site-packages/patsy/desc.py:164: in from_formula; tree = parse_formula(tree_or_string); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:148: in parse_formula; _atomic_token_types); ../../../miniconda3/lib/python3.6/site-packages/patsy/infix_parser.py:210: in infix_parse; for token in token_source:; ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:94: in _tokenize_formula; yield _read_python_expr(it, end_tokens); ../../../miniconda3/lib/python3.6/site-packages/patsy/parse_formula.py:44: in _read_python_expr; for pytype, token_string, origin in it:; ../../../miniconda3/lib/python3.6/site-packages/patsy/util.py:332: in next; return six.advance_iterator(self._it); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . code = ''. def python_tokenize(code):; # Since formulas can only contain Python expressions, and Python; # expressions cannot meaningfully c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530:62,adapt,adapting,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/398#issuecomment-451762530,1,['adapt'],['adapting']
Modifiability,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051:430,extend,extend,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289#issuecomment-429919051,1,['extend'],['extend']
Modifiability,"Thank you very much. I could remove the graph slot and this error is gone, but now I have a new error: . ```; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-2-aae861244dfa> in <module>; ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] for k in keys]; 211",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885:493,layers,layers,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885,5,['layers'],['layers']
Modifiability,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444:19,adapt,adapt,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292#issuecomment-429443444,1,['adapt'],['adapt']
Modifiability,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336:131,plugin,plugins,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425038336,3,['plugin'],"['plugin', 'plugins']"
Modifiability,"Thank you! worked for me; > ; > ; > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244:146,config,config,146,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-583628244,3,['config'],['config']
Modifiability,"Thanks @giovp for your quick reply! I upgraded pandas and ran your code with the pbmc dataset. This ran fine. On my own dataset it is still giving the same error. So maybe something is wrong with the way I created adata. Because my code ran fine before upgrading scanpy and I found this issue: https://github.com/theislab/single-cell-tutorial/issues/28#issue-576248363 I thought it might be a real bug. ; After running your example I will just look into how I created adata to see if I can find the error. ; This is what it looks like now: ; ```; AnnData object with n_obs × n_vars = 2773 × 3783 ; obs: 'n_genes', 'plate', 'platebatch', 'stage', 'well_no', 'ERCC_genes', 'n_total_counts', 'percent_mito', 'n_counts', 'percent_ribo', 'percent_protein_coding', 'percent_lincRNA', 'sum_lincRNA', 'percent_antisense', 'sum_antisense', 'percent_miRNA', 'sum_miRNA', 'percent_bidirectional_promoter_lncRNA', 'sum_bidirectional_promoter_lncRNA', 'percent_snoRNA', 'n_counts_norm', 'Chat_norm_expr', 'cellnr', 'louvain', 'velocity_self_transition', 'lineages', 'root_cells', 'end_points', 'velocity_pseudotime'; var: 'ENS_names', 'geneid', 'feature', 'chr', 'fullname', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'velocity_gamma', 'velocity_r2', 'velocity_genes'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'plate_colors', 'stage_colors', 'umap', 'velocity_graph', 'velocity_graph_neg', 'velocity_settings', 'rank_genes_groups'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'velocity_tsne', 'velocity_umap'; varm: 'PCs'; layers: 'Ms', 'Mu', 'spliced', 'unspliced', 'variance_velocity', 'velocity'; ```. The adata.X of the pbmc data is `scipy.sparse.csr.csr_matrix`; My adata.X is `numpy.ndarray`. This probably results in the problem of the difference in dimensions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097:1545,layers,layers,1545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1114#issuecomment-601076097,1,['layers'],['layers']
Modifiability,"Thanks a lot for the useful comments & I am a great admirer of scanpy, @falexwolf . I am trying to cluster cells based on specific gene sets (same as @biskra). I am working with loom files & after the data preprocessing and finding out highly variable genes, I have subgrouped the HVG(s) into five different categories by functional annotation/pathway enrichment analysis. Then I tried to subset 'adata' to the gene group I am interested in to carry out the embedding & clustering:. adata = adata[:, adata.var['highly_variable']]. #From the highly variable genes, let's say I want to use Gene1, Gene2,... Gene500 for the Louvain clustering instead of the PCA. Then, I tried to do what you suggested before:. #I have nothing stored under adata.obsm; adata.var['highly_variable'] = adata[['gene1', 'gene2', 'gene3', 'gene4']].X. I am getting this error despite the fact that these genes are in the HVG list: ; #KeyError: ""None of [Index(['Map7d1', 'Ndufa2', 'Klc2', 'Slc35b2'], dtype='object')] are in the [index]"". If this works, then the community graph can be computed:; sc.pp.neighbors(adata, use_rep='highly_variable'). I shall be grateful if you can help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487964976:243,variab,variable,243,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487964976,2,['variab'],['variable']
Modifiability,"Thanks for a quick response and the comments. > The main change here is passing None instead of 0 to total, right?. It was actually setting it in the contructor, rather than assigning it to the tqdm object (the latter doesn't work). Here's before:; ![old](https://user-images.githubusercontent.com/46717574/100207740-3b88d880-2f08-11eb-882f-cae14be0837e.png); and after:; ![new](https://user-images.githubusercontent.com/46717574/100207756-3fb4f600-2f08-11eb-85f8-5938ff04572d.png). > Also: this makes some errors with files still existing make much more sense. I had no idea KeyboardInterrupt doesn't inherit from Exception. I didn't know that either, so I looked it up (it actually inherits from `BaseException` among other things:; https://docs.python.org/3/library/exceptions.html#exception-hierarchy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404:602,inherit,inherit,602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733582404,2,['inherit'],"['inherit', 'inherits']"
Modifiability,"Thanks for implementing this! I used it to regress out total counts and cell cycle scores before highly variable gene selection, and it worked well. The clusters are better separated without artifacts, unlike running regressing function after HVG.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2731#issuecomment-1800547868:104,variab,variable,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2731#issuecomment-1800547868,1,['variab'],['variable']
Modifiability,"Thanks for posting that code, it's very helpful for figuring this stuff out. I tried running that code on one of our example datasets, and wasn't able to reproduce your results (however, one of the variables `pos_coord` wasn't defined):. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the datas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:198,variab,variables,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,1,['variab'],['variables']
Modifiability,"Thanks for that. I did mean `adata00a`, whoops! I'm expecting the `ValueError` was due to the `.var_names` having different shapes?. I think the main issue here is that `AnnData.concatenate` expects the variables (`.var_names`) to be shared, and appends the objects along the observation axis. If your data is read in with observations in the `var` axis, you can use `AnnData.transpose` to fix that. It looks like something different is happening in the snippet I sent than the one you posted before. I'm pretty sure I didn't modify any variables when I added the print statements to your snippet. Any idea why the results would be different?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929:203,variab,variables,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/914#issuecomment-554597929,2,['variab'],['variables']
Modifiability,"Thanks for the PR! I've just renamed the variable to be a bit more clear. I do think this test could be a bit better (e.g. check that the structure of the object is correct), but also this is an improvement so LGTM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2170#issuecomment-1061626692,1,['variab'],['variable']
Modifiability,"Thanks for the PR! We've been thinking about refactoring this part of the package, and this looks like an interesting way to do it. However, we're not accepting any additions to the `external` module anymore. Instead we are pointing people to the broader [scverse ecosystem](https://scverse.org/packages/#ecosystem). We may be interested in using this as a direct dependency but may need to do some research into this first + request/ add a few features in `Marsilea` such as dot plots. cc @grst",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208:45,refactor,refactoring,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2512#issuecomment-1597429208,1,['refactor'],['refactoring']
Modifiability,"Thanks for the PR!. While I like the idea of being able to be more flexible around how the dot plots are made, I'm not sure I like the idea of adding more argument specific behavior. That is, I don't like that passing a boolean flag changes the meaning of the values for the `var_names` and `groupby` arguments. E.g. the `group_by` is the selection of `obs` corresponding to the plots rows, while `var_names` was the variable for the columns. What could maybe be done instead is to have an argument for column grouping. Instead of. ```python; sc.pl.dotplot(adata, var_names='C1QA', groupby=['louvain', 'sampleid'], groupby_expand=True); ```. It could be more like. ```python; sc.pl.dotplot(adata, var_names='C1QA', group_by='louvain', group_cols='sampleid'); ```. I would ideally like a solution here to work for all the other ""grouped plots"" as well. I'll write a bit more about this in the parent issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001:67,flexible,flexible,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055#issuecomment-987012001,2,"['flexible', 'variab']","['flexible', 'variable']"
Modifiability,"Thanks for the PR, that’s it. We know about the two broken tests (e.g. see #3068), they can be ignored for this PR. Not all changes I made in #3097 were necessary, just re-adding the `if len(gene_pool) < len(var_names)` branch. I made the refactoring PR since your changes already refactored the function in a good way, I just went a bit further so the repeated code for “get row/col means of a gene subset of `adata`” code could be reused, and your `get_indexer` change would only need to be added in a single location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2921#issuecomment-2149299253:239,refactor,refactoring,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2921#issuecomment-2149299253,2,['refactor'],"['refactored', 'refactoring']"
Modifiability,"Thanks for the demo code! now its clear to me. I adapted the test to use `np.var(..,dtype=np.float64)` as ground truth, making the internal datatype conversion explicit. Any other requests? I think everything else is ready :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131:49,adapt,adapted,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1732#issuecomment-801986131,1,['adapt'],['adapted']
Modifiability,"Thanks for the explanations @ivirshup! This makes quite a bit more sense to me now (the block sparse matrix stuff). If I understand the `.raw` removal alternative correctly, then you would want to add masks to every operation in scanpy that is not DE and work with `.layers`? I assume that e.g., MT or ribo genes are mainly removed for cellular representation analysis. Some people will also want to remove them from DE analysis to have a set of results that are easy to interpret and have less multiple testing burden. It seems to me that adding masking like this would be quite a large endeavour, no?. > What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I don't see this as such a big issue. If you assume anything filtered out was removed because it was predominantly 0, then it would not have been included in the HVG set of that dataset anyway. So you can assume it would not be in the HVG intersection for that dataset and if you add it, then a 0 for each cell would probably not be that problematic. And whether this was due to a particular cell type being poorly represented can be answered by the gene set that you do have for these cells. Typically there is sufficient gene-gene covariance that you still keep this signal somehow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661:267,layers,layers,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-822616661,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"Thanks for the response. I tried that several times, but couldn’t get the install to work:. “Cannot find the C core of igraph on this system using pkg-config” etc. Any other advice would of course be appreciated greatly. From: MalteDLuecken [mailto:notifications@github.com]; Sent: Friday, May 24, 2019 3:54 PM; To: theislab/scanpy <scanpy@noreply.github.com>; Cc: Moos, Malcolm <Malcolm.Moos@fda.hhs.gov>; Comment <comment@noreply.github.com>; Subject: Re: [theislab/scanpy] igraph problems (#138). The above issue was fixed by installing python-igraph and not igraph. Just run; pip install python-igraph. —; You are receiving this because you commented.; Reply to this email directly, view it on GitHub<https://github.com/theislab/scanpy/issues/138?email_source=notifications&email_token=AMEIEFZ5Y3DOAD5JWFWTCXTPXBBWZA5CNFSM4E5ZJQRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWGNRAA#issuecomment-495769728>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AMEIEF3USEC33LKKFK4X4R3PXBBWZANCNFSM4E5ZJQRA>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986:151,config,config,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/138#issuecomment-495920986,1,['config'],['config']
Modifiability,"Thanks for the suggestion. Actually, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:676,variab,variable,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368,1,['variab'],['variable']
Modifiability,"Thanks for the tutorial!. Let's get most of this right now that no one has still used the function. We don't want to be consistent with Scater, we want to be consistent with the rest of Scanpy and the other python ecosystem. - Can we replace all occurances of `features` with `variables`? We had quite some discussions whether an AnnData is samples of features or observations of variables, and throughout, we stick with the latter convention. It's a very simple change. ; - Can we replace `exprs_values` with `expr_type` or something more suggestive of the fact that it's just a string denoting the kind of expression values? ; - Can we replace `total_features_by_counts` with `n_genes`? Why so complicated? And in contrast to `total_counts`, `features` does not suggest that it's a number, so there has to be an `n_...` before it, otherwise it completely breaks the convention.; - Can we call `feature_controls`, `control_variables`, which would be a much more intuitive name? ; - Why is `n_cells_by_{expr_values}` not simply `n_cells`? Am I missing something?. Regarding `n_counts` versus `total_counts`, I mentioned already that I see that `total_counts` has some advantages when starting to compare with quantile counts, etc. Also, it doesn't require an `n_` as it's clear that it's a number. But for all the rest that I mentioned above, I don't see these arguments. What do you think?. I really like that you use an `inplace` parameter instead of the usual `copy`, we might have exaggerated it in some places.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398:277,variab,variables,277,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436124398,2,['variab'],['variables']
Modifiability,"Thanks for the update. Now is clear. We do not offer that possibility as most of those functions are based on seaborn, thus, simply passing the relevant data to seaborn will get you the image that you want. Nevertheless, I would like to take a look. How do you think this should work. Just add a variable to show the genes that you would like to see. Or you mean a more generic function just to make split plots between any two categories for the genes that you want to see?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626:296,variab,variable,296,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1448#issuecomment-707551626,1,['variab'],['variable']
Modifiability,"Thanks for the wishes! :). If it's not much work for you: could you paste your workaround here? In my tests, the reading of old AnnData backing files worked fine, but I only tested from version to version... 0.2.8 is already quite old for the speed with which Scanpy evolves, so I probably missed something. In principle, Scanpy should be fully backward compatible; several people have written pipelines and stored files that still have to run with more recent versions of Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745:267,evolve,evolves,267,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56#issuecomment-354906745,1,['evolve'],['evolves']
Modifiability,"Thanks for your comment Jason!; Don't you think the sentence before the one you quoted:; ""If None, mpl.rcParams[""axes.prop_cycle""] is used unless the categorical variable already has colors stored in adata.uns[""{var}_colors""].""; in combination with the default being ""None"" would make this clear?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845:162,variab,variable,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2311#issuecomment-1256966845,1,['variab'],['variable']
Modifiability,"Thanks for your replies and tips on the using fenced code blocks! I did verify that removing `adata.varm` before saving the `.raw` object gets rid of this problem. . On a tangentially related note, is there an easy way to restore a new adata object from a `.raw` object? There are some cases where I would like to re-do some analysis on the full, `.raw` object that still retains the original non-transformed variables.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/884#issuecomment-554084273:409,variab,variables,409,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884#issuecomment-554084273,1,['variab'],['variables']
Modifiability,"Thanks to everyone who commented here. Hey @flying-sheep, it passes the tests in https://github.com/scverse/scanpy/blob/e285c0f6ec77631d14d748d0927d38aae4391886/tests/test_aggregated.py; please let me know if I should fix or refactor anything; Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3180#issuecomment-2325176926:225,refactor,refactor,225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3180#issuecomment-2325176926,1,['refactor'],['refactor']
Modifiability,"Thanks, worked for me. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023:135,config,config,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-584071023,3,['config'],['config']
Modifiability,That could also work. But this would require a bit of a rewrite. I think the current solution is simpler and also really fast.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797:56,rewrite,rewrite,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2022529797,1,['rewrite'],['rewrite']
Modifiability,"That is a wonderful solution! I will give it a shot shortly. Thank you so much for your help!. Here's some example code of how Seurat handles cluster scoring and merging with random forests and OOBE:. ```; pbmc <- ValidateClusters(pbmc, pc.use = 1:30, top.genes = 30). pbmc <- BuildClusterTree(pbmc, ; do.reorder = T, ; reorder.numeric = T). node.scores <- AssessNodes(pbmc). node.scores[order(node.scores$oobe,decreasing = T),] -> node.scores. nodes.merge <- node.scores[which(node.scores[,2] > 0.1),]; nodes.to.merge <- sort(nodes.merge$node) ; pbmc.merged <- pbmc. for (n in nodes.to.merge); {; pbmc.merged <- MergeNode(pbmc.merged, n); }. ```. Here's an explanation, as this code was derived from this recent (and awesome) publication:; ; From page 6 of the Supplementary Methods of Plass et al 2018: http://science.sciencemag.org/content/early/2018/04/18/science.aaq1723. To prevent obtaining spurious clusters result of overclustering, the robustness of the clusters was calculated using the function AssessNodes from Seurat. For each cluster, the average expression of all variable genes (4910) is computed and a phylogenetic tree based on the distance matrix in gene expression space is computed. Next, it computes an Out of Bag Error for a random forest classifier trained on each internal node split of the tree. We recursively build a tree and assessed all its nodes, merging all clusters with an out of bag error bigger than 0.1 until no such nodes were found.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410:1080,variab,variable,1080,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/362#issuecomment-440912410,1,['variab'],['variable']
Modifiability,"That is interesting... do you know where the randomness is coming in? I think `sc.pp.highly_variable_genes()` can have some variability. These two VMs have the same operating system and hardware otherwise, right? I've had reproducibility issues moving between Fedora 25 and 28. In the end the libraries we use rely on underlying kernel numerics. There's a limit to how reproducible one can be. This only really becomes an issue if the biological interpretation is no longer consistent. Of course we'd like to be reproducible before then as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704:124,variab,variability,124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620862704,1,['variab'],['variability']
Modifiability,"That issue report mentions setting the `PYTHONHASHSEED` environment variable to `0` (next to all the seed setting) worked to create a fully reproducible workflow. If that doesn't work for you, it might be good to continue the discussion there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306:68,variab,variable,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620657306,1,['variab'],['variable']
Modifiability,"That's a great idea. It might require some reorganization, though, because currently `use_raw` is checked two places: once in `sc.pl.scatter()`, because it needs to know whether to look for variables in raw or not when deciding how to call `_scatter_obs()`, and again in `_scatter_obs()` itself. And it would probably be bad to do `adata = adata.raw.to_adata()` twice?. On another note, some pytests that are in files I did not edit are now failing because they can't find `anndata.tests` to import. I'm not sure if I messed something up by adding tests to `test_plotting.py` or whether this is a different issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046:190,variab,variables,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964269046,1,['variab'],['variables']
Modifiability,"That's an interesting idea and I see how it would be useful. I don't think it's going to be easy to implement, since I believe our code is heavily based around having groups of observations on one axis, groups of variables on the other. . Definitely something to keep in mind for a refactor though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914:213,variab,variables,213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-863787914,2,"['refactor', 'variab']","['refactor', 'variables']"
Modifiability,"That’s exactly backwards: I find it annoying if packages modify state on import. We already jump through hoops in our testing framework to work around our misbehavior:. https://github.com/theislab/scanpy/blob/681ce93e7e58956cb78ef81bc165558b84d6ebb0/scanpy/tests/conftest.py#L4-L6. `import matplotlib.pyplot [as plt]` means “I’m an end user who just opened a notebook and I want the kitchen sink, give me everything and configure everything”. Libraries shouldn’t do it and scanpy is one. When we still had `scanpy.api` there would have been a case for importing pyplot there, as `scanpy.api` was for interactive use. Now we don’t have any excuses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212:420,config,configure,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-523026212,1,['config'],['configure']
Modifiability,That’s super redundant now. Please extract all that text from `doc_scatter_bulk` into another variable and import and use that one instead.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/557#issuecomment-476508242:94,variab,variable,94,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/557#issuecomment-476508242,1,['variab'],['variable']
Modifiability,"That’s the way anyway. I think first step would be to rewrite our `test` extra in terms of a) what’s needed for testing and b) what really are scanpy features being tested:. ```toml; test = [; 'pytest',; 'scanpy[dask]',; 'scanpy[zarr]',; ]; ```. then we can extra-by-extra make parts of our test suite optional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088726702:54,rewrite,rewrite,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2211#issuecomment-1088726702,1,['rewrite'],['rewrite']
Modifiability,"The CLI option is different from the config option …. no review necessary, simple fix",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2537:37,config,config,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2537,1,['config'],['config']
Modifiability,"The `layers` of an `AnnData` object are closest to the assays from Seurat. You should be able to store whatever transformations of the expression matrix you want in there. . In general, I do something like:. ```python; adata.layers[""counts""] = adata.X.copy(); sc.pp.normalize_total(adata); sc.pp.log1p(adata); ```. As a side note, I don't think we'd recommend using scaled data, but you can read more on that from these [tutorial notebooks](https://github.com/theislab/single-cell-tutorial) or [this related paper](https://www.embopress.org/doi/10.15252/msb.20188746).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806:5,layers,layers,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089#issuecomment-596279806,2,['layers'],['layers']
Modifiability,"The `leidenalg` package was originally built for flexibility, and you can easily plugin new quality functions. As a result, some of the admin stuff is being done less efficiently than it could be done. In `igraph`, there is less flexibility, so that the implementation can be made more efficient. Additionally, some of the iteration over neighbours in the `leidenalg` package is less efficient than how it is implemented in `igraph` at the moment. This could be made more efficient though, but it is something that requires quite some rewriting, for which I would first need to find the time. I'm not sure how large the speed gains of this would be immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975:81,plugin,plugin,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040092975,1,['plugin'],['plugin']
Modifiability,"The data also allows to detect a next issue: When multiple genes have the same value of `disp_cut_off`. Can be found if here e.g. dont do `sc.pp.normalize_total`:. ```py; import scanpy as as; adata = sc.datasets.pbmc3k(); # sc.pp.normalize_total(adata, target_sum=10000); sc.pp.log1p(adata); sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000); adata.var[""highly_variable""].sum(); ```; ```; 10367; ```; Which is due to many genes having the value selected for the `disp_cut_off` here, having . ...`x[n-2]` = `x[n-1 ]` = `x[n]` = `x[n+1] `= `x[n+2]`... https://github.com/scverse/scanpy/blob/b918a23eb77462837df90d7b3a30a573989d4d48/src/scanpy/preprocessing/_highly_variable_genes.py#L408-L418. I tried to check how Seurat is proceeding in such a case, expecting to see how it breaks the ties. (data downloaded from [here](https://cf.10xgenomics.com/samples/cell/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz)); Here I'm actually not sure how to turn off the `scale.factor` argument? Its set to 10'000 by default. ```R; library(dplyr); library(Seurat); library(patchwork). pbmc.data <- Read10X(data.dir = ""filtered_gene_bc_matrices/hg19/""). pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k""). pbmc <- NormalizeData(pbmc, normalization.method = ""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot"", nfeatures = 10000). length(VariableFeatures(pbmc)); ```; ```; 2292; ```; However, it turns out Seurat seems to restrict to the genes which are variable in the sense of passing the set mean threshold and normalized dispersion thresholds. These thresholds are ignored in scanpy if the number of genes is given. So not really an insight of how to break ties in this case. Would suggest to make a new issue, which the potential project on comparing the frameworks could address.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888:1412,Variab,VariableFeatures,1412,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3157#issuecomment-2255759888,2,"['Variab', 'variab']","['VariableFeatures', 'variable']"
Modifiability,"The docs don’t say it does, so this would be an enhancement, not a bug fix. it’s not high priority, since it’s easy to just do `ax = plt.subplot(); sc.pl.rank_genes_groups(adata, ax=ax)`. contributions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205#issuecomment-2437741950:48,enhance,enhancement,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205#issuecomment-2437741950,1,['enhance'],['enhancement']
Modifiability,"The docs for `sc.pl.scatter` say . > The palette can be a valid ListedColormap name ('Set2', 'tab20', …). but setting `palette` to a string throws an error. ```python; adata = sc.datasets.paul15(); sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""); ```. ```pytb; ... storing 'paul15_clusters' as categorical; Trying to set attribute `.uns` of view, copying.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-698-58a5366a0f70> in <module>; 1 adata = sc.datasets.paul15(); ----> 2 sc.pl.scatter(adata, ""Cma1"", ""Irf8"", color='paul15_clusters', palette=""Set2""). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; --> 128 return _scatter_obs(**args); 129 if (; 130 (x in adata.var.keys() or x in adata.obs.index). ~/.local/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 273 palettes = [palette for _ in range(len(keys))]; 274 for i, palette in enumerate(palettes):; --> 275 palettes[i] = _utils.default_palette(palette); 276 ; 277 if basis is not None:. TypeError: 'str' object does not support item assignment; ```. I get no error if I use any of `sc.pl.palettes`. I also get no error setting `palette=""Set2""` in `sc.pl.umap`, `sc.pl.draw_graph` etc... #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; MulticoreTSNE NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1438:779,layers,layers,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1438,1,['layers'],['layers']
Modifiability,"The following code produces the desired plot (up to permutation of the subplots and the image size) in case of only one row of subplots:. ```python; if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]. g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds). if stripplot:; grouped_df = obs_tidy.groupby(x); for ax_id, key in zip(range(g.axes.shape[1]), keys):; sns.stripplot(y=y, data=grouped_df.get_group(key), jitter=jitter, size=size, color=""black"", ax=g.axes[0, ax_id], **kwds); ```. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93485925-e5922600-f903-11ea-9edb-0f7523a67c0d.png). Seems a bit hacky to me. What do you think @fidelram?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891:239,adapt,adapting,239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694279891,1,['adapt'],['adapting']
Modifiability,"The function `sc.pl.dotplot()` can be used with processed data or raw data by setting `use_raw=True`. The % cells that express a gene is obtained by counting cells with expression above `expression_cutoff` (which by default is 0.0). . However this is likely to produce a wrong result when using scaled and centered data (i.e. after `pp.scale()` ). Unless I'm missing something, the percentage of cells expressing a gene should only be computed from raw data. . Although this is the default usage of the dotplot function in scanpy notebooks, the default use in Seurat is the opposite, and it seems easy to use `sc.pl.dotplot()` in scaled data without noticing this. Maybe a variable similar to `use_raw` to uncouple the use of raw data for the colormap and dotsize could solve this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1361:673,variab,variable,673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1361,1,['variab'],['variable']
Modifiability,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/308#issuecomment-430674069:785,variab,variables,785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308#issuecomment-430674069,1,['variab'],['variables']
Modifiability,"The latest version of scanpy added the possibility to enhance the plots in multiple ways. For this you need to use the new classes documented here https://scanpy.readthedocs.io/en/stable/api/scanpy.plotting.html#classes. In particular you want to check the function `add_totals()`: https://scanpy.readthedocs.io/en/stable/api/scanpy.pl.StackedViolin.add_totals.html#scanpy.pl.StackedViolin.add_totals. For your case you can do:; ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}; sc.pl.StackedViolin(adata, markers, groupby='bulk_labels').add_totals().show(); ```. Other options using the function that you are familiar with is:. ```PYTHON; # here return_fig=True is used; plot = sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', return_fig=True); plot.add_totals().show(); ```. You can find further info here: https://github.com/theislab/scanpy/pull/1210",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193:54,enhance,enhance,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1450#issuecomment-707544193,1,['enhance'],['enhance']
Modifiability,"The main change here is passing `None` instead of `0` to `total`, right?. Also: this makes some errors with files still existing make much more sense. I had no idea `KeyboardInterrupt` doesn't inherit from `Exception`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444:193,inherit,inherit,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1507#issuecomment-733508444,1,['inherit'],['inherit']
Modifiability,"The only issue I can think of was when I was creating the object. Before I used to transfer the `adata.obs` dataframe to a new one by doing `adata_new.obs = adata_old.obs`. When I did this in `scanpy==1.7.1` the transfer didnÄt show any errors, but it didn't copy. This was fixed when I added the `.copy()` to that command. . When I ran the same thing on a macbook pro, the labels somehow disappeared after calculating highly variable genes. . I have been using this notebook since `scanpy==1.6` and it didn't give me any problems until I upgraded to `scanpy==1.7.1`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441:426,variab,variable,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1701#issuecomment-787874441,1,['variab'],['variable']
Modifiability,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:55,config,configuration,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['config'],['configuration']
Modifiability,"The scanpy.read_10X_mtx works well for reading in the STARsolo output matrices, which are based on the CellRanger Outputs. . However, it would be nice to have a function or modification of the read_10X_mtx function (e.g. a boolean for STARsolo velocyto) to automate inputting the velocyto matrices that STARsolo outputs and placing them in the appropriate layers. A boolean switch for filtered versus raw matrices would be a good addition as well.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1860:356,layers,layers,356,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1860,1,['layers'],['layers']
Modifiability,"The scanpyToCellbrowser function has an option useRaw that will use the; .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; the matrix and all annotations, or anndataToTsv to write just the matrix.; Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top; > variable genes. Is there a way to output the raw matrix including all genes?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560:189,variab,variable,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-498194560,2,['variab'],['variable']
Modifiability,"The size of my dataset is:. ```AnnData object with n_obs × n_vars = 19091 × 23315```. Here is the full code:. ```; import scanpy as sc; import pandas as pd; import numpy as np. from anndata import AnnData. def harmony_integrate(; adata: AnnData,; key: str,; basis: str = ""X_pca"",; adjusted_basis: str = ""X_pca_harmony"",; **kwargs,; ):; try:; import harmonypy; except ImportError:; raise ImportError(""\nplease install harmonypy:\n\n\tpip install harmonypy""). X = adata.obsm[basis].astype(np.float64). harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs). adata.obsm[adjusted_basis] = harmony_out.Z_corr.T. adata = sc.read_h5ad('adata.h5ad'). adata_merge = adata.copy(); adata_merge.X = adata_merge.layers['counts']; sc.experimental.pp.highly_variable_genes(adata_merge, n_top_genes=3000, batch_key='batch'). adata_merge = adata_merge[:, adata_merge.var['highly_variable']].copy(); sc.experimental.pp.normalize_pearson_residuals(adata_merge); adata_merge.layers['apr'] = adata_merge.X.copy(); sc.tl.pca(adata_merge, svd_solver=""arpack""); adata_merge.obsm['X_pca_30'] = adata_merge.obsm['X_pca'][:, :30]. adata1 = adata_merge.copy(); adata2 = adata_merge.copy(); ```. The frist test:. ```; # scanpy 1.9.6 that changes of this PR won't have taken effect yet.; # I copy the harmony_integrate from https://github.com/scverse/scanpy/blob/75cb4e750efaccc1413cb204ffa49d21db017079/scanpy/external/pp/_harmony_integrate.py; harmony_integrate(adata1, key='batch', basis='X_pca_30'); harmony_integrate(adata2, key='batch', basis='X_pca_30'); np.testing.assert_array_equal(adata1.obsm[""X_pca_harmony""], adata2.obsm[""X_pca_harmony""]); ```. It raised the Error:. ```; AssertionError: ; Arrays are not equal. Mismatched elements: 567291 [/](https://vscode-remote+ssh-002dremote-002bnansha.vscode-resource.vscode-cdn.net/) 572730 (99.1%); Max absolute difference: 1.20792265e-12; Max relative difference: 4.37537551e-09; x: array([[-0.954048, -7.21621 , -1.601975, ..., 0.059509, -0.436056,; 0.564897],; [-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227:709,layers,layers,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2655#issuecomment-1823084227,2,['layers'],['layers']
Modifiability,"The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |; |--------|--------|; | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3243:117,variab,variables,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243,2,['variab'],['variables']
Modifiability,"The variable folder has one file in .h5ad format as input or raw data. No, I execute the code correctly because every time I run this command or move forward with other commands, the number on the kernel increases without any error message. But in a folder, no object is generated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943:4,variab,variable,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817693943,1,['variab'],['variable']
Modifiability,"The variable y_axis is something I introduced in my latest PR. If you; update to the master branch you should see those changes. On Tue, Dec 4, 2018 at 2:49 AM pritykin <notifications@github.com> wrote:. > I would like to use stacked_violin plot with variable y-axis limits,; > particularly when swap_axes=True. Examples here; > <https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c>,; > particularly code in line 7, show this. How do I do this? When I use it now; > with my code, it always chooses a uniform y-axis limit for all genes. Which; > option do I use for variable y-axis limits?; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/386>, or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1VtN8DWjBSDb-YjUImPvquAJapH3ks5u1dSzgaJpZM4Y_wfC>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759:4,variab,variable,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/386#issuecomment-445273759,3,['variab'],['variable']
Modifiability,"There is a sentence at the bottom of https://www.uni-kassel.de/fb07/fileadmin/datas/fb07/5-Institute/IVWL/Kosfeld/lehre/spatial/SpatialEconometrics2.pdf slide 8 about this, but it is not very clear. In my case on this specific data scaling each row from 0 to 1 lead to I in expected range. But then sometimes I get the values in expected range also without scaling. maybe these extreme values I was getting were due to the variability problem. Regarding how to repeat it:; I currently have an odd dataset and if I run my jupytyer cell multiple times I sometimes get different results. It is odd. Sometimes also I's are within range [-1,1] and sometimes they explode.; <img width=""599"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115678736-1d59c400-a352-11eb-9f94-630faceba08d.png"">. Sometimes the differences are very extreme:; <img width=""629"" alt=""image"" src=""https://user-images.githubusercontent.com/47607471/115752535-7baa9500-a39a-11eb-93ae-ca0edd95a3cd.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639:423,variab,variability,423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-824634639,1,['variab'],['variability']
Modifiability,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:298,variab,variables,298,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['variab'],['variables']
Modifiability,"They'll both be affected by the [resolution limit](https://www.pnas.org/content/104/1/36), which might be what you're referring to. This is a well-described problem for Modularity with the configuration null model that it only optimally detects communities within a certain size range relative to the size of the network. For me heavy-tailed networks are PPIs.. KNNs are a lot more regular than that. I'm not sure what a weighted KNN graph would be... are you talking about the PhenoGraph approach?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915:189,config,configuration,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-483313915,1,['config'],['configuration']
Modifiability,"This PR addresses #646 by adding the option to pass a dict to the plotting functions heatmap, dotplot, matrixplot, tracksplot and stacked_violin. . Now, when `var_names` is a dictionary the `var_group_labels` and `var_group_positions` are set such that the dictionary key is a label and the group is the dict values. In the following example the 'brackets' plot on top of the image are prepared based on the markers dictionary:. ```PYTHON; marker_genes_dict = {'B-cell': ['CD79A', 'MS4A1'], ; 'T-cell': 'CD3D',; 'T-cell CD8+': ['CD8A', 'CD8B'],; 'NK': ['GNLY', 'NKG7'],; 'Myeloid': ['CST3', 'LYZ'],; 'Monocytes': ['FCGR3A'],; 'Dendritic': ['FCER1A']}; # use marker genes as dict to group them; ax = sc.pl.dotplot(pbmc, marker_genes_dict, groupby='bulk_labels'); ```; ![image](https://user-images.githubusercontent.com/4964309/58255475-5dcaf480-7d6d-11e9-83f6-bb4ebc8e33a7.png). This PR also introduces a small change in `sc.pl.stacked_violin` by setting `cut=0` as default parameter for `seaborn.violin`. This produces in my opinion better plots by removing the extension of the violin past extreme points. This is specially useful to avoid the violin plot to extend below zero expression values. . **Update**: I set the dependencies to `matplotlib==3.0.*` and `scipy==1.2` to solve failing tests. More details in the conversation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/661:1160,extend,extend,1160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661,1,['extend'],['extend']
Modifiability,This PR adds `.layers` support for PCA and regress_out.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2588:15,layers,layers,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2588,1,['layers'],['layers']
Modifiability,"This PR adds extra functionality to `pl.violin` adding the option produce stacked violin plots for each `key` passed. The new optional boolean argument `stripplot` was added to add/remove the stripplot on top of the violin plots. An example image is:. ![image](https://user-images.githubusercontent.com/4964309/41411458-3e105336-6fdd-11e8-8e18-07e49fe7c8d1.png). Similarly, I added `pl.heatmap` that plots variables ordered by an observation as follows:. ![image](https://user-images.githubusercontent.com/4964309/41411511-6996f334-6fdd-11e8-93ff-176b89743d83.png). An example notebook using these visualizations is [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/175:406,variab,variables,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/175,1,['variab'],['variables']
Modifiability,"This PR adds multiprocessing to the pp.regress_out function. . Here some benchmarks:. ```python; import numpy as np; import pandas as pd; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. # create a matrix with 20.000 cells with 3000 genes; adata = AnnData(random(20000, 3000, density=0.6, format='csr')); ```; **Benchmark using ordinal variables**; ```python; # create a categorical column and run regress out using ; # the categorical column; adata.obs['batch'] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0])); %timeit res = sc.pp.regress_out(adata, keys='batch', n_jobs=20, copy=True); ```; > 8.44 s ± 292 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; # import previous version of the function (which I saved in the file simple_old.py); from scanpy.preprocessing.simple_old import regress_out_old; %timeit res_old = regress_out_old(adata, keys='batch', n_jobs=1, copy=True); ```; > 1min 5s ± 4.45 s per loop (mean ± std. dev. of 7 runs, 1 loop each). **Compare that the previous and the new output are the same**; ```python; np.array_equal(res.X, res_old.X); ```; > True. **Benchmark using ordinal variables**; ```python; adata.obs['percent_mito'] = np.random.rand(adata.X.shape[0]); adata.obs['n_counts'] = adata.X.sum(axis=1). %timeit res2 = sc.pp.regress_out(adata, keys=['n_counts', 'percent_mito'], n_jobs=32, copy=True); ```; > 4.99 s ± 501 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; %timeit res2_old = regress_out_old(adata, keys=['n_counts', 'percent_mito'], n_jobs=1, copy=True); ```; > 41.2 s ± 7.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each). ```python; np.array_equal(res2.X, res2_old.X); ```; > True",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/164:374,variab,variables,374,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/164,2,['variab'],['variables']
Modifiability,"This PR adds some utilities for choosing which values to use for `X` inside scanpy functions. These utilities include a getter and setter for representations of the observations and some checks for testing these functions. This approach has been applied to `sc.pp.scale` and `sc.pp.log1p` as demonstrations, and because that code was in need of a refactor. I think it results in cleaner code with more standardized behaviors. The implementation of `sc.pp.scale` is just a hammered out version of what I commented here: https://github.com/theislab/scanpy/pull/1135#issuecomment-608200735. Still todo:. - [x] Figure out how documentation should work. In particular automatic documentation with type variables; - [x] Figure out tab completion for arguments not in the default/ fallback definition. This PR would:. * Close #1089; * Supercede #1135",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1173:347,refactor,refactor,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1173,2,"['refactor', 'variab']","['refactor', 'variables']"
Modifiability,"This PR extends the original PR #512 by @gokceneraslan which adds the `standard_scaling` parameter to matrixplot. . I added the same functionality to dotplot, heatmap and stacked_violin. Also, I integrated PR #524 by @sjfleming which adds a `smallest_dot` option to dotplot.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/528:8,extend,extends,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/528,1,['extend'],['extends']
Modifiability,"This PR introduces the following changes:. * `swap_axes` option was added to `pl.matrixplot` and `pl.heatmap`. When `swap_axes=True`, the x axis contains cells and the y axis contains genes (#349).; * added `show_genes_labels` to `pl.heatmap`. This allows to have compact heatmaps without overlapping gene labels.; * added lines to separate categories in `pl.heatmap`.; * changed categories colors in `pl.heatmap` by the colors found in `adata.uns`; * removed empty space that was present in different plots; * added a `layer` option to specify which layer to use for plotting. ; * added a new visualization called `pl.tracksplot`. ; * changed `if <variable> is True` by `if <variable>` after @flying-sheep remarks. ; * added `setup()` from matplotlib.testing; * reduced dpi of test images to 40.; * added var_groups plot for stacked_violin when `swap_axes=True` ; * improved layout of stacked_violin (default width and linewidth of violin plots). As an example, here is how some of the changes look like. I am using the `rank_genes_groups_*` plots because they contain more visual elements. Further examples can be seen here: https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c. **heatmap** with `swap_axes=True, show_gene_labels=False`:; ![image](https://user-images.githubusercontent.com/4964309/48777204-46358500-ecd2-11e8-8ced-e772e0987f95.png). **matrixplot** with `swap_axes=True`:. ![image](https://user-images.githubusercontent.com/4964309/48777447-dffd3200-ecd2-11e8-9720-31e084eec0f4.png). **new *tracksplot***: Each *track* contains the var (genes) values sorted and colored according to the categories used:; ![image](https://user-images.githubusercontent.com/4964309/48777284-7c730480-ecd2-11e8-8e3b-ab4a02969311.png). **tracksplot** using the results of `sc.tl.rank_genes_groups`:; ![image](https://user-images.githubusercontent.com/4964309/48777641-6fa2e080-ecd3-11e8-90be-e3742058eb99.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/369:649,variab,variable,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/369,2,['variab'],['variable']
Modifiability,"This adds two new convenience functions to `utils`. ## `obs_values_df`. Basically does the data access part of the scatter plots (actually copied the core of the code from there). Basically, lets you get a data frame of values from obs, obsm, and expression matrix back as a dataframe. I'd planned on this being the data access part of `ridge_plot` PR, but I've found it generally useful for data access. Also finding a feature-ful KDE that isn't buggy has been an issue for the ridge plots. This uses the obsm access I had suggested to @gokceneraslan in #613. I'm also open to adding a `var_values_df` to this PR, I just haven't had a use case yet. ## `rank_genes_groups_df`. Returns a dataframe of differential expression results, because accessing DE results right now is a pain. This was a part of #467, but I can just remove it from there. ## Whats left to do. Docs, but it's boilerplate. Do we have centralized docstrings for things like `gene_symbols`, `use_raw`, `layers`, and `adata`?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/619:972,layers,layers,972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619,1,['layers'],['layers']
Modifiability,This collects new features of version 0.3.1 while extending other issues such as https://github.com/theislab/scanpy/issues/45.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/51:50,extend,extending,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/51,1,['extend'],['extending']
Modifiability,"This fixes #415, by allowing one to find variable genes using the `equal_frequency` option. It also adds and option to change the number of bins for cell ranger flavor. I originally tried to copy the implementation in Seurat, which would allow a test similar to what's already present for the `equal_width` implementation. However the Seurat code has an error:; ```R; else if (binning.method==""equal_frequency"") {; data_x_bin <- cut(x = gene.mean, breaks = c(-1,quantile(gene.mean[gene.mean>0],probs=seq(0,1,length.out=num.bin)))); }; ```; The `-1` in the code makes it such that there is always only one value in the first bin, which goes from -1 to the minimum value. Not sure why they have this, but then we get different answers since the Scanpy code in `highly_variable_genes` always makes bins that have only one gene significant (to correct the other error from Seurat that normally excludes these bins/genes, which often contain some highly-expressed genes). Additionally, the `cut` function in R sometimes returns bin edges with different rounding than the Seurat implementation since Seurat does not modify the default `dig.lab = 3`. In contrast, I believe pandas uses the actual cutoffs in the data.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/572:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/572,1,['variab'],['variable']
Modifiability,"This fixes the `UnboundLocalError: local variable 'ig_layout' referenced before assignment` exception that happens in following scenario:. ```; sc.tl.louvain(adata); sc.tl.paga(adata); sc.pl.paga(adata); sc.tl.draw_graph(adata); ```. Since there is no else statement for use_paga check, ig_layout is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/124:41,variab,variable,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/124,1,['variab'],['variable']
Modifiability,"This fixes:; ```; Traceback (most recent call last):; File ""/private/var/folders/df/6xqpqpcd7h73b6jpx9t6cwhw0000gn/T/tmpn9tl9wf0/job_working_directory/000/2/configs/tmp_r9i0cvx"", line 15, in <module>; sc.pl.dpt_timeseries(; File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 171, in dpt_timeseries; timeseries_as_heatmap(; File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/scanpy/plotting/_utils.py"", line 206, in timeseries_as_heatmap; pl.colorbar(shrink=0.5); File ""/Users/mvandenb/miniconda3/envs/anndata/lib/python3.8/site-packages/matplotlib/pyplot.py"", line 2188, in colorbar; raise RuntimeError('No mappable was found to use for colorbar '; RuntimeError: No mappable was found to use for colorbar creation. First define a mappable such as an image (with imshow) or a contour set (with contourf).; ```; I see that in other places plt.colobar is used in this module you're; doing the same thing. I believe this broke in; 64f04d8.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1654:157,config,configs,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1654,1,['config'],['configs']
Modifiability,"This includes fixes for both #469 and #470 . #469 was a small indexing error. To fix #470, a `rankby_abs` check is included in the `logreg` section of the method that mirrors the `rankby_abs` checks in the other two methods. This PR additionally updates `select_groups` function in `scanpy/utils.py.` I was having some issues when the clusters that I was using were labelled by integers (i.e. when `adata.obs[key].cat.catagories.values.dtype` was some form of integer) AND when I was looking at a subset of the clusters (e.g. `groups=[0,1]`, not when `groups='all'`). At the start of the `rank_genes_groups` function, these cluster labels are converted into strings in the `groups_order` variable. In the `select_groups` function (line 667 of the original utils.py file), however, we call ; ``` ; np.where(adata.obs[key].cat.categories.values == name)[0][0]; ```; which fails with an error (since `name` is a string from `select_groups` and the elements of `adata.obs[key].cat.categories.values` are integers). Thus, this PR includes a check for the `dtype` of `adata.obs[key].cat.categories.values` - if it is numeric, we instead look at ; ``` ; np.where(adata.obs[key].cat.categories.values == float(name))[0][0]; ```. This error should only appear if the cluster labels are integers (since this is the only time that the cluster labels are converted to strings for `groups_order` in `rank_genes_groups`) but the above fix should also work if the cluster labels are any floating point numbers (just in case the `rank_genes_groups` is ever generalized in this way). ([Here](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.scalars.html) is a link to the numpy type hierarchy). Edit: added a line number",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/471:688,variab,variable,688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/471,1,['variab'],['variable']
Modifiability,This is a pull request to integrate the Self-Assembling Manifold (SAM) algorithm into scanpy. A brief summary of the method:; SAM iteratively rescales the expressions of genes based on their spatial variability along the intrinsic manifold of the data. Extensive benchmarking has shown this approach to improve dimensionality reduction and feature selection for both 'easy' and 'challenging' datasets. SAM is especially powerful when analyzing datasets with only subtle differences in gene expression between cell types. More information can be found in the eLife publication: https://elifesciences.org/articles/48994. I still need to write the test script as well as ensure that the added code follows the BLACK coding style. Please let me know if there are any other issues I should fix prior to merging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/903:199,variab,variability,199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/903,1,['variab'],['variability']
Modifiability,This is again a configuration problem. You don't have http://igraph.org/python/ installed. That's a library with thousands of users and citations. It has a very powerful and fast C++ core that allows treating dataset sizes with a million cells. I realize that I misspecified this in Scanpy's automatic installation in the requirements file. I just updated this and will push it to the master branch. You simply need to type `pip install python-igraph` and then everything should work.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126:16,config,configuration,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35#issuecomment-324589126,1,['config'],['configuration']
Modifiability,"This is already allowed, we just don't have a separate argument for it. Just pass a list of the variables you'd like to groupby. For example:. ```python; import scanpy as sc, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); genes = list(pbmc.uns[""rank_genes_groups""][""names""][0]). pbmc.obs[""batch""] = np.random.choice([""a"", ""b""], pbmc.n_obs); sc.pl.dotplot(pbmc, genes, [""louvain"", ""batch""]); ```. ![tmp](https://user-images.githubusercontent.com/8238804/110577481-d30fef80-81b6-11eb-93f3-4adb2f269e78.jpg)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272:96,variab,variables,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1729#issuecomment-794866272,1,['variab'],['variables']
Modifiability,"This is just being built up, but yes, the layers have the same constraints as in loom files and are meant to enable the transformation of loom files to AnnData objects without loss of information. Here's the stub of the docs: https://anndata.readthedocs.io/en/latest/anndata.AnnData.layers.html. Here's the very first draft https://github.com/theislab/scvelo of our stochastic RNA velocity package... There will be more information on this very soon. :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407:42,layers,layers,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/236#issuecomment-414602407,2,['layers'],['layers']
Modifiability,This is solved in PR #425 (which also includes other enhancements),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/465#issuecomment-461456817:53,enhance,enhancements,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/465#issuecomment-461456817,1,['enhance'],['enhancements']
Modifiability,"This is the anndata working properly:; ```; AnnData object with n_obs × n_vars = 24759 × 29612; obs: 'sample', 'batch', 'n_counts'; var: 'ensembl_id', 'n_cells'; layers: 'counts'; ```; After normalization and logarithmize it with:; ```; adata.X = sc.pp.normalize_total(adata, inplace=False)['X']; adata.X = sc.pp.log1p(adata.X); ```; And computing PCAs, neighbors and UMAP coordinates this is a plot showing the expression of GRIK1 f.e:. ![image](https://github.com/scverse/scanpy/assets/94078098/b2a1cc8e-d4d8-4a32-91e3-a2a55857a140). Then, this is the anndata that after normalization does not show gene expression in UMAP:; ```; AnnData object with n_obs × n_vars = 17217 × 33704; obs: 'Age', 'Condition', 'Origin', 'Region', 'Sex', 'Subject', 'louvain', 'louvain6', 'obs_names', 'sample', 'batch', 'dataset'; var: 'dispersions', 'dispersions_norm', 'gene_ids', 'highly_variable', 'means', 'n_cells', 'var_names'; obsm: 'X_umap'; layers: 'counts'; ```; Because this anndata has pre-computed UMAP coordinates and the raw data was normalized with sizefactors in R, when reading the file, adata.X is already normalized, and if I plot the UMAP for SLC5A11 f.e this is the result: ; ![image](https://github.com/scverse/scanpy/assets/94078098/9e0c6958-b882-4f28-b7ac-dda5d58cbcba). However, if I select the raw counts of this anndata (stored in layers['counts']) and normalize it with `sc.pp.normalize `function and logarithmize it, this is the output of `sc.pl.umap` (it doesn't matter re-computing PCAs, neighbors and UMAP):; ![image](https://github.com/scverse/scanpy/assets/94078098/365ec629-3eea-4e58-ad0d-6ff3004d3c13). UMAP after recomputing PCAs, etc:; ![image](https://github.com/scverse/scanpy/assets/94078098/6727a53b-31ac-414a-b93d-55baa1688f85)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2556#issuecomment-1643597794:162,layers,layers,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2556#issuecomment-1643597794,3,['layers'],['layers']
Modifiability,"This is the origin of the error:. Some of the cells don't have neighbours at the variable adata_ref.obsp['distances'].tolil().rows:. ```; array([list([223, 280, 316, 5791]), list([3877, 5899, 7766, 7807]),; list([165, 304, 423, 713]), ..., list([]),; list([94, 865, 7077, 7666]), list([])], dtype=object); ## (the maximum 4 elements of each list comes from having run sc.pp.neighbors(adata_ref, n_neighbors = 5)) ##; ```. The above array is impossible to stack with np.stack due to the sublists having different lengths. A potential solution might be filtering out those cells without neighbours, though this is suboptimal. I have tried it and new rows remain empty. Only after repeating it a second time, it works:. ```; DEFINED_NEIGHB_NUM =5; sc.pp.pca(adata_ref); sc.pp.neighbors(adata_ref, n_neighbors = DEFINED_NEIGHB_NUM ); sc.tl.umap(adata_ref). b = np.array(list(map(len,adata_ref.obsp['distances'].tolil().rows))) == DEFINED_NEIGHB_NUM -1; adata_ref = adata_ref[b]; ```. A better solution would be correcting the Nearest Neighbour assignment so that it doesn't create empty distance lists. Did I understand this correctly?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104437780:81,variab,variable,81,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104437780,1,['variab'],['variable']
Modifiability,"This issue references the following line of code:. https://github.com/theislab/scanpy/blob/560bd5d348a502d5152eaf20f5f8bef794562a97/scanpy/plotting/_dotplot.py#L185. The documentation accurately describes the `standard_scale='var'` normalization strategy as ; ""Whether or not to standardize the given dimension between 0 and 1, meaning for each variable or group, subtract the minimum and divide each by its maximum."". Something about this normalization has bothered me for a long time, and I finally realized: it's the subtraction of the minimum value. This subtraction means that the minimum valued dot will have color = 0. Imagine a case with only two `groupby` groupings: healthy and disease. In that case, one of the dots will always have color 0, and the other will have color 1. Totally binary, no matter how close the actual values are. I feel that this kind of normalization is very misleading for scRNA-seq data!. A random example follows:. This image makes it look like these genes are very specific to one tissue or another ---------------; ![image](https://user-images.githubusercontent.com/10214815/112070104-6d6f1c00-8b43-11eb-9944-b113da55a567.png). But in reality, if we had scaled by just dividing by the max (and not first subtracting the min), then we'd see -------; ![image](https://user-images.githubusercontent.com/10214815/112070028-4add0300-8b43-11eb-8317-412b74b274aa.png). which is much more realistic, and much closer to what you'd see if you used a log-scaling instead, and made several different plots for genes expressed at different levels overall. I leave it up for discussion, but I would suggest two types of fixes:; 1. delete the above line 185 (and the other places it shows up...); 2. allow the user to specify a custom normalization function (but change the default to `x / max(x)` instead of `(x - min(x)) / max(x)`)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757:345,variab,variable,345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757,1,['variab'],['variable']
Modifiability,"This line:. https://github.com/scverse/scanpy/blob/383a61b2db0c45ba622f231f01d0e7546d99566b/pyproject.toml#L162. means that pytest imports that module together with the other plugins, and *then* runs tests. That means that it will `import scanpy.testing._pytest` (i.e. `scanpy` and everthing imported in there) before pytest-cov is loaded and can do anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122:175,plugin,plugins,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122,1,['plugin'],['plugins']
Modifiability,"This looks great!. A few ideas:. * For having an outline to separate overlapping clusters, I don't think I like that one of the outlines would be plotted over the other cluster. In the plots shown above (https://github.com/theislab/scanpy/pull/794#issuecomment-523515331) I think the upper image is less clear about the extent of the overlap than the lower one, and suggests a greater importance of group `3`. Maybe there could be some indication of ambiguity for the region of overlap?; * For the string based quantile selection, is there another package which allows writing operations like this? My concern is that string based DSLs can get messy. It would be nice to make sure we're choosing a unambiguous spec which we can extend in the future and use in other functions. An example of a spec would be SQL reduction operations (like `PERCENTILE_DISC`), but hopefully there would be something less verbose.; * For the basis argument, could we not require the key in `obsm` start with `X_`? I'm thinking the key would just go through a check like:. ```python; if basis in adata.obsm:; basis_key = basis; elif f""X_{basis}"" in adata.obsm:; basis_key = f""X_{basis}""; else:; raise KeyError(; f""Could not find entry in `obsm` for '{basis}'.\n""; f""Available keys are: {list(adata.obsm.keys())}.""; ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596:728,extend,extend,728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794#issuecomment-523732596,1,['extend'],['extend']
Modifiability,"This may be related to this issue:; https://github.com/theislab/scanpy/issues/918#issue-522668041. I was running:. `sc.tl.umap(bdata, init_pos='paga')`. But it gave me this error:. ```; TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)) with parameters (array(float64, 1d, C), array(float64, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7fca8d70fc80>)); [2] During: typing of call at /usr/local/lib/python3.6/dist-packages/umap/umap_.py (795). File ""../../usr/local/lib/python3.6/dist-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/936:476,parameteriz,parameterized,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/936,1,['parameteriz'],['parameterized']
Modifiability,"This might be true the expected behaviour is not mentioned here.; Then as you pointed out it is more of an enhancement, which would be to make it match other plotting functions behaviour ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3205#issuecomment-2446164660:107,enhance,enhancement,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3205#issuecomment-2446164660,1,['enhance'],['enhancement']
Modifiability,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141:166,plugin,plugins,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-425031141,2,['plugin'],"['plugin', 'plugins']"
Modifiability,This pull request is for calculating and plotting cell densities on an embedded representation. This is especially useful together with an `.obs` covariate to calculate and visualize cell densities over conditions. Code is adapted from raw version by @sophietr . Still work in progress...,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/543:223,adapt,adapted,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/543,1,['adapt'],['adapted']
Modifiability,"This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s ; after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3294:301,variab,variable,301,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294,1,['variab'],['variable']
Modifiability,"This will be the discussion that no one ever wants since developers like us tend to bring our mice and keyboards to fight when it comes to questions like these, but hey I thought at some point it should be had anyways haha. @ivirshup recently introduced black as the formatting standard for Scanpy. I am not a fan of blacks formatting, but the idea of consistent formatting for big open source projects is great! So +1 from me. Anyways, currently black formats with 88 characters per line, which makes, especially with black, for lots and lots of line breaks and encourages bad practices like unspecific short variable names etc. Modern Python programming is not C programming from the 80s. Have a read at Linus rant on the 80 character limit in the Linux kernel and why the Linux kernel does **not** enforce it: https://lkml.org/lkml/2020/5/29/1038 . Applying black with a 120 characters limit removes about 1500 lines. That's 1500 lines that you have to scroll less and in my opinion the result is more readable. What do you think?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1694:610,variab,variable,610,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1694,1,['variab'],['variable']
Modifiability,"This worked for me as well.; Amazing thanks!. > I had the same issue, and it turns out setting up channels solves the problem as follows:; > ; > ```; > conda config --add channels defaults; > conda config --add channels bioconda; > conda config --add channels conda-forge; > ```; > ; > Ref:; > https://bioconda.github.io/recipes/scanpy/README.html; > https://bioconda.github.io/user/install.html#set-up-channels",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267:158,config,config,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-613099267,3,['config'],['config']
Modifiability,"To not repeat ourselves @ivirshup (I think) suggested this. Let’s see if readthedocs supports this. If so, this should soon be visible: https://icb-scanpy.readthedocs-hosted.com/en/inherit-requirements/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/898:181,inherit,inherit-requirements,181,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898,1,['inherit'],['inherit-requirements']
Modifiability,"Totally forgot about this one sorry :(. **The problem** ; In `scatter_base`: https://github.com/theislab/scanpy/blob/040e61ff50836d4a6cdd7da7482dcb4ee50d05ae/scanpy/plotting/_utils.py#L736-L740. For non categorical variables, this code gets the current figure and adds a separate axis on which the colorbar is plotted.; Therefore, the axes objects on which the data is plotted do not contain a legend object.; Instead, `fig` should contain the colorbar axis and we could maybe manage to manipulate it as a workaround. There is also this DeprecationWarning popping up.; ```pytb; MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance. In a future version, a new instance will always be created and returned. Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.; ax_cb = fig.add_axes(rectangle); ```. **Current workaround (also for all other sort of plots)**; The problem here is really that we don't have two separate figures / axes aren't handled correctly.; Good news is, that there is a way around using `plt.subplots` and using given `Axes` objects. even if we want to plot 2 plots side by side in a jupyter notebook (original post here: https://stackoverflow.com/questions/21754976/ipython-notebook-arrange-plots-horizontally).; However, `sc.pl.scatter` isn't exposing the figure object but only the axis. But if we specify `show=False`, it returns the axis and we can obtain the figure object using `matplotlib.pyplot.gcf()`.; Store these figures in a list and pass them to the `plot_nice()` function which will plot all your figures side by side until it runs out of space, after which it will create a linebreak and continue. Therefore, you can specify how many figures you want to plot per line, using the individual `figsize` argument. For my example it would look like this:; ```python; from flow_layout import plot_nice # import the required plot",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283:215,variab,variables,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1258#issuecomment-713492283,1,['variab'],['variables']
Modifiability,"Tried to install via `$ pip3 install -e .` but returned this error:; ```; Obtaining file://path/to/scanpy_1.4/scanpy; Complete output from command python setup.py egg_info:; /path/to/miniconda3/envs/bio/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""path/to/scanpy/setup.py"", line 11, in <module>; from scanpy import __author__, __email__; File ""path/to/scanpy/scanpy/__init__.py"", line 26, in <module>; check_versions(); File ""path/to/scanpy/scanpy/utils.py"", line 38, in check_versions; .format(__version__, anndata.__version__)); NameError: name '__version__' is not defined. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in path/to/scanpy/; ```; The variable `__version__` in line 38 in utils.py is not defined.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/482:1036,variab,variable,1036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/482,1,['variab'],['variable']
Modifiability,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:688,variab,variable,688,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,Unable to subset a new adata by highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['variab'],['variable']
Modifiability,Use matplotlib 3.1 and adapt tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1020:23,adapt,adapt,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1020,1,['adapt'],['adapt']
Modifiability,Using RMM works but only to a certain extend. As far as I understand it you can oversubscribe VRAM to a maximum of 2X. If you go above that you’ll get a memory alloc error.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372:38,extend,extend,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1107449372,1,['extend'],['extend']
Modifiability,"Using `coverage run` is intentional. Please just configure the shell so it exits when a line fails instead of changing this. The reason is at we have `addopts = ['-p', 'scanpy.testing.pytest']` (or so). If we use `pytest --cov`, coverage for things run during import breaks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956852933:49,config,configure,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956852933,1,['config'],['configure']
Modifiability,"Very strange. `variable` is some assigned name after an internal `pandas.melt`. . First, I would not recommend to plot all `adata.var_names` unless they are fewer (<30). But that seems not to be the problem. To discard a problem with seaborn violin plot, can you try `sc.pl.matrixplot` instead?. Also, do you get the same output in both cases after. ```; adata.obs.head(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/318#issuecomment-431813153:15,variab,variable,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/318#issuecomment-431813153,1,['variab'],['variable']
Modifiability,We could then also consider extending it with rapids single cell @Intron7 . More of a `scverse reproducibility` page and maybe also bring in scvi tools,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696:28,extend,extending,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-2047411696,1,['extend'],['extending']
Modifiability,"We have a weird temporary global variable called `sc.pl._utils._tmp_cluster_pos`. We use it for storing the positions of cluster centroids (actually the centroids of any categorical variable for any sort of embedding). The weird part is that it's set in scatterplot functions (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_anndata.py#L468 and https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/scatterplots.py#L809) and used only by `sc.pl.paga_compare` (https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/paga.py#L119). First, it's not obvious where paga_compare finds centroids (it was a mystery to me until recently). Second, the current design is error-prone (see a corner case https://github.com/theislab/scanpy/issues/686). Therefore, there should be a better place to store cluster centroids :). I'm not following the discussion about the future of AnnData, but maybe having something like `adata.uns['obs_category_leiden']` and storing colors and centroids in it e.g. `adata.uns['obs_category_leiden']['colors']` and `adata.uns['obs_category_leiden']['centroids']['X_umap']` would be more structured.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/938:33,variab,variable,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/938,2,['variab'],['variable']
Modifiability,"We probably have two problems:. 1. CI doesn’t run; 2. Scanpy got harder to use for people. I think the first [is easy to fix](https://github.com/numba/numba/blob/c13c840a8f1f038c1e78472db472a8f19a0bd564/numba/core/config.py#L309): We just `export NUMBA_THREADING_LAYER=workqueue` in our tests. The second is harder, but first I want to note something:. > This was fine in the past, since pynndescent/ umap were forcing a workqueue backend which is always available. I wouldn’t call that situation *fine*, doing things at import time or even just requiring a certain value as configurable global state is bad behavior. This means our solution for the second shouldn’t be that we hardcode a threading layer to use here. We could make it configurable on our end or something, but no import time global state change. #1933 only fixes CI … also bad issue number, yikes!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964:214,config,config,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1931#issuecomment-874649964,3,['config'],"['config', 'configurable']"
Modifiability,"We should definitely maintain the type in layers, and that means maintaining the type in .X makes sense too. We should also take care not to downcast more incompatible types: int32 can be expressed as float64, but not in float32. int64 has to stay int64.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634:42,layers,layers,42,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-558138634,1,['layers'],['layers']
Modifiability,"We should have a discussion about this separate from #265. There’s three options how to implement them. 1. Python has a built-in way of registering “entry points” which might be suitable. [A tutorial](https://amir.rachum.com/blog/2017/07/28/python-entry-points/). ```py; setup(; # ...; entry_points={; 'scanpy.extensions': ['myextension = my.extension.module'],; },; ); ```. 2. Some projects have a module like `scanpy.ext`. An extension is a module installed into there (which is possible in python):. ```py; setup(; name='scanpy-ext-myextension',; # ...; packages=['scanpy.ext.myextension'],; ); ```. 3. Some projects provide a config value like `scanpy.settings.extensions` to which module names can be `append`ed. Unlike the first two, this means the user has to enable them explicitly. Pytest uses [both 1 and 3](https://docs.pytest.org/en/latest/writing_plugins.html) for its plugins. Sphinx uses [both 1 and 3](http://www.sphinx-doc.org/en/master/extdev/index.html) for different things: they want their “builders” to be automatically discovered and the other extensions to be explicitly enabled. ---. Another option for 2. is to use [flask’s approach](http://flask.pocoo.org/docs/1.0/extensiondev/) of just naming modules `scanpy_*` which is too cute and magic to me. Veto.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271:630,config,config,630,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271,2,"['config', 'plugin']","['config', 'plugins']"
Modifiability,"Well, `project_dir` is configurable:. https://github.com/theislab/scanpydoc/blob/02a0fcb5b5ddfd1f9427c27e736e83126f6cfc64/scanpydoc/rtd_github_links.py#L144. but why does it go to `__init__.py`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/437#issuecomment-456209252:23,config,configurable,23,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/437#issuecomment-456209252,1,['config'],['configurable']
Modifiability,"Well, the documentation of `highest_expr_genes` doesn’t say that it supports layers, but this is a very sensible feature request.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437659493:77,layers,layers,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437659493,1,['layers'],['layers']
Modifiability,"Went with properties for everything except private variables. Took a little longer than 10 minutes, but I think it's mostly there. Got all the tests to pass on my machine, but I bet other things will fail. I also haven't tested what'll happen with the docs, though I did have to modify some of the documentation code. * The `verbosity` settings might be trouble. I changed the value again... but this should stop an error I'm getting with bbknn and be consistent with the python logging module. ; * The settings imports are ugly. Wasn't sure how to import a variable from a parent module. Is this possible?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-479505756:51,variab,variables,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-479505756,2,['variab'],"['variable', 'variables']"
Modifiability,"We’re thinking about making the backend configurable through something like https://github.com/frankier/sklearn-ann (that specific one doesn’t seem maintained though). A recipe for this is found here: https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py. Faiss does seem nice as an option, but a hard dependency on something that isn’t on PyPI is out of the question.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399:40,config,configurable,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2519#issuecomment-1603957399,1,['config'],['configurable']
Modifiability,What is the content of the variable `folder`? There must be an error message or else you are not executing the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273:27,variab,variable,27,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1795#issuecomment-817683273,1,['variab'],['variable']
Modifiability,"What is there in the latest update:; - The simple adoption, at the heart of this PR, that `flavor=seurat_v3_paper` matches Seurat better when using `batch_key`.; - The `flavor=seurat_v3` remains untouched, hence not a breaking change.; - The doc is more detailed now. What is not there:; - Refactoring of single vs multi batch. Reason: While this effort will enhance code maintenance, it may quickly require almost the entire _highly_variable_genes.py to be touched. Suggest to do this thorough & separately?; - orthogonality of flavor and ordering. Reason: I think this is very hard to understand and match against other methods for users. . > If it makes sense to offer a common set of orderings for all flavors, it should definitely be a separate option. Does it make sense? There isn't benchmarking literature I know, and the flavors don't offer a decoupled ordering choice themselves. From user issues, I experience the consistency with other tools to be the primary concern.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285:290,Refactor,Refactoring,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792#issuecomment-1919485285,2,"['Refactor', 'enhance']","['Refactoring', 'enhance']"
Modifiability,"What stays the same:. - `pip install scanpy`; - `pip install . `; - `pip install git+https://...`; - you can install your deps with conda; - you can do a dev install. What changes:. - Please check the [install docs](https://scanpy.readthedocs.io/en/flit-for-isaac/installation.html#development-version), in short:; - `pip install -e .[every,single,extra]` → `flit install -s` for dev installs; - `beni pyproject.toml > environment.yml` for conda; - Extremely simple `flit build` and `flit publish`. Maybe install `keyring` to store your publish password, and you know everything you need to.; - `flit build` doesn’t clutter your dev directory with `build/` and `*.egg-info/` junk, it just creates `dist/scanpy-*{.whl,.tar.gz}`.; - No more obscure stuff nobody understands (MANIFEST.in, package_data, …); - Centralized setup configuration in pyproject.toml instead of spread over multiple files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527:824,config,configuration,824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527,1,['config'],['configuration']
Modifiability,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/171:56,variab,variables,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171,3,['variab'],"['variable', 'variables']"
Modifiability,"When running QC on a single-cell dataset with less than 500 features and leaving argument defaults, an index error is raised. This is because the default for `percent_top` assumes genomics data with 500+ genes. Such a number is not necessarily common for other OMICS, like metabolomics (typically 150-300). ```python; adata = anndata.AnnData(np.random.random(100**2).reshape((100, 100))); sc.preprocessing._qc.describe_obs(adata). Traceback (most recent call last):; File ""~/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py"", line 3398, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-61-7293f2baee95>"", line 1, in <cell line: 1>; sc.preprocessing._qc.describe_obs(adata); File ""~/conda/lib/python3.8/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""~/conda/lib/python3.8/site-packages/scanpy/preprocessing/_qc.py"", line 397, in top_segment_proportions; raise IndexError(""Positions outside range of features.""); IndexError: Positions outside range of features.; ```; (scanpy==1.9.1). In my case, I can not directly specify `percent_top`, because the ScanPy QC is called from a third-party library, which leaves all defaults. Ideally, defaults for unspecified arguments would be compatible with all inputs that the user specifies explicitly. I have some questions:; - Now as the signature's default is None, is there a way to still make the signature in the documentation show the actual default? Or should I also change the docstring's Params section?; - I chose to have the `if` condition in argument order at the top (before `parallel`). Tell me if you prefer me to merge it with the percent_top condition in the middle (line 116), or to refactor the tuple out as a constant.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2346:1778,refactor,refactor,1778,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2346,1,['refactor'],['refactor']
Modifiability,"When using **scanpy.pl.paga_path**, I experience the same error as @plrlhb12 (TypeError: **float() argument must be a string or a number, not 'csr_matrix'**) and I can also only generate a plot after deleting adata.raw. As a consequence, I can only plot genes that are filtered for high variability during preprocessing and still present in adata.var.gene_ids. ; I would be glad if there was a way to make it work without deleting adata.raw and therefore being able to plot also non-highly variable genes! Thank you!. **Versions:**; > anndata==0.7.4 matplotlib==3.3.0 numpy==1.19.1 pandas==1.1.0 scanpy==1.6.0 scipy==1.5.2 sklearn==0.23.1 igraph==0.8.2 leidenalg==0.8.1 umap==0.4.6",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766:287,variab,variability,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1295#issuecomment-690431766,2,['variab'],"['variability', 'variable']"
Modifiability,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and there’s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095:207,adapt,adapted,207,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607#issuecomment-483195095,1,['adapt'],['adapted']
Modifiability,Works for me! I’d say we refactor the helper function in a separate PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/824#issuecomment-530443204:25,refactor,refactor,25,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824#issuecomment-530443204,1,['refactor'],['refactor']
Modifiability,"Yeah the marker assignments here are random, that might be a bit confusing indeed. The only point is that the colors for the clusters (columns) match with the *names* of the marker gene lists (rows). So if, in an actual dataset, you now name the correct clusters ""T cells"", ""B cells"" etc. in you obs.clusters variable, and match those names with your marker gene dict keys, everything will match up. ; Does that make sense?; I just didn't go through annotations of the clusters here, that would be a bit tedious. But try it out on your own (correctly annotated) anndata object if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875:309,variab,variable,309,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1511#issuecomment-734848875,1,['variab'],['variable']
Modifiability,"Yeah, AnnData doesn’t serialize arbitrary attributes to disk. I assume the output of `fit_transform` is cell×gene? Then you could do `all_data.layers['magic'] = ...`. If the output is `cell×y` with `y != n_genes` then you should do `all_data.obsm['magic'] = ...`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/426#issuecomment-454072095:143,layers,layers,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/426#issuecomment-454072095,1,['layers'],['layers']
Modifiability,"Yeah, I assumed it might be useful for other algorithms that use a variable number of PCs as input as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559186183:67,variab,variable,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559186183,1,['variab'],['variable']
Modifiability,"Yeah, I can't reproduce it with a canned dataset either --- I'm doing something a bit weird and transforming imaging mass cytometry data into AnnData objects (hence the `imctools` dependency). I have an object that looks like:; ```{python}; AnnData object with n_obs × n_vars = 68865 × 29; obs: 'nuclei_counts', 'n_antibodies_by_intensity', 'log1p_n_antibodies_by_intensity', 'total_intensity', 'log1p_total_intensity', 'n_counts'; var: 'ab_mass', 'ab_name', 'n_cells_by_intensity', 'mean_intensity', 'log1p_mean_intensity', 'pct_dropout_by_intensity', 'total_intensity', 'log1p_total_intensity', 'highly_variable'; uns: 'spatial', 'log1p', 'pca',; obsm: 'X_spatial', 'X_spatial_lowres', 'X_pca'; varm: 'PCs'; layers: 'cleaned', 'normed', 'lognormed'; ```. I will probably raise this with `pynndescent` then because; ```; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=15) # <-- works; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=11) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""euclidean"", n_neighbors=5) # <-- crashes; sc.pp.neighbors(imc42, n_pcs=10, metric=""correlation"", n_neighbors=5) # <-- crashes; ```. Sorry for hijacking this issue @giovp and @TiongSun .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223:710,layers,layers,710,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1696#issuecomment-797647223,1,['layers'],['layers']
Modifiability,"Yeah, that looks pretty weird. In your example, where did the variable `integrated_anterior` come from?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049165791:62,variab,variable,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2143#issuecomment-1049165791,1,['variab'],['variable']
Modifiability,"Yeah, that makes absolute sense. . Even better would be to add layers similar to `adata.X` and pass that into any function. Something like `adata.obs_custom`. In which case `adata.obs_custom` will inherit the same properties as that of `adata.obs` and users can make as many as they need in an organized manner. It will also allow users to store different values with the same column name (of course in different layers). e.g. `adata.obs_custom['same_column_name']` and `adata.obs_custom2['same_column_name']`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839:63,layers,layers,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621581839,3,"['inherit', 'layers']","['inherit', 'layers']"
Modifiability,"Yeah, the raw data has been indeed assigned before I subsetted through highly variable genes. It might be this sort of mismatching that is problematic.; However I encounter the same problem when trying to plot a layer. The layers should contain the same set of genes as the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456776304:78,variab,variable,78,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456776304,2,"['layers', 'variab']","['layers', 'variable']"
Modifiability,"Yeah, the task is running fine, but it's not including the license locally. It's also including a different set of files than flit does, which seems like a configuration issue. I think we need to add some more checks to the build task.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328:156,config,configuration,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1909#issuecomment-874368328,1,['config'],['configuration']
Modifiability,"Yes I would like to separate both `sample` and `leiden_r1`, I should create a new variable `adata.obs['leiden+sample']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609699372:82,variab,variable,82,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2530#issuecomment-1609699372,1,['variab'],['variable']
Modifiability,"Yes, I saw that too but I was hoping it could be prevented as it seems suboptimal to me and is probably confusing when looking at the code. Is there a specific reason for using `seaborn.FacetGrid` instead of `seaborn.catplot` (see [here](https://seaborn.pydata.org/generated/seaborn.catplot.html#seaborn.catplot))? Replacing the lines. ```python; # ...; kwds.setdefault('cut', 0); kwds.setdefault('inner'). if multi_panel and groupby is None and len(ys) == 1:; # This is a quick and dirty way for adapting scales across several; # keys if groupby is None.; y = ys[0]; g = sns.FacetGrid(obs_tidy, col=x, col_order=keys, sharey=False). # don't really know why this gives a warning without passing `order`; g = g.map(sns.violinplot, y, orient='vertical', scale=scale, order=keys, **kwds); ```. [here](https://github.com/theislab/scanpy/blob/f704f724529def21769ee6407f9b47b5c161564c/scanpy/plotting/_anndata.py#L736) by. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); ```. gives the desired plot (except for different size). The strip plot can be added on top by calling `seaborn.stripplot` afterwards:. ```python; if multi_panel and groupby is None and len(ys) == 1:; g = sns.catplot(y=y, data=obs_tidy, kind=""violin"", col=x, col_order=keys, sharey=False, order=keys, **kwds); if stripplot:; sns.stripplot(y=y, data=obs_tidy, jitter=jitter, color=""black""); ```. At the moment, I am just unsure how to plot to each subplot of the new `g`. It might be possible to loop through the plot grid so not to add everything on top of the last plot. ![master_violin_multi_panel](https://user-images.githubusercontent.com/28675704/93460801-16626300-f8e4-11ea-8f46-ed7ff64d8efb.png). Besides the not yet solved strip plot problem, would that be a valid alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139:497,adapt,adapting,497,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1420#issuecomment-694154139,1,['adapt'],['adapting']
Modifiability,"Yes, I'll send an example in a bit, recovered variable genes seem wildly discrepant. I can get to this tomorrow! Thanks for your quick response",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1865046956:46,variab,variable,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1865046956,1,['variab'],['variable']
Modifiability,"Yes, Joshua, thank you... it makes sense that it takes the nonzero overlap - this is what I meant with ""boolean gene expression"". 🙂 And yes, on PCA this does not make sense at all. OK, you dug out the private function `tools._utils.choose_representation`. This returns the PCA representation if the data matrix has more than 50 variables: See the documentation of the **use_rep** argument [here](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors) and the code https://github.com/theislab/scanpy/blob/8e06ff6ecfab892240b58d2206e461685216a926/scanpy/tools/_utils.py#L22-L43. This behavior is intended as it is rarely advisable to compute distances on an uncompressed data matrix with more than 50 dimensions. Don't you think so? If `.X` is already a 100-dimensional compressed latent representation of another model, then, of course, a PCA on top of that could be nonsense - here I'd agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/177#issuecomment-399892886:328,variab,variables,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/177#issuecomment-399892886,1,['variab'],['variables']
Modifiability,"Yes, but if the user needs the raw counts of all genes, he/she shouldn't deal with ""unnormalizing"" things (which is non-trivial for beginners, but not for you 😄). So, it's better to adapt scanpy to easier workflows, not the other way around due to the limitations of scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468:182,adapt,adapt,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819784468,1,['adapt'],['adapt']
Modifiability,"Yes, for sure, one of the two commits I just pushed adds a few tests for various cases of `sc.pl.scatter()` with `basis=None`. You're totally right that plotting the transposed matrix (i.e., cells as points in the scatterplot, instead of genes) does not work. I think this is a separate problem from the one I'm fixing here, in response to the issue #1097. It seems to me there are two problems going on:; 1. The wrapper function `sc.pl.scatter()` mistakenly raises a ValueError if x, y, or color are var_names that exist only in raw but not in the base layer, even if `use_raw=True`. The underlying `_scatter_obs()` has no problem dealing with this situation, so to solve this, `sc.pl.scatter()` just needs to call `_scatter_obs()` in this case instead of raising a ValueError. This PR fixes that.; 2. When x, y, and/or color are variables found in `obs.index` or `var.keys()`, `sc.pl.scatter()` makes a transposed version of `adata`, but as you said, `adata.raw` does not get transposed. This leads to an `AttributeError` on this line of `_scatter_obs()`:. https://github.com/theislab/scanpy/blob/cab9f781f9fdee2eeebf05a84c2ce5f717afa514/scanpy/plotting/_anndata.py#L250; `AttributeError: 'NoneType' object has no attribute 'obs_vector'`. I'm not sure what the correct way to handle this is, so for now, I'm; * adding a commit so that this PR does not modify anything in the part of `sc.pl.scatter()` dealing with transposition; * adding a parametrized test to `test_plotting.py` that tests all use-cases of `sc.pl.scatter()` where `basis=None`, _except_ for the one where `use_raw=True` and x/y/color are per-obs variables, as that didn't work before this commit or after it. It's still in the parameter list but commented out for now. I'd be happy to help fix it since I've familiarized myself with this code pretty well, but we might need to discuss how this case should be handled first, and my suggestion would be to deal with this in a separate PR/bug fix/commit. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242:831,variab,variables,831,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027#issuecomment-964253242,2,['variab'],['variables']
Modifiability,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:141,variab,variable,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,2,['variab'],['variable']
Modifiability,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:441,extend,extended,441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467,1,['extend'],['extended']
Modifiability,"Yes, that is what I currently do. It is just a matter of aesthetics. Since I have a large number of variables that I generate with custom functions, I wanted to store them separately based on what they represent in separate modules under (`adata.uns`). . Adding everything to `adata.obs` quickly gets cluttered. No worries just wanted to see if it was an option. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538:100,variab,variables,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1189#issuecomment-621299538,1,['variab'],['variables']
Modifiability,"Yes, then this could be extended in scanpy. I imagine this would be very useful for reference mapping visualisations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133898960:24,extend,extended,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133898960,1,['extend'],['extended']
Modifiability,"Yes, they are in that file: https://github.com/theislab/scanpy/blob/master/scanpy/plotting/palettes.py. Of course, user palettes are also accepted. Let me know if you need more. We'll extend this in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-390478463:184,extend,extend,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-390478463,1,['extend'],['extend']
Modifiability,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023:319,variab,variable,319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-764850023,1,['variab'],['variable']
Modifiability,"You already set that, so the config isn’t modified at runtime",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204:29,config,config,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204,1,['config'],['config']
Modifiability,You can store different forms of the matrix in `layers` and often choose which one to use with the `layers` argument. It really depends on the function whether it expects normalized or count data. Most functions should mention it in the documentation if the expect count data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801:48,layers,layers,48,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-867418801,2,['layers'],['layers']
Modifiability,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):; 1. Using fewer genes is computationally less expensive for downstream analysis.; 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430:238,variab,variable,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578#issuecomment-759366430,1,['variab'],['variable']
Modifiability,"[sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) uses Pearson residuals from “regularized negative binomial regression,” to correct for the sequencing depth. After regressing out total number of UMIs (and other variables if given) it ranks the genes based on their residual variances and therefore also acts as a HVG selection method. This function replaces `sc.pp.normalize_total` and `sc.pp.highly_variable_genes` and requires raw counts in ``adata.X``.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1271:252,variab,variables,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1271,1,['variab'],['variables']
Modifiability,"\numba\core\lowering.py in lower_function_body(self); 257 bb = self.blkmap[offset]; 258 self.builder.position_at_end(bb); --> 259 self.lower_block(block); 260 self.post_lower(); 261 return entry_block_tail. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\lowering.py in lower_block(self, block); 271 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 272 loc=self.loc, errcls_=defaulterrcls):; --> 273 self.lower_inst(inst); 274 self.post_block(block); 275 . ~\AppData\Local\Continuum\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\errors.py in new_error_context(fmt_, *args, **kwargs); 750 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 751 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 752 reraise(type(newerr), newerr, tb); 753 ; 754 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numba\core\utils.py in reraise(tp, value, tb); 79 if value.__traceback__ is not tb:; 80 raise value.with_traceback(tb); ---> 81 raise value; 82 ; 83 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4079:36: error: '%.2747' defined with type 'i64' but expected 'i32'; %"".2748"" = icmp eq i32 %"".2746"", %"".2747""; ^. File ""..\..\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 412:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. During: lowering ""id=13[LoopNest(index_variable = parfor_index.264, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1341:13274,config,config,13274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1341,1,['config'],['config']
Modifiability,"] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; adata = sc.read_csv(dir + 'GSM5226574_C51ctr_raw_counts.csv.gz').T; import scvi ; sc.pp.filter_genes(adata, min_cells = 10); sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); ```. ```pytb; >>> sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\A",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1115,plugin,plugins,1115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['plugin'],['plugins']
Modifiability,"].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X value of MALAT1 in cell: (0, 0)	142.0. Run 2: after sc.pp.normalize_total: ; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862. Run 3: normalization, specifing argument layer=None; normalizing counts per cell; finished ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2687,layers,layers,2687,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ../../miniconda3/envs/scanpy/lib/python3.9/functools.py:888: in wrapper; > return dispatch(args[0].__class__)(*args, **kw); > scanpy/preprocessing/_simple.py:888: in scale_anndata; > X, adata.var[""mean""], adata.var[""std""] = do_scale(; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:468: in _compile_for_args; > error_rewrite(e, 'typing'); > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; > ; > e = TypingError('Failed in nopython mode pipeline (step: nopython frontend)\nnon-precise type pyobject\nDuring: typing of ...y the following argument(s):\n- argument 0: Cannot determine Numba type of <class \'scipy.sparse._csr.csr_matrix\'>\n'); > issue_type = 'typing'; > ; > def error_rewrite(e, issue_type):; > """"""; > Rewrite and raise Exception `e` with help supplied based on the; > specified issue_type.; > """"""; > if config.SHOW_HELP:; > help_msg = errors.error_extras[issue_type]; > e.patch_message('\n'.join((str(e).rstrip(), help_msg))); > if config.FULL_TRACEBACKS:; > raise e; > else:; > > raise e.with_traceback(None); > E numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); > E non-precise type pyobject; > E During: typing of argument at /home/zeth/PycharmProjects/scanpy/scanpy/preprocessing/_simple.py (763); > E ; > E File ""scanpy/preprocessing/_simple.py"", line 763:; > E def do_scale(X, maxv, nthr):; > E <source elided>; > E # t0= time.time(); > E s = np.zeros((nthr, X.shape[1])); > E ^ ; > E ; > E This error may have been caused by the following argument(s):; > E - argument 0: Cannot determine Numba type of <class 'scipy.sparse._csr.csr_matrix'>; > ; > ../../miniconda3/envs/scanpy/lib/python3.9/site-packages/numba/core/dispatcher.py:409: TypingError; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717:1791,Rewrite,Rewrite,1791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2457#issuecomment-1540006717,1,['Rewrite'],['Rewrite']
Modifiability,"__dealloc__; KeyError: 0; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-255f06b48663> in <module>(); 1 adata_backed = sc.read(""tmp.h5ad"", backed=""r""); 2 sc.pl.pca(adata_backed, color=""0""); ----> 3 sc.pl.pca(adata_backed[:, :5], color=""0""). /usr/local/lib/python3.6/site-packages/scanpy/plotting/tools/__init__.py in pca(adata, color, use_raw, sort_order, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save, ax); 114 title=title,; 115 show=False,; --> 116 save=False, ax=ax); 117 utils.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:1868,layers,layers,1868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,"_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale, smallest_dot, title, colorbar_title, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, vmin, vmax, vcenter, norm, **kwds); 982 return dp; 983 else:; --> 984 dp.make_figure(); 985 savefig_or_show(DotPlot.DEFAULT_SAVE_PREFIX, show=show, save=save); 986 show = settings.autoshow if show is None else show. ~/github/scanpy/scanpy/plotting/_baseplot_class.py in make_figure(self); 606 mainplot_height = len(self.categories) * category_height; 607 mainplot_width = (; --> 608 len(self.var_names) * category_width + self.group_extra_size; 609 ); 610 if self.are_axes_swapped:. AttributeError: 'DotPlot' object has no attribute 'group_extra_size'; ```. First, what's up with the printed error?. Second, I think subsetting the groups and specifying the order can be done at the same time. This is the behaviour of the `groups` kwarg for variable axis of `sc.pl.rank_genes_groups`. This is also the behaviour of `var_names`. I'd noticed some related behaviour I can't quite remember while fixing up #1529. Noticed this specific case while looking at #1914. #### Versions. <details>; <summary> </summary>. ```python; -----; anndata 0.7.7.dev4+g49739eb; scanpy 1.9.0.dev7+g092376d2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.7.dev4+g49739eb; anyio NA; appnope 0.1.0; argon2 20.1.0; asciitree NA; attr 20.3.0; babel 2.8.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli 1.0.9; certifi 2020.06.20; cffi 1.14.0; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dask 2021.05.0; dateutil 2.8.1; decorator 4.4.2; fasteners NA; fsspec 2021.06.0; google NA; h5py 3.2.1; idna 2.10; igraph 0.9.6; ipykernel 5.5.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.6.0; kiwisolver 1.2.0; le",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1915:2291,variab,variable,2291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1915,1,['variab'],['variable']
Modifiability,"_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> 57 yield (key, self[key]); 58 ; 59 def __len__(self) -> int:. /opt/conda/lib/python3.7/site-packages/loompy/graph_man",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:1151,layers,layers,1151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,1,['layers'],['layers']
Modifiability,"`. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; In [1]: sc.pp.combat(adata, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()). In [2]: sc.pp.highly_variable_genes(adata); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-2-7727f5f928cd>"", line 1, in <module>; sc.pp.highly_variable_genes(adata). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=fl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:1442,variab,variables,1442,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['variab'],['variables']
Modifiability,"`E ImportError: cannot import name 'settings' from partially initialized module 'scanpy' (most likely due to a circular import) (/home/vsts/work/1/s/scanpy/__init__.py); `. Meh, it's hell to track this down now. I assume that autopep8 removed an unused variable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073:253,variab,variable,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-785044073,1,['variab'],['variable']
Modifiability,"```; Traceback (most recent call last):; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx/events.py"", line 101, in emit; results.append(listener.handler(self.app, *args)); File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/sphinx_autodoc_typehints/__init__.py"", line 446, in process_docstring; formatted_annotation = format_annotation(; File ""/home/docs/checkouts/readthedocs.org/user_builds/icb-scanpy/envs/1828/lib/python3.8/site-packages/scanpydoc/elegant_typehints/formatting.py"", line 126, in format_annotation; arg_name = variables[""argname""].replace(r""\_"", ""_""); KeyError: 'argname'; ```. Not sure why RTD fails with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735:643,variab,variables,643,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1828#issuecomment-1004638735,1,['variab'],['variables']
Modifiability,"```py; %matplotlib inline; import scanpy; ```. when importing scanpy from a jupyter notebook I get this warning, because apparently scanpy calls `matplotlib.use()`. if at all, it should only do that after checking that no backend is already selected. ```; /home/icb/philipp.angerer/.local/lib/python3.5/site-packages/matplotlib/__init__.py:1401: UserWarning: This call to matplotlib.use() has no effect; because the backend has already been chosen;; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,; or matplotlib.backends is imported for the first time. warnings.warn(_use_error_msg); ```. and this, which isn’t actually a `Warning`, is printed to stdout (why?). ```; ... WARNING: did not find DISPLAY variable needed for interactive plotting; --> try ssh with `-X` or `-Y`; setting `sett.savefigs = True`; ```. in an interactive notebook or other shell, `sett.savefigs` shouldn’t be automatically set to `True`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/16:721,variab,variable,721,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/16,1,['variab'],['variable']
Modifiability,"```python; import scanpy as sc; adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean""); ```. ```; AnnData object with n_obs × n_vars = 11 × 765; obs: 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; layers: 'mean'; ```. ```python; sc.get.aggregate(adata.obsm[""X_umap""], by=adata.obs[""louvain""].array, func=""mean""); ```. ```; {'mean': array([[ -6.18019123, -6.12846152],; [ -3.10995685, 8.4991954 ],; [ 6.30307056, -2.15245383],; [ -4.72268065, -3.24033642],; [-11.94002487, -5.39480163],; [ -1.39242794, 6.6239316 ],; [ 4.3991326 , -0.16749119],; [ 4.847834 , -9.30549509],; [-10.41891144, -1.15700949],; [ -7.91249486, -4.06782072],; [ 1.12418592, -6.94506866]])}; ```. So it returns an `AnnData` when an `AnnData` is passed, but a dict when a less structured object is passed. This is probably because it's `singledispatched` under the hood, but IDK that this behaviour is great. I think it could make more sense for this to either:. * Always return an `AnnData`; * Throw an error if something other than an AnnData is passed in. A third option is that we document this behaviour, but I generally don't love it. There are other places that we do something like this, i.e. return a different type depending on the input. However, I feel like there's more of a loss of information here and less of an obvious return type. Maybe in future this could get a `return_type: type[AnnData] | type[Dict] | type[xr.Dataset] = AnnData` argument that controls what is returned?. WDYT @ilan-gold @Intron7?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2930:276,layers,layers,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2930,1,['layers'],['layers']
Modifiability,"```restructuredtext; Returns ; ------- ; adata : :class:`~scanpy.api.AnnData` ; Annotated data matrix, where obsevations/cells are named by their ; barcode and variables/genes by gene name. The data matrix is stored in ; `adata.X`, cell names in `adata.obs_names` and gene names in ; `adata.var_names`. The gene IDs are stored in `adata.obs['gene_ids']`. ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/179#issuecomment-398095606:160,variab,variables,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/179#issuecomment-398095606,1,['variab'],['variables']
Modifiability,"`adata.raw.var_names` will have a different set of variables than `adata.var_names`, see #2018. This is by design, to allow you to have a reduced set of features in a dense matrix in `adata.X`, but have the full dataset in `adata.raw`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296:51,variab,variables,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-968888296,1,['variab'],['variables']
Modifiability,"`log2(TP10K+1)` values are more interpretable than `log(TP10K+1)`, which uses natural logarithm, therefore it'd be great to have an option on the base of the log. It's one of the requests also here #45 . Since neither of np.log or np.log1p accepts any base arguments(isn't this unbelievable), I did it with simple numba functions here: . https://nbviewer.ipython.org/gist/gokceneraslan/2744cfeda702fb9b9e48d0216427372c?flush_cache=true. I won't have time to send a PR, but feel free to pursue further, adapt the notebook and merge (if you have time). PS: Also see https://github.com/numpy/numpy/issues/14969.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929:502,adapt,adapt,502,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929,1,['adapt'],['adapt']
Modifiability,"`logg.warning` vs. `logg.warn`. I liked the short and verbal `.warn` better. I know there is some confusion, because of how python's core warning and logging modules possible, but meanwhile, several other packages have adapted Scanpy's logging module. All of them now need to change each line from `logg.warn` to `logg.warning` and even I will tend to make a lot of errors being used to `logg.warn` (still most of the time using emacs without autosuggest...). So, as Isaac, I'd also like the equivalent `logg.warn` function but wouldn't even deprecate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754:219,adapt,adapted,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/676#issuecomment-499027754,1,['adapt'],['adapted']
Modifiability,"`n_genes_user` is supposed to limit the length of the returned tables. however, one still needs to search all genes `n_genes` (`== X.shape[1]`) in order to get the top-scoring ones. this is the rationale behind the two variables and the naming",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/71#issuecomment-359410549:219,variab,variables,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/71#issuecomment-359410549,1,['variab'],['variables']
Modifiability,`plot_scatter` throws error when sparse layers used for color,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/700:40,layers,layers,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/700,1,['layers'],['layers']
Modifiability,"a import stacked_violin; 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); 308 ; 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 819 if isinstance(var_names, str):; 820 var_names = [var_names]; --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 822 ; 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 510 ; 511 def __getitem__(self, index):; --> 512 oidx, vidx = self._normalize_indices(index); 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); 538 obs, var = super(Raw, self)._unpack_index(packed_index); 539 obs = _normalize_index(obs, self._adata.obs_names); --> 540 var = _normalize_index(var, self.var_names); 541 return obs, var; 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); 270 raise KeyError(; 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 272 .format(index)); 273 return positions.values; 274 else:; ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222:1957,layers,layers,1957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222,1,['layers'],['layers']
Modifiability,"a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9cbf-f154f291aa14.png) . @LuckyMD @Zethson what would be your best practice?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1596,variab,variability,1596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638,2,['variab'],['variability']
Modifiability,a==0.7.4 umap==0.4.6 numpy==1.19.0 scipy==1.5.1 pandas==1.1.5 scikit-learn==0.23.1 statsmodels==0.12.1 python-igraph==0.8.0 louvain==0.6.1 leidenalg==0.8.3; ```. The new environment has ; ```; scanpy==1.6.1 anndata==0.7.5 umap==0.4.6 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.1 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3; ```; Full new conda environment:; ```; name: single_cell_analysis; channels:; - defaults; dependencies:; - _libgcc_mutex=0.1=main; - argon2-cffi=20.1.0=py37h7b6447c_1; - async_generator=1.10=py37h28b3542_0; - attrs=20.3.0=pyhd3eb1b0_0; - backcall=0.2.0=pyhd3eb1b0_0; - bleach=3.3.0=pyhd3eb1b0_0; - ca-certificates=2021.1.19=h06a4308_0; - certifi=2020.12.5=py37h06a4308_0; - cffi=1.14.4=py37h261ae71_0; - dbus=1.13.18=hb2f20db_0; - decorator=4.4.2=pyhd3eb1b0_0; - defusedxml=0.6.0=py_0; - entrypoints=0.3=py37_0; - expat=2.2.10=he6710b0_2; - fontconfig=2.13.0=h9420a91_0; - freetype=2.10.4=h5ab3b9f_0; - glib=2.66.1=h92f7085_0; - gst-plugins-base=1.14.0=h8213a91_2; - gstreamer=1.14.0=h28cd5cc_2; - icu=58.2=he6710b0_3; - importlib_metadata=2.0.0=1; - ipykernel=5.3.4=py37h5ca1d4c_0; - ipython=7.20.0=py37hb070fc8_1; - ipython_genutils=0.2.0=pyhd3eb1b0_1; - ipywidgets=7.6.3=pyhd3eb1b0_1; - jedi=0.17.0=py37_0; - jinja2=2.11.3=pyhd3eb1b0_0; - jpeg=9b=h024ee3a_2; - jsonschema=3.2.0=py_2; - jupyter=1.0.0=py37_7; - jupyter_client=6.1.7=py_0; - jupyter_console=6.2.0=py_0; - jupyter_core=4.7.1=py37h06a4308_0; - jupyterlab_pygments=0.1.2=py_0; - jupyterlab_widgets=1.0.0=pyhd3eb1b0_1; - ld_impl_linux-64=2.33.1=h53a641e_7; - libedit=3.1.20191231=h14c3975_1; - libffi=3.3=he6710b0_2; - libgcc-ng=9.1.0=hdf63c60_0; - libpng=1.6.37=hbc83047_0; - libsodium=1.0.18=h7b6447c_0; - libstdcxx-ng=9.1.0=hdf63c60_0; - libuuid=1.0.3=h1bed415_2; - libxcb=1.14=h7b6447c_0; - libxml2=2.9.10=hb55368b_3; - markupsafe=1.1.1=py37h14c3975_1; - mistune=0.8.4=py37h14c3975_1001; - nb_conda=2.2.1=py37_0; - nb_conda_kernels=2.3.1=py37h06a4308_0; - nbclient,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:1994,plugin,plugins-base,1994,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['plugin'],['plugins-base']
Modifiability,"aac@Mimir:~/tmp/genomic-features-docs; $ conda activate test-2978 ; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; from scanpy._compat imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help.; [ ... ]. In [3]: from scanpy._compat import pkg_version. In [4]: pkg_version(""anndata""); Out[4]: <Version('0.9.0')>. In [5]: quit(); (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ pip install -U anndata; Requirement already satisfied: anndata in /Users/isaac/miniforge3/envs/test-2978/lib/python3.12/site-packages (0.9.0); Collecting anndata; Downloading anndata-0.10.6-py3-none-any.whl.metadata (6.6 kB); [ ... ]; Downloading anndata-0.10.6-py3-none-any.whl (122 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.1/122.1 kB 2.1 MB/s eta 0:00:00; Downloading array_api_compat-1.6-py3-none-any.whl (36 kB); Installing collected packages: array-api-compat, anndata; Attempting uninstall: anndata; Found existing installation: anndata 0.9.0; Uninstalling anndata-0.9.0:; Successfully uninstalled anndata-0.9.0; Successfully installed anndata-0.10.6 array-api-compat-1.6; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ conda list | grep anndata; anndata 0.10.6 pypi_0 pypi; (test-2978) isaac@Mimir:~/tmp/genomic-features-docs; $ ipython; imPython 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 21:00:12) [Clang 16.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.22.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: from scanpy._compat import pkg_version. In [2]: pkg_version(""anndata""); Out[2]: <Version('0.10.6')>; ```. </details>. Interesting to see that this seems to work now!. I do think the recommended solution here is ""don't do this"", but I'm considering just using `anndata.__version__` here since that will work when the package metadata is broken.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757:1858,enhance,enhanced,1858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978#issuecomment-2039433757,1,['enhance'],['enhanced']
Modifiability,"aceback (most recent call last); <ipython-input-8-463060c90a0b> in <module>(); ----> 1 sc.pl.violin(adata_counts, keys='dropout_per_gene'). ~/scanpy/scanpy/plotting/anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, scale, order, multi_panel, show, xlabel, rotation, save, ax, **kwds); 630 X_col = adata.raw[:, key].X; 631 else:; --> 632 X_col = adata[:, key].X; 633 obs_df[key] = X_col; 634 if groupby is None:. ~/anndata/anndata/base.py in __getitem__(self, index); 1303 def __getitem__(self, index):; 1304 """"""Returns a sliced view of the object.""""""; -> 1305 return self._getitem_view(index); 1306 ; 1307 def _getitem_view(self, index):. ~/anndata/anndata/base.py in _getitem_view(self, index); 1306 ; 1307 def _getitem_view(self, index):; -> 1308 oidx, vidx = self._normalize_indices(index); 1309 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1310 . ~/anndata/anndata/base.py in _normalize_indices(self, index); 1283 obs, var = super(AnnData, self)._unpack_index(index); 1284 obs = _normalize_index(obs, self.obs_names); -> 1285 var = _normalize_index(var, self.var_names); 1286 return obs, var; 1287 . ~/anndata/anndata/base.py in _normalize_index(index, names); 261 return slice(start, stop, step); 262 elif isinstance(index, (int, str)):; --> 263 return name_idx(index); 264 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 265 # here, we replaced the implementation based on name_idx with this. ~/anndata/anndata/base.py in name_idx(i); 248 raise IndexError(; 249 'Key ""{}"" is not valid observation/variable name/index.'; --> 250 .format(i)); 251 i = i_found[0]; 252 return i. IndexError: Key ""dropout_per_gene"" is not valid observation/variable name/index.; ```. The whole thing works for:; ```; sc.pl.violin(adata_counts.T, keys='dropout_per_gene'); sc.pl.violin(adata_counts, keys='dropout_per_cell); ```. So it's clearly just not taking `.var` columns for `sc.pl.violing()`. . I've also reproduced this with `adata = sc.datasets.blob()`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/375:2036,variab,variable,2036,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/375,2,['variab'],['variable']
Modifiability,adapted from https://github.com/scverse/anndata/blob/main/.azure-pipelines.yml. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2478:0,adapt,adapted,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2478,1,['adapt'],['adapted']
Modifiability,"adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""] have very different meanings and it's good to have them separate, I think. Considering that PCA looks for the genes marked True in adata.var[""highly_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:552,variab,variable,552,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,2,['variab'],['variable']
Modifiability,add option to keep genes and store bool array of highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284:56,variab,variable,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284,1,['variab'],['variable']
Modifiability,added support for individual cmaps for continous variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1489:49,variab,variables,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1489,1,['variab'],['variables']
Modifiability,alizer 2.0.4 pyhd3eb1b0_0 ; chex 0.1.81 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.2.1 py39hca03da5_0 ; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; cython 3.0.10 pypi_0 pypi; debugpy 1.6.7 py39h313beb8_0 ; decorator 5.1.1 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; dm-tree 0.1.7 py39h2666b31_0 conda-forge; docrep 0.3.2 pyh44b312d_0 conda-forge; et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge; etils 1.6.0 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.1 pypi_0 pypi; executing 0.8.3 pyhd3eb1b0_0 ; flax 0.6.1 pyhd8ed1ab_1 conda-forge; fonttools 4.53.0 py39hfea33bf_0 conda-forge; freetype 2.12.1 hadb7bae_2 conda-forge; fsspec 2024.6.1 pyhff2d567_0 conda-forge; future 1.0.0 pyhd8ed1ab_0 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forge; glib 2.80.2 h535f939_0 conda-forge; glib-tools 2.80.2 h4c882b9_0 conda-forge; glpk 5.0 h6d7a090_0 conda-forge; gmp 6.3.0 h7bae524_2 conda-forge; grpc-cpp 1.46.4 hb15be72_9 conda-forge; gst-plugins-base 1.24.4 h8a8f8c8_0 conda-forge; gstreamer 1.24.4 h430e707_0 conda-forge; h5py 3.11.0 nompi_py39h534c8c8_102 conda-forge; hdf5 1.14.3 nompi_hec07895_105 conda-forge; icu 73.2 hc8870d7_0 conda-forge; idna 3.7 py39hca03da5_0 ; igraph 0.10.13 h762ac30_0 conda-forge; importlib-metadata 7.0.1 py39hca03da5_0 ; importlib_metadata 7.0.1 hd3eb1b0_0 ; importlib_resources 6.4.0 pyhd8ed1ab_0 conda-forge; ipykernel 6.28.0 py39hca03da5_0 ; ipython 8.15.0 py39hca03da5_0 ; ipywidgets 8.1.2 py39hca03da5_0 ; jax 0.3.15 pyhd8ed1ab_0 conda-forge; jaxlib 0.3.15 cpu_py39hb5f911d_3 conda-forge; jedi 0.18.1 py39hca03da5_1 ; jinja2 3.1.4 py39hca03da5_0 ; joblib 1.4.2 pyhd8ed1ab_0 conda-forge; json5 0.9.6 pyhd3eb1b0_0 ; jsonschema 4.19.2 py39hca03da5_0 ; jsonschema-specifications 2023.7.1 py39hca03da5_0 ; jupyter 1.0.0 py39hca03da5_9 ; jupyter-lsp 2.2.0 py39hca03da5_0 ; jupyter_client 8.6.0 py39hca03da5_0 ; jupyter_console 6.6.3 py39hca03da5_0 ; jupyter_core 5.7.2 py39hca03da5_0 ; jupyter_even,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:7976,plugin,plugins-base,7976,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['plugin'],['plugins-base']
Modifiability,allowing dotplot to use two variables in groupby as x and y axis,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2055:28,variab,variables,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2055,1,['variab'],['variables']
Modifiability,"ally, I am using cellxgene which takes the; h5ad file as an input. when using anndata.write() function, it only output; the anndata.X as the expression matrix. And also there is no option of; useRaw here.; Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an; error saying its wrong shape.; Do you have any suggestions?. Thanks a lot!. On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <; notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the; > .raw matrix, if present, for the .tsv export.; >; > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the; > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write; > the matrix and all annotations, or anndataToTsv to write just the matrix.; > Or use code from there to write your own.; >; > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:; >; > > Hi, the expression matrix I exported from adata.write only have the top; > > variable genes. Is there a way to output the raw matrix including all; > genes?; > >; > > —; > > You are receiving this because you were mentioned.; > > Reply to this email directly, view it on GitHub; > > <; > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073; > >,; > > or mute the thread; > > <; > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ; > >; > > .; > >; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368:1038,variab,variable,1038,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499091368,1,['variab'],['variable']
Modifiability,"an this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110:2846,variab,variable,2846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110,2,['variab'],['variable']
Modifiability,"args; 4346 ); 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast); 6256 ; 6257 new_data = self._data.fillna(; -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast; 6259 ); 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs); 573 ; 574 def fillna(self, **kwargs):; --> 575 return self.apply(""fillna"", **kwargs); 576 ; 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs); 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy); 437 ; --> 438 applied = getattr(b, f)(**kwargs); 439 result_blocks = _extend_blocks(applied, result_blocks); 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast); 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):; 1935 values = self.values if inplace else self.values.copy(); -> 1936 values = values.fillna(value=value, limit=limit); 1937 return [; 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 206 else:; 207 kwargs[new_arg_name] = new_arg_value; --> 208 return func(*args, **kwargs); 209 ; 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit); 1871 elif is_hashable(value):; 1872 if not isna(value) and value not in self.categories:; -> 1873 raise ValueError(""fill value must be in categories""); 1874 ; 1875 mask = codes == -1. ValueError: fill value must be in categories; ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/809:3846,variab,variable,3846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809,2,['variab'],['variable']
Modifiability,"aster branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). This is exactly the code on https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html using the dataset downloaded per the instructions. The umap step does not produce the correct result. tsne however produces sensible results.; ```python; import numpy as np; import pandas as pd; import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'). results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results. adata = sc.read_10x_mtx(; 'data/filtered_gene_bc_matrices/hg19/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) . adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`. sc.pl.highest_expr_genes(adata, n_top=20, ). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],; jitter=0.4, multi_panel=True). sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'). adata = adata[adata.obs.n_genes_by_counts < 2500, :]; adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:1208,variab,variable,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"atching, some corner cases where numerics might have turned narrow outcomes, so actually 1991/2000. ```py; pmbc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). # introduce a dummy ""technical covariate""; this is used in Seurat's SelectIntegrationFeatures; pbmc.obs[""dummy_tech""] = ""source_1""; pbmc.obs.loc[pbmc.obs.index[500:1000], ""dummy_tech""] = ""source_2""; pbmc.obs.loc[pbmc.obs.index[1000:1500], ""dummy_tech""] = ""source_3""; pbmc.obs.loc[pbmc.obs.index[1500:2000], ""dummy_tech""] = ""source_4""; pbmc.obs.loc[pbmc.obs.index[2000:], ""dummy_tech""] = ""source_5"". # default settings in scanpy are the same as for Seurat; seurat_v3_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3"", batch_key=""dummy_tech"", inplace=False); seurat_v3_paper_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_paper"", batch_key=""dummy_tech"", inplace=False); seurat_v3_implementation_hvg = sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3_implementation"", batch_key=""dummy_tech"", inplace=False). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat_batch = pd.read_csv(""scanpy/scanpy/tests/_scripts/seurat_hvg_v3_batch.csv"", index_col=0). seu = pd.Index(pbmc3k_tutorial_FindVariableGenes_seurat_batch[""x""].values); ```. Matching genes `'seurat_v3'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_hvg[seurat_v3_hvg.highly_variable].index)); ```; `764`. Matching genes `'seurat_v3_paper'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_paper_hvg[seurat_v3_paper_hvg.highly_variable].index)); ```; `1990`. Matching genes `'seurat_v3_implementation'` and `FindVariableGenes`; ```; len(seu.intersection(seurat_v3_implementation_hvg[seurat_v3_implementation_hvg.highly_variable].index)); ```; `1990`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:5772,adapt,adapted,5772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Modifiability,"ated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max_value] = max_value; if return_mean_var:; return X, mean, var; else:; return X. @scale.register(AnnData); def scale_anndata(; adata: AnnData,; *,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; ) -> Optional[AnnData]:; adata = adata.copy() if copy else adata; view_to_actual(adata); adata.X, adata.var[""mean""], adata.var[""std""] = scale(; X, ; zero_center=zero_center, ; max_value=max_value, ; copy=False, # because a copy has already been made, if it were to be made; return_mean_var=True; ); if copy:; return adata. @scale.register(sparse.spmatrix); def scale_sparse(; X, ; *, ; zero_center: bool = True,; copy=False,; **kwargs; ):; # need to add the following here to make inplace logic work; if zero_center:; logg.info(; '... as `zero_center=True`, sparse input is '; 'densified and may lead to large memory consumption'; ); X = X.toarray(); copy = False # Since the data has been copied; return scale_array(X, zero_center=zero_center, copy=copy, **kwargs); ```. </details>. I actually really like this pattern of having an underlying function which has all the logic, but then dispatching through wrappers for the argument handling. It splits out the cases quite nicely, and makes the code flexible. This pattern is very common in Julia, and fairly common in Bioconductor packages.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:3555,flexible,flexible,3555,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['flexible'],['flexible']
Modifiability,"ath = EXAMPLE_DATA.fetch(filename); sample_adata = sc.read_10x_h5(path); sample_adata.var_names_make_unique(); adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""); adata.obs_names_make_unique(); print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse; adata.var[""mt""] = adata.var_names.str.startswith(""MT-""); # ribosomal genes; adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")); # hemoglobin genes; adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True; ). sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter=0.4,; multi_panel=True,; ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data; adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts; sc.pp.normalize_total(adata); # Logarithmize the data; sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""); sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata); sc.tl.umap(adata); sc.tl.leiden(; adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2; ). sc.pl.pca(; adata,; color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],; dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],; ncols=2,; size=2,; ). sc.pp.neighbors(adata). sc.pl.umap(; adata,; color=""sample"",; # Setting a smaller point size to get prevent overlap; size=2,; ). ### runs forever:; sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3228:2065,layers,layers,2065,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228,1,['layers'],['layers']
Modifiability,"ayer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py; import numpy as np; import pandas as pd; import anndata as ad. # Create a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orie",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1355,layers,layers,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['layers'],['layers']
Modifiability,bbknn integrates multiple variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2004:26,variab,variables,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2004,1,['variab'],['variables']
Modifiability,"been added.; * **update**: method chaining was added to avoid explosion of parameters as suggested in #956. The caveat is that a different name is needed to keep the current functionality of `sc.pl.dotplot`. Now is possible to do `sc.pl.DotPlot(adata, ....).add_dendrogram(size=1.2).swap_axes().show()`. ![image](https://user-images.githubusercontent.com/4964309/77526545-13ce0600-6e8b-11ea-996a-3d548f52f200.png). ![image](https://user-images.githubusercontent.com/4964309/77529251-8c36c600-6e8f-11ea-8446-c1dd18374cd4.png). Open issues:; * we need a better way to filter the output from rank_genes_groups (and I would also consider a better name). One option is to filter while plotting.; * to correctly plot the results from `sc.tl.rank_genes_groups` is better to report the results for all genes (`n_genes`). The default is to keep the top 100 genes per group. Thus, for many genes, the pvalue, logfoldchange etc in other groups is not saved.; * added new parameters, for some I would be happy to have some feedback. One is `style` that has two options `square color` and `dot color`. **Update**; The following is now possible (notice the method chaining):. ```PYTHON; adata = sc.datasets.pbmc68k_reduced(); markers = {'T-cell': ['CD3D', 'CD3E', 'IL32'], 'B-cell': ['CD79A', 'CD79B', 'MS4A1'], 'myeloid': ['CST3', 'LYZ']}; sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \; .add_totals(sort='descending', size=1.2) \; .legend(color_title='log(UMI count+1)', width=1.6)\; .style(color_map='RdBu_r', dot_min=0.3)\; .show(); ```; ![image](https://user-images.githubusercontent.com/4964309/77777939-889e6d00-7050-11ea-842e-cf2a11739df6.png). TODO:; - [x] Remove small secondary ticks; - [x] Adapt matrixplot to new object model; - [ ] Adapt heatmap to object model; - [x] Adapt stackedviolin to object model; - [ ] Incorporate modifications from #1116 ; - [x] Fix call from rank_genes_groups; - [x] 'black' code; - [x] Update docstrings; - [ ] Update tests; - [ ] Update tutorial and readthedocs",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1127:2398,Adapt,Adapt,2398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1127,3,['Adapt'],['Adapt']
Modifiability,"before normolization, you can do adata.layers['counts']=adata.X.copy() to add the counts of all genes to the layers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2261#issuecomment-1153040097:39,layers,layers,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2261#issuecomment-1153040097,2,['layers'],['layers']
Modifiability,"bles especially if they have some outliers. This deeply saddens us and forces us to watch a few more episodes of Stranger Things and it certainly doesn't help :(. I would like to hear your thoughts about how to fix that. But before that, as a responsible person, I did some homework and I spent around 34 minutes to understand how color normalization works in matplotlib (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html) and tried to implement a custom normalization class (https://matplotlib.org/3.1.1/tutorials/colors/colormapnorms.html#custom-normalization-manually-implement-two-linear-ranges). . My idea is simply to specify vmin/vmax in terms of quantiles of the color vector which can be shared between variables instead of a specific value. One way, I thought, might be to pass a `norm` argument with a custom normalization object to our lovely `plot_scatter`. However, as far as I understand, it's not possible because in the quantile function in the custom normalization class requires the entire color vector for each continuous variable which is not super convenient because it's too much preprocessing to find different quantile values for each variable and pass a vmin/vmax vector to the plotting function. Not user-friendly and still requires modifications in the code :(. Instead, I added two ugly arguments named `vmin_quantile` and `vmax_quantile` to the `plot_scatter` function which allows me to specify a single quantile value for vmin/vmax which is then translated into real values separately for each variable:. ![image](https://user-images.githubusercontent.com/1140359/62720493-20731c00-b9d8-11e9-9dc9-f91cf052c4e1.png). This solved my problem but I was wondering if it makes sense to add this to scanpy. What do you think?. Finally, here is the way I use it:. `plot_scatter(adata, basis='umap', color=['louvain', 'NKG7', 'GNLY', 'KIT'], cmap='Reds', vmax_quantile=0.999)`. PS: It needs a few more lines to be accessible from other functions like sc.pl.umap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/775:1441,variab,variable,1441,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/775,3,['variab'],['variable']
Modifiability,"bols; > under the column adata.var[“gene_name”]. When I call:; > sc.pl.umap(adata, color=['ENSMUSG00000074637']); > It plots no problem. However, when I call:; > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'); > I get the following error:; >; > Traceback (most recent call last):; >; >; >; > File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>; >; > sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; >; > return plot_scatter(adata, basis='umap', **kwargs); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter; >; > use_raw=use_raw, gene_symbols=gene_symbols); >; >; >; > File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values; >; > .format(value_to_plot, adata.obs.columns)); >; >; >; > ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',; >; > 'louvain'],; >; > dtype='object'); >; >; > Inspecting adata.var[""gene_name""] give:; >; > index; >; > ENSMUSG00000002459 Rgs20; >; > ENSMUSG00000033740 St18; >; > ENSMUSG00000067879 3110035E14Rik; >; > ENSMUSG00000025912 Mybl1; >; > ENSMUSG00000016918 Sulf1; >; > ENSMUSG00000025938 Slco5a1; >; > ENSMUSG00000025930 Msc; >; > ENSMUSG00000025921 Rdh10; >; > ENSMUSG00000025777 Gdap1; >; > ENSMUSG00000025776 Crispld1; >; > ENSMUSG00000025927 Tfap2b; >; > ENSMUSG00000025931 Paqr8; >; > ENSMUSG00000026158 Ogfrl1; >; > ...; >; >; > I'm not sure what I'm doing wrong here. I can do just about anything using; > the ensembl ids, but I am having a lot of trouble using the gene symbols. I; > would like to be able to use the gene symbols in the plots for umap,; > violin, pca, etc. Any help would be much appreciated. Thanks!; >;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-472840788:1447,variab,variable,1447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-472840788,1,['variab'],['variable']
Modifiability,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:305,extend,extending,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['extend'],['extending']
Modifiability,"bs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata, max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(); sc._settings.ScanpyConfig.n_jobs = os.cpu_count(); sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''; from sklearn.manifold import TSNE; from scanpy.tools._utils import _choose_representation; X = _choose_representation(adata, n_pcs=tsne_n_pcs); X_tsne = TSNE().fit_transform(X.astype(np.float32)); adata.obsm['X_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:2785,variab,variable,2785,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,['variab'],['variable']
Modifiability,"by using as.loom function in Seurat3. After closing the file with $close.all(), I'm trying to read loom file by read_loom function in scanpy, but I have this error:. ```; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-7-aed61d3d5eef> in <module>; 1 import scanpy as sc; ----> 2 a = sc.read_loom('brain10x.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype); 156 ; 157 if X_name not in lc.layers.keys(): X_name = ''; --> 158 X = lc.layers[X_name].sparse().T.tocsr() if sparse else lc.layers[X_name][()].T; 159 ; 160 layers = OrderedDict(). /opt/conda/lib/python3.7/site-packages/loompy/loom_layer.py in sparse(self, rows, cols); 109 col: List[np.ndarray] = []; 110 i = 0; --> 111 for (ix, selection, view) in self.ds.scan(items=cols, axis=1, layers=[self.name]):; 112 if rows is not None:; 113 vals = view.layers[self.name][rows, :]. /opt/conda/lib/python3.7/site-packages/loompy/loompy.py in scan(self, items, axis, layers, key, batch_size); 597 for key, layer in vals.items():; 598 lm[key] = loompy.MemoryLoomLayer(key, layer); --> 599 view = loompy.LoomView(lm, self.ra[ordering], self.ca[ix + selection], self.row_graphs[ordering], self.col_graphs[ix + selection], filename=self.filename, file_attrs=self.attrs); 600 yield (ix, ix + selection, view); 601 ix += cols_per_chunk. /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in __getitem__(self, thing); 96 if type(thing) is slice or type(thing) is np.ndarray or type(thing) is int:; 97 gm = GraphManager(None, axis=self.axis); ---> 98 for key, g in self.items():; 99 # Slice the graph matrix properly without making it dense; 100 (a, b, w) = (g.row, g.col, g.data). /opt/conda/lib/python3.7/site-packages/loompy/graph_manager.py in items(self); 55 def items(self) -> Iterable[Tuple[str, sparse.coo_matrix]]:; 56 for key in self.keys():; ---> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598:1040,layers,layers,1040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598,1,['layers'],['layers']
Modifiability,cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.1.3; executing 1.2.0; fastjsonschema 2.18.1; filelock 3.12.4; flake8 6.0.0; Flask 3.0.0; fonttools 4.43.1; fqdn 1.5.1; frozenlist 1.4.0; fsspec 2023.6.0; future 0.18.3; gensim 4.3.2; gitdb 4.0.10; GitPython 3.1.36; gmpy2 2.1.2; greenlet 3.0.0; h5py 3.9.0; holoviews 1.17.1; huggingface-hub 0.17.3; humanfriendly 10.0; hvplot 0.8.4; hyperlink 21.0.0; idna 3.4; igraph 0.10.4; imagecodecs 2023.1.23; imageio 2.31.1; imagesize 1.4.1; imbalanced-learn 0.11.0;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:4668,Config,ConfigArgParse,4668,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['Config'],['ConfigArgParse']
Modifiability,"ckages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 20.0.2 . ubuntu; 20.04. pip list; Package Version Location ; ------------------------- -------------------- -----------------------------; analysaurus 0.0.1 /home/ubuntu/code/analysaurus; anndata 0.7.5 ; ansi2html 1.5.2 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2465,config,config,2465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['config'],['config']
Modifiability,"cked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, everyone:; Many users probably do not rely on pp.normalize_total for downstream analysis, but I found a strange default behavior that I think is worth mentioning.; pp.normalize_total() normalized my .layers['counts'] as well; The documentation is a bit murky; not sure if that is the expected behavior when layer is unspecified, but; such default behavior would undermine anyone who wishes to save the count information before RPKM normalization. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:1004,layers,layers,1004,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"co.cross_plot is one high level possibility to construct complex heatmaps with the 'central data heatmap + annotation heatmaps' layout. Among other things, it can automatically cluster columns or rows based on the central data heatmap and apply the clustering to the annotation heatmaps. It can also plot dendrograms. This is an experimental function with some quirks, I did want to improve the concept soon-ish.; - co.heatmap is the base heatmap plotting function in codaplot. It provides a simple way to plot categorical heatmaps and add spacers within heatmaps. Both tasks are not trivial with matplotlib base plot functions. This would be helpful for adding categorical annotation heatmaps, even if you don't want to use co.cross_plot as it is right now.; - i have an alternative function to co.heatmap in my snippets library which is capable of creating heatmaps using rectangle or circle patches with size and color aesthetics, but i havent added it to codaplot yet. You can always create circle patch heatmaps with standard scatterplots, but this has drawbacks when you want to be able to add spacers within the plot or when you want full control of the circle patch sizes (so that they fit perfectly within the row at maximum size). From what I understand such a patch based function would be helpful, right?. I would be happy to contribute some base functionality for this issue by adding improvements to codaplot, ie provide the circle patch heatmap function and a better complex heatmap function than the currently available co.cross_plot. I do plan on maintaining codaplot for the foreseeable future and have been using it for my own projects for quite a while now. At the moment it's a relatively small library (when you subtract the experimental modules) and could be quickly refactored into a single scanpy module if something happens and I find myself unable to maintain and expand the library over the next years. . Does using codaplot for this issue sound at all interesting to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103:2150,refactor,refactored,2150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2194#issuecomment-1145123103,1,['refactor'],['refactored']
Modifiability,colouring of highly variable genes on pl.filter_genes_dispersion,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/39:20,variab,variable,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/39,1,['variab'],['variable']
Modifiability,"conda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2434,layers,layers,2434,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['layers'],['layers']
Modifiability,"continue this discussion rather than start a new one, though I can do that if you prefer. I have an AnnData object `adata` with ensembl ids as `adata.var_name` and mouse gene symbols under the column `adata.var[“gene_name”]`. When I call:; `sc.pl.umap(adata, color=['ENSMUSG00000074637'])`; It plots no problem. However, when I call:; `sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name')`; I get the following error:; ```; Traceback (most recent call last):. File ""<ipython-input-559-05c51c5cc5d6>"", line 1, in <module>; sc.pl.umap(adata, color=['Sox2'], gene_symbol='gene_name'). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 29, in umap; return plot_scatter(adata, basis='umap', **kwargs). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 275, in plot_scatter; use_raw=use_raw, gene_symbols=gene_symbols). File ""/anaconda3/lib/python3.6/site-packages/scanpy/plotting/_tools/scatterplots.py"", line 670, in _get_color_values; .format(value_to_plot, adata.obs.columns)). ValueError: The passed `color` Sox2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['timepoint', 'replicate_id', 'n_genes', 'percent_mito', 'n_counts',; 'louvain'],; dtype='object'); ```; Inspecting adata.var[""gene_name""] give:; ```; index; ENSMUSG00000002459 Rgs20; ENSMUSG00000033740 St18; ENSMUSG00000067879 3110035E14Rik; ENSMUSG00000025912 Mybl1; ENSMUSG00000016918 Sulf1; ENSMUSG00000025938 Slco5a1; ENSMUSG00000025930 Msc; ENSMUSG00000025921 Rdh10; ENSMUSG00000025777 Gdap1; ENSMUSG00000025776 Crispld1; ENSMUSG00000025927 Tfap2b; ENSMUSG00000025931 Paqr8; ENSMUSG00000026158 Ogfrl1; ...; ```; I'm not sure what I'm doing wrong here. I can do just about anything using the ensembl ids, but I am having a lot of trouble using the gene symbols. I would like to be able to use the gene symbols in the plots for umap, violin, pca, etc. Any help would be much appreciated. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455#issuecomment-472756442:1252,variab,variable,1252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455#issuecomment-472756442,1,['variab'],['variable']
Modifiability,correlation between cell types and continuous variables stored in .obs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1855:46,variab,variables,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1855,1,['variab'],['variables']
Modifiability,correlation between cell types and obs variables,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1826:39,variab,variables,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1826,1,['variab'],['variables']
Modifiability,"crash the kernel, but does bog it down and causes everything to to take much more time than necesarry. ; I am working in a conda env on a Win 10 , 64bit, x64 system; the problem also occurs using the pbmc3k dataset. ### Minimal code sample. ```python; # example with own data, but same happens with pbmc3k data; sc.pp.filter_cells(em_adata, min_genes=200); sc.pp.filter_genes(em_adata, min_cells=3); em_adata.shape; # [out] -> (42753, 21636). sc.pp.calculate_qc_metrics(em_adata, qc_vars=[""mt""], percent_top=None, log1p=False, inplace=True); em_adata.obs[""outlier_mt""] = em_adata.obs.pct_counts_mt > 15; em_adata.obs[""outlier_total""] = em_adata.obs.total_counts > 30000; em_adata.obs[""outlier_ngenes""] = em_adata.obs.n_genes_by_counts > 6000; em_adata = em_adata[~em_adata.obs[""outlier_mt""], :]; em_adata = em_adata[~em_adata.obs[""outlier_total""], :]; em_adata = em_adata[~em_adata.obs[""outlier_ngenes""], :]; sc.pp.filter_genes(em_adata,min_cells=1). sc.pp.scrublet(em_adata); em_adata.layers['counts'] = em_adata.X.copy(); sc.pp.normalize_total(em_adata); sc.pp.log1p(em_adata); sc.pp.highly_variable_genes(em_adata,flavor='seurat'); sc.pl.highly_variable_genes(em_adata); em_adata = em_adata[:, em_adata.var[""highly_variable""]]; em_adata.shape; # [out] -> (41749, 1425); sc.pp.pca(em_adata, n_comps=50); sc.pp.neighbors(em_adata); sc.tl.umap(em_adata); sc.tl.leiden(em_adata,flavor='igraph',n_iterations=2,random_state=1653,directed=False); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32; ```. ### Versions. <details>. ```; conda env:; # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-binding",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:1501,layers,layers,1501,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['layers'],['layers']
Modifiability,"cumentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illustrating when and how to use these parameters effectively. 3. **Consistency in Documentation Style**: Ensure that the documentation style is consistent across different parameters, making it easier for users to read and understand. ### Conclusion; Enhancing the documentation of the `MatrixPlot` class will improve the library's usability and user experience. . I am new to open-source contribution and I am eager to contribute to this enhancement, and welcome any additional input or guidance from the project maintainers and community.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:2435,enhance,enhancement,2435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhancement']
Modifiability,"d remove the graph slot and this error is gone, but now I have a new error: . ```; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-2-aae861244dfa> in <module>; ----> 1 adata = sc.read_loom('dataset.loom'). /opt/conda/lib/python3.7/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. /opt/conda/lib/python3.7/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . /opt/conda/lib/python3.7/site-packages/anndata/base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. /opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init_dict(data, index, columns, dtype=dtype); 393 elif isinstance(data, ma.MaskedArray):; 394 import numpy.ma.mrecords as mrecords. /opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 210 arrays = [data[k] for k in keys]; 211 ; --> 212 return arrays_t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885:1023,layers,layers,1023,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-487609885,1,['layers'],['layers']
Modifiability,"dFragment-->; </body>; </html>. Both tutorial adatas after a successful ingest:; ```. (AnnData object with n_obs × n_vars = 700 × 208; obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap', 'rep'; varm: 'PCs'; obsp: 'distances', 'connectivities',; AnnData object with n_obs × n_vars = 2638 × 208; obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'; var: 'n_cells'; uns: 'draw_graph', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'; varm: 'PCs'; obsp: 'distances', 'connectivities'); ```. Now my data, adata_ref:. <html><body>; <!--StartFragment--><div class=""lm-Widget p-Widget lm-Panel p-Panel jp-Cell-inputWrapper""><div class=""lm-Widget p-Widget jp-InputArea jp-Cell-inputArea"">.   | celltype | louvain; -- | -- | --; cell1 | hepatic stellate cells | 1; cell2 | cholangiocytes | 1; ... | ... | ... <p>8439 rows × 2 columns</p>; </div></div><!--EndFragment-->; </body>; </html>. and my adata that I wish to ingest:. <html><body>; <!--StartFragment-->.   | louvain; -- | --; cell1 | 0; cell2 | 0; ... <!--EndFragment-->; </body>; </html>. Both my adata files have the same 40 variables and pca/umaps, they look like this:. ```; (AnnData object with n_obs × n_vars = 8989 × 40; obs: 'louvain'; uns: 'pca', 'neighbors', 'umap', 'louvain', 'louvain_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities',; AnnData object with n_obs × n_vars = 8439 × 40; obs: 'celltype', 'louvain'; uns: 'pca', 'neighbors', 'umap', 'louvain', 'louvain_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'); ```. I suspect the error stems from the Nearest Neighbours. Or maybe my number of variables (40) is too small?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104240383:2742,variab,variables,2742,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2085#issuecomment-1104240383,2,['variab'],['variables']
Modifiability,don't configure black,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2701:6,config,configure,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2701,1,['config'],['configure']
Modifiability,"don’t worry, i think they should really default to a better locale: many people will get their Dockerfiles by adapting existing ones instead of finding that specific doc site, i think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361:110,adapt,adapting,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-344299361,1,['adapt'],['adapting']
Modifiability,dotplot with x axis being one variable and y axis being another variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876:30,variab,variable,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876,2,['variab'],['variable']
Modifiability,"e /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:2467,Layers,Layers,2467,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['Layers'],['Layers']
Modifiability,"e a small data matrix; data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations; obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]); var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object; adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function; adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; sc.pl.highest_expr_genes(adata, layer='normalised'); ```. ### Error output. ```pytb; Output exceeds the size limit. Open the full output data in a text editor; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[32], line 17; 15 # Test layer call function; 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test; ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'); 19 # Test layer call function; 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 98 height = (n_top * 0.2) + 1.5; 99 fig, ax = plt.subplots(figsize=(5, height)); --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds); 101 ax.set_xlabel(""% of total counts""); 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318:1544,layers,layers,1544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318,1,['layers'],['layers']
Modifiability,"e and there was a two year old thread on the subject (https://github.com/scverse/scanpy/issues/2073), but none of the solutions worked on my machine. I tried version control of all the packages, most importantly numpy as it was mentioned as a problem. I was wondering if this you all have any newer solutions for this issue. Due to my inexperience im really not sure what is causing this issue and therefore what to provide you all with. I also just realized that after a clean install trying to pip3 install scikit-misc returns this . (scanpy_env) user@Mac ~ % pip3 install scikit-misc ; Requirement already satisfied: scikit-misc in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (0.3.1); Requirement already satisfied: numpy>=1.22.3 in /opt/miniconda3/envs/scanpy_env/lib/python3.9/site-packages (from scikit-misc) (1.26.4). Still getting the error though... Hardware: M2 max mac, macos 15 beta (could this be it somehow?). ### Minimal code sample. Original error upon running highly variable genes; ```python; <details>. ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/scanpy/preprocessing/_highly_variable_genes.py:66, in _highly_variable_genes_seurat_v3(adata, flavor, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 try:; ---> 66 from skmisc.loess import loess; 67 except ImportError:. ModuleNotFoundError: No module named 'skmisc'. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); Cell In[14], line 1; ----> 1 doublet_training_data = sc.pp.highly_variable_genes(adata, n_top_genes=6000, subset=True, flavor='seurat_v3'); 2 doublet_training_data. File /opt/miniconda3/envs/scanpyenvt/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:1770,variab,variable,1770,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['variab'],['variable']
Modifiability,"e checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Since a few weeks ago (at least), the tests in `test_metrics.py` started failing because the exact equality tests no longer consistently returned the bit-for-bit same float. Something like it has been observed in https://github.com/scverse/scanpy/pull/1740#discussion_r596827747. #2687 disables the exact comparison, but we should figure out why it’s happening and if we can restore exact precision. ### Minimal code sample. ```console; $ git switch 1.9.5; $ pytest scanpy/tests/test_metrics.py; ```. ### Error output. ```pytb; =================================== FAILURES ===================================; __________________________ test_morans_i_consistency ___________________________. def test_morans_i_consistency():; pbmc = pbmc68k_reduced(); pbmc.layers[""raw""] = pbmc.raw.X.copy(); g = pbmc.obsp[""connectivities""]; ; > assert eq(; sc.metrics.morans_i(g, pbmc.obs[""percent_mito""]),; sc.metrics.morans_i(pbmc, vals=pbmc.obs[""percent_mito""]),; ); E AssertionError: assert False; E + where False = eq(0.13099293222276961, 0.13099293222276967); E + where 0.13099293222276961 = <function morans_i at 0x7f354779d9d0>(<700x700 sparse matrix of type '<class 'numpy.float64'>'\n	with 9992 stored elements in Compressed Sparse Row format>, index\nAAAGCCTGGCTAAC-1 0.023856\nAAATTCGATGCACA-1 0.027458\nAACACGTGGTCTTT-1 0.016819\nAAGTGCACGTGCTA-1 0.011797\nACACGAACGGAGTG-1 0.017277\n ... \nTGGCACCTCCAACA-8 0.008840\nTGTGAGTGCTTTAC-8 0.022068\nTGTTACTGGCGATT-8 0.012821\nTTCAGTACCGGGAA-8 0.014169\nTTGAGGTGGAGAGC-8 0.010886\nName: percent_mito, Length: 700, dtype: float32); E + where <function morans_i at 0x7f354779d9d0> = <module 'scanpy.metrics' from '/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/scanpy/metrics/__init__.py'>.morans_i; E + wher",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2688:1048,layers,layers,1048,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2688,1,['layers'],['layers']
Modifiability,"e not already familiar with https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview. ; Also, ok for having `uns` changes in another PR, I can work on that as soon as this is merged.; > Update: heard back, the `library_id` should be fine, at least for this version.; > . good !. > > support for multiple slices should be first; > ; > I'm not sure I'm convinced of this. I've also already got some code ready to go for the connectivities and some examples of what can be done with it.; > ; > I'd like to hear what kind of stuff you want to be able to do with multiple slices. Are you interested in stitching together slides or holding arbitrary slides in an AnnData? I think I'd like to see a more fleshed out idea of what kinds of analysis could be done here before deciding on what kind of an API this should have, and cases we should be ready to handle.; > . support for multiple slices and concatenation of anndata objects is by far the priority to me. It's a really useful functionality since:; * most people don't work with one slide; * having the same anndata object containing scRNA-seq as well as matched visium tissue would allow for a very straightforward approach to integration and label propagation (with ingest/bbknn). This would also be extremely useful for the tutorial (which I can't update until anndata supports multiple tissues). I am very interested to see the applications of spatial connectivities you think can be useful. I see the potential but I don't think it's straightforward to make use of that info (especially because in essence the spatial graph derived from visium is completely homogeneous, hence lack of structure).; ; > Also, I think spatial plotting code should get moved out of `sc.pl.embedding` before we allow plotting multiple slides at a time. Why is that? `sc.pl.spatial` is essentially a scatterplot that calls `sc.pl.embedding` yet using another method (circles instead of scatter, but inherits all the arguments)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855:2875,inherit,inherits,2875,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1088#issuecomment-596965855,1,['inherit'],['inherits']
Modifiability,"e sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/wo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1320,layers,layers,1320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,['layers'],['layers']
Modifiability,"e(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_value(value, key); 200 self._data[key] = value. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:89, in AlignedMapping._validate_value(self, val, key); 83 dims = tuple((""obs"", ""var"")[ax] for ax in self.axes); 84 msg = (; 85 f""Value passed for key {key!r} is of incorrect shape. ""; 86 f""Values of {self.attrname} must match dimensions {dims} of parent. ""; 87 f""Value had shape {actual_shape} while it should have had {right_shape}.""; 88 ); ---> 89 raise ValueError(msg); 91 if not self._allow_df and isinstance(val, pd.DataFrame):; 92 name = self.attrname.title().rstrip(""s""). ValueError: Value passed for key 'mean' is of incorrect shape. Values of layers must match dimensions ('obs', 'var') of parent. Value had shape (11, 2) while it should have had (11, 765).; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0rc2.dev19+ga6126980; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.3.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; h5py 3.10.0; igraph 0.11.3; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.41.1; markupsafe 2.1.4; matplotlib 3.8.3; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.2.1; parso 0.8.3; pexpect 4.9.0; prompt_toolkit 3.0.43; psutil 5.9.8; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 15.0.1; pygments 2.17.2; pyparsing 3.1.1; pytz 2023.4; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.0; sparse 0.15.1; stack_data 0.6.3; tblib 3.0.0; texttable 1.7.0; threadpoolctl ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:3605,layers,layers,3605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"e**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards favoring the style of `seurat_v3_paper`. Better naming suggestions more than welcome. **Examples**; - Good when no `batch_key` used:; ```py; import numpy as np; import pandas as pd; import scanpy as sc. # load exactly the data from seurat tutorial. pbmc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). print(pbmc). # default settings in scanpy are the same as for Seurat; sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3""). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat = pd.read_csv(""scanpy/scanpy/scanpy/tests/_scripts/seurat_hvg_v3.csv.gz"", index_col=0). # This is used to order and rank the hvg when no batch information used; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance.standardized""],; pbmc.var[""variances_norm""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""mean""],; pbmc.var[""means""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance""],; pbmc.var[""variances""]),; ); ```. - Discrepancy when `batch_key` is used: The flavor `'seurat_v3'` shows 764/2000 genes overlap with Seurat. The new suggested flavors `'seurat_v3_paper'` and `'seurat_v3_implementation'` show a 1990/2000 selected genes overlap with Seurat. The 10 non-matching genes contain 1 gene renamed by Seurat and therefore not found matching, some corner ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:3561,adapt,adapted,3561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['adapt'],['adapted']
Modifiability,"e, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being passed to sanitize anndata, and then say ""please don't pass subsetted anndata objects to plotting functions"" or something like that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3661,variab,variables,3661,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variables']
Modifiability,"e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/scanpy/plotting/tools/paga.py"", line 930, in paga_path; idcs_group = np.argsort(adata.obs['dpt_pseudotime'].values[; File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2688, in __getitem__; return self._getitem_column(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/frame.py"", line 2695, in _getitem_column; return self._get_item_cache(key); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/generic.py"", line 2489, in _get_item_cache; values = self._data.get(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/internals.py"", line 4115, in get; loc = self.items.get_loc(item); File ""/miniconda3/envs/mulled-v1-9b7030900b5a2e199b0cdbb5894abd70134735de4070acfe326d220bc105c4a2/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 3080, in get_loc; return self._engine.get_loc(self._maybe_cast_indexer(key)); File ""pandas/_libs/index.pyx"", line 140, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/index.pyx"", line 162, in pandas._libs.index.IndexEngine.get_loc; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1492, in pandas._libs.hashtable.PyObjectHashTable.get_item; File ""pandas/_libs/hashtable_class_helper.pxi"", line 1500, in pandas._libs.hashtable.PyObjectHashTable.get_item; KeyError: 'dpt_pseudotime'; ```. My test dataset was probably not an adapted one (I did not find any test to reproduce) but is it an expected behavior? Should we provide as input always a `adata` with `dpt_pseudotime`? Maybe it would be interesting to detail that in the documentation.; Anything I can do to help there?. Bérénice",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/328:2123,adapt,adapted,2123,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/328,1,['adapt'],['adapted']
Modifiability,"e:; 138 self.genlower = self.GeneratorLower(self); ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_normal_function(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 ; ; ~/miniforge3/envs/scVelo/lib/python3.8/contextlib.py in __exit__(self, type, value, traceback); 129 value = type(); 130 try:; --> 131 self.gen.throw(type, value, traceback); 132 except StopIteration as exc:; 133 # Suppress StopIteration *unless* it's the same exception that; ; ~/miniforge3/envs/scVelo/lib/python3.8/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 ; ; LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32; ; File ""../../../../miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^; ; During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /Users/depretis.stefano/miniforge3/envs/scVelo/lib/python3.8/site-packages/umap/layouts.py (52); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1799:9790,config,config,9790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1799,1,['config'],['config']
Modifiability,"ecent call last); Input In [51], in <module>; 4 adata2.obs['group'] = adata2.obs.index.to_series().str.startswith(""A"").astype(str); 5 fig, axes = plt.subplots(1, 2); ----> 6 sc.pl.violin(adata2, keys=['CD8A', 'CD8B'], groupby=""group"", ax=axes). File /opt/conda/envs/analysis/lib/python3.8/site-packages/scanpy/plotting/_anndata.py:835, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 833 axs = [ax]; 834 for ax, y, ylab in zip(axs, ys, ylabel):; --> 835 ax = sns.violinplot(; 836 x=x,; 837 y=y,; 838 data=obs_tidy,; 839 order=order,; 840 orient='vertical',; 841 scale=scale,; 842 ax=ax,; 843 **kwds,; 844 ); 845 if stripplot:; 846 ax = sns.stripplot(; 847 x=x,; 848 y=y,; (...); 854 ax=ax,; 855 ). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/_decorators.py:46, in _deprecate_positional_args.<locals>.inner_f(*args, **kwargs); 36 warnings.warn(; 37 ""Pass the following variable{} as {}keyword arg{}: {}. ""; 38 ""From version 0.12, the only valid positional argument ""; (...); 43 FutureWarning; 44 ); 45 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); ---> 46 return f(**kwargs). File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:2408, in violinplot(x, y, hue, data, order, hue_order, bw, cut, scale, scale_hue, gridsize, width, inner, split, dodge, orient, linewidth, color, palette, saturation, ax, **kwargs); 2405 if ax is None:; 2406 ax = plt.gca(); -> 2408 plotter.plot(ax); 2409 return ax. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:1043, in _ViolinPlotter.plot(self, ax); 1041 def plot(self, ax):; 1042 """"""Make the violin plot.""""""; -> 1043 self.draw_violins(ax); 1044 self.annotate_axes(ax); 1045 if self.orient == ""h"":. File /opt/conda/envs/analysis/lib/python3.8/site-packages/seaborn/categorical.py:761, in _ViolinPlotter.draw_violins(self, ax); 759 def draw_violins(self, ax):",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2136:1758,variab,variable,1758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2136,1,['variab'],['variable']
Modifiability,"ed in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scipy/sparse/data.py:132: RuntimeWarning: invalid value encountered in expm1; result = op(self._deduped_data()); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: overflow encountered in square; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py:18: RuntimeWarning: invalid value encountered in subtract; var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: overflow encountered in log1p; mean = np.log1p(mean); /home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py:86: RuntimeWarning: invalid value encountered in log1p; mean = np.log1p(mean); Traceback (most recent call last):; File ""../../scvi/scvi_adata.py"", line 75, in <module>; sc.pp.highly_variable_genes(adata); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 257, in highly_variable_genes; flavor=flavor); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 92, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins); File ""/home/sfleming/anaconda3/envs/scvi/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 233, in cut; ""cannot specify integer `bins` when input data "" ""contains infinity""; ValueError: cannot specify integer `bins` when input data contains infinity; ```. Indeed, if I do `np.expm1(3701)` I get an overflow. I think it will be necessary to come up with a way to calculate highly variable genes without doing `expm1` on the raw counts, due to this overflow issue.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/763:2294,variab,variable,2294,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/763,1,['variab'],['variable']
Modifiability,"ef map_embedding(self, method):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/umap/umap_.py in transform(self, X); 2715 else:; 2716 epsilon = 0.24 if self._knn_search_index._angular_trees else 0.12; -> 2717 indices, dists = self._knn_search_index.query(; 2718 X, self.n_neighbors, epsilon=epsilon; 2719 ). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pynndescent/pynndescent_.py in query(self, query_data, k, epsilon); 1564 """"""; 1565 if not hasattr(self, ""_search_graph""):; -> 1566 self._init_search_graph(); 1567 ; 1568 if not self._is_sparse:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pynndescent/pynndescent_.py in _init_search_graph(self); 1054 ); 1055 else:; -> 1056 diversify_csr(; 1057 reverse_graph.indptr,; 1058 reverse_graph.indices,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . ~/miniconda3/envs/flng/lib/python3.8/site-packages/numba/core/caching.py in save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2406:3528,config,config,3528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2406,1,['config'],['config']
Modifiability,"en reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; import pandas as pd; import numpy as np; import urllib.request; ; url = ""https://humancellatlas.usegalaxy.eu/datasets/11ac94870d0bb33a8d1d2e6bc37a120e/display?to_ext=h5ad""; # 75 MB anndata; urllib.request.urlretrieve(url, '11ac94.h5ad'). adata = sc.read('11ac94.h5ad'). sc.settings.figdir = '.'. sc.pl.rank_genes_groups_dotplot(; adata,; save='.png',; show=False,; gene_symbols='Symbol',; n_genes=10,; log=False,; use_raw=False,; ); ```. ```pytb; Traceback (most recent call last):; File ""/private/var/folders/23/wwh84vd95rncnf5mx753cpgh0000gp/T/tmpenni0vju/job_working_directory/000/11/configs/tmp4tnm4xxb"", line 15, in <module>; sc.pl.rank_genes_groups_dotplot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 647, in rank_genes_groups_dotplot; return _rank_genes_groups_plot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py"", line 426, in _rank_genes_groups_plot; _pl = dotplot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py"", line 913, in dotplot; dp = DotPlot(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_dotplot.py"", line 127, in __init__; BasePlot.__init__(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_baseplot_class.py"", line 105, in __init__; self.categories, self.obs_tidy = _prepare_dataframe(; File ""/usr/local/lib/python3.8/site-packages/scanpy/plotting/_anndata.py"", line 1845, in _prepare_dataframe; obs_tidy = get.obs_df(; File ""/usr/local/lib/python3.8/site-packages/scanpy/get/ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1796:1107,config,configs,1107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1796,1,['config'],['configs']
Modifiability,"ent_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; ima",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2202,plugin,plugins,2202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['plugin'],['plugins']
Modifiability,"error message:. ```; computing UMAP; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/nr/miniconda3/lib/python3.7/site-packages/scanpy/tools/_umap.py"", line 145, in umap; verbose=settings.verbosity > 3,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 1005, in simplicial_set_embedding; verbose=verbose,; File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 401, in _compile_for_args; error_rewrite(e, 'typing'); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/dispatcher.py"", line 344, in error_rewrite; reraise(type(e), e, None); File ""/home/nr/miniconda3/lib/python3.7/site-packages/numba/six.py"", line 668, in reraise; raise value.with_traceback(tb); numba.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend); Invalid use of type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)) with parameters (array(float64, 1d, C), array(float32, 1d, C)); Known signatures:; * (array(float32, 1d, A), array(float32, 1d, A)) -> float32; * parameterized; [1] During: resolving callee type: type(CPUDispatcher(<function rdist at 0x7f8ffd913050>)); [2] During: typing of call at /home/nr/miniconda3/lib/python3.7/site-packages/umap/umap_.py (795). File ""miniconda3/lib/python3.7/site-packages/umap/umap_.py"", line 795:; def optimize_layout(; <source elided>. dist_squared = rdist(current, other); ^. This is not usually a problem with Numba itself but instead often caused by; the use of unsupported features or an issue in resolving types. To see Python/NumPy features supported by the latest release of Numba visit:; http://numba.pydata.org/numba-doc/latest/reference/pysupported.html; and; http://numba.pydata.org/numba-doc/latest/reference/numpysupported.html. For more information about typing errors and how to debug them visit:; http://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile. If you think your code should work with Numba, please rep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/948:1383,parameteriz,parameterized,1383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/948,1,['parameteriz'],['parameterized']
Modifiability,"exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (fail",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:1585,variab,variable,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['variab'],['variable']
Modifiability,extend options to aggregate ranks across batches for HVG selection,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2151:0,extend,extend,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2151,1,['extend'],['extend']
Modifiability,"f all, thank you for your great platform! . When I try to export a SPRING project I get the following error (it seems that the class NeighborsView is not defined; I have a 'neighbors' key in .uns): . <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```; sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). AnnData object with n_obs × n_vars = 8757 × 20679 ; obs: 'SeqRun', 'Biological replicate', 'nCount_RNA', 'nCount_SCT', 'nFeature_RNA', 'nFeature_SCT', 'novelty', 'orig_ident', 'percent_mt', 'sc_leiden_res_48.75', 'State', 'ImmGen'; var: 'Selected', 'sct_detection_rate', 'sct_gmean', 'sct_residual_mean', 'sct_residual_variance', 'sct_variable', 'sct_variance'; uns: 'Biological replicate_colors', 'ImmGen_colors', 'State_colors', 'leiden', 'neighbors', 'state'; obsm: 'X_pca', 'X_umap'; varm: 'pca_feature_loadings'; layers: 'norm_data', 'scale_data'; obsp: 'connectivities', 'distances'; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-208-9f15be957dd9> in <module>; 1 sc.external.exporting.spring_project(adata, '/Users/mariusmessemaker/Documents/Project/mempel/SPRING', 'X_umap', subplot_name='Mempel', cell_groupings=['State', 'ImmGen', 'Biological replicate'], ; ----> 2 custom_color_tracks=None, total_counts_key='nCount_RNA', neighbors_key='neighbors', overwrite=False). ~/miniconda3/envs/py36-sc/lib/python3.6/site-packages/scanpy/external/exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, neighbors_key, overwrite);",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1260:1198,layers,layers,1198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1260,1,['layers'],['layers']
Modifiability,"f six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Found 3 genes with zero variance.; Fitting L/S model and finding priors. Finding parametric adjustments. Adjusting data. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: invalid value encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()). In [2]: sc.pp.highly_variable_genes(adata); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-2-7727f5f928cd>"", line 1, in <module>; sc.pp.highly_variable_genes(adata). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, nan, nan]).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172:2129,variab,variable,2129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172,1,['variab'],['variable']
Modifiability,flake8 config malformed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1783:7,config,config,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1783,1,['config'],['config']
Modifiability,"fo(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_ses",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3082,config,config,3082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it allows for a more functional style of writing a processing pipeline_?. Hence, I'm sorry that I tend to not merge your pull request as is. Either you restore everything else that was there and solely add the inplace `np.log1p` or I'd do that. :smile:. Have a good Sunday!; Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1959,variab,variable,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['variab'],['variable']
Modifiability,"ft_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actual(. /usr/local/lib/python3.6/site-packages/anndata/base.py in _init_as_view(self, adata_ref, oidx, vidx); 705 self._varm = ArrayView(adata_ref.varm[vidx_normalized], view_args=(self, 'varm')); 706 # hackish solution here, no copy should be necessary; --> 707 uns_new = deepcopy(self._adata_ref._uns); 708 # need to do the slicing before setting the updated self._n_obs, self._n_vars; 709 self._n_obs = self._adata_ref.n_obs # use the original n_obs here. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in deepcopy(x, memo, _nil); 178 y = x; 179 else:; --> 180 y = _reconstruct(x, memo, *rv); 181 ; 182 # If is its own copy, don't memoize. /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/copy.py in _reconstruct(x, memo, func, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:3433,layers,layers,3433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,1,['layers'],['layers']
Modifiability,"ge](https://user-images.githubusercontent.com/8238804/90855459-0332a500-e3c3-11ea-8b7b-0ba997664f93.png). </details>. The current default of true is a bit weird for ""on data"":. ```python; sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data""); sc.pl.umap(brain, color=""leiden"", groups=[""0"", ""1""], legend_loc=""on data"", na_in_legend=False); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855740-a8e61400-e3c3-11ea-99fa-d9cdcd3320ed.png); ![image](https://user-images.githubusercontent.com/8238804/90855745-abe10480-e3c3-11ea-88fd-9c794c95773d.png). </details>. ## Missing color. The missing color can now be specified with `na_color`. This defaults to transparent for spatial plots, and light gray for all other embedding based plots. ```python; with plt.rc_context({""figure.dpi"": 150}):; sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""]); sc.pl.spatial(brain, color=[""leiden_missing"", ""Bc1_missing""], na_color=(.8, .8, .8, .2)); ```. <details>; <summary> Images </summary>. ![image](https://user-images.githubusercontent.com/8238804/90855677-894eeb80-e3c3-11ea-91a5-51049080af45.png); ![image](https://user-images.githubusercontent.com/8238804/90855880-05493380-e3c4-11ea-878b-492872198b7f.png). </details>. ## Tests. I've added a parameterized regression test around a perhaps-too-cute test case. <details>; <summary> Test case </summary>. ```python; sc.pl.spatial(adata, color=""label""); ```. ![image](https://user-images.githubusercontent.com/8238804/90856156-ab953900-e3c4-11ea-83da-9caf5fb5d82e.png). </details>. This test makes a lot of files, so I'll rebase the revisions away before merge. ## Possible problems. * I'm hoping I haven't missed any edge cases, but would appreciate some testing from @giovp and @fidelram.; * What do you think about the interaction between `groups` and `legend_loc=""on data""`? I'd like to keep `na_in_legend` as a simple boolean, but this does change the current behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238:1951,parameteriz,parameterized,1951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1356#issuecomment-678052238,1,['parameteriz'],['parameterized']
Modifiability,"genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no error. While running: `adata_orig.X.sum(1)` gives me the error as : ; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-23-951a31c71c45> in <module>; ----> 1 adata_orig.X.sum(1). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```. PS: I downloaded this` .h5ad` file from a published research paper to perform some analysis over it, would be happy to provide you the link to same if required. . Also this is the environment that I am working in:; `scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.1 scipy==1.6.2 pandas==1.2.4 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.6`. Hello @LuckyMD, tagging you for just in case you might be knowing the resolution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:2896,layers,layers,2896,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,good catch! Thank you for reporting this. ; Pinging @Koncopd since I believe you were involved in major refactoring of this. ; Thank you @rpeys,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951:104,refactor,refactoring,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1292#issuecomment-704767951,1,['refactor'],['refactoring']
Modifiability,"group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); > 819 if isinstance(var_names, str):; > 820 var_names = [var_names]; > --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); > 822; > 823 if 'color' in kwds:; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); > 1983 matrix = adata[:, var_names].layers[layer]; > 1984 elif use_raw:; > -> 1985 matrix = adata.raw[:, var_names].X; > 1986 else:; > 1987 matrix = adata[:, var_names].X; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); > 510; > 511 def __getitem__(self, index):; > --> 512 oidx, vidx = self._normalize_indices(index); > 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; > 514 else: X = self._adata.file['raw.X'][oidx, vidx]; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); > 538 obs, var = super(Raw, self)._unpack_index(packed_index); > 539 obs = _normalize_index(obs, self._adata.obs_names); > --> 540 var = _normalize_index(var, self.var_names); > 541 return obs, var; > 542; >; > ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); > 270 raise KeyError(; > 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; > --> 272 .format(index)); > 273 return positions.values; > 274 else:; >; > Cheers,; > Samuele; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/438#issuecomment-456707222>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1XEPWhKAyeK0sWLrAzmqJvm45H-vks5vGBhDgaJpZM4aMT_6>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910:3239,variab,variables,3239,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456735910,1,['variab'],['variables']
Modifiability,"he 45th variable is the only one who's value changes; * This is especially weird since all values are changing if I run the function on the full set of variables; * If I use a smaller interval size (10), I don't get any varying results; * If I run morans I for each value individually, I also don't get any varying results. I'm now checking to see if I take random subsample of the variables, compute morans I for those values together, is it always the same variables which are inconsistent?. -----------------. This is so weird. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from functools import partial. def check_subset(g, X, idx, tries=5, func=sc.metrics.morans_i):; sub = X[idx]; result = np.ones(sub.shape[0], dtype=bool); first = func(g, sub). for i in range(tries):; result &= (first == func(g, sub)). return result. morans_i = partial(check_subset, adata.obsp[""connectivities""], adata.X.T.copy(), func=sc.metrics.morans_i). # Take adata.n_vars samples of 100 variables each; samples = np.random.choice(adata.n_vars, (adata.n_vars, 100)). # This takes a while; results = np.vstack([morans_i(samples[idx]) for idx in range(adata.n_vars)]). df = pd.DataFrame({; ""var_idx"": samples.flatten(),; ""consistent"": results.flatten(), ; ""sample"": np.repeat(np.arange(adata.n_vars), 100),; ""order"": np.tile(np.arange(100), adata.n_vars),; }). df.groupby(""order"").mean()[""consistent""].plot(); ```. ![image](https://user-images.githubusercontent.com/8238804/116065566-7e72f600-a6ca-11eb-9ad6-5c991ed79910.png). ---------------------. </details>. Progress!. Minimal reproducer:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[:411].copy(); sc.pp.neighbors(pbmc); res = [sc.metrics.morans_i(pbmc.obsp[""connectivities""], pbmc.X.T) for i in range(3)]. np.equal(res[0], res[1]); ```. I can trigger the bug by changing the dataset size. . I think this may be a memory alignment issue. If `X` is a sparse matrix, I don't get any inconsistency in the results (which would ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541:1557,variab,variables,1557,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-826712541,1,['variab'],['variables']
Modifiability,"he best trees into a search forest; --> 967 tree_scores = [; 968 score_linked_tree(tree, self._neighbor_graph[0]); 969 for tree in self._rp_forest; 970 ]; 971 if self.verbose:; 972 print(ts(), ""Worst tree score: {:.8f}"".format(np.min(tree_scores))). File ~/miniconda3/envs/test/lib/python3.10/site-packages/pynndescent/pynndescent_.py:968, in <listcomp>(.0); 961 self._search_forest = [; 962 convert_tree_format(tree, self._raw_data.shape[0]); 963 for tree in rp_forest; 964 ]; 965 else:; 966 # convert the best trees into a search forest; 967 tree_scores = [; --> 968 score_linked_tree(tree, self._neighbor_graph[0]); 969 for tree in self._rp_forest; 970 ]; 971 if self.verbose:; 972 print(ts(), ""Worst tree score: {:.8f}"".format(np.min(tree_scores))). File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:487, in _DispatcherBase._compile_for_args(self, *args, **kws); 485 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 486 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 487 raise e; 488 finally:; 489 self._types_active_call = []. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:420, in _DispatcherBase._compile_for_args(self, *args, **kws); 418 return_val = None; 419 try:; --> 420 return_val = self.compile(tuple(argtypes)); 421 except errors.ForceLiteralArg as e:; 422 # Received request for compiler re-entry with the list of arguments; 423 # indicated by e.requested_args.; 424 # First, check if any of these args are already Literal-ized; 425 already_lit_pos = [i for i in e.requested_args; 426 if isinstance(args[i], types.Literal)]. File ~/miniconda3/envs/test/lib/python3.10/site-packages/numba/core/dispatcher.py:965, in Dispatcher.compile(self, sig); 963 with ev.trigger_event(""numba:compile"", data=ev_details):; 964 try:; --> 965 cres = self._compiler.compile(args, return_type); 966 except errors.ForceLiteralArg as e:; 967 def folded(args, kws):. File ~/miniconda3/envs/test/lib/python3.10/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2191:3359,config,config,3359,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2191,1,['config'],['config']
Modifiability,"he environment containing useful variables and data for testing (a sample scRNA dataset) after changing scanpy's source code. However, scanpy cannot be reloaded. This means that to test, one has to stop the kernel, restart, reload all of the data needed for a plot and then test a plotting function that was just modified (for instance). . Here is a way to demonstrate the reload failure easily:; 1. open utils.py and add the print statement to track the descend_classes_and_funcs() function. ```py; #utils.py; def annotate_doc_types(mod: ModuleType, root: str):; for c_or_f in descend_classes_and_funcs(mod, root):; print(c_or_f) #added line to track descend_classes_and_funcs() function--TR; c_or_f.getdoc = partial(getdoc, c_or_f); ```. 2. open ipython. ```py; import scanpy as sc; # prints out a bunch of function names from the descend_classes_and_funcs() function. import importlib; importlib.reload(sc); # endless loop of function names from the descend_classes_and_funcs() function; # due to recursive yield statement; ```. So what is the purpose of this function? And can it be altered to allow reload? It is called when __init__.py is run by sc.annotate_doc_types(sys.modules[__name__], 'scanpy'). . ```py; #utils.py. def descend_classes_and_funcs(mod: ModuleType, root: str):; for obj in vars(mod).values():; if not getattr(obj, '__module__', getattr(obj, '__qualname__', getattr(obj, '__name__', ''))).startswith(root):; continue; if isinstance(obj, Callable):; yield obj; if isinstance(obj, type):; yield from (m for m in vars(obj).values() if isinstance(m, Callable)); elif isinstance(obj, ModuleType):; yield from descend_classes_and_funcs(obj, root); ```. _________________________________________________________. It is possible to remove the scanpy manually by:. ```py; import sys; sys.modules.pop('scanpy'); ```. and then import scanpy from scratch. Is that the preferred way to re-import scanpy for developers working to enhance scanpy package?. Thank you for your work on scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/468:2079,enhance,enhance,2079,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/468,1,['enhance'],['enhance']
Modifiability,"hi @falexwolf, it still isn't working. The problem is with categorical variables, I'm currently doing this before subsetting:. ```; cat_columns = adata.obs.select_dtypes(['category']).columns; adata.obs[cat_columns] = adata.obs[cat_columns].astype(str); del cat_columns; ```; but it's really annoying, specially when using scvelo. Can you look into it? Also something problematic is that the `adata.uns['variable_color']` doesn't delete after you delete `adata.obs['variable']` so when you run into the subsetting problem my fix doesn't work if this is the situation and I've to manually delete the columns one by one... perhaps make this part of `sanitize_anndata`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979:71,variab,variables,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/363#issuecomment-458386979,2,['variab'],"['variable', 'variables']"
Modifiability,"hi Alex, I'm not sure how to test this either. I tried to run in a new clean conda environment but cant use more than 50% of the cells in pre-processing (otherwise the process is killed at even the normalization stage, ""pp.normalize_ per_cell"" and uses over 120GB RAM); The file from 10X i'm using is the link to ""1M_neurons aggr - Gene / cell matrix HDF5 (filtered)"" from https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/ ; This is what i type into the terminal; $ python cluster_mouse_brain.py 1M_neurons_filtered_gene_bc_matrices_h5.h5; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; reading 1M_neurons_filtered_gene_bc_matrices_h5.h5 (0:01:19.78); running recipe zheng17; filtered out 3983 genes that are detected in less than 1 counts; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; Killed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/511#issuecomment-469664995:566,Variab,Variable,566,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/511#issuecomment-469664995,3,['Variab'],['Variable']
Modifiability,"hi,. finally managed to add some tests. Had to refactor the original `test_score_genes.py` a little, I hope that's ok: The one test that was already there still exists, I just pulled out the creation of the adata into a separate function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235:47,refactor,refactor,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1196#issuecomment-626233235,1,['refactor'],['refactor']
Modifiability,high variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/188:5,variab,variable,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/188,1,['variab'],['variable']
Modifiability,highly variable genes + batch_key --> reciprocal condition number error,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2669:7,variab,variable,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2669,1,['variab'],['variable']
Modifiability,"hon-input-2-2626ee07d023> in <module>; ----> 1 sc.read_h5ad('/xchip/beroukhimlab/michelle/jjeang_plgg/sc_labeled.h5ad'). ~/.local/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; leidenalg 0.8.0; llvmlite 0.38.1; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.5.2; mpl_toolkits NA; natsort 8.1.0; numba 0.55.2; numexpr 2.7.1; numpy 1.21.6; packaging 21.3; pandas 1.4.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:2794,layers,layers,2794,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['layers'],['layers']
Modifiability,"hon37\lib\site-packages\matplotlib\cbook\deprecation.py in wrapper(*args, **kwargs); 449 ""parameter will become keyword-only %(removal)s."",; 450 name=name, obj_type=f""parameter of {func.__name__}()""); --> 451 return func(*args, **kwargs); 452 ; 453 return wrapper. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in __init__(self, ax, cmap, norm, alpha, values, boundaries, orientation, ticklocation, extend, spacing, ticks, format, drawedges, filled, extendfrac, extendrect, label); 489 else:; 490 self.formatter = format # Assume it is a Formatter or None; --> 491 self.draw_all(); 492 ; 493 def _extend_lower(self):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in draw_all(self); 506 # sets self._boundaries and self._values in real data units.; 507 # takes into account extend values:; --> 508 self._process_values(); 509 # sets self.vmin and vmax in data units, but just for the part of the; 510 # colorbar that is not part of the extend patch:. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colorbar.py in _process_values(self, b); 961 expander=0.1); 962 ; --> 963 b = self.norm.inverse(self._uniform_y(self.cmap.N + 1)); 964 ; 965 if isinstance(self.norm, (colors.PowerNorm, colors.LogNorm)):. c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in inverse(self, value); 1218 if not self.scaled():; 1219 raise ValueError(""Not invertible until scaled""); -> 1220 self._check_vmin_vmax(); 1221 vmin, vmax = self.vmin, self.vmax; 1222 . c:\users\pawandeep\appdata\local\programs\python\python37\lib\site-packages\matplotlib\colors.py in _check_vmin_vmax(self); 1179 raise ValueError(""minvalue must be less than or equal to maxvalue""); 1180 elif self.vmin <= 0:; -> 1181 raise ValueError(""minvalue must be positive""); 1182 ; 1183 def __call__(self, value, clip=None):; ValueError: minvalue must be positive",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2003:3592,extend,extend,3592,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2003,1,['extend'],['extend']
Modifiability,"hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 327 in _main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 273 in wrap_session; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 320 in pytest_cmdline_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 198 in console_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igraph 0.11.4; imageio 2.34.0; importlib-metadata 7.0.1; importlib-resources 6.1.1; iniconfig 2.0.0; intervaltree 3.1.0; Jin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:6591,config,config,6591,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['config'],['config']
Modifiability,"how to add the raw counts to my h5ad object?; Any idea how to convert the three data slot from Seurat [raw, data, scaled.data] to h5ad layers?. ```; SeuratObject@assays$RNA@counts; SeuratObject@assays$RNA@data; SeuratObject@assays$RNA@scale.data; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210:135,layers,layers,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1875#issuecomment-868781210,1,['layers'],['layers']
Modifiability,"htly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero is automatic - if they are not infinity before. An argument can be made for setting these values to `inf` without zero centering and to `0` or `nan` with zero centering. That is something for you to decide (I guess).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1953,Variab,Variables,1953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['Variab'],['Variables']
Modifiability,"https://github.com/theislab/scanpy/blob/2a7fb7cdf56ff44ef6338c7dba3b84b4b32d216c/scanpy/plotting/_tools/__init__.py#L286. `scanpy.pl.rank_genes_groups()` by default presents the `scores` from `adata.uns['rank_genes']['scores']`. However, why is it not possible to change to `scores` `logfoldchanges`. The code at line 286 does not use any modularity or possible parameterization. A possible change would be to introduce a new parameter `plotby`, which corresponds to `adata.uns['rank_genes'][f'{plotby}']` or similar. . I do realize that it is easy to extract the relevant data and plot them by myself. However, there is value in making it easier to choose what to exactly plot from the `rank_genes_groups`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1152:362,parameteriz,parameterization,362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1152,1,['parameteriz'],['parameterization']
Modifiability,"images.githubusercontent.com/8238804/144901891-45c3a8aa-1b56-4521-abc1-66f968a59d23.png); > ; > ```python; > sc.pl.heatmap(; > pbmc,; > var_names=[""LDHB"", ""LYZ"", ""CD79A""],; > row_groups=""louvain"",; > col_groups=""sampleid""; > ); > ```; > ; > ![image](https://user-images.githubusercontent.com/8238804/144902398-e967c1db-53c1-4b44-bcbf-8dfedcf06e58.png); > ; > What do you think about that?. Thanks @ivirshup !. I like these lines you suggested- perhaps I can adopt to make it more elegant when creating color_df/size_df:; ```; import scanpy as sc, pandas as pd, numpy as np. pbmc = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc.obs[""sampleid""] = np.repeat([""s1"", ""s2""], pbmc.n_obs / 2); df = sc.get.obs_df(pbmc, [""LDHB"", ""louvain"", ""sampleid""]). summarized = df.pivot_table(; index=[""louvain"", ""sampleid""],; values=""LDHB"",; aggfunc=[np.mean, np.count_nonzero]; ); color_df = summarized[""mean""].unstack(); size_df = summarized[""count_nonzero""].unstack(). # I don't think the var_names or groupby variables are actually important here; sc.pl.DotPlot(; pbmc,; var_names=""LDHB"", groupby=[""louvain"", ""sampleid""], # Just here so it doesn't error; dot_color_df=color_df, dot_size_df=size_df,; ).style(cmap=""Reds"").show(); ```; this is the output:; ![image](https://user-images.githubusercontent.com/10910559/145053489-c550d5a7-a8fe-4a61-b672-9103ccf1d228.png); some work are needed to modify the grid/axis size, legend and scale. Actually this is the reason I work on top of the _dotplot and _baseplot function/ classes to implement the solution- to make the plots the same style with scanpy dotplot without doing too much work on the cosmetics. But I can certainly change grouby_expand from bool to an actual variable `group_cols` as you suggested in #2055 . Or should we call it `col_groups` as you did in your sc.pl.heatmap pseudo code? ; I'd be more than happy to make it more generalized, i.e., to sc.pl.heatmap, but I may need some time to understand sc.pl.heatmap first. The plotting functions a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664:3170,variab,variables,3170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-988045664,1,['variab'],['variables']
Modifiability,"imal code sample (that we can copy&paste without having any data). ```python; adata.obs['sex'].cat.categories.tolist(); ```. ```pytb; ['F', 'M', 'U']; ```. ```python; adata.obs['age_groups'].cat.categories.tolist(); ```. ```pytb; ['Old', 'YoungAdult', 'Pediatric', 'Fetal', 'NewBorn']; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['sex']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	sex. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/sinhar/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py:340: RuntimeWarning: divide by zero encountered in true_divide; (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max(); Adjusting data; ```. ```python; sc.pp.combat(adata, key='384plate', covariates=['age_group']); ```. ```pytb; Standardizing Data across genes. Found 34 batches. Found 1 categorical variables:; 	age_group. Found 0 numerical variables:; 	. ---------------------------------------------------------------------------; LinAlgError Traceback (most recent call last); <timed eval> in <module>. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace); 204 # standardize across genes using a pooled variance estimator; 205 logg.info(""Standardizing Data across genes.\n""); --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key); 207 ; 208 # fitting the parameters on the standardized data. ~/.local/lib/python3.8/site-packages/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key); 102 ; 103 # compute pooled variance estimator; --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T); 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]); 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). ~/.local/lib/python3.8/sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1606:1427,variab,variables,1427,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606,1,['variab'],['variables']
Modifiability,"ine 244, in info; return settings._root_logger.info(msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 56, in info; return self.log(INFO, msg, time=time, deep=deep, extra=extra); File ""/home/vsts/work/1/s/scanpy/logging.py"", line 43, in log; super().log(level, msg, extra=extra); Message: "" finished: added\n 'X_draw_graph_fr', graph_drawing coordinates (adata.obsm)""; Arguments: (); --- Logging error ---; Traceback (most recent call last):; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/logging/__init__.py"", line 1084, in emit; stream.write(msg + self.terminator); ValueError: I/O operation on closed file.; Call stack:; File ""/opt/hostedtoolcache/Python/3.8.8/x64/bin/pytest"", line 8, in <module>; sys.exit(console_main()); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main; code = main(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/sit",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3014,config,config,3014,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"ing, same NaNs and same error when trying to run ```sc.pp.highly_variable_genes()```:. ```; In [1]: sc.pp.combat(adata_Combat, key='sample'); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/anndata/_core/anndata.py:21: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variabl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1228,variab,variables,1228,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variables']
Modifiability,"ion in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```py; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:1444,variab,variable,1444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,1,['variab'],['variable']
Modifiability,"ion(self, fndesc); 188 # Init argument values; 189 self.extract_function_arguments(); --> 190 entry_block_tail = self.lower_function_body(); 191 ; 192 # Close tail of entry block. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_function_body(self); 214 bb = self.blkmap[offset]; 215 self.builder.position_at_end(bb); --> 216 self.lower_block(block); 217 self.post_lower(); 218 return entry_block_tail. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/lowering.py in lower_block(self, block); 228 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 229 loc=self.loc, errcls_=defaulterrcls):; --> 230 self.lower_inst(inst); 231 self.post_block(block); 232 . ~/.conda/envs/rpy/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback); 133 value = type(); 134 try:; --> 135 self.gen.throw(type, value, traceback); 136 except StopIteration as exc:; 137 # Suppress StopIteration *unless* it's the same exception that. ~/.conda/envs/rpy/lib/python3.9/site-packages/numba/core/errors.py in new_error_context(fmt_, *args, **kwargs); 749 newerr = errcls(e).add_context(_format_msg(fmt_, args, kwargs)); 750 tb = sys.exc_info()[2] if numba.core.config.FULL_TRACEBACKS else None; --> 751 raise newerr.with_traceback(tb); 752 ; 753 . LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Storing i64 to ptr of i32 ('dim'). FE type int32. File ""../../../../../../../.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py"", line 52:; def rdist(x, y):; <source elided>; result = 0.0; dim = x.shape[0]; ^. During: lowering ""dim = static_getitem(value=$8load_attr.2, index=0, index_var=$const10.3, fn=<built-in function getitem>)"" at /public/home/ycxiang_zju/.conda/envs/rpy/lib/python3.9/site-packages/umap/layouts.py (52); ```; ​; sc.pp.filter_cells(unspliced, min_genes=200); dyn.pl.basic_stats(spliced)`; I am wondering how to solve this problem. Will I need to re-create a virtual environment with lower python verison?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796:9724,config,config,9724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-803866796,1,['config'],['config']
Modifiability,"irmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am following the tutorial but everytime I try to run a violin plot the kernel crashes, this doesnt happen with other seaborn graphs. I have tried updating packeges, changing environment, etc, etc & nothing works any help would be great !!. ![image](https://github.com/scverse/scanpy/assets/127498480/b5cc12b1-00af-4919-abd0-7ea99b72cade). ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(; 'data/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`; adata; sc.pl.highest_expr_genes(adata, n_top=20, ); sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'); sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0-4, multi_panel=True); ```. ### Error output. ```pytb; Kernel Restarting; The kernel for Tests/scanpytutorial/Untitled.ipynb appears to have died. It will restart automatically.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840:1111,variab,variable,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840,2,['variab'],"['variable', 'variables-axis']"
Modifiability,"irst concatenated into a single dataframe.; - The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via ~~np.nanmin~~ np.nanmean. Another column which counts ""in how many batches a gene is detected as hvg"" is also created. ~~I'm not 100% certain about nanmin, but I think it works better than mean or max, because it forces the process to pick genes with high dispersion even in the ""worst"" batches.~~; - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list.; - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462765-af5f1880-6396-11e9-95fb-dddb05d94214.png). ```python; sc.pp.highly_variable_genes(ad); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462767-bd149e00-6396-11e9-95e4-31c52241a747.png). ```python; sc.pp.highly_variable_genes(ad, batch_key='batch', n_top_genes=1000); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```. ![image](https://user-images.githubusercontent.com/1140359/56462820-999e2300-6397-11e9-81e0-ee4aff03668a.png). ```python;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/614:1457,adapt,adapted,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614,1,['adapt'],['adapted']
Modifiability,"it should definitely work. on a properly configured system (including docker images), the encoding should be UTF-8. you’re right, we should probably do it. the only reason we didn’t yet is that we open quite a few files in the codebase, and if one of those open calls expects UTF-8, it’ll break again, but this time deeper down and harder to reproduce.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072:41,config,configured,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/43#issuecomment-343484072,1,['config'],['configured']
Modifiability,"itch the t-SNE implementation to openTSNE at the very least. For the recipes, there' already something similar in the preprocessing module. So I'd imagine calling standard t-SNE with `sc.tl.tsne` and the recipes like `sc.tl.tsne.recipe_multiscale`. > And luckily it is possible! I can even see two approaches. (1) Either use k=15 kNN graph with the uniform similarity kernel. As I said, and as Pavlin knows, this yields result that is very similar to using perplexity=30. (2) Or use k=15 kNN graph with UMAP weights, normalize it as t-SNE expects it to be normalized and use that. My expectation is that it would yield very similar results, but I haven't actually tried it. I very much prefer option 1. If I understand option 2 correctly, we would normalize the 15 neighbors to essentially `perplexity=5`. I've never once found a case where that is useful, so having this as the default behaviour in scanpy seems like a really bad idea (I foresee a lot of issues in the style ""why is t-SNE not working?""). Using a uniform kernel produces results that are virtually indistinguishable from vanilla t-SNE, so that's fine IMO, and it's faster as well. It's still less than the default `perplexity=30`, but this seems like the best option. Whatever we agree on, the same can be applied to the ingest functionality, so adding that would also be straightforward. > Moreover, we could make the standard t-SNE available by extending sc.pp.neighors with method=""tsne"" (there are several methods there already). I don't understand this, why would this belong on `sc.pp.neighbors`? The graph weighing should go into the `sc.tl.tsne` call. Are the UMAP weights assigned to the graph in `sc.pp.neighbors`? That seems questionable. I would expect the output to be a directed, unweighted graph, and let each method take care of the graph. If anything, I'd expect it to weight it using the Jaccard index of shared nearest neighbors, which seems to me like pretty much the standard thing to do in single-cell analysis.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360:1465,extend,extending,1465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-748670360,1,['extend'],['extending']
Modifiability,"ite; cwd: /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/; Complete output (29 lines):; running install; running build; got version from file /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/llvmlite/_version.py {'version': '0.34.0', 'full': 'c5889c9e98c6b19d5d85ebdd982d64a03931f8e2'}; running build_ext; /usr/bin/python /tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py; LLVM version... Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 105, in main_posix; out = subprocess.check_output([llvm_config, '--version']); File ""/usr/lib/python3.10/subprocess.py"", line 420, in check_output; return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,; File ""/usr/lib/python3.10/subprocess.py"", line 501, in run; with Popen(*popenargs, **kwargs) as process:; File ""/usr/lib/python3.10/subprocess.py"", line 966, in __init__; self._execute_child(args, executable, preexec_fn, close_fds,; File ""/usr/lib/python3.10/subprocess.py"", line 1842, in _execute_child; raise child_exception_type(errno_num, err_msg, err_filename); FileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 191, in <module>; main(); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 181, in main; main_posix('linux', '.so'); File ""/tmp/pip-install-i_hyvl0s/llvmlite_d91917e9522a491da51d00bc9034d43e/ffi/build.py"", line 107, in main_posix; raise RuntimeError(""%s failed executing, please point LLVM_CONFIG ""; RuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config; error: command '/usr/bin/python' failed with exit code 1; ----------------------------------------; ```. Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2105:2236,config,config,2236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2105,3,['config'],['config']
Modifiability,"iterally nobody is using this function for arrays and sparse matrices. I'm not sure about that. I got the impression from https://github.com/theislab/scanpy/issues/1030#issuecomment-607952458 and (IIRC) a conversation with @scottgigante that people would like these functions to work on arrays as well as AnnData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:951,Variab,Variables,951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['Variab'],['Variables']
Modifiability,"itle, size_title, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, swap_axes, dot_color_df, show, save, ax, return_fig, **kwds); 930 dot_color_df=dot_color_df,; 931 ax=ax,; --> 932 **kwds,; 933 ); 934 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_dotplot.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, expression_cutoff, mean_only_expressed, standard_scale, dot_color_df, dot_size_df, ax, **kwds); 142 layer=layer,; 143 ax=ax,; --> 144 **kwds,; 145 ); 146 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_baseplot_class.py in __init__(self, adata, var_names, groupby, use_raw, log, num_categories, categories_order, title, figsize, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, ax, **kwds); 111 num_categories,; 112 layer=layer,; --> 113 gene_symbols=gene_symbols,; 114 ); 115 . ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). ~/anaconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/_anndata.py in <dictcomp>(.0); 1837 # translate the column names to the symbol names; 1838 obs_tidy.rename(; -> 1839 columns={var_names[x]: symbols[x] for x in range(len(var_names))},; 1840 inplace=True,; 1841 ). NameError: free variable 'symbols' referenced before assignment in enclosing scope; ```. #### Versions. <details>. scanpy==1.6.0 anndata==0.7.4 umap==0.3.10 numpy==1.18.4 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.8.2 louvain==0.6.1 leidenalg==0.8.0. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1636:2649,variab,variable,2649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636,1,['variab'],['variable']
Modifiability,"ize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #173; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Docs:; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/release-notes/index.html#version-1-10; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pp.scrublet_simulate_doublets.html; - https://icb-scanpy--2703.com.readthedocs.build/en/2703/api/generated/scanpy.pl.scrublet_score_distribution.html. ### How to review this PR. I made tests quantitative before this PR, so note that the only change that modified tests is 42143d88a0d499130fac8e5ca60eef0c19163734. In that PR, I make it so there are no longer any duplicate simulated doublets being created. This is necessary to be able to support any neighborhood detection algorithm. I also feel like it makes more sense. This is the only algorithmic change to upstream. Please use your own judgement to check if this makes sense to you. ### TODO:. - [x] remove unused utils (plotting, preprocessing); - [ ] figure out what remaining utils to replace with ours; - [x] PCA/SVD: https://github.com/scverse/scanpy/blob/bf5f1f9343f5729df6f90f7c68363682022e0480/scanpy/preprocessing/_scrublet/__init__.py#L415-L417; - [x] mean_center, normalize_variance, zscore: small enough to be left alone I think; - [ ] get_knn_graph: no need to have multiple implementations here, but our current implementation automatically calculates connectivities, which this doesn’t need https://github.com/scverse/scanpy/pull/2723; - [ ] refactor so the class API matches the way we use it ; - [x] switch to dataclass for centralized attr defs; - [x] use non-deprecated random state style; - [x] use our logging instead of print statements",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2703:1975,refactor,refactor,1975,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2703,1,['refactor'],['refactor']
Modifiability,"l components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata,max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); ```. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L202",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3100:2752,variab,variable,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3100,2,['variab'],['variable']
Modifiability,"l tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; obs: 'n_counts'; var: 'gene_ids'; adata_dist = AnnData object with n_obs × n_va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1384,config,configfile,1384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['config'],['configfile']
Modifiability,"lace=True). print(""Run 1: initial values after simple processing: ""); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). print(""\nRun 2: after sc.pp.normalize_total: ""); sc.pp.normalize_total(adata, target_sum=1e4); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()) # Note that this changed too; print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2240,layers,layers,2240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,log1p warns adata.X is logged when it may not be (when other layers are logged),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1333:61,layers,layers,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1333,1,['layers'],['layers']
Modifiability,"lots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 658 # check if value to plot is in var; 659 elif use_raw is False and value_to_plot in adata.var_names:; --> 660 color_vector = adata[:, value_to_plot].X; 661 ; 662 elif use_raw is True and value_to_plot in adata.raw.var_names:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1307 def __getitem__(self, index):; 1308 """"""Returns a sliced view of the object.""""""; -> 1309 return self._getitem_view(index); 1310 ; 1311 def _getitem_view(self, index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1311 def _getitem_view(self, index):; 1312 oidx, vidx = self._normalize_indices(index); -> 1313 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1314 ; 1315 def _remove_unused_categories(self, df_full, df_sub, uns):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 662 if not isinstance(X, AnnData):; 663 raise ValueError('`X` has to be an AnnData object.'); --> 664 self._init_as_view(X, oidx, vidx); 665 else:; 666 self._init_as_actual(. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 723 self._X = None; 724 else:; --> 725 self._init_X_as_view(); 726 ; 727 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 750 shape = (; 751 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 752 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 753 ); 754 if np.isscalar(X):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 148 return 1; 149 else:; --> 150 return len(idx). TypeError: object of type 'numpy.int64' has no len(); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-456954317:2233,layers,layers,2233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-456954317,1,['layers'],['layers']
Modifiability,"ls.savefig_or_show('pca_scatter', show=show, save=save); 118 if show == False: return axs. /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 110 show=show,; 111 save=save,; --> 112 ax=ax); 113 elif x is not None and y is not None:; 114 if ((x in adata.obs.keys() or x in adata.var.index). /usr/local/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _scatter_obs(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 371 c = adata.raw[:, key].X; 372 elif key in adata.var_names:; --> 373 c = adata[:, key].X if layers[2] == 'X' else adata[:, key].layers[layers[2]]; 374 c = c.toarray().flatten() if issparse(c) else c; 375 elif is_color_like(key): # a flat color. /usr/local/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 1292 def __getitem__(self, index):; 1293 """"""Returns a sliced view of the object.""""""; -> 1294 return self._getitem_view(index); 1295 ; 1296 def _getitem_view(self, index):. /usr/local/lib/python3.6/site-packages/anndata/base.py in _getitem_view(self, index); 1296 def _getitem_view(self, index):; 1297 oidx, vidx = self._normalize_indices(index); -> 1298 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1299 ; 1300 # this is used in the setter for uns, if a view. /usr/local/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 674 if not isinstance(X, AnnData):; 675 raise ValueError('`X` has to be an AnnData object.'); --> 676 self._init_as_view(X, oidx, vidx); 677 else:; 678 self._init_as_actua",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263:2657,layers,layers,2657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263,2,['layers'],['layers']
Modifiability,"lt colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(); ```; ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```; def style(; self,; cmap: str = DEFAULT_COLORMAP,; color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,; dot_max: Optional[float] = DEFAULT_DOT_MAX,; dot_min: Optional[float] = DEFAULT_DOT_MIN,; .....; ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://github.com/theislab/scanpydoc/blob/875b441212830678cf9fc81c52f5af29bbb8715f/scanpydoc/elegant_typehints/formatting.py#L101-L107",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633:1659,rewrite,rewrites,1659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633,1,['rewrite'],['rewrites']
Modifiability,"lues; 247 self.extract_function_arguments(); --> 248 entry_block_tail = self.lower_function_body(); 249 ; 250 # Close tail of entry block. ~\anaconda3\lib\site-packages\numba\lowering.py in lower_function_body(self); 271 bb = self.blkmap[offset]; 272 self.builder.position_at_end(bb); --> 273 self.lower_block(block); 274 ; 275 self.post_lower(). ~\anaconda3\lib\site-packages\numba\lowering.py in lower_block(self, block); 286 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block); 290 . ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 723 from numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtract.5, 1))]{130: <ir.Block at C:\Users\lyciansarpedon\anaco",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:12636,config,config,12636,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:2167,config,configure,2167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,1,['config'],['configure']
Modifiability,"make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Plotting with scanpy.pl gives attribute error. . ### Minimal code sample. ```python; sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); Cell In[47], line 1; ----> 1 sc.pl.violin(adata, [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""], jitter=0.4, multi_panel=True). File ~/anaconda3/lib/python3.11/site-packages/scanpy/plotting/_anndata.py:795, in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 790 if multi_panel and groupby is None and len(ys) == 1:; 791 # This is a quick and dirty way for adapting scales across several; 792 # keys if groupby is None.; 793 y = ys[0]; --> 795 g = sns.catplot(; 796 y=y,; 797 data=obs_tidy,; 798 kind=""violin"",; 799 scale=scale,; 800 col=x,; 801 col_order=keys,; 802 sharey=False,; 803 order=keys,; 804 cut=0,; 805 inner=None,; 806 **kwds,; 807 ); 809 if stripplot:; 810 grouped_df = obs_tidy.groupby(x). File ~/anaconda3/lib/python3.11/site-packages/seaborn/categorical.py:2932, in catplot(data, x, y, hue, row, col, kind, estimator, errorbar, n_boot, units, seed, order, hue_order, row_order, col_order, col_wrap, height, aspect, log_scale, native_scale, formatter, orient, color, palette, hue_norm, legend, legend_out, sharex, sharey, margin_titles, facet_kws, ci, **kwargs); 2929 linecolor = plot_kws.pop(""linecolor"", ""auto""); 2930 linecolor = p._complement_color(linecolor, color, p._hue_map); -> 2932 p.plot_violins(; 2933 width=width,; 2934 dodge=dod",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:1111,adapt,adapting,1111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['adapt'],['adapting']
Modifiability,making Cell ranger's normalized dispersion (for highly variable gene selection) not absolute,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/705:55,variab,variable,55,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/705,1,['variab'],['variable']
Modifiability,"manually edit such minor things that can be easily addressed in the plotting code 😄 . What we are discussing is maybe a minor thing here, but if we can minimize the dependency on Illustrator (which is a pricey proprietary software with a highly unintuitive interface in my very subjective opinion) to make publication-ready figures, I think it's a HUGE plus for the community. I think this is related to the philosophy of scanpy. To sum it up in a broader context, I think enabling people to have high-quality, publication-ready figures without mastering matplotlib and/or Illustrator must be one of the top items of the `scanpy constitution` :). I know many colleagues who already nicely memorized the entire scanpy API but asking them also to know bits and pieces of a beast like mpl might be too much IMO. Based on your final suggestion, I can imagine myself trying to remember ""Was it `var_ticklabels_kwargs={""fontstyle"": ""italic""}` or `var_ticklabels_kwargs={""font_style"": ""italic""}` or `var_ticklabels_kwds={""fontstyle"": ""italic""}` or `ticklabels_var_kwargs={""fontstyle"": ""italic""}` etc. I even spend 45 seconds everyday to remember this damn thing here `plt.rcParams[""figure.figsize""]` :). > I don't really like that `set_figure_params` modifies plots not generated by scanpy. I totally understand this from the coding and engineering perspective, very ugly and violates several principles of good design. But on the other hand, it makes the life of many practitioners easier by setting the plotting config early on in a ""Scanpy notebook session"", which is clearly created to do research on single-cell genomics with scanpy, without rerunning things several times. For example, I use plotnine sometimes for publications too, but I hate writing `theme(text=element_text(family='Arial'))` every time I make a figure. plotnine is a general-purpose plotting library so it's not their problem, but people use Scanpy for both data exploration AND publications. So I think we can do better than that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906:2206,config,config,2206,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1913#issuecomment-875885906,1,['config'],['config']
Modifiability,"map(knn_indices, knn_dists, n_obs, n_neighbors, set_op_mix_ratio, local_connectivity); 385 # umap 0.5.0; 386 warnings.filterwarnings(""ignore"", message=r""Tensorflow not installed""); --> 387 from umap.umap_ import fuzzy_simplicial_set; 388 ; 389 X = coo_matrix(([], ([], [])), shape=(n_obs, 1)). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/__init__.py in <module>; 1 from warnings import warn, catch_warnings, simplefilter; ----> 2 from .umap_ import UMAP; 3 ; 4 try:; 5 with catch_warnings():. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/umap/umap_.py in <module>; 45 ); 46 ; ---> 47 from pynndescent import NNDescent; 48 from pynndescent.distances import named_distances as pynn_named_distances; 49 from pynndescent.sparse import sparse_named_distances as pynn_sparse_named_distances. /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/pynndescent/__init__.py in <module>; 13 numba.config.THREADING_LAYER = ""workqueue""; 14 ; ---> 15 __version__ = pkg_resources.get_distribution(""pynndescent"").version. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in get_distribution(dist); 464 dist = Requirement.parse(dist); 465 if isinstance(dist, Requirement):; --> 466 dist = get_provider(dist); 467 if not isinstance(dist, Distribution):; 468 raise TypeError(""Expected string, Requirement, or Distribution"", dist). /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in get_provider(moduleOrReq); 340 """"""Return an IResourceProvider for the named module or requirement""""""; 341 if isinstance(moduleOrReq, Requirement):; --> 342 return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]; 343 try:; 344 module = sys.modules[moduleOrReq]. /opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py in require(self, *requirements); 884 included, even if they were already activated in this working set.; 885 """"""; --> 886 needed = self.resolve(parse_requ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169:2775,config,config,2775,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169,1,['config'],['config']
Modifiability,"me', 'patient_id'; var: 'feature_types'; uns: 'hvg', 'leiden', 'neighbors', 'pca', 'umap'; obsm: 'X_pca', 'X_pca_harmony', 'X_umap'; layers: 'raw'; ```. After this, when I tried running the command:; `sc.pl.highest_expr_genes(adata_orig, n_top=20, )`. I get the following output:; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-22-c4ab6dadfa42> in <module>; ----> 1 sc.pl.highest_expr_genes(adata_orig ). C:\ProgramData\Anaconda3\lib\site-packages\scanpy\plotting\_qc.py in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds); 65 ; 66 # compute the percentage of each gene per cell; ---> 67 norm_dict = normalize_total(adata, target_sum=100, inplace=False); 68 ; 69 # identify the genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no er",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:1947,layers,layers,1947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['layers'],['layers']
Modifiability,monypy 0.0.9; holoviews 1.18.3; hpack 4.0.0; httpcore 1.0.4; httpx 0.27.0; hyperframe 6.0.1; idna 3.6; igraph 0.11.4; imagecodecs 2024.1.1; imageio 2.34.0; importlib_metadata 7.0.2; importlib_resources 6.1.3; inflect 7.0.0; ipykernel 6.29.3; ipylab 1.0.0; ipython 8.22.2; ipywidgets 8.1.2; isoduration 20.11.0; jedi 0.19.1; Jinja2 3.1.3; joblib 1.3.2; json5 0.9.22; jsonpointer 2.4; jsonschema 4.21.1; jsonschema-specifications 2023.12.1; jupyter_client 8.6.0; jupyter_core 5.7.1; jupyter-events 0.9.0; jupyter-lsp 2.2.4; jupyter_server 2.13.0; jupyter_server_proxy 4.1.0; jupyter_server_terminals 0.5.2; jupyterlab 4.1.4; jupyterlab_pygments 0.3.0; jupyterlab_server 2.25.3; jupyterlab_widgets 3.0.10; kiwisolver 1.4.5; lamin_utils 0.13.0; lazy_loader 0.3; legacy-api-wrap 1.4; leidenalg 0.10.2; linkify-it-py 2.0.3; llvmlite 0.42.0; locket 1.0.0; louvain 0.8.1; lz4 4.3.3; mapclassify 2.6.1; Markdown 3.5.2; markdown-it-py 3.0.0; MarkupSafe 2.1.5; matplotlib 3.8.3; matplotlib-inline 0.1.6; mdit-py-plugins 0.4.0; mdurl 0.1.2; mistune 3.0.2; msgpack 1.0.7; mudata 0.2.3; multidict 6.0.5; multipledispatch 0.6.0; munkres 1.1.4; muon 0.1.5; natsort 8.4.0; nbclient 0.8.0; nbconvert 7.16.2; nbformat 5.9.2; nbproject 0.10.1; nest_asyncio 1.6.0; networkx 3.2.1; notebook 7.1.1; notebook_shim 0.2.4; numba 0.59.0; numcodecs 0.12.1; numpy 1.24.4; nvtx 0.2.10; omnipath 1.0.8; openpyxl 3.1.2; orjson 3.9.15; overrides 7.7.0; packaging 24.0; pandas 1.5.3; pandocfilters 1.5.0; panel 1.3.8; param 2.0.2; parso 0.8.3; partd 1.4.1; patsy 0.5.6; pexpect 4.9.0; pickleshare 0.7.5; pillow 10.2.0; pip 24.0; pkgutil_resolve_name 1.3.10; platformdirs 4.2.0; pooch 1.8.1; prometheus_client 0.20.0; prompt-toolkit 3.0.42; protobuf 4.25.3; psutil 5.9.8; ptxcompiler 0.8.1; ptyprocess 0.7.0; pure-eval 0.2.2; pyarrow 14.0.2; pyarrow-hotfix 0.6; pycparser 2.21; pyct 0.5.0; pydantic 1.10.14; pyee 8.1.0; Pygments 2.17.2; pylibcugraph 24.2.0; pylibraft 24.2.0; pynndescent 0.5.11; pynvml 11.4.1; pyparsing 3.1.2; pyppetee,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:4083,plugin,plugins,4083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['plugin'],['plugins']
Modifiability,"move `*` if you think there should be some positional ones, especially for pearson residuals). > the ""is median rank a good way to do HVG selection across batches""-issue (see this code comment). thanks for the explanation @jlause , I think is clear and it makes sense that it's the same as Seurat V3. > the question what the final names of the functions should be (see @ivirshup's last post). for `normalize_pearson_residual`, i think it makes sense to keep `normalize` in, as it's not the same type of transformation compared to `log1p`. For the HVG genes, I understand that same API but different function is not nice, but I also think is not nice if the function name change after functions get outside experimental module. For instance, as it is now, it would be `sc.experimental.pp.highly_variable_genes` -> `sc.pp.highly_variable_genes`. Otherwise, it would be `sc.experimental.pp.pearson_deviant_genes ` -> `sc.pp.highly_variable_genes` , which I don't think it is a smooth transition. ; If/when we eventually refactor `highly_variable_genes`, it wouldn't matter (there would be changes in function name anyway), but then again we'd have to consider backward compatibility as well. Furthermore, as it is now, it is true that it's the same `highly_variable_genes` API, but it belongs to the experimental module. Therefore, users would/should not assume the same functionality. In my opinion it's clearer this way as `sc.experimental.pp.highly_variable_genes` provides method in the experiemntal module that do HVG selection (and for now, it happens that only pearson residuals are available). > docs consistency (see @ivirshup's last post); > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. what do you have in mind @ivirshup ? happy to help out but don't think I know what you are referring to. . I will be on vacation until 14th of Sept, will have a look at remaining comments when I'm back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513:1271,refactor,refactor,1271,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-909055513,1,['refactor'],['refactor']
Modifiability,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4212,config,config,4212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['config'],['config']
Modifiability,"mple. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_recipes.py in; 4 from anndata import AnnData; 5; ----> 6 from . import _simple as pp; 7 from ._deprecated.highly_variable_genes import filter_genes_dispersion, filter_genes_cv_deprecated; 8 from ._normalization import normalize_total. ~/.conda/envs/scanpy/lib/python3.8/site-packages/scanpy/preprocessing/_simple.py in; 8 from typing import Union, Optional, Tuple, Collection, Sequence, Iterable; 9; ---> 10 import numba; 11 import numpy as np; 12 import scipy as sp. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/init.py in; 32; 33 # Re-export decorators; ---> 34 from numba.core.decorators import (cfunc, generated_jit, jit, njit, stencil,; 35 jit_module); 36. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/decorators.py in; 10; 11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning; ---> 12 from numba.stencils.stencil import stencil; 13 from numba.core import config, extending, sigutils, registry; 14. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/stencils/stencil.py in; 9 from llvmlite import ir as lir; 10; ---> 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry; 12 from numba.core.typing.templates import (CallableTemplate, signature,; 13 infer_global, AbstractTemplate). ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/registry.py in; 2; 3 from numba.core.descriptors import TargetDescriptor; ----> 4 from numba.core import utils, typing, dispatcher, cpu; 5; 6 # -----------------------------------------------------------------------------. ~/.conda/envs/scanpy/lib/python3.8/site-packages/numba/core/dispatcher.py in; 13; 14 from numba import _dispatcher; ---> 15 from numba.core import utils, types, errors, typing, serialize, config, compiler, sigutils; 16 from numba.core.compiler_lock import global_compiler_lock; 17 from numba.core.typeconv.rules import default_type_manager. ~/.conda/envs/scanpy/lib/python3.8/s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1797:2126,config,config,2126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1797,2,"['config', 'extend']","['config', 'extending']"
Modifiability,"mport scanpy as sc; pbmc = sc.datasets.pbmc3k(); log_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch""); ```. ```pytb; >>> import scanpy as sc; g_anndata = sc.pp.log1p(pbmc, copy=True); pbmc.layers['log_transformed'] = log_anndata.X.copy(). # This errors, because X is not normalized and flavor=""seurat"" requires normalizes data.; # ValueError: cannot specify integer `bins` when input data contains infinity; sc.pp.highly_variable_genes(pbmc, flavor=""seurat"", subset=False, inplace=False). # This works, we pass log tranformed data; pbmc.uns['log1p'] = log_anndata.uns['log1p']; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False). # This raises ValueError again; pbmc.obs['batch'] = 'A'; column_index = pbmc.obs.columns.get_indexer(['batch']); pbmc.obs.iloc[slice(pbmc.n_obs//2, None), column_index] = 'B'; sc.pp.highly_variable_genes(pbmc, layer=""log_transformed"", flavor=""seurat"", subset=False, inplace=False, batch_key=""batch"")>>> pbmc = sc.datasets.pbmc3k(). >>> log_anndata = sc.pp.log1p(pbmc, copy=True); >>> pbmc.layers['log_transformed'] = log_anndata.X.copy(); >>> ; >>> # This errors, because X is not normalized and flavor=""seurat"" requires norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2396:1480,layers,layers,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2396,1,['layers'],['layers']
Modifiability,"mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variable]; 2 adata. ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in __getitem__(self, index); 1114 def __getitem__(self, index: Index) -> ""AnnData"":; 1115 """"""Returns a sliced view of the object.""""""; -> 1116 oidx, vidx = self._normalize_indices(index); 1117 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1118 . ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\anndata.py in _normalize_indices(self, index); 1095 ; 1096 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:; -> 1097 return _normalize_indices(index, self.obs_names, self.var_names); 1098 ; 1099 # TODO: this is not quite complete... ~\anaconda3\envs\Python3812\lib\site-packages\anndata\_core\index.py in _normalize_indices(index, names0, names1); 34 ax0, ax1 = unpack_index(ind",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:2035,variab,variable,2035,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['variab'],['variable']
Modifiability,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587:1330,layers,layers,1330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587,1,['layers'],['layers']
Modifiability,"n the package for the given example below. The experiment was run on AWS r7i.24xlarge.; ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3279:1355,variab,variable,1355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279,1,['variab'],['variable']
Modifiability,"n(); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main; ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:3933,config,config,3933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"n(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger in loggers:; handlers = getattr(logger, 'handlers', []); for handler in handlers:; logger.removeHandler(handler); ```. Next time we come across a reproducible case of this happening, w",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:4092,config,config,4092,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"n3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2448,layers,layers,2448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['layers'],['layers']
Modifiability,"n312\site-packages\pandas\io\common.py:765, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options); [761](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:761) if compression == ""gzip"":; [762](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:762) if isinstance(handle, str):; [763](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:763) # error: Incompatible types in assignment (expression has type; [764](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:764) # ""GzipFile"", variable has type ""Union[str, BaseBuffer]""); --> [765](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:765) handle = gzip.GzipFile( # type: ignore[assignment]; [766](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:766) filename=handle,; [767](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:767) mode=ioargs.mode,; [768](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:768) **compression_args,; [769](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/pandas/io/common.py:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:16735,variab,variable,16735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['variab'],['variable']
Modifiability,"nData objects. > In the very early days of Scanpy, I thought it'd be nice to also accept other formats of data matrices. I still think it would be nice to support that, it just requires factoring the code better. I agree recursively calling the same function for argument handling gets very confusing. However, I think we could do something more like this (note, it's not tested yet, and could be cleaner... it's my ten minute version):. <details>; <summary> Alternative implementation of scale </summary>. ```python; @singledispatch; def scale(X, *args, **kwargs):; """"""\; Scale data to unit variance and zero mean.; .. note::; Variables (genes) that do not display any variation (are constant across; all observations) are retained and set to 0 during this operation. In; the future, they might be set to NaNs.; Parameters; ----------; data; The (annotated) data matrix of shape `n_obs` × `n_vars`.; Rows correspond to cells and columns to genes.; zero_center; If `False`, omit zero-centering variables, which allows to handle sparse; input efficiently.; max_value; Clip (truncate) to this value after scaling. If `None`, do not clip.; copy; If an :class:`~anndata.AnnData` is passed,; determines whether a copy is returned.; Returns; -------; Depending on `copy` returns or updates `adata` with a scaled `adata.X`,; annotated with `'mean'` and `'std'` in `adata.var`.; """"""; return scale_array(X, *args, **kwargs). @scale.register(np.ndarray); def scale_array(; X,; zero_center: bool = True,; max_value: Optional[float] = None,; copy: bool = False,; return_mean_var=False,; ):; if copy:; X = X.copy(); if not zero_center and max_value is not None:; logg.info( # Be careful of what? This should be more specific; '... be careful when using `max_value` '; 'without `zero_center`.'; ); if max_value is not None:; logg.debug(f'... clipping at max_value {max_value}'); mean, std = _scale(X, zero_center) # the code from here could probably just be ; # do the clipping; if max_value is not None:; X[X > max",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735:1317,variab,variables,1317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1135#issuecomment-608200735,1,['variab'],['variables']
Modifiability,"nDataReadError Traceback (most recent call last); <ipython-input-20-38a594ec7d06> in <module>; ----> 1 adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'). /opt/conda/lib/python3.7/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 424 d[k] = read_dataframe(f[k]); 425 else: # Base case; --> 426 d[k] = read_attribute(f[k]); 427 ; 428 d[""raw""] = _read_raw(f, as_sparse, rdasp). /opt/conda/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:1890,layers,layers,1890,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['layers'],['layers']
Modifiability,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1175:2015,variab,variable,2015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1175,1,['variab'],['variable']
Modifiability,"ng: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).; ""(https://pypi.org/project/six/)."", FutureWarning); scanpy==1.4.6 anndata==0.7.1 umap==0.4.1 numpy==1.18.1 scipy==1.4.1 pandas==1.0.3 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-igraph==0.8.0; Standardizing Data across genes. Found 11 batches. Found 0 numerical variables:; 	. Fitting L/S model and finding priors. Finding parametric adjustments. /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_combat.py:338: RuntimeWarning: divide by zero encountered in true_divide; change = max((abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()); Adjusting data. In [2]: np.sum(~np.isnan(adata_Combat.X)); Out[2]: 0. In [3]: np.sum(np.isnan(adata_Combat.X)); Out[3]: 7644442. In [4]: sc.pp.highly_variable_genes(adata_Combat); extracting highly variable genes; Traceback (most recent call last):. File ""<ipython-input-4-a706aaf6f1f8>"", line 1, in <module>; sc.pp.highly_variable_genes(adata_Combat). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 235, in highly_variable_genes; flavor=flavor,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py"", line 65, in _highly_variable_genes_single_batch; df['mean_bin'] = pd.cut(df['means'], bins=n_bins). File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 265, in cut; duplicates=duplicates,. File ""/home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/pandas/core/reshape/tile.py"", line 381, in _bins_to_cuts; f""Bin edges must be unique: {repr(bins)}.\n"". ValueError: Bin edges must be unique: array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,; nan, nan, nan, nan, nan, nan, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922:1756,variab,variable,1756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1172#issuecomment-616468922,1,['variab'],['variable']
Modifiability,"niconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 176 try:; --> 177 return func(elem, *args, **kwargs); 178 except Exception as e:. ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 526 if encoding_type:; --> 527 EncodingVersions[encoding_type].check(; 528 group.name, group.attrs[""encoding-version""]. ~/miniconda3/lib/python3.8/enum.py in __getitem__(cls, name); 343 def __getitem__(cls, name):; --> 344 return cls._member_map_[name]; 345 . KeyError: 'dict'. During handling of the above exception, another exception occurred:. AnnDataReadError Traceback (most recent call last); <ipython-input-17-97568eff5295> in <module>; ----> 1 adata=anndata.read_h5ad('./visium_merge_inter_upload.h5ad'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 419 d[k] = read_dataframe(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). ~/miniconda3/lib/python3.8/functools.py in wrapper(*args, **kw); 872 '1 positional argument'); 873 ; --> 874 return dispatch(args[0].__class__)(*args, **kw); 875 ; 876 funcname = getattr(func, '__name__', 'singledispatch function'). ~/miniconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. ### Minimal code sample (that we can copy&paste without having any data). ```python; # Your code here; ```. ```pytb; [Paste the error output produced by the above code here]; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2376:1828,layers,layers,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2376,1,['layers'],['layers']
Modifiability,no highly variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/935:10,variab,variable,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/935,1,['variab'],['variable']
Modifiability,"no, not great, but it work's and one should now be much better settled for the future with AnnData. for example, the gene plots and different subgroups work. if you have a good suggestion for a default color map for continuous and categorial columns in smp, I'm very happy to adapt it. :). https://github.com/falexwolf/collab_alex/blob/master/scanpy/examples/maehr17.md. or here directly in the main readme. https://github.com/theislab/scanpy#moignard15",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236:276,adapt,adapt,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2#issuecomment-278282236,1,['adapt'],['adapt']
Modifiability,"normalize total operates inplace. in the code example above `adata.layers[""counts""]` and `adata.X` are referencing the same object. You should add `.copy()` if you want independent behavior",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389#issuecomment-1371002904:67,layers,layers,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389#issuecomment-1371002904,1,['layers'],['layers']
Modifiability,normalize_total affects layers,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:24,layers,layers,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"npy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. aggregate throws error when aggregating `obsm` or `varm`. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc68k_reduced(); sc.get.aggregate(adata, by=""louvain"", func=""mean"", obsm=""X_umap""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1208,layers,layers,1208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"ns, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(); 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(); 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(); 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .core.anndata import AnnData, Raw; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(); 46 LayersBase, Layers; 47 ); ---> 48 from .. import h5py; 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView; 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 4 from typing import Optional, Union, KeysView, NamedTuple; 5 ; ----> 6 import h5py; 7 import numpy as np; 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900:1332,Layers,LayersBase,1332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900,2,['Layers'],"['Layers', 'LayersBase']"
Modifiability,"nted yet."". NotImplementedError: Failed to write value for uns/umap/params/random_state, since a writer for type <class 'numpy.random.mtrand.RandomState'> has not been implemented yet. The above exception was the direct cause of the following exception:. NotImplementedError Traceback (most recent call last); <ipython-input-2-1dd6b1c7e996> in <module>; 4 pbmc = sc.datasets.pbmc68k_reduced(); 5 sc.tl.umap(pbmc, random_state=np.random.RandomState(10)); ----> 6 pbmc.write(""tmp.h5ad""). ~/github/anndata/anndata/_core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense, as_dense); 1988 compression_opts=compression_opts,; 1989 force_dense=force_dense,; -> 1990 as_dense=as_dense,; 1991 ); 1992 . ~/github/anndata/anndata/_io/h5ad.py in write_h5ad(filepath, adata, force_dense, as_dense, dataset_kwargs, **kwargs); 110 write_attribute(f, ""varp"", adata.varp, dataset_kwargs=dataset_kwargs); 111 write_attribute(f, ""layers"", adata.layers, dataset_kwargs=dataset_kwargs); --> 112 write_attribute(f, ""uns"", adata.uns, dataset_kwargs=dataset_kwargs); 113 if adata.isbacked:; 114 adata.file.open(filepath, ""r+""). /usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/functools.py in wrapper(*args, **kw); 838 '1 positional argument'); 839 ; --> 840 return dispatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). ~/github/anndata/anndata/_io/h5ad.py in write_attribute_h5ad(f, key, value, *args, **kwargs); 124 if key in f:; 125 del f[key]; --> 126 _write_method(type(value))(f, key, value, *args, **kwargs); 127 ; 128 . ~/github/anndata/anndata/_io/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 284 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 285 for sub_key, sub_value in value.items():; --> 286 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs=dataset_kwargs); 287 ; 288 . /usr/local/Cellar/python/3.7.6_1/Frameworks/Python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1131:2126,layers,layers,2126,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1131,1,['layers'],['layers']
Modifiability,"o a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. kwargs = {""multiplicative_replacement"":""auto""}. sc.pp.normalize(adata, function=clr, **kwargs) # -> addata object with the transformation as one of the layers; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:2621,layers,layers,2621,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['layers'],['layers']
Modifiability,"obs):; 135 ing.map_labels(col, labeling_method[i]). /usr/local/lib/python3.7/dist-packages/scanpy/tools/_ingest.py in neighbors(self, k, queue_size, epsilon, random_state); 469 self._nnd_idx.search_rng_state = rng_state; 470 ; --> 471 self._indices, self._distances = self._nnd_idx.query(test, k, epsilon); 472 ; 473 else:. /usr/local/lib/python3.7/dist-packages/pynndescent/pynndescent_.py in query(self, query_data, k, epsilon); 1564 """"""; 1565 if not hasattr(self, ""_search_graph""):; -> 1566 self._init_search_graph(); 1567 ; 1568 if not self._is_sparse:. /usr/local/lib/python3.7/dist-packages/pynndescent/pynndescent_.py in _init_search_graph(self); 1061 self._distance_func,; 1062 self.rng_state,; -> 1063 self.diversify_prob,; 1064 ); 1065 reverse_graph.eliminate_zeros(). /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 432 e.patch_message('\n'.join((str(e).rstrip(), help_msg))); 433 # ignore the FULL_TRACEBACKS config, this needs reporting!; --> 434 raise e; 435 ; 436 def inspect_llvm(self, signature=None):. /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in _compile_for_args(self, *args, **kws); 365 argtypes.append(self.typeof_pyval(a)); 366 try:; --> 367 return self.compile(tuple(argtypes)); 368 except errors.ForceLiteralArg as e:; 369 # Received request for compiler re-entry with the list of arguments. /usr/local/lib/python3.7/dist-packages/numba/core/compiler_lock.py in _acquire_compile_lock(*args, **kwargs); 30 def _acquire_compile_lock(*args, **kwargs):; 31 with self:; ---> 32 return func(*args, **kwargs); 33 return _acquire_compile_lock; 34 . /usr/local/lib/python3.7/dist-packages/numba/core/dispatcher.py in compile(self, sig); 823 raise e.bind_fold_arguments(folded); 824 self.add_overload(cres); --> 825 self._cache.save_overload(sig, cres); 826 return cres.entry_point; 827 . /usr/local/lib/python3.7/dist-packages/numba/core/caching.py in save_overload(self, sig, data); 669 """"""; 670 with se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1951:2849,config,config,2849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1951,1,['config'],['config']
Modifiability,"obs[""condition""] = np.tile([""c1"", ""c2""], int(pbmc.n_obs / 2)). ## plot one gene, one column grouping variable; sc.pl.dotplot(pbmc, var_names='C1QA', groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171329-f5fafb2b-0695-41d9-b313-eac9ea218836.png); ```; ## plot two genes, one column grouping variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups='sampleid'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171410-45f77f03-3487-4b7f-86da-658284608b05.png); ```; ## plot two genes, tow column group variable; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171470-58df0907-a15b-4b7f-afa3-3578728177e0.png); ```; ## or we could use the same varaibles as y axis; sc.pl.dotplot(pbmc, var_names=['C1QA', 'CD19'], groupby=['sampleid', 'condition'], col_groups='louvain'); ```; ![image](https://user-images.githubusercontent.com/10910559/147171544-849a93f4-99cd-493e-9f2b-f5662f03e797.png). For the heatmap, I think you were referring to `sc.pl.matrixplot`. `sc.pl.heatmap` is a different function which plot a cell as a row and a gene as a column. `col_groups` was also added to `sc.pl.matrixplot`:; ```; ## plot two genes, tow column group variable; sc.pl.matrixplot(pbmc, var_names=['C1QA', 'CD19'], groupby='louvain', col_groups=['sampleid', 'condition']); ```; ![image](https://user-images.githubusercontent.com/10910559/147171604-183f7210-276c-4fdb-b173-477e00e636c0.png); For the `row_groups` you proposed in your hypothetical `sc.pl.heatmap` implementation, it is equivalent to the current `groupby` argument in `sc.pl.dotplot`/`sc.pl.matrixplot`. I think it might be good to keep it as is for now- for this kind of changes it might be good to do a coordinated update on all plotting functions because I see quite a few functions use the `groupby` argument.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049:1653,variab,variable,1653,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1876#issuecomment-999969049,1,['variab'],['variable']
Modifiability,"ocals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions(); commit: ef5a8ee57c2aef7778a069a49101a8998718e6d5. python; 3.8.5. pip; 20.0.2 . ubuntu; 20.04. pip list; Package Version Location ; ------------------------- --------------",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2328,config,config,2328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['config'],['config']
Modifiability,"odule>; ----> 1 sc.pl.umap(adata, gene_symbols = 'Symbol', color = ['Tnnt2']). /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 275 color_vector, categorical = _get_color_values(adata, value_to_plot,; 276 groups=groups, palette=palette,; --> 277 use_raw=use_raw); 278 ; 279 # check if higher value points should be plot on top. /anaconda3/envs/scanpy/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in _get_color_values(adata, value_to_plot, groups, palette, use_raw); 665 raise ValueError(""The passed `color` {} is not a valid observation annotation ""; 666 ""or variable name. Valid observation annotation keys are: {}""; --> 667 .format(value_to_plot, adata.obs.columns)); 668 ; 669 return color_vector, categorical. ValueError: The passed `color` Tnnt2 is not a valid observation annotation or variable name. Valid observation annotation keys are: Index(['Sample', 'n_counts', 'n_genes', 'percent_mito', 'log_counts',; 'louvain'],; dtype='object'). ```. adata.var contains the column ""Symbol"" and ""Tnnt2"" is present:. `adata.var[adata.var['Symbol'] == 'Tnnt2']`. . Symbol | type | highly_variable | means | dispersions | dispersions_norm; -- | -- | -- | -- | -- | --; Tnnt2 | protein_coding | True | 0.923869 | 4.090601 | 11.370244. run with:; `scanpy==1.3.7 anndata==0.6.17 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/455:1728,variab,variable,1728,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/455,2,['variab'],['variable']
Modifiability,"of scanpy. ### What happened?. Hi,. When I try to run the code in https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html# I get an error. ### Minimal code sample. ```python; import numpy as np; import scanpy as sc; import seaborn as sns; from scipy.stats import median_abs_deviation. sc.settings.verbosity = 0; sc.settings.set_figure_params(; dpi=80,; facecolor=""white"",; frameon=False,; ). adata = sc.read_10x_h5(; filename=""filtered_feature_bc_matrix.h5"",; backup_url=""https://figshare.com/ndownloader/files/39546196"",; ). import anndata2ri; import logging. import rpy2.rinterface_lib.callbacks as rcb; import rpy2.robjects as ro. rcb.logger.setLevel(logging.ERROR); ro.pandas2ri.activate(); anndata2ri.activate(). %load_ext rpy2.ipython. %%R; library(SoupX). adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp); sc.pp.log1p(adata_pp). sc.pp.pca(adata_pp); sc.pp.neighbors(adata_pp); sc.tl.leiden(adata_pp, key_added=""soupx_groups""). # Preprocess variables for SoupX; soupx_groups = adata_pp.obs[""soupx_groups""]. cells = adata.obs_names; genes = adata.var_names; data = adata.X.T. adata_raw = sc.read_10x_h5(; filename=""raw_feature_bc_matrix.h5"",; backup_url=""https://figshare.com/ndownloader/files/39546217"",; ); adata_raw.var_names_make_unique(); data_tod = adata_raw.X.T. %%R -i data -i data_tod -i genes -i cells -i soupx_groups -o out . # specify row and column names of data; rownames(data) = genes; colnames(data) = cells; # ensure correct sparse format for table of counts and table of droplets; data <- as(data, ""sparseMatrix""); data_tod <- as(data_tod, ""sparseMatrix""). # Generate SoupChannel Object for SoupX ; sc = SoupChannel(data_tod, data, calcSoupProfile = FALSE). # Add extra meta data to the SoupChannel object; soupProf = data.frame(row.names = rownames(data), est = rowSums(data)/sum(data), counts = rowSums(data)); sc = setSoupProfile(sc, soupProf); # Set cluster information in SoupChannel; sc = setClusters(sc, soupx_groups). # Estimate con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2685:1245,variab,variables,1245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2685,1,['variab'],['variables']
Modifiability,"ok, do you mind updating both scanpy and anndata and try again? There has been a major refactoring of plotting functions in 1.6.0 and it could address your issue",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034:87,refactor,refactoring,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704312034,1,['refactor'],['refactoring']
Modifiability,"olcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; return self._hookexec(self, self.get_hookimpls(), kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec; return self._inner_hookexec(hook, methods, kwargs); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>; self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall; res = hook_impl.function(*args); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main; return wrap_session(config, _main); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session; session.exitstatus = doit(config, session) or 0; File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main; config.hook.pytest_runtestloop(session=session); File ""/opt/hostedtoolcache/Python/3.8.8/x64/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__; ```. </details>. This seems to be a known problem caused by setting up logging at import: https://github.com/pytest-dev/pytest/issues/5502. Why touching logging at import at all is considered a bug in the setup, I am not sure. Adding this as a required teardown may fix the problem (https://github.com/pytest-dev/pytest/issues/5502#issuecomment-647157873):. ```python; def clear_loggers():; """"""Remove handlers from all loggers""""""; import logging; loggers = [logging.getLogger()] + list(logging.Logger.manager.loggerDict.values()); for logger in loggers:; handlers = getattr(logger, 'handlers', []); for handler in handlers:; logger.removeHandler(handler); ```. Next time we come across a reproducible case of this happening, we should try this out.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1736:4225,config,config,4225,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1736,1,['config'],['config']
Modifiability,"olin; 306 return stacked_violin(adata, gene_names, groupby, var_group_labels=group_names,; --> 307 var_group_positions=group_positions, show=show, save=save, **kwds); 308 ; 309 elif plot_type == 'tracksplot':. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in stacked_violin(adata, var_names, groupby, log, use_raw, num_categories, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, stripplot, jitter, size, scale, order, swap_axes, show, save, row_palette, **kwds); 819 if isinstance(var_names, str):; 820 var_names = [var_names]; --> 821 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 822 ; 823 if 'color' in kwds:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in __getitem__(self, index); 510 ; 511 def __getitem__(self, index):; --> 512 oidx, vidx = self._normalize_indices(index); 513 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 514 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_indices(self, packed_index); 538 obs, var = super(Raw, self)._unpack_index(packed_index); 539 obs = _normalize_index(obs, self._adata.obs_names); --> 540 var = _normalize_index(var, self.var_names); 541 return obs, var; 542 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/anndata/base.py in _normalize_index(index, names); 270 raise KeyError(; 271 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 272 .format(index)); 273 return positions.values; 274 else:; ```. Cheers,; Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222:2892,variab,variables,2892,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/438#issuecomment-456707222,1,['variab'],['variables']
Modifiability,"om_object = loompy.connect('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. However i would like to open it with scanpy by:. ```py; loom_file = sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom', validate=False); ```. and i get the following error:. ```pytb; ---------------------------------------------------------------------------; Exception Traceback (most recent call last); <ipython-input-26-3a0e0ee3248f> in <module>(); ----> 1 loom_file=sc.read_loom('Aerts_Fly_AdultBrain_Filtered_57k.loom',validate=False). ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\readwrite\read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names, dtype, **kwargs); 184 var=var,; 185 layers=layers,; --> 186 dtype=dtype); 187 return adata; 188 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 670 layers=layers,; 671 dtype=dtype, shape=shape,; --> 672 filename=filename, filemode=filemode); 673 ; 674 def _init_as_view(self, adata_ref: 'AnnData', oidx: Index, vidx: Index):. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, layers, dtype, shape, filename, filemode); 848 # annotations; 849 self._obs = _gen_dataframe(obs, self._n_obs,; --> 850 ['obs_names', 'row_names', 'smp_names']); 851 self._var = _gen_dataframe(var, self._n_vars, ['var_names', 'col_names']); 852 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _gen_dataframe(anno, length, index_names); 285 _anno = pd.DataFrame(; 286 anno, index=anno[index_name],; --> 287 columns=[k for k in anno.keys() if k != index_name]); 288 break; 289 else:. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy); 390 dtype=dtype, copy=copy); 391 elif isinstance(data, dict):; --> 392 mgr = init",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/924:1101,layers,layers,1101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/924,3,['layers'],['layers']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. ts=time.time(); sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)); ```; <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:2463,variab,variable,2463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call . https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/prepro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:2488,variab,variable,2488,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call; https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preproc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3280:2385,variab,variable,2385,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280,1,['variab'],['variable']
Modifiability,"omponents to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata, max_value=10); print(""Total scale time : %s"" % (time.time()-ts)); t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(); sc._settings.ScanpyConfig.n_jobs = os.cpu_count();",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:2517,variab,variable,2517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,['variab'],['variable']
Modifiability,"on3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__(). TypeError: a bytes-like object is required, not 'list'; ```. Conda list output. ```; # packages in environment at /wynton/home/state/alkhairohr/miniconda3/envs/python_env:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; anndata 0.7.3 pypi_0 pypi; attrs 19.3.0 py_0 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; blas 1.0 mkl ; bleach 3.1.5 pyh9f0ad1d_0 conda-forge; blosc 1.19.0 hd408876_0 ; bzip2 1.0.8 h7b6447c_0 ; ca-certificates 2020.1.1 0 ; certifi 2020.6.20 py36_0 ; colorlover 0.3.0 py_0 conda-forge; cycler 0.10.0 py36_0 ; dbus 1.13.6 he372182_0 conda-forge; decorator 4.4.2 py_0 conda-forge; defusedxml 0.6.0 py_0 conda-forge; entrypoints 0.3 py36h9f0ad1d_1001 conda-forge; expat 2.2.9 he1b5a44_2 conda-forge; fontconfig 2.13.1 he4413a7_1000 conda-forge; freetype 2.10.2 he06d7ca_0 conda-forge; get-version 2.1 pypi_0 pypi; glib 2.63.1 h3eb4bd4_1 ; gst-plugins-base 1.14.0 hbbd80ab_1 ; gstreamer 1.14.0 hb31296c_0 ; h5py 2.10.0 pypi_0 pypi; hdf5 1.10.4 hb1b8bf9_0 ; icu 58.2 hf484d3e_1000 conda-forge; importlib-metadata 1.6.1 py36h9f0ad1d_0 conda-forge; importlib_metadata 1.6.1 0 conda-forge; intel-openmp 2020.1 217 ; ipyevents 0.7.1 py_0 conda-forge; ipykernel 5.3.0 py36h95af2a2_0 conda-forge; ipython 7.15.0 py36h9f0ad1d_0 conda-forge; ipython_genutils 0.2.0 py_1 conda-forge; ipywidgets 7.5.1 py_0 conda-forge; jedi 0.17.1 py36h9f0ad1d_0 conda-forge; jinja2 2.11.2 pyh9f0ad1d_0 conda-forge; joblib 0.15.1 py_0 ; jpeg 9d h516909a_0 conda-forge; jsonschema 3.2.0 py36h9f0ad1d_1 conda-forge; jupyter 1.0.0 py_2 conda-forge; jupyter_client 6.1.3 py_0 conda-forge; jupyter_console 6.1.0 py_1 conda-forge; jupyter_core 4.6.3 py36h9f0ad1d_1 conda-forge; kiwisolver 1.2.0 py36hfd86e86_0 ; ld_impl_linux-64 2.33.1 h53a641e_7 ; legacy-api-wrap 1.2 pypi_0 pypi; leidenalg 0.8.0 pypi_0 pypi; libedit 3.1.20191231 h7b6447c_0 ; libffi 3.3 he6710b0_1 ; libgcc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1293:6203,plugin,plugins-base,6203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1293,1,['plugin'],['plugins-base']
Modifiability,"or highly expressed genes is probably more meaningful difference than for a lowly expressed one.; - m=0 s=1 z-standardisation can be problematic if you have a small vs large population that has high expression (relatively to the size of whole data) - in former case you get much higher scores than in the latter as whole data seems higher; - If you have many cells doing [0,1] across cells rather than groups and then averaging may be good option as well (see below). Potential problem if you have outlier cells (less likely if log-norm before), but in this case you could normalise from 1st to 99th percentile - still may be problem for rare populations but at least you can regulate the threshold (so better than z-standardisation maybe).; - [0,1] on groups is not too bad when you expect large variation anyway by definition (e.g. plotting top data-defined markers), but agreed too often misleading so should not be default. Example: See gene Trp53bp1 under old - not much difference across groups:; - [0,1] on groups - seems very variable; ![image](https://user-images.githubusercontent.com/47607471/160437189-dd2c3deb-786e-4317-a4cf-fd31fdbd7f19.png); - no normalisation (currently only other option) - bad for multiple marker comparison; ![image](https://user-images.githubusercontent.com/47607471/160437292-daf03941-1e9a-44ed-8942-f2a180ec2c85.png); - max_abs scale on groups - probably still exaggerates variability; ![image](https://user-images.githubusercontent.com/47607471/160456753-c211d7da-1f72-46f3-9355-87eebc649472.png); - [0,1] on cells (before averaging) - the only one that does not exaggerate between group variability; similar with max_abs scaling on cells - probably as some cells are 0 - but much better in terms of time for large sparse matrices. however, this type of normalisation makes differences way less distinct - problem as weak markers look indistinguishable across groups.; ![image](https://user-images.githubusercontent.com/47607471/160437395-4604c2ef-2cf2-46fb-9c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638:1218,variab,variable,1218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1757#issuecomment-1080962638,1,['variab'],['variable']
Modifiability,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2115,layers,layers,2115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,2,['layers'],['layers']
Modifiability,"ose docs are out of date (current version is `v1.10.1`). There's an up to date PDF on their bioconductor page, but I don't think I can link to the function from there. How about this: <details>; <summary>Updated docstring</summary>. ```python; def calculate_qc_metrics(adata, expr_type=""counts"", var_type=""genes"", qc_vars=(),; percent_top=(50, 100, 200, 500), inplace=False):; """"""; Calculate quality control metrics. Calculates a number of qc metrics for an AnnData object, see section ; Returns for specifics. Largely based on `calculateQCMetrics` from scater; [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix. Parameters; ----------; adata : :class:`~anndata.AnnData`; Annotated data matrix.; expr_type : `str`, optional (default: `""counts""`); Name of kind of values in X.; var_type : `str`, optional (default: `""genes""`); The kind of thing the variables are.; qc_vars : `Container`, optional (default: `()`); Keys for boolean columns of `.var` which identify variables you could ; want to control for (e.g. ""ERCC"" or ""mito"").; percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`); Which proportions of top genes to cover. If empty or `None` don't; calculate.; inplace : bool, optional (default: `False`); Whether to place calculated metrics in `.obs` and `.var`. Returns; -------; Union[NoneType, Tuple[pd.DataFrame, pd.DataFrame]]; Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or; updates `adata`'s `obs` and `var`. Observation level metrics include:. * `total_{var_type}_by_{expr_type}`; E.g. ""total_genes_by_counts"". Number of genes with positive counts ; in a cell.; * `total_{expr_type}`; E.g. ""total_counts"". Total number of counts for a cell.; * `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`; E.g. ""pct_counts_in_top_50_genes"". Cumulative percentage of counts ; for 50 most expressed genes in a cell.; * `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`; E.g. ""total_counts_mito"". Total number ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688:1019,variab,variables,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/424#issuecomment-454024688,1,['variab'],['variables']
Modifiability,"ot args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwds). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:199, in AlignedActualMixin.__setitem__(self, key, value); 198 def __setitem__(self, key: str, value: V):; --> 199 value = self._validate_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1887,layers,layers,1887,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,2,['layers'],['layers']
Modifiability,"ould I use there? (As an aside, I don't understand why using UMAP connectivites is the default for clustering at all. From what I can tell, the standard way of weighing the KNNG for graph-based clustering in single-cell is to use the Jaccard index of the mutual nearest neighbors to weigh the edges). From an implementation standpoint, the `sc.pp.tsne_negihbors` will inevitably have to call the UMAP KNNG construction, since I can see that it's not split out in the code-base. `sc.pp.neighbors` calls the UMAP implementation directly, and since the goal is to use the same KNNG construction procedure, t-SNE will have to call the same UMAP function, and override the weights afterward. Much like `gauss` does now. It would probably make more sense to split out the KNNG construction from the UMAP weight calculation, but that seems like a lot of work. Or maybe not. In the latest UMAP release from a few days ago, they split out the graph construction into `pynndescent`. Either way, refactoring this doesn't belong in this PR. Alternatively, we could construct our own KNNG in `sc.pp.tsne_neighbors` using Annoy, which openTSNE does by default. But that seems suboptimal, because the design philosophy seems to be re-use the same graph for everything. . What I think would make more sense is to remove the connectivity calculation from the `sc.pp.neighbors` altogether, and have that calculate an unweighted KNNG. Since different methods need their own connectivities anyways, that should be done in each method separately. So UMAP connectivities would be calculated in `sc.tl.umap`, and the Louvain Jaccard connectivities in `sc.tl.louvain`. Then, you wouldn't be assigning any preference to any one connectivity method. From what I can tell, there's no evidence the UMAP connectivities are better than the others in any way, especially not for clustering. If you have any information on this, I'd be really curious to know. Ultimately, the decision is up to you guys, since this is more of a desi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009:2888,refactor,refactoring,2888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-759374009,1,['refactor'],['refactoring']
Modifiability,"ower_normal_function(self, fndesc); 246 # Init argument values; 247 self.extract_function_arguments(); --> 248 entry_block_tail = self.lower_function_body(); 249 ; 250 # Close tail of entry block. ~\anaconda3\lib\site-packages\numba\lowering.py in lower_function_body(self); 271 bb = self.blkmap[offset]; 272 self.builder.position_at_end(bb); --> 273 self.lower_block(block); 274 ; 275 self.post_lower(). ~\anaconda3\lib\site-packages\numba\lowering.py in lower_block(self, block); 286 with new_error_context('lowering ""{inst}"" at {loc}', inst=inst,; 287 loc=self.loc, errcls_=defaulterrcls):; --> 288 self.lower_inst(inst); 289 self.post_block(block); 290 . ~\anaconda3\lib\contextlib.py in __exit__(self, type, value, traceback); 128 value = type(); 129 try:; --> 130 self.gen.throw(type, value, traceback); 131 except StopIteration as exc:; 132 # Suppress StopIteration *unless* it's the same exception that. ~\anaconda3\lib\site-packages\numba\errors.py in new_error_context(fmt_, *args, **kwargs); 723 from numba import config; 724 tb = sys.exc_info()[2] if config.FULL_TRACEBACKS else None; --> 725 six.reraise(type(newerr), newerr, tb); 726 ; 727 . ~\anaconda3\lib\site-packages\numba\six.py in reraise(tp, value, tb); 667 if value.__traceback__ is not tb:; 668 raise value.with_traceback(tb); --> 669 raise value; 670 ; 671 else:. LoweringError: Failed in nopython mode pipeline (step: nopython mode backend); Failed in nopython mode pipeline (step: nopython mode backend); LLVM IR parsing error; <string>:4053:36: error: '%.2725' defined with type 'i64' but expected 'i32'; %"".2726"" = icmp eq i32 %"".2724"", %"".2725""; ^. File ""..\..\anaconda3\lib\site-packages\scanpy\preprocessing\_qc.py"", line 399:; def top_segment_proportions_sparse_csr(data, indptr, ns):; <source elided>; partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype); for i in numba.prange(indptr.size - 1):; ^. [1] During: lowering ""id=13[LoopNest(index_variable = parfor_index.260, range = (0, $122binary_subtrac",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1147:12598,config,config,12598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1147,1,['config'],['config']
Modifiability,"owngrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, da",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:1186,layers,layers,1186,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,1,['layers'],['layers']
Modifiability,"ows where to go to read certain formats.... scanpy? muon? squidpy? Scanpy has read visium but squidpy is the spatial package? I can analyze atac data in scanpy but need to use muon to read the file?. Seurat has basically every reader one would need. This kind of fractured environment is not going to help us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this wo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1159,enhance,enhanced,1159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['enhance'],['enhanced']
Modifiability,pl.pca_loadings fails if `subset=False` when computing highly variables genes.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/315:62,variab,variables,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/315,1,['variab'],['variables']
Modifiability,"pl.scatter(adata, x='n_counts', y='n_genes', save=""_gene_count""). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _get_obs_array(self, k, use_raw, layer); 1527 obs.keys and then var.index.""""""; 1528 if use_raw:; -> 1529 return self.raw.obs_vector(k); 1530 else:; 1531 return self.obs_vector(k=k, layer=layer). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in obs_vector(self, k); 408 as `.obs_names`.; 409 """"""; --> 410 a = self[:, k].X; 411 if issparse(a):; 412 a = a.toarray(). ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in __getitem__(self, index); 331 ; 332 def __getitem__(self, index):; --> 333 oidx, vidx = self._normalize_indices(index); 334 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 335 else: X = self._adata.file['raw.X'][oidx, vidx]. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_indices(self, packed_index); 360 obs, var = unpack_index(packed_index); 361 obs = _normalize_index(obs, self._adata.obs_names); --> 362 var = _normalize_index(var, self.var_names); 363 return obs, var; 364 . ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in _normalize_index(index, names); 153 return slice(start, stop, step); 154 elif isinstance(index, (np.integer, int, str)):; --> 155 return name_idx(index); 156 elif isinstance(index, (Sequence, np.ndarray, pd.Index)):; 157 # here, we replaced the implementation based on name_idx with this. ~/miniconda3/envs/py3/lib/python3.6/site-packages/anndata/core/anndata.py in name_idx(i); 140 raise IndexError(; 141 'Key ""{}"" is not valid observation/variable name/index.'; --> 142 .format(i)); 143 i = i_found[0]; 144 return i; ```. I don't understand why anndata thinks that . IndexError: Key ""n_counts"" is not valid observation/variable name/index. even though it's clearly in adata.obs... any suggestions what to do? add print statements to the various functions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728:2232,variab,variable,2232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728,2,['variab'],['variable']
Modifiability,"plotting\_tools\scatterplots.py"", line 542, in umap; return embedding(adata, 'umap', **kwargs); File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 207, in embedding; use_raw=use_raw, gene_symbols=gene_symbols,; File ""C:\ProgramData\Miniconda3\lib\site-packages\scanpy\plotting\_tools\scatterplots.py"", line 865, in _get_color_values; values = adata.raw.obs_vector(value_to_plot); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 413, in obs_vector; idx = self._normalize_indices((slice(None), k)); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 364, in _normalize_indices; var = _normalize_index(var, self.var_names); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 155, in _normalize_index; return name_idx(index); File ""C:\ProgramData\Miniconda3\lib\site-packages\anndata\core\anndata.py"", line 142, in name_idx; .format(i)); IndexError: Key ""XKR4"" is not valid observation/variable name/index. ```; However, the gene XKR4 did exist in the var_names:; ```; >>> post_adata.var_names; Index(['XKR4', 'RP1', 'SOX17', 'MRPL15', 'LYPLA1', 'TCEA1', 'RGS20', 'ATP6V1H',; 'OPRK1', 'NPBWR1',; ...; '2700089I24RIK', 'RAB11FIP2', 'E330013P04RIK', 'NANOS1', 'EIF3A',; 'FAM45A', 'SFXN4', 'PRDX3', 'GRK5', 'CSF2RA'],; dtype='object', length=16249); ```. The anndata object looked as below and it was fine when I tried to show the louvain clusters:. ```; >>> post_adata; AnnData object with n_obs × n_vars = 88291 × 16249; obs: 'CellID', 'batch_indices', 'labels', 'local_means', 'local_vars', 'louvain', 'clusters'; var: 'gene_id'; uns: 'neighbors', 'louvain', 'louvain_colors'; obsm: 'X_scVI', 'X_umap'. >>> sc.pl.umap(post_adata, color=['louvain']); ```. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...; ```. #### Versions:; scanpy==1.4.5.post3 anndata==0.6.22.post1 umap==0.3.10 numpy==1.18.1 scipy==1.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1039:1398,variab,variable,1398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1039,1,['variab'],['variable']
Modifiability,"pt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 327 in _main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 273 in wrap_session; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/main.py"", line 320 in pytest_cmdline_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_callers.py"", line 102 in _multicall; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_manager.py"", line 119 in _hookexec; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pluggy/_hooks.py"", line 501 in __call__; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 175 in main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/_pytest/config/__init__.py"", line 198 in console_main; File ""/opt/hostedtoolcache/Python/3.9.18/x64/bin/pytest"", line 8 in <module>; /home/vsts/work/_temp/1dc6f140-196e-4393-a84a-ebdaa5dcda61.sh: line 1: 1811 Illegal instruction (core dumped) pytest. ##[error]Bash exited with code '132'.; ##[section]Finishing: PyTest; ```. ### Versions. <details>. ```; anndata 0.10.5.post1; annoy 1.17.3; array_api_compat 1.4.1; asciitree 0.3.3; attrs 23.2.0; cfgv 3.4.0; click 8.1.7; cloudpickle 3.0.0; contourpy 1.2.0; coverage 7.4.1; cycler 0.12.1; dask 2024.2.0; dask-glm 0.3.2; dask-ml 2023.3.24; decorator 5.1.1; Deprecated 1.2.14; distlib 0.3.8; distributed 2024.2.0; exceptiongroup 1.2.0; fasteners 0.19; fbpca 1.0; filelock 3.13.1; fonttools 4.49.0; fsspec 2024.2.0; future 0.18.3; geosketch 1.2; get-annotations 0.1.2; graphtools 1.5.3; h5py 3.10.0; harmonypy 0.0.9; identify 2.5.35; igra",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:6471,config,config,6471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['config'],['config']
Modifiability,"py log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1457,config,configured,1457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['config'],['configured']
Modifiability,"pytables is starting to throw warnings, so it may be time for a rewrite and moving the function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043:64,rewrite,rewrite,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879682043,1,['rewrite'],['rewrite']
Modifiability,pytest can also be configured in TOML!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1375:19,config,configured,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1375,1,['config'],['configured']
Modifiability,"python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole annd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3398,variab,variable,3398,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variable']
Modifiability,"r branch of scanpy. ### What happened?. I'm trying to use `sc.pl.spatial` with the dataset that is available on 10X Visium with the sample ID `CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma`. I can open and do some basic QC just fine, but when I try to plot, I get the error `TypeError: can't multiply sequence by non-int of type 'float`. ### Minimal code sample. ```python; import scanpy as sc; import anndata as an; import pandas as pd; import numpy as np; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import scanorama. sc.set_figure_params(facecolor=""white"", figsize=(8, 8)); sc.settings.verbosity = 3. # Loading dataset; adata = sc.read_visium(; path=r""\external"",; count_file=""CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5"",; load_images=True,; source_image_path=r""\spatial"",; ). adata.var_names_make_unique(). # Quality control; adata.var[""mito""] = adata.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mito""], percent_top=None, log1p=False, inplace=True; ); sc.pl.spatial(adata); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""\scanpy\plotting\_tools\scatterplots.py"", line 1002, in spatial; File ""\plotting\_tools\scatterplots.py"", line 391, in embedding; # if user did not set alpha, set alpha to 0.7; File ""\scanpy\plotting\_utils.py"", line 1107, in circles; if scale_factor != 1.0:; TypeError: can't multiply sequence by non-int of type 'float'; ```; The json file on the spatial folder with the scale factors is as follows:. ```json; {; ""regist_target_img_scalef"": 0.16836435,; ""tissue_hires_scalef"": 0.056121446,; ""tissue_lowres_scalef"": 0.016836435,; ""fiducial_diameter_fullres"": 384.18505640709947,; ""spot_diameter_fullres"": 256.12337093806633; }; ```. `tissue_hires_scalef` is being passed as `scale_factor` variable and hence why it's throwing the error; ### Versions. <details>. ```; scanpy 1.9.6; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778:2144,variab,variable,2144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778,1,['variab'],['variable']
Modifiability,"ranch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hello Scanpy,; It's very smooth to subset the adata by HVGs when doing `adata = adata[:, adata.var.highly_variable]` in the Scanpy pipeline. But when using the same coding to subeset a new raw adata, it generate errors. Could you please help me to check this issue?; Thanks!; Best,; YJ. ### Minimal code sample (that we can copy&paste without having any data). ```python; ACT_sub2 = sc.read('C:/Users/Park_Lab/Documents/ACT_sub2.h5ad') # Scanpy proceeded data; ACT_sub2; AnnData object with n_obs × n_vars = 2636 × 5000; obs: 'leiden', 'n_genes', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_rpl', 'pct_counts_rpl', 'total_counts_rps', 'pct_counts_rps'; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand', 'n_cells', 'mt', 'rpl', 'rps', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'leiden', 'leiden_colors', 'neighbors', 'pca', 'rank_genes_groups', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; layers: 'ambiguous', 'matrix', 'spliced', 'unspliced'; obsp: 'connectivities', 'distances'. adata = sc.read_loom(filename='C:/Users/Park_Lab/Documents/cellsorted_Apc_Cracd_Tumor_KPVDV.loom') # raw data; adata.var_names_make_unique(); adata; AnnData object with n_obs × n_vars = 13499 × 32285; var: 'Accession', 'Chromosome', 'End', 'Start', 'Strand'; layers: 'matrix', 'ambiguous', 'spliced', 'unspliced'. adata.var['highly_variable']=ACT_sub2.var['highly_variable']; adata = adata[:, adata.var.highly_variable] # subset the raw data by ACT_sub2's highly variable genes. KeyError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_9544/4098982234.py in <module>; ----> 1 adata = adata[:, adata.var.highly_variabl",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2095:1480,layers,layers,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2095,1,['layers'],['layers']
Modifiability,rank_genes_groups refactoring,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/723:18,refactor,refactoring,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/723,1,['refactor'],['refactoring']
Modifiability,rank_genes_groups refactoring 2nd try,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156:18,refactor,refactoring,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156,1,['refactor'],['refactoring']
Modifiability,"ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(); sc.pp.log1p(adata); print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes; sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression; for marker in markers:; adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes; adata = adata[:, adata.var.highly_variable]. ts=time.time(); #Regress out confounding factors (number of counts, mitochondrial gene expression); mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX); n_counts = np.array(adata.X.sum(axis=1)); adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts; adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']); print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(); sc.pp.scale(adata); print(""Total scale time : %s"" % (time.time()-ts)); ```; add timer around _get_mean_var call . https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099:2756,variab,variable,2756,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099,1,['variab'],['variable']
Modifiability,read_10x_mtx() with annData 0.10.4 only reads one variable,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2825:50,variab,variable,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2825,1,['variab'],['variable']
Modifiability,"recent call last); Cell In[3], line 1; ----> 1 sc.get.aggregate(pbmc, by=""louvain"", func=""mean"", obsm=""X_umap""). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/functools.py:909, in singledispatch.<locals>.wrapper(*args, **kw); 905 if not args:; 906 raise TypeError(f'{funcname} requires at least '; 907 '1 positional argument'); --> 909 return dispatch(args[0].__class__)(*args, **kw). File /mnt/workspace/repos/scanpy/scanpy/get/_aggregated.py:272, in aggregate(adata, by, func, axis, mask, dof, layer, obsm, varm); 264 # Actual computation; 265 layers = aggregate(; 266 data,; 267 by=categorical,; (...); 270 dof=dof,; 271 ); --> 272 result = AnnData(; 273 layers=layers,; 274 obs=new_label_df,; 275 var=getattr(adata, ""var"" if axis == 0 else ""obs""),; 276 ); 278 if axis == 1:; 279 return result.T. File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:271, in AnnData.__init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx); 269 self._init_as_view(X, oidx, vidx); 270 else:; --> 271 self._init_as_actual(; 272 X=X,; 273 obs=obs,; 274 var=var,; 275 uns=uns,; 276 obsm=obsm,; 277 varm=varm,; 278 raw=raw,; 279 layers=layers,; 280 dtype=dtype,; 281 shape=shape,; 282 obsp=obsp,; 283 varp=varp,; 284 filename=filename,; 285 filemode=filemode,; 286 ). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/anndata.py:501, in AnnData._init_as_actual(self, X, obs, var, uns, obsm, varm, varp, obsp, raw, layers, dtype, shape, filename, filemode); 498 self._clean_up_old_format(uns); 500 # layers; --> 501 self._layers = Layers(self, layers). File /mnt/workspace/mambaforge/envs/scanpy-dev/lib/python3.11/site-packages/anndata/_core/aligned_mapping.py:331, in Layers.__init__(self, parent, vals); 329 self._data = dict(); 330 if vals is not None:; --> 331 self.update(vals). File <frozen _collections_abc>:949, in update(self, other, **kwd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2929:1624,layers,layers,1624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2929,1,['layers'],['layers']
Modifiability,"resolve the dependencies, and for that purpose gets all the installed packages’ metadata, then tries to figure out a configuration of upgrades that makes things work. No idea why it sees “1.7.0rc2” and decides “I’ll update this even when not asked to update”. Maybe raise this issue with pip?. > I'm not sure what you mean by this. Does flit install -s --deps=develop not count as reinstalling? Are you counting flit install -s as a development install?. Yes, that’s a reinstall in some development mode. My point was that if a scanpy pre-1.7 version really was installed, maybe pip was correct to update to 1.7 for some reason. However since we’re past 1.7 now, unless you haven’t git-pulled yet, I assume your dev install’s metadata has gone stale. I guess pip wouldn’t uninstall your dev install if you had run `git pull && flit install -s`, therefore updating the metadata. But I could be wrong, as I have no idea why pip thought it necessary to touch scanpy when installing scvelo. > I think pinning pip to an old version is worse than using a common, even if non-standard, installation method. Our setup.py is a compatibility shim solely for fallback use, not something to be relied upon in any part of our process. Usually when something does an arbitrary change making our life harder, our approach is pinning it temporarily until it fixed that or the infrastructure has adapted to its whims, right?. > It looks like the direction the discussion is headed is PEP 427 is wrong, and pip is right. Accepted PEPs are specs, so only pip and flit can be right or wrong (as they implement it). If people decide that what pip does happens to be *better* than the currently spec-compliant behavior, the spec can be changed accordingly. Until then pip is wrong, so we should pin its version to one that accepts spec-compliant behavior. (we can change our approach if that happens to drag on too long). I see you already commented in pypa/pip#9628, so I guess that’s the better place for following that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298:1569,adapt,adapted,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1527#issuecomment-783309298,1,['adapt'],['adapted']
Modifiability,"rized tab (Thienpont_T-cell_v4_R_fixed.loom). Here are the relevant parts of my conda environment; ```; loompy 2.0.15 <pip>; python 3.6.6 h5001a0f_0 conda-forge; anndata 0.6.11 <pip>; scanpy 1.3.2 <pip>; ```; Here is the error below, I don't know if this is an issue with the latest loompy version? Thanks so much for any help! -Orr; ```; import scanpy.api as sc ; sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-2-3e86e297513a> in <module>; 1 import scanpy.api as sc; ----> 2 sc.read_loom('./Thienpont_T-cell_v4_R_fixed.loom'). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_loom(filename, sparse, cleanup, X_name, obs_names, var_names); 142 filename = fspath(filename) # allow passing pathlib.Path objects; 143 from loompy import connect; --> 144 with connect(filename, 'r') as lc:; 145 ; 146 if X_name not in lc.layers.keys(): X_name = ''. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in connect(filename, mode, validate, spec_version); 1151 Note: if validation is requested, an exception is raised if validation fails.; 1152 	""""""; -> 1153 return LoomConnection(filename, mode, validate=validate, spec_version=spec_version). ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/loompy/loompy.py in __init__(self, filename, mode, validate, spec_version); 82 lv = loompy.LoomValidator(version=spec_version); 83 if not lv.validate(filename):; ---> 84 raise ValueError(""\n"".join(lv.errors) + f""\n{filename} does not appead to be a valid Loom file according to Loom spec version '{spec_version}'""); 85 ; 86 self._file = h5py.File(filename, mode). ValueError: Row attribute 'Gene' dtype object is not allowed; Row attribute 'Regulons' dtype [('SPI1_extended_(1805g)', 'u1'), ('SPI1_(1756g)', 'u1'), ('KLF5_extended_(1521g)', 'u1'), ('EHF_extended_(1513g)', 'u1'), ('STAT1_extended_(1443g)', 'u1'),",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/320:1252,layers,layers,1252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/320,1,['layers'],['layers']
Modifiability,"rning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 15:; @jit(float32[:](float32[:, :]), nogil=True); def l2_norm(in_matrix):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: Code running in object mode won't allow parallel execution despite nogil=True.; @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""l2_norm"" failed type inference due to: Invalid use of Function(<function norm at 0x7f637ad4ca70>) with argument(s) of type(s): (axis=Literal[int](1), x=array(float32, 2d, A)); * parameterized; In definition 0:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; In definition 1:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<function norm at 0x7f637ad4ca70>); [2] During: typing of call at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (16). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 16:; def l2_norm(in_matrix):; return np.linalg.norm(x=in_matrix, axis=1); ^. @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:3854,parameteriz,parameterized,3854,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,7,['parameteriz'],['parameterized']
Modifiability,"rocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. Hi, I'm trying to follow [Analytic Pearson residuals](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); but after getting first 2 nomalizations, I'm stucked with Pearson residuals normalization. It seems like a devided by 0 issue? I don't know. What else I can do to check if the matrix is the problem. already did gene (`min_cells=3`) and cell filtering(`min_genes=200`).; ### Minimal code sample (that we can copy&paste without having any data). ```python; analytic_pearson = sc.experimental.pp.normalize_pearson_residuals(adata, inplace=False); ```. ```pytb; /home/sxykdx/miniconda3/envs/scanpy/lib/python3.10/site-packages/scanpy/experimental/pp/_normalization.py:59: RuntimeWarning: invalid value encountered in divide; residuals = diff / np.sqrt(mu + mu**2 / theta); ```; `analytic_pearson[""X""]` ; Output:; ```py; array([[-0.08038502, -0.10195383, -0.24513291, ..., 1.47699586,; -0.08709449, -0.16926342],; [-0.08623174, -0.109369 , 3.53739781, ..., -0.54014355,; -0.09342913, -0.18157157],; [-0.07625086, -0.09671059, -0.2325321 , ..., -0.47777291,; 12.02085262, 6.06603257],; ...,; [-0.02799957, -0.03551299, -0.08540438, ..., -0.17560885,; -0.03033674, -0.05896328],; [-0.02840246, -0.03602399, -0.08663319, ..., -0.17813493,; -0.03077326, -0.05981169],; [-0.02914286, -0.03696307, -0.08889143, ..., -0.18277714,; -0.03157547, -0.06137084]]); ```. ```py; adata.layers[""analytic_pearson_residuals""] = analytic_pearson[""X""]; adata.layers[""analytic_pearson_residuals""].sum(1); ```; Output:; `array([nan, nan, nan, ..., nan, nan, nan])`. #### Versions. scanpy==1.9.3 anndata==0.9.1 umap==0.5.3 numpy==1.23.5 scipy==1.10.1 pandas==2.0.1 scikit-learn==1.2.2 statsmodels==0.13.5 python-igraph==0.10.3 pynndescent==0.5.10. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2496:1817,layers,layers,1817,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2496,2,['layers'],['layers']
Modifiability,"rovement; While the overall structure of the documentation is good, certain parameters are not described in detail, which might lead to ambiguity in their application. Notably:. - **Parameters like `use_raw`, `log`, `num_categories`, `categories_order`, etc.**: The existing documentation does not provide enough context or explanation about what each of these parameters does, their expected data types, default values, and how they influence the behavior of the plot. - **Complex Parameters**: Parameters that involve more complex concepts or data structures, such as `var_names`, `groupby`, `var_group_positions`, and `values_df`, would benefit significantly from more detailed descriptions and examples. - **Method `style` and Its Parameters**: The `style` method within the `MatrixPlot` class modifies plot visual parameters, but the implications and use cases of changing parameters like `cmap`, `edge_color`, and `edge_lw` are not well-explained. ### Suggested Improvements; To address these issues, I recommend the following enhancements:. 1. **Detailed Parameter Explanations**: Expand on the description of each parameter, especially those that are complex or not self-explanatory. This should include the type of data expected, default values, and a clear explanation of the parameter’s role and impact. 2. **Include Examples and Use Cases**: For complex parameters, providing examples or typical use cases can be extremely helpful. This could be in the form of small code snippets or scenarios illustrating when and how to use these parameters effectively. 3. **Consistency in Documentation Style**: Ensure that the documentation style is consistent across different parameters, making it easier for users to read and understand. ### Conclusion; Enhancing the documentation of the `MatrixPlot` class will improve the library's usability and user experience. . I am new to open-source contribution and I am eager to contribute to this enhancement, and welcome any additional input or guida",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2766:1522,enhance,enhancements,1522,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2766,1,['enhance'],['enhancements']
Modifiability,"rror with sc.tl.rank_genes_groups that my reference needs to be one of groupby=[cats], but as you can see in the traceback below, it appears identical. It has something to do with the data types, I've been dealing with this for a few weeks now and on some data sets I am eventually able to figure it out and get it to run, on some I am not. I think this is more of a pandas issue than scanpy, so I am wondering what version of pandas you recommend for anndata 0.7.6? Or if you know a simple workaround to make sure that the datatypes match? This has been a massive headache. I use this list(zip()) syntax in my code frequently, have never had an issue with datatypes inside of a zipped object... so this may be a simple pythonic question, but if there is a different method of accomplishing the same thing thing that doesn't introduce this error I would be happy to hear that as well. In the example below, in my adata.obs I have a column 'condition' that is a categorical variable of biological condition for differential expression, and so all cells belong to either group 0 or 1. I have tried seemingly every combination of having these, and the leiden clusters column be integers, strings, objects, before converting to the categorical dtype in line 7 below, always get the same error, and the reference = item always appears identical to the first item in the groupby = list. ### Minimal code sample (that we can copy&paste without having any data). ```python; cluster_method='leiden'; n_genes=1000; g1n='Control'; adata.obs['condition']=adata.obs['condition'].astype('category'); adata.obs[cluster_method]=adata.obs[cluster_method].astype('category'); pairs = list(zip(adata.obs['condition'], adata.obs[cluster_method])); adata.obs['pairs_'+cluster_method]=pairs; adata.obs['pairs_'+cluster_method]=adata.obs['pairs_'+cluster_method].astype('category'); pairs_set = list(set(pairs)); s = sorted(pairs_set); half = int((len(s)/2)); list1 = s[:half]; list2 = s[half:]; lz_cluster_method = list(zip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:1263,variab,variable,1263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['variab'],['variable']
Modifiability,"s #2088 and adresses #1733; <!-- Only check the following box if you did not include release notes -->; - [x] Tests included or not required because: They are required and some suggested; - [x] Release notes; - [x] Doc update - depending on feedback here; - [x] Doc update - guidance scanpy vs seurat. **Context**; As discussed in issues #2088 and #1733, `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=SOME_KEY)` potentially differs in the implementation of how HVGs are ranked from its Seurat counterpart:; - either by sorting by number-of-batches-in-which-genes-are-highly-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1310,variab,variable,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['variab'],['variable']
Modifiability,"s this problem. There's two reasons for this:. ### Only a different set of variables. Raw only differs from the main object by variables. But we just as often want to remove observations (doublet detection for example). To account for this, I think it makes sense to just have two different anndata objects. ### absolutely everything. I don't think we really can expect to have everything. There are always going to be analyses that require going back to the BAM. If ""single file"" is the issue, we could definitely allow something like:. ```python; with h5py.File(""analysis.h5"") as f:; processed = ad.read_h5ad(f[""processed""]); raw = ad.read_h5ad(f[""raw""]); ```. -----------------------------. @LuckyMD . > Integration works better with HVGs typically. I'm thinking of the case where I have a few datasets saved as `h5ad` that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when y",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:1075,variab,variable,1075,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variable']
Modifiability,"s, some valuese get lost. . I wasn't able to make a minimal reproducible example, but I obfuscated the `obs` table of my real data and can share it here: ; https://www.dropbox.com/scl/fi/jsbrb2ulki7mmih2242kc/adata_aggregate_bug.h5ad?rlkey=qczuaf2v5vlwb00zyuxmzjkix&dl=1. ### Minimal code sample. ```python; >>> test_adata = sc.read_h5ad(""adata_aggregate_bug.h5ad""). >>> test_adata.obs[""patient_id""].nunique(); 69. >>> test_adata.obs.isnull().sum(); patient_id 0; timepoint 0; external_batch_id 0; dtype: int64. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 15. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:1418,variab,variables,1418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['variab'],['variables']
Modifiability,"s/pytorch_latest/lib/python3.8/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing); 1319 ; 1320 with option_context(""display.max_seq_items"", 10, ""display.width"", 80):; -> 1321 raise KeyError(; 1322 ""Passing list-likes to .loc or [] with any missing labels ""; 1323 ""is no longer supported. "". KeyError: ""Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: CategoricalIndex(['1Ery'], categories=['1Ery', '2Ery', '3Ery', '4Ery', '5Ery', '6Ery', '7MEP', '8Mk', ...], ordered=False, name='paul15_clusters', dtype='category'). See https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike""; ```. #### Versions. <details>. sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.2.0; anndata 0.7.6; autoreload NA; backcall 0.2.0; cffi 1.14.5; configobj 5.0.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; git 3.1.14; gitdb 4.0.7; google NA; gpytorch 1.4.1; h5py 3.2.1; igraph 0.9.6; inferelator NA; ipykernel 5.5.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; matplotlib 3.4.1; mpl_toolkits NA; natsort 7.1.1; numba 0.53.1; numexpr 2.7.3; numpy 1.20.2; packaging 20.9; pandas 1.2.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.18; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pytz 2021.1; scanpy 1.8.2; scipy 1.6.3; seaborn 0.11.1; sinfo 0.3.1; sitecustomize NA; six 1.15.0; sklearn 0.24.2; smmap 4.0.0; statsmodels 0.12.2; storemagic NA; supirfactor NA; tables 3.6.1; texttable 1.6.3; torch 1.9.0+cu102; tornado 6.1; tqdm 4.60.0; traitlets 5.0.5; typing_extensions NA; umap 0.5.1; wcwidth 0.2.5; zmq 22.0.3; -----; IPython 7.22.0; jupyter_client 6.1.12; jupyter_core ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2078:6211,config,configobj,6211,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2078,1,['config'],['configobj']
Modifiability,"sc.pl.heatmap now can execute dendrogram by clusters, can it be extended to dendrogram by genes ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037:64,extend,extended,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1077#issuecomment-592423037,1,['extend'],['extended']
Modifiability,"sc.pl.scatter() is a wrapper for _scatter_obs(). It checks to make sure; the variable names the caller is requesting to plot exist in var and/or; obs, but does not take into account whether it should look in raw based; on the use_raw flag, as _scatter_obs() does. This leads to errors when a; user asks to plot variables that are in the raw but not the filtered; matrix of adata. This commit fixes that bug. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2027:77,variab,variable,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2027,2,['variab'],"['variable', 'variables']"
Modifiability,"sc.pp.scale is giving me an error when I run it for the first time, then running fine on the second. This occurs when I run on an object generated using .copy() . ![Screen Shot 2019-07-08 at 10 55 34 AM](https://user-images.githubusercontent.com/26631928/60820560-a8fb6400-a16f-11e9-9915-7d808561af69.png). In terms of the numbers at the end--I have 2176 cells and 1600 highly variable genes. . If I run on an object not generated using copy, I get ""Trying to set attribute `.obs` of view, making a copy."" but it finishes on first run. . If I try to regress out counts first, I get. ![Screen Shot 2019-07-08 at 11 12 57 AM](https://user-images.githubusercontent.com/26631928/60821330-58850600-a171-11e9-9a50-666694bf2c1c.png). One additional oddity--if I run sc.pphighly_variable_genes with flavor = 'seurat' instead of flavor = 'cell_ranger' and call sc.pp.regress_out(Bcell, 'n_counts') prior to running sc.pp.scale(Bcell, max_value = 10) I don't get any error. If I don't run regress_counts but have the 'seurat' flavor I get ; ![Screen Shot 2019-07-08 at 11 22 04 AM](https://user-images.githubusercontent.com/26631928/60822004-9f273000-a172-11e9-814d-dfc83155b488.png). Really not sure what's happening but figured I should let you know. Thanks!. sc.settings.verbosity = 3. scanpy==1.4.3 anndata==0.6.21 umap==0.3.9 numpy==1.16.4 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/731:377,variab,variable,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/731,1,['variab'],['variable']
Modifiability,"sc.read_h5ad(results_file); sc.pp.highly_variable_genes(adataMNN, batch_key = 'sample'); var_select = adataMNN.var.highly_variable_nbatches > 1; var_genesMNN = var_select.index[var_select]; datasets = [adataMNN[adataMNN.obs['sample'] == sa].copy() for sa in adataMNN.obs['sample'].cat.categories]; sc.external.pp.mnn_correct(*datasets, var_subset=var_genesMNN, batch_key='sample'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: Code running in object mode won't allow parallel execution despite nogil=True.; @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:14: NumbaWarning: ; Compilation is falling back to object mode WITH looplifting enabled because Function ""l2_norm"" failed type inference due to: Invalid use of Function(<function norm at 0x7f637ad4ca70>) with argument(s) of type(s): (axis=Literal[int](1), x=array(float32, 2d, A)); * parameterized; In definition 0:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; In definition 1:; TypeError: norm_impl() got an unexpected keyword argument 'x'; raised from /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/typing/templates.py:539; This error is usually caused by passing an argument of a type that is unsupported by the named function.; [1] During: resolving callee type: Function(<function norm at 0x7f637ad4ca70>); [2] During: typing of call at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (16). File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 16:; def l2_norm(in_matrix):; return np.linalg.norm(x=in_matrix, axis=1); ^. @jit(float32[:](float32[:, :]), nogil=True); /home/auesro/anaconda3/envs/Scanpy/l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:1410,parameteriz,parameterized,1410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['parameteriz'],['parameterized']
Modifiability,"scanpy version 1.3.7,. I noticed that some figures does not return an ax handle.; Specifically rank_genes_groups_heatmap does not return an axis becouse _rank_genes_groups_plot does not return an axis from the matrix plot. Only tracksplot and stacked_violin seems to return an axis in [_rank_genes_groups_plot](https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_tools/__init__.py#L258). Also I'm wondering why not all plotting functions return an axes handle (list) always.; It's easy to capture in a variable if not wished to print to screen and easier to keep track of than having to both capture and determine the show=False state, and it would mirror behavior of other figure packages. Any specific reason you do not do this?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/419:514,variab,variable,514,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/419,1,['variab'],['variable']
Modifiability,"scanpy version 1.3.7. NOTE!: During typing this issue out i figured out how to solve it and save figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using yo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:802,extend,extended,802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['extend'],['extended']
Modifiability,"se:; 3612 if name in self._info_axis:. ~/anaconda3/lib/python3.6/site-packages/pandas/core/accessor.py in __get__(self, instance, owner); 52 # this ensures that Series.str.<method> is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. T",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3337,variab,variable,3337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['variab'],['variable']
Modifiability,shared high variable genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1578:12,variab,variable,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578,1,['variab'],['variable']
Modifiability,small refactor,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2890:6,refactor,refactor,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2890,1,['refactor'],['refactor']
Modifiability,"sorry, copilot hallucinated me a layer variable!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437815137:39,variab,variable,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318#issuecomment-2437815137,1,['variab'],['variable']
Modifiability,"sounds good!. but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/271#issuecomment-432728241:64,extend,extend,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271#issuecomment-432728241,1,['extend'],['extend']
Modifiability,"sounds to me like you know what you're doing. Where do you get your PCA from? Seurat? Otherwise I would consider running `sc.pp.pca(adata, svd_solver='arpack')` with the highly variable gene set and your pre-processed data from Seurat. Also, for the future, an easier way to go between Seurat and Scanpy might be [anndata2ri](https://www.github.com/flying-sheep/anndata2ri). I have an example notebook for that [here](https://www.github.com/LuckyMD/Code_snippets).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498843181:177,variab,variable,177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498843181,1,['variab'],['variable']
Modifiability,"ster/spatial/integration-scanorama.ipynb. @flying-sheep mentioned that the scanpy tests filter out warnings and indeed you can reproduce these by e.g.,:; ```sh; pytest -W error::FutureWarning -n auto scanpy/tests/test_plotting.py; ```. ### Error output. - [x] `…/scanpy/plotting/_tools/scatterplots.py:401:`. > UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored. - [x] `…/scanpy/plotting/_tools/__init__.py:1269:`. > FutureWarning: The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect. ; > `_ax = sns.violinplot(`. - [x] `…/scanpy/preprocessing/_simple.py:274:`. > ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; > `adata.var[""n_cells""] = number`. - [x] `…/scanpy/plotting/_stacked_violin.py:503: FutureWarning:`. > Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect. ; > `row_ax = sns.violinplot(`. ### Versions. <details>. ```; -----; anndata 0.10.4; scanpy 1.10.0.dev191+gf7f5d5c6; -----; IPython 8.20.0; PIL 10.2.0; asciitree NA; asttokens NA; cffi 1.16.0; cloudpickle 3.0.0; cycler 0.12.1; cython_runtime NA; dask 2024.1.0; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fasteners 0.19; h5py 3.10.0; igraph 0.10.8; iniconfig NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.41.1; markupsafe 2.1.3; matplotlib 3.8.2; mpl_toolkits NA; msgpack 1.0.7; natsort 8.4.0; numba 0.58.1; numcodecs 0.12.1; numpy 1.26.3; packaging 23.2; pandas 2.1.4; parso 0.8.3; pexpect 4.9.0; pluggy 1.3.0; prompt_toolkit 3.0.43; psutil 5.9.7; ptyprocess 0.7.0; pure_eval 0.2.2; py NA; pygments 2.17.2; pyparsing 3.1.1; pytest 7.4.4; pytz 2023.3.post1; scipy 1.11.4; session_info 1.0.0; setuptools 68.2.2; setuptools_scm NA; sitecustomize NA; six 1.16.0; sklearn 1.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2839:1531,variab,variable,1531,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2839,1,['variab'],['variable']
Modifiability,"sure! in short: alex said he didn’t like the switch to type annotations at all, citing a few gripes. i went on to fix them at various places (fixes are now in) and argued against a few others. i convinced alex that we should (slowly and carefully) adapt type annotations. the only thing that was missing is a consensus on how to best pretty-print `typing.Union`, because alex was not a fan of the name and clumsiness. I preferred `a, b, or c`, he just `a, b, c`. i explained why `a, b, c` is a bad convention, but alex insisted to go with it because (sadly) everyone is doing it. from there on we went deeper into algebraic types and so on. without need really, as we already decided on what to do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977:248,adapt,adapt,248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-443255977,1,['adapt'],['adapt']
Modifiability,"t scanpy as sc I get the following error:. LookupError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1082,config,config,1082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,1,['config'],['config']
Modifiability,"tall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a longer planning discussion about how configuration works. Agreed, probably in an extra issue. > I'm wondering if we couldn't cut down on the need to explain by adopting a convention of referencing relevant settings in any function that access them? For example, the docs for expression_atlas would have a reference to dataset_dir?. sounds great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1772,config,configs,1772,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,3,['config'],"['configs', 'configuration']"
Modifiability,"tations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 932 idcs = idcs[idcs_group]; 933 if key in adata.obs_keys(): x += list(adata.obs[key].values[idcs]); --> 934 else: x += list(adata_X[:, key].X[idcs]); 935 if ikey == 0:; 936 groups += [group for i in range(len(idcs))]. ~\Anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 1301 def __getitem__(self, index):; 1302 """"""Returns a sliced view of the object.""""""; -> 1303 return self._getitem_view(index); 1304 ; 1305 def _getitem_view(self, index):. ~\Anaconda3\lib\site-packages\anndata\base.py in _getitem_view(self, index); 1305 def _getitem_view(self, index):; 1306 oidx, vidx = self._normalize_indices(index); -> 1307 return AnnData(self, oidx=oidx, vidx=vidx, asview=True); 1308 ; 1309 # this is used in the setter for uns, if a view. ~\Anaconda3\lib\site-packages\anndata\base.py in __init__(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, oidx, vidx); 667 if not isinstance(X, AnnData):; 668 raise ValueError('`X` has to be an AnnData object.'); --> 669 self._init_as_view(X, oidx, vidx); 670 else:; 671 self._init_as_actual(. ~\Anaconda3\lib\site-packages\anndata\base.py in _init_as_view(self, adata_ref, oidx, vidx); 724 self._X = None; 725 else:; --> 726 self._init_X_as_view(); 727 ; 728 self._layers = AnnDataLayers(self, adata_ref=adata_ref, oidx=oidx, vidx=vidx). ~\Anaconda3\lib\site-packages\anndata\base.py in _init_X_as_view(self); 751 shape = (; 752 get_n_items_idx(self._oidx, self._adata_ref.n_obs),; --> 753 get_n_items_idx(self._vidx, self._adata_ref.n_vars); 754 ); 755 if np.isscalar(X):. ~\Anaconda3\lib\site-packages\anndata\utils.py in get_n_items_idx(idx, l); 141 return 1; 142 else:; --> 143 return len(idx). TypeError: object of type 'numpy.int64' has no len()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/333:2404,layers,layers,2404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/333,1,['layers'],['layers']
Modifiability,"th issues around categorical values. To me this suggests a need to have separate arguments for the two cases (`order_categorical`, `order_continuous`), though this raises issues with ""vectorizing"" the argument. Docstrings for these arguments would look something like:. ```rst; order_continuous: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""ascending""; How to order points in plots colored by continuous values. Options include:; * ""current"": use current ordering of AnnData object; * ""random"": randomize the order; * ""ascending"": points with the highest value are plotted on top; * ""descending"": points with lowest value are plotted on top; order_categorical: Literal[""current"", ""random"", ""ascending"", ""descending""] = ""random""; How to order non-null categorical points in the plot. Uses same options as order_continuous.; ```. In this case, `sort_order` would be deprecated, and tell the user to use `order_continuous` instead. ## Potential extensions. * We could also allow users to pass `Callable[Vector, Vector[int]]`s (e.g. function which takes color vector, returns vector of integers) as arguments. ## Possible issues. ### Vectorization could be complicated. Vectorization of argument unclear/ maybe not possible. That is, what if I want the same variable twice, but ordered differently? This would look like: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8""], order_continuous=[""ascending"", ""descending""]); ```. Now what if I wanted to also plot a categorical value? Is this: . ```python; sc.pl.umap(adata, color=[""CD8"", ""CD8"", ""leiden""], order_continuous=[""ascending"", ""descending"", None]); ```. ### Null values. This solution assumes we still want null values plotted on bottom. Should there be control over that?. ## Some references for other libraries:. * [`altair.Sort`](https://altair-viz.github.io/user_guide/generated/core/altair.Sort.html#altair.Sort); * (I'm actually not sure if other libraries do this, datashader does `max`/ `min`/ `mean` which is sorta similar?)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554:1598,variab,variable,1598,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-776508554,1,['variab'],['variable']
Modifiability,"thank you for your thoughts and suggestions! great! :smile:. - `vars` is fine, `genes` is also fine as this is a function that will predominantly used for RNA-seq, I guess, `features` would be fine if we had adopted a different convention; - `expr_type` is better than `vars_type`; we could do `values_type`, which is suggestive and anticipates that at some point, `.X` might get deprecated and replaced by `.values`; but maybe a simple `suffix` parameter is better?; - why not simply `n_genes` and `n_genes_{suffix} if suffix is not None`? if people want to distinguish between different processing steps or parts of the data matrix? but right now, the canonical use would only complete these things once for the raw data; I hardly imagine computing this stuff on imputed counts or normed expression... if people want, ok, they have the suffix argument for... if people do multi-omics with anndata (we're starting to do this a lot), yes, they should also be able to do it, but a `suffix` would be fine in this case, too; regarding `by_{suffix}`: I usually associate conditioning on something when I read `by` and I guess many people do, are we sure we want this here?; - `control_variables` is more clear than the super-generic `variables` argument, which usually indicates in scanpy that you want a function to restrict to a set of variables; but here, it's different; - largely similar thoughts on `n_cells` vs `n_cells_{suffix}`; - `n_...` versus `total_...`; sure you're absolutely right, `total_vars` is much easier to swallow than `n_fluorescence`; so, I'm ok with `total_...` if you like to move forward with that; on the other hand, having `n_...` for things that are numbers and counted and `sum_...` for things that are simply sums within columns or rows would be even clearer, I imagine; but maybe confusing to implement; your decision! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119:1230,variab,variables,1230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436378119,2,['variab'],['variables']
Modifiability,"thanks for getting this started!. since this new modality has different signal characteristics, I wanted to bring up for discussion:. ### normalization choice: for the incoming geometric normalization, any justification for choosing that one over others? . in your https://github.com/theislab/scanpy-tutorials/pull/14 (10x PBMC dataset of ~30 Totalseq antibodies), the antibody panel is similar to that used in mass cytometry datasets, but different papers seem to prefer different transforms -- which begs the question, now that similar panels are being used, which transform makes the most sense in terms of:; - preserving visual interpretation of absent/low/med/high (corresponding to expectations of cell subsets); - handling a variety of marker distribution shapes (unimodal/bimodal/trimodal, skewed shapes); - making it easier to spot nonspecific antibody staining / off-target effects; - not introducing more bias in downstream differential comparisons (fits with assumptions about variable distribution properties, based on the commonly used statistical testing methods). absent a convincing answer, it may be worth implementing multiple as options, leaving the choice to the user, and just documenting these use-cases through citations; eventually, someone can make a notebook that compares the behaviors, biological expectations, and/or impacts on statistical comparisons to inform which method should be the default. While the CITEseq paper applied CLR, it's not obvious that one is better than the ones used in more time-tested fields like mass cytometry and flow cytometry. ```python; def CLR_transform(df):; '''; implements the CLR transform used in CITEseq (need to confirm in Seurat's code); https://doi.org/10.1038/nmeth.4380; '''; logn1 = np.log(df + 1); T_clr = logn1.sub(logn1.mean(axis=1), axis=0); return T_clr. def asinh_transform(df, cofactor=5):; '''; implements the hyperbolic arcsin transform used in CyTOF/mass cytometry; https://doi.org/10.1038/nmeth.4380; '''; T_cytof = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691:989,variab,variable,989,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-635963691,1,['variab'],['variable']
Modifiability,"that I want to integrate. What if a highly variable gene in one dataset just isn't present in another? Is it because it wasn't found in that dataset at all, or because it was only present in a few cells? If it was only present in a few cells, how can I be sure a particular cell type wasn't just poorly represented in that dataset?. I feel like it's helpful to have the all the measured genes present, so that when you do gather your datasets together you can select features from the full set. > > This does run into memory usage problems if want do a densifying transform on the data; > Don't understand this entirely... I was thinking about what happens if you do something like `sc.pp.scale`, where you don't have any 0s in your expression matrix anymore, so it has to be stored as a dense matrix. I believe this is why `raw` was even introduced originally, since the normalization workflow then was feature selection -> scale. It was wasteful to store the entire set of variables as a dense matrix, especially since you're only using a small subset of the features. > and not sure what a block sparse matrix type is. Block sparse matrices are a good storage structure when you've got ""blocks"" of dense values in you matrix. For example, this is what the sparsity structure might look like in a random sparse matrix:. ```; ⡠⠄⠀⠨⡯⢀⠀⠐⢡⠀⠠⢀⠠⢂⠀⠐⠀⠐⠐⣢⠀⢂⠀⠈⠒⣂⠂⠀; ⠆⢌⠁⡁⠈⠀⡀⠖⠂⠀⠁⠂⠀⠉⠐⡀⠀⠀⠈⠠⠄⠉⢀⡀⠀⠀⠀⠂; ⠑⠀⠠⠀⠃⠀⠀⠅⢀⠠⠄⡀⠅⠂⢀⠪⠀⠦⢀⠀⢃⠈⢀⠌⠚⠀⠀⠃; ⠁⠂⡃⠈⠀⢀⠀⠙⢀⠥⠀⠀⠄⡁⠀⠠⠈⠀⠈⠃⠂⠠⣀⠀⠈⣁⠁⠆; ⡀⠐⠐⠠⠀⠐⢐⡄⣂⠀⠀⠘⠀⠀⠀⠠⠂⠀⡀⠨⠁⠀⠀⠀⠁⠁⠣⠤; ⠀⡐⢀⢢⠀⠁⠔⠀⠁⠀⠃⠀⢀⢀⠐⠃⠄⠀⡇⠊⠄⠀⡈⢀⠀⠀⣀⠆; ⠀⢐⣤⡄⠠⠂⠃⡈⠘⠀⠀⠀⡂⠰⢄⠊⡂⠀⠐⠂⠀⠄⠀⠀⢱⠩⠈⢀; ⢁⠀⠑⠚⠁⠂⠂⠐⠁⠀⠀⢀⠠⠀⠐⠈⠈⡨⠀⠂⠀⡈⠈⠁⡐⣀⢁⠂; ⠀⠀⠀⠁⠀⠠⠅⠁⡠⠇⢐⠀⠀⠖⢉⣀⠀⢀⠀⠠⡀⠀⡀⢰⠁⠂⢉⠂; ⠀⠀⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:2007,variab,variables,2007,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,1,['variab'],['variables']
Modifiability,"the idea is you want to take advantage of the very nice macro scanpy provides to make multiple umap subplots, when you specify multiple variables to color by (`sc.pl.umap(colors=['cell_type', 'other'])`). However, for each of those subplots, you might like to place the legend in different places, e.g. on the data for the cell types, but off to the side for the other variable. Unfortunately, scanpy only allows you to specify `legend_loc` once for all the subplots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2309#issuecomment-1257066552:136,variab,variables,136,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2309#issuecomment-1257066552,2,['variab'],"['variable', 'variables']"
Modifiability,"this makes it easy to have a basic code style in place without configuring individual editors: http://editorconfig.org. my PyCharm and your [Emacs](https://github.com/editorconfig/editorconfig-emacs#readme) both support it (Emacs with that plugin). if you don’t want to install the plugin, it’s at least useful for me when i switch machines (or future contributors)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/5:63,config,configuring,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/5,3,"['config', 'plugin']","['configuring', 'plugin']"
Modifiability,"tion with `inplace=False`, you'll get a ""nice"" object that is convenient to handle. If you call a function `sc.tl.function` in a pipeline with `inplace=True` but later on, you'll want this nice object, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a sc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:3864,rewrite,rewrite,3864,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['rewrite'],['rewrite']
Modifiability,"tion/pca.py in _fit(self, X); 380 ; 381 X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,; --> 382 copy=self.copy); 383 ; 384 # Handle n_components==None. ~/anaconda2/envs/scanpy/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator); 556 "" a minimum of %d is required%s.""; 557 % (n_features, array.shape, ensure_min_features,; --> 558 context)); 559 ; 560 if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:. ValueError: Found array with 0 feature(s) (shape=(44495, 0)) while a minimum of 1 is required.; ```. The `pca` code doesn't error here, because `highly_variable_intersection` makes `'highly_variable' in adata.var.keys()` evaluate to `True`:; ```; if use_highly_variable is True and 'highly_variable' not in adata.var.keys():; raise ValueError('Did not find adata.var[\'highly_variable\']. '; 'Either your data already only consists of highly-variable genes '; 'or consider running `pp.highly_variable_genes` first.'); if use_highly_variable is None:; use_highly_variable = True if 'highly_variable' in adata.var.keys() else False; if use_highly_variable:; logg.info(' on highly variable genes'); adata_comp = adata[:, adata.var['highly_variable']] if use_highly_variable else adata. ```; ```pytb; adata.var.keys(); Index(['mito', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts',; 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts',; 'n_cells', 'highly_variable', 'means', 'dispersions',; 'dispersions_norm', 'highly_variable_nbatches',; 'highly_variable_intersection'],; dtype='object'); ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.5.post2 anndata==0.7.1 umap==0.3.10 numpy==1.17.0 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.3 statsmodels==0.11.0dev0+630.g4565348 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032:2948,variab,variable,2948,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032,2,['variab'],['variable']
Modifiability,"tion=0.5, random_state=seed); sc.pl.umap(adata, color=['batch', 'leiden'], alpha=0.3); print(adata.uns['neighbors']['connectivities'].sum()). adata_neigh = adata.copy(); sc.pp.neighbors(adata_neigh, metric='cosine', random_state=seed); sc.tl.umap(adata_neigh, random_state=seed); sc.tl.leiden(adata_neigh, resolution=0.6, random_state=seed); sc.pl.umap(adata_neigh, color=['batch', 'leiden'], alpha=0.3); print(adata_neigh.uns['neighbors']['connectivities'].sum()); ```. Our matrices are the same (the sums are 4918.370372081173 and 5005.088472351332), so the random generation works, but then the UMAPs and clustering solutions are different. For the adata run with `sc.pp.neighbors` (left are batches and right are `leiden` cluster labels):; Mine; ![image](https://user-images.githubusercontent.com/35657291/73087502-e12b7f80-3ed2-11ea-9df9-177cec32d208.png). Hers; ![image](https://user-images.githubusercontent.com/35657291/73087537-f0aac880-3ed2-11ea-8c45-115023be9bff.png). For the adata run with `sce.pp.bbknn`:; Mine; ![image](https://user-images.githubusercontent.com/35657291/73087518-ea1c5100-3ed2-11ea-97de-4b6ce0e0d389.png). Hers; ![image](https://user-images.githubusercontent.com/35657291/73087545-f43e4f80-3ed2-11ea-82b9-a5dcb5d804b7.png). Our PCA decomposition has the same coordinates, so we discard the PCA as the source of variability. Also, since both UMAP and leiden look different, we think the source might come from the neighbor calculation. When running `adata.uns['neighbors']['connectivities'].sum()` I get 801.5580058219996 and 1204.5274490986717 for `adata` and `adata_neigh`. I don't have her values now, but the values using the _real_ dataset were in the order of 5000, and they were of by less than 0.001; so we are confused that with such a small difference on the sum, the results can be so different. I attach the adatas for you to inspect them if you need more info. [adatas.zip](https://github.com/theislab/scanpy/files/4109668/adatas.zip). Thanks for the help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1009:2595,variab,variability,2595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1009,1,['variab'],['variability']
Modifiability,"took your position back when this behaviour was added. The closest issue I could find was [theislab/anndata#115](https://github.com/theislab/anndata/issues/115), but I remember having a longer discussion with @flying-sheep about this.; > ; > However, I'm now pretty convinced that strings as categoricals is pretty necessary for computational reasons. There isn't a good ""array of strings"" in python, so all operations on those kinds are reaaaaally slow. Also, most of the time strings really are encoding a categorical variable. If they aren't, they should be unique (so we don't convert). I would still argue it's better to complain and get the user to fix it or convert to a categorical internally for the purposes of the function. It's not the conversion that I find the issue it's that there is no way to control it. I would be totally fine with including `sanitize_andata()` in examples, just as a function called by the user rather than internally. An example of a non-unique non-categorical variable would be alternative gene annotations. It's relatively common for more than one ENSEMBL idea to map to a gene symbol but that's not really a ""category"" that's useful for anything. Maybe the threshold of one repeated value is too high and it should be like 10%? That would probably become too unpredictable though. > I would generally agree with this. I don't like that `highly_variable_genes` will add mean and variance measures to the object. I guess I don't see converting strings to categoricals as being a big change, since most operations on them will have identical results. I wouldn't say the means/variances are side effects, more of an intermediate value. That's what the function says it's calculating and they are useful to have for later. Changing unrelated columns is what bugs me. > I do see how this would cause problems with `R` since this isn't the case with `factors`. I wonder if there this could be solved in the converter?. I don't think there is any good way for a conver",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499:1019,variab,variable,1019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1747#issuecomment-801731499,1,['variab'],['variable']
Modifiability,trackplot enhancement,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1195:10,enhance,enhancement,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1195,1,['enhance'],['enhancement']
Modifiability,"ts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X). adata = sc.datasets.pbmc3k(); adata.layers['counts'] = adata.X; cell = adata.obs.index[1]; adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). print(""\nRun 3: normalization, specifing argument layer=None""); sc.pp.normalize_total(adata, target_sum=1e4, layer = None); print('sum of count layer in designated cell: ', adata[cell,:].layers['counts'].sum()); print('obs[total_counts] value in cell: ', adata[cell,:].obs['total_counts'][0]); print('.X.sum() value in cell: ', adata[cell,:].X.sum()); print('sum of count layer of MALAT1 in cell: ', adata[cell,'MALAT1'].layers['counts']); print('.X value of MALAT1 in cell: ', adata[cell,'MALAT1'].X); ```. ```pytb; #Output:; Run 1: initial values after simple processing: ; sum of count layer in designated cell: 4903.0; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 4903.0; sum of count layer of MALAT1 in cell: (0, 0)	142.0; .X value of MALAT1 in cell: (0, 0)	142.0. Run 2: after sc.pp.normalize_total: ; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862. Run 3: normalization, specifing argument layer=None; normalizing counts per cell; finished (0:00:00); sum of count layer in designated cell: 10000.049; obs[total_counts] value in cell: 4903.0; .X.sum() value in cell: 10000.049; sum of count layer of MALAT1 in cell: (0, 0)	289.61862; .X value of MALAT1 in cell: (0, 0)	289.61862; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2389:2921,layers,layers,2921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2389,1,['layers'],['layers']
Modifiability,"turns a object with labels. I think this is mostly done, though I'm considering changing the calling convention. Here's an example of usage:. ```python; import scanpy as sc; import seaborn as sns. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.leiden(pbmc); sns.heatmap(sc.metrics.confusion_matrix(""bulk_labels"", ""leiden"", pbmc.obs)); ```. ![image](https://user-images.githubusercontent.com/8238804/68737959-1a28b780-0639-11ea-8576-4cf1907066d9.png). I've copied `seaborn`s calling convention here, but I think that could change. Right now the above call is equivalent to:. ```python; sc.metrics.confusion_matrix(pbmc.obs[""bulk_labels""], pbmc.obs[""louvain""]); ```. But I wonder if it would make more sense to have the DataFrame go first if it's provided. I've also based the API around my usage of confusion matrices, so I'm very open to more general feedback on this. My reason for including it here was the amount of code it took wrapping `sklearn.metrics.confusion_matrix` to get useful output. ## `sc.metrics.gearys_c` ([Wiki page](https://en.wikipedia.org/wiki/Geary%27s_C)). Calculates autocorrelation on a measure on a network. Used in [VISION](https://doi.org/10.1038/s41467-019-12235-0) for ranking gene sets. This is useful for finding out whether some per-cell measure is correlated with the structure of a connectivity graph. In practice, I've found it useful for identifying features that look good on a UMAP:. ```python; import numpy as np; pbmc.layers[""logcounts""] = pbmc.raw.X. %time gearys_c = sc.metrics.gearys_c(pbmc, layer=""logcounts""); # CPU times: user 496 ms, sys: 3.88 ms, total: 500 ms; # Wall time: 74.9 ms; to_plot = pbmc.var_names[np.argsort(gearys_c)[:4]]; sc.pl.umap(pbmc, color=to_plot, ncols=2); ```. ![image](https://user-images.githubusercontent.com/8238804/68736833-e304d700-0635-11ea-87f4-ac066f3e270c.png). It can also be useful to rank components of dimensionality reductions (example with ICA: https://github.com/theislab/scanpy/issues/767#issuecomment-552756716).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/915:2428,layers,layers,2428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/915,1,['layers'],['layers']
Modifiability,"ty sure the logical conclusion of any long discussion about types is that everything should be done in Haskell. I don't like the use of branches with `isinstance` because it breaks polymorphism, which is a key part of pythonic code to me. @falexwolf, I completely agree with you on ""what makes a good docstring"". The knowledge overhead for numeric python doesn't include type theory, so the docs should be interpretable without them. Ideally, interfaces are simple and the documentation makes the expected behavior clear. I'm still not sure I totally understand what the intent of the ""type"" vs. ""class"" system is in python, so I'm often a little unsure what to do with heavily typed code. That said, if expected behaviors could be encapsulated (both formally and intuitively) with some abstract types (representing interfaces or traits) that would be a nicer solution. I don't think we're near that point in python. ## Lattices. Sorry about not giving some info on lattices, I'd thought you didn't want to get into it. It's the [partially ordered set kind](https://en.wikipedia.org/wiki/Lattice_(order)) of lattice, where each type is an element or subset. I'll give a short python based example (ignoring that `Union[]` can't be instantiated). <details>. <summary>The code:</summary>. ```python; from typing import Any, Union. class A():; pass. class B(A):; pass. class C(A):; pass. class D():; pass. class E(D):; pass; ```. </details>. that defines a lattice, which can be represented as a DAG like this:. ```; Any; / \; A D; / \ |; B C E; \ | /; Union[]; ```. It's partially ordered in that you can't say A contains E or vice-versa, but you can say things like A is contains B, and `Any` is a supertype of (contains) everything else. I think that how you're viewing it is pretty close, except the elements are types instead of their properties. My mental model has types being a collection of properties, and being a subtype means an object inherits it's supertypes properties, and can have more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545:1968,inherit,inherits,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-444715545,1,['inherit'],['inherits']
Modifiability,"typo, `variabes->variables`. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [ ] Closes #; - [X] Tests included or not required because:; small change in documentation; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because:; minor change in documentation",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2793:7,variab,variabes,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2793,2,['variab'],"['variabes', 'variables']"
Modifiability,"ues, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision); 676 skip_blank_lines=skip_blank_lines); 677 ; --> 678 return _read(filepath_or_buffer, kwds); 679 ; 680 parser_f.__name__ = name. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _read(filepath_or_buffer, kwds); 438 ; 439 # Create the parser.; --> 440 parser = TextFileReader(filepath_or_buffer, **kwds); 441 ; 442 if chunksize or iterator:. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, f, engine, **kwds); 785 self.options['has_index_names'] = kwds['has_index_names']; 786 ; --> 787 self._make_engine(self.engine); 788 ; 789 def close(self):. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in _make_engine(self, engine); 1012 def _make_engine(self, engine='c'):; 1013 if engine == 'c':; -> 1014 self._engine = CParserWrapper(self.f, **self.options); 1015 else:; 1016 if engine == 'python':. /usr/local/lib/python3.6/site-packages/pandas/io/parsers.py in __init__(self, src, **kwds); 1706 kwds['usecols'] = self.usecols; 1707 ; -> 1708 self._reader = parsers.TextReader(src, **kwds); 1709 ; 1710 passed_names = self.names is None. pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.__cinit__(). EmptyDataError: No columns to parse from file; ```. </details>. The arguments I've passed there are whats in the documentation for the function, so I'd figured I'd give them a shot first. I've also tried `host=""www.ensembl.org/biomart""`, but had no such luck. This is after installing the `bioservices` module via `pip` and it creating some config file the first time I tried to run the query.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242:3335,config,config,3335,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242,1,['config'],['config']
Modifiability,"use `cmap`. In general you can use any option available for; `matplotlib.pyplot.scatter` including vmin, vmax, etc. On Mon, Nov 26, 2018 at 11:01 PM aopisco <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> @fidelram; > <https://github.com/fidelram> how to we change the color palette for; > numerical variables? currently setting palette = 'Oranges' only works for; > the categorical ones; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/156#issuecomment-441815667>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1RFS_19jC__9pOo04OZkjN_hVZvvks5uzGTBgaJpZM4UCLlA>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074:331,variab,variables,331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/156#issuecomment-441950074,1,['variab'],['variables']
Modifiability,"v(dir + 'GSM5226574_C51ctr_raw_counts.csv.gz').T; import scvi ; sc.pp.filter_genes(adata, min_cells = 10); sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); ```. ```pytb; >>> sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 422, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip instal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1507,plugin,plugins,1507,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['plugin'],['plugins']
Modifiability,"var_names[0:4], groupby='celltype', color_map = 'Reds'); ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); <ipython-input-72-4fc81df5ca3f> in <module>; ----> 1 sc.pl.dotplot(adata_2, adata_2.var_names[0:4], groupby='celltype', color_map = 'Reds'). ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, color_map, dot_max, dot_min, standard_scale, smallest_dot, figsize, dendrogram, gene_symbols, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1809 num_categories,; 1810 layer=layer,; -> 1811 gene_symbols=gene_symbols,; 1812 ); 1813 . ~\Anaconda3\envs\UMCU\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer, gene_symbols); 2911 matrix = adata[:, var_names].layers[layer]; 2912 elif use_raw:; -> 2913 matrix = adata.raw[:, var_names].X; 2914 else:; 2915 matrix = adata[:, var_names].X. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in __getitem__(self, index); 94 ; 95 def __getitem__(self, index):; ---> 96 oidx, vidx = self._normalize_indices(index); 97 ; 98 # To preserve two dimensional shape. ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\raw.py in _normalize_indices(self, packed_index); 154 obs, var = unpack_index(packed_index); 155 obs = _normalize_index(obs, self._adata.obs_names); --> 156 var = _normalize_index(var, self.var_names); 157 return obs, var; 158 . ~\Anaconda3\envs\UMCU\lib\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 91 not_found = indexer[positions < 0]; 92 raise KeyError(; ---> 93 f""Values {list(not_found)}, from {list(indexer)}, ""; 94 ""are not valid obs/ var names or indices.""; 95 ). KeyError: ""Values ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], from ['Rgs20', 'Oprk1', 'St18', 'Gm26901'], are not valid obs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652:1626,layers,layers,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-704271652,1,['layers'],['layers']
Modifiability,"ve figures with all elements showing with the help of ; `fig1.savefig(""name.svg"" bbox_inches='tight')`, which solves 90% of the issue I had. I also noticed that these functions have a built in save function where I found the answer. I added a question at the bottom which is still of interest to me. However, feel free to rank this as non essential.; --------------------------------------. First of all, the new figure plotting functions looks amazing.; I just have a few issues that I hope I can get some help with.; I seem to often get the behavior of figures from scanpy after I plot that the elements like xtick labels and other important features are hidden due to figure boarders or that boarders are extended far beyond the plotting are. Due to the way the axes is constructed I can't simply do a fig.tight_layout(). Even with the grid_spec specific tight_layout https://matplotlib.org/users/tight_layout_guide.html#use-with-gridspec I get the same result.; I get the sense that the figures looks ok in a notebook, perhaps, where these elements can be seen, but that does unfortunate not translate to my workflow of manually saving figures and plotting with qt or tk backends to be able to get a quick overview. Below is an example of a fig.savefig(""test.png""); for the command; ```; sc.pl.matrixplot(adata, var_names=genes_ranked_by_loading_in_PC[:topg], groupby='hpf', use_raw=None, cmap='RdBu_r', swap_axes=True); ```; ![test](https://user-images.githubusercontent.com/715716/50937406-77644300-1441-11e9-8713-8bebdf94c26b.png). The over extending bounders can't be seen here due to the white background but the missing x-tics are clear. (Note I realized how to at least save figures with a 'tight' bounding box so that issue is solved.). Q: Is it possible to get an interactive figure (as in plotting with qt or tk) where elements are visible as with fig.tight_layout() for ordinary axes using your plotting functions? @fidelram tagging you here as I assume you might be able to know this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/418:1642,extend,extending,1642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/418,1,['extend'],['extending']
Modifiability,"w, log, num_categories, color_map, dot_max, dot_min, figsize, dendrogram, var_group_positions, var_group_labels, var_group_rotation, layer, show, save, **kwds); 1350 if isinstance(var_names, str):; 1351 var_names = [var_names]; -> 1352 categories, obs_tidy = _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer=layer); 1353 ; 1354 # for if category defined by groupby (if any) compute for each var_name. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\plotting\_anndata.py in _prepare_dataframe(adata, var_names, groupby, use_raw, log, num_categories, layer); 1983 matrix = adata[:, var_names].layers[layer]; 1984 elif use_raw:; -> 1985 matrix = adata.raw[:, var_names].X; 1986 else:; 1987 matrix = adata[:, var_names].X. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in __getitem__(self, index); 495 ; 496 def __getitem__(self, index):; --> 497 oidx, vidx = self._normalize_indices(index); 498 if self._adata is not None or not self._adata.isbacked: X = self._X[oidx, vidx]; 499 else: X = self._adata.file['raw.X'][oidx, vidx]. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_indices(self, packed_index); 523 obs, var = super()._unpack_index(packed_index); 524 obs = _normalize_index(obs, self._adata.obs_names); --> 525 var = _normalize_index(var, self.var_names); 526 return obs, var; 527 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in _normalize_index(index, names); 268 raise KeyError(; 269 'Indices ""{}"" contain invalid observation/variables names/indices.'; --> 270 .format(index)); 271 return positions.values; 272 else:. KeyError: 'Indices ""[\'mamo\', \'mab21\', \'ChaT\', \'VGlut\']"" contain invalid observation/variables names/indices.'; ```. I do NOT get the error when I select to 'color' for either genes in the sc.pl.umap command. Moreover my adata contains all genes since the beggining so it is not subsetting anything. . All the help is appreciated :)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/593:2069,variab,variables,2069,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/593,2,['variab'],['variables']
Modifiability,"when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use; ```; tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]; ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var); ```; to ""reset"" the `.X` matrix (maybe there's a better way?); or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```; scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/826:180,variab,variable,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826,3,['variab'],['variable']
Modifiability,"wishes below: -->; ... I'm in the process of redesigning my most used workflows from https://github.com/jolespin/soothsayer to wrap around ScanPy. I come from microbial ecology but do a good amount of machine learning. In microbial ecology, the community is shifting towards a compositional data analysis (CoDA) approach which has fundamentals firmly rooted in mathematics. . Here is some literature about broad-scale applications across all NGS datasets: ; * [A field guide for the compositional analysis of any-omics data](https://academic.oup.com/gigascience/article/8/9/giz107/5572529); * [Understanding sequencing data as compositions: an outlook and review](https://academic.oup.com/bioinformatics/article/34/16/2870/4956011). it is even catching attention in scRNA-seq too: ; * [scCODA is a Bayesian model for compositional single-cell data analysis](https://www.nature.com/articles/s41467-021-27150-6). Anyways, off my soap box. As mentioned, I'm in the process of adapting my workflows to take advantage of ScanPy's power but I'm having a few difficulties. The first incorporating custom transformations. In future versions, would it be possible to create an API that is similar to [scanpy.pp.normalize_total)(https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.normalize_total.html) but allows for a custom metric? . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2475:1423,adapt,adapting,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2475,1,['adapt'],['adapting']
Modifiability,"y on my end; I've been able to reproduce the error with my own data and one of the scanpy built-in test datasets). Sorry the code chunks are broken up/a little long; I am using the scran normalization approach outlined in the [single cell tutorial](https://github.com/theislab/single-cell-tutorial). ```python; adata = sc.datasets.pbmc3k(); sc.pp.filter_genes(adata, min_cells = 1). # scran normalization; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after = 1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps = 15); sc.pp.neighbors(adata_pp); sc.tl.leiden(adata_pp, key_added = 'groups', resolution = 0.5); input_groups = adata_pp.obs['groups']; data_mat = adata.X.T; ```; ```python; %%R -i data_mat -i input_groups -o size_factors; size_factors = sizeFactors(computeSumFactors(SingleCellExperiment(list(counts = data_mat)), ; clusters = input_groups, ; min.mean = 0.1)); ```; ```python; del adata_pp; adata.obs['size_factors'] = size_factors; adata.layers['counts'] = adata.X.copy(); adata.X /= adata.obs['size_factors'].values[:,None]; sc.pp.log1p(adata); adata.X = sp.sparse.csr_matrix(adata.X); adata.raw = adata. sc.pp.highly_variable_genes(adata, flavor = 'cell_ranger', n_top_genes = 2000); sc.pp.pca(adata, n_comps = 50, use_highly_variable = True, svd_solver = 'arpack'); sc.pp.neighbors(adata); sc.tl.umap(adata); adata.uns['log1p'] # this produces: {'base': None}; adata.write('test.h5ad'). adata = sc.read('test.h5ad'); adata.uns['log1p'] # the now produces: {}; sc.tl.leiden(adata, key_added='leiden_r1.0'); sc.tl.rank_genes_groups(adata, groupby='leiden_r1.0', key_added='rank_genes_all', method='wilcoxon') # this causes an error; ```. ```pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); Input In [13], in <cell line: 2>(); 1 # Calculate marker genes; ----> 2 sc.tl.rank_genes_groups(adata, groupby='leiden_r1.0', key_added='rank_genes_all', method='wilcoxon', use_raw=F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2181:2690,layers,layers,2690,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2181,1,['layers'],['layers']
Modifiability,"y-variable and then breaking ties with median-rank-in-batches (this is described in [Stuart et al. 2019](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf), and implemented in Seurat's [`SelectIntegrationFeatures`](https://satijalab.org/seurat/reference/selectintegrationfeatures)*).; - OR by sorting first by median-rank-in-batches and breaking ties with number-of-batches-in-which-genes-are-highly-variable (this is how `""seurat_v3""` in scanpy is currently implemented); ; causing quite some discrepancy in the results. *I am not an R expert, so this might not be correct: Digging into the code of `SelectIntegrationFeatures`, I suspect the genes _above_ a treshold level of batches in which they are HVGs are [ordered by their median rank](https://github.com/satijalab/seurat/blob/41d19a8a55350bff444340d6ae7d7e03417d4173/R/integration.R#L2988), in contrary to the textual description in Stuart et al.; and only the genes displaying this threshold of number of batches in which they are highly variable are ranked by their median rank - to decide which are kept as highly variable. This would have an effect on the ordering of the very top genes, but NOT on the actual genes which are selected by `SelectIntegrationFeatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:1907,variab,variable,1907,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,2,['variab'],['variable']
Modifiability,"y_variable""] (regardless of the value of the batch_key option), using adata.var[""highly_variable_intersection""] for filtering is not a good idea. If there is confusion between adata.var[""highly_variable""] and adata.var[""highly_variable_intersection""]:. If the user specifies n_top_genes, adata.var[""highly_variable""] contains top variable genes in the list of genes sorted by number of batches they are detected as variable (ties broken using dispersion). If mean/dispersion filters are provided, we apply these cutoffs to mean mean/dispersion across batches to construct a unified adata.var[""highly_variable""]. adata.var[""highly_variable_intersection""] is a very strict definition that I personally avoid using at all, but it also depends on the experimental setting and batch_key itself. Therefore, there is a mistake in the following code:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata_hvg = adata[:, adata.var.highly_variable_intersection].copy(); sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. This possibly removes many genes that are identified as highly variable in adata.var.highly_variable because adata_hvg = adata[:, adata.var.highly_variable_intersection] keeps only a subset of highly variable genes (see the definitions above). If one wants to use the strict definition, correct usage would be:. ```python; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=10, min_disp=0.1, batch_key=""source""); adata.var.highly_variable = adata.var.highly_variable_intersection; sc.tl.pca(adata_hvg, svd_solver='arpack', n_comps = 30, use_highly_variable=True) # both the default None and True will error; see below; ```. which is what @LuckyMD proposes, IIUC. I think what we should do here is to print a more informative error in PCA, smt like `HVGs identified by sc.pp.highly_variable_genes cannot be found in adata.`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607:1452,variab,variable,1452,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1032#issuecomment-616740607,2,['variab'],['variable']
Modifiability,"ython3.7/site-packages (from matplotlib==3.0.*->scanpy) (0.10.0); Requirement already satisfied: pytz>=2017.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from pandas>=0.21->scanpy) (2019.3); Requirement already satisfied: get-version>=2.0.4 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (2.1); Requirement already satisfied: setuptools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from legacy-api-wrap->scanpy) (42.0.2.post20191203); Requirement already satisfied: numexpr>=2.6.2 in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from tables->scanpy) (2.7.0); Requirement already satisfied: more-itertools in /home/tsundoku/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.7; python_version < ""3.8""->scanpy) (7.2.0); ```. ```; conda install -c bioconda scanpy; ```. ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: |; /; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Package pandas conflicts for:; scanpy -> pandas[version='>=0.21']; Package tqdm conflicts for:; scanpy -> tqdm; Package setuptools conflicts for:; scanpy -> setuptools; Package patsy conflicts for:; scanpy -> patsy; Package seaborn conflicts for:; scanpy -> seaborn; Package pytables conflicts for:; scanpy -> pytables; Package umap-learn conflicts for:; scanpy -> umap-learn[version='>=0.3.0']; Package networkx conflicts for:; scanpy -> networkx; Package readline conflicts for:; python=3.7 -> r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:10953,flexible,flexible,10953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['flexible'],['flexible']
Modifiability,"ython; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns; import anndata; import matplotlib as mpl; import scipy. sc.logging.print_versions(); # scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.3 scipy==1.3.1 ; # pandas==0.25.3 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. sp = sc.datasets.pbmc3k(); sc.pp.normalize_total(sp,target_sum=1e6,key_added='norm_factor'); sc.pp.log1p(sp); sp.raw=sp; sc.pp.highly_variable_genes(sp, n_top_genes=2000); sc.pl.highly_variable_genes(sp); sp = sp[:, sp.var['highly_variable']]; sc.pp.scale(sp, max_value=10); sc.tl.pca(sp, svd_solver='arpack'); sc.pl.pca_variance_ratio(sp, log=True); sc.pp.neighbors(sp, n_neighbors=10, n_pcs=30); sc.tl.diffmap(sp); sc.pp.neighbors(sp, n_neighbors=20, use_rep='X_diffmap'); sc.tl.louvain(sp,resolution=1); sc.tl.paga(sp); _, axs = plt.subplots(ncols=1, figsize=(24, 10), gridspec_kw={'wspace': 0.05, 'left': 0.12}); # Modified this call because pos_coord wasn't defined:; # sc.pl.paga(sp,color='louvain',layout='fa',pos=pos_coord,threshold=0.2,ax=axs) ; sc.pl.paga(sp,color='louvain',layout='fa',threshold=0.2,ax=axs); from scanpy.tools._utils import get_init_pos_from_paga as init; sc.tl.umap(sp,init_pos=init(sp)); sc.pl.umap(sp,color='louvain'); ```. The final plot looks normal enough:. ![image](https://user-images.githubusercontent.com/8238804/69206364-8c9d1880-0ba0-11ea-8180-3bbd0b8c825e.png). Right now, there are a lot of variables in this script. There's a few things to try:. * Check if `pos_coord` is causing the issue; * I noticed your scanpy version wasn't the same as the current release, could you update that?; * If you run the script with the dataset I used, does your plot still have those strange rectangular layouts?; * Can you cut down the number of commands you used, and potentially even the amount of data? This will limit the number of variables that could be causing the behavior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868:1766,variab,variables,1766,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/918#issuecomment-555819868,2,['variab'],['variables']
Modifiability,"| 297 |; | Updated | 14.91 |; | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python; import time; import numpy as np. import pandas as pd. import scanpy as sc; from sklearn.cluster import KMeans. import os; import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '); warnings.simplefilter('ignore'); input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):; print('Downloading import file...'); wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes; MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out; markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells; min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed; max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes; min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this; n_top_genes = 4000 # Number of highly variable genes to retain. # PCA; n_components = 50 # Number of principal components to compute. # t-SNE; tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means; k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs; sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(); tr=time.time(); adata = sc.read(input_file); adata.var_names_make_unique(); adata.shape; print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(); # To reduce the number of cells:; USE_FIRST_N_CELLS = 1300000; adata = adata[0:USE_FIRST_N_CELLS]; adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell); sc.pp.filter_cells(adata, max_genes=max_genes_per_cell); sc.pp.filter_genes(adata, min_cells=min_cells_per_gene); sc.pp.normalize_total(adata, target_sum=1e4); print(""Total filter and normalize time ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3284:1390,variab,variable,1390,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284,1,['variab'],['variable']
Modifiability,"⠀⠂⠠⢠⡁⡄⡌⠀⠀⠠⢅⠀⠄⠀⢕⢐⠀⠄⡂⢀⠂⠀⠂⠈⡸⠂; ⠀⠀⠀⢐⡂⠀⢀⠐⠀⠰⡀⠑⡀⠀⠠⠀⠐⢀⠈⠆⠤⠄⢀⠀⣀⠢⡀⠀; ⠂⢀⢪⢘⠀⢀⠩⠅⢄⠄⠠⠠⠐⠀⠀⢀⠠⠂⠀⠁⡘⠀⠀⠐⠢⡐⠀⠀; ⢀⠌⡘⠘⠂⠄⢀⠀⢠⠔⠈⢀⠈⠀⠀⠠⡀⡂⠄⢀⠀⠀⠀⠁⠔⢈⢰⠀; ⠁⠐⡀⡠⠀⠐⠠⠈⠀⢀⠀⠘⠂⠀⠀⠀⠐⠰⠄⡡⠠⡀⠀⠀⠂⠠⠁⠐; ```. While this is one with blocks along the diagonal:. ```; ⠿⣧⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠿⠿⠿⠿⣧⣤⣤⣤⣤⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠛⠛⠛⠛⠛⠛⣤⣤⣤⣤⣤⣤⣤⣤⣤⣤⡄⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠿⠿⠿⠿⠿⠿⠿⠿⠿⠿⣧⣤⠀⠀⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣶⣶⠀⠀⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠛⢻⣶⡆⠀; ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⣿⣿; ```. When you have blocks of dense values, you can just store those dense blocks as regular arrays along with offsets. > but can't you subset sparse matrices based on masks? Should be fairly easy to just skip indices that are not in the mask. Yes, this should be fine. The issue I was thinking of is more when you want to do something like `scale`-ing your expression. > Or mito/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc. > In this case you might need these genes also during an analysis pipeline (and not just for data storage), so you would like to have them in a separate ""raw"" container that is otherwise not touched. If don't want them to be used as features for any analyses on `X`, they could be stored in `obsm`. If you want to use them for some analyses, (like DE), then they can just be masked out for others. > I would be a bit hesitant to not have a replacement for .raw. I think `layers` satisfies this. It just doesn't allow you to have a different set of variables (that is, not just a subset) for DE than the rest of the object has. But, having the different set of variables is what makes `raw` difficult to work with. > introduce a new .frozenraw or sth like that where just the raw data is stored and it's essentially read-only after assignment?. I'd note that `.raw` is already supposed to be read-only.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472:4201,layers,layers,4201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-820902472,3,"['layers', 'variab']","['layers', 'variables']"
Modifiability,"我同样面临着这个 bug; 我的代码是; ```python; #genes_to_plot = ['Blvrb','Klf1','Serpina3f','Coro1a','Napsa','Ly6c2']; genes_to_plot = ['Blvrb',#MEP marker; 'Klf1',#MEP marker; 'Serpina3f']#CMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); genes_to_plot = ['Coro1a',#CMP and GMP marker; 'Napsa',#GMP marker; 'Ly6c2']#GMP marker; sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); ```; 错误信息：; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-13-61edd8063c25> in <module>(); 3 'Klf1',#MEP marker; 4 'Serpina3f']#CMP marker; ----> 5 sc.pl.scatter(adata,'cd34_log','fcgr_log',color=genes_to_plot,color_map='coolwarm'); 6 genes_to_plot = ['Coro1a',#CMP and GMP marker; 7 'Napsa',#GMP marker. ~/anaconda3/lib/python3.7/site-packages/scanpy/plotting/_anndata.py in scatter(adata, x, y, color, use_raw, layers, sort_order, alpha, basis, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, legend_fontoutline, color_map, palette, frameon, right_margin, left_margin, size, title, show, save, ax); 124 (x in adata.obs.keys() or x in adata.var.index); 125 and (y in adata.obs.keys() or y in adata.var.index); --> 126 and (color is None or color in adata.obs.keys() or color in adata.var.index); 127 ):; 128 return _scatter_obs(**args). ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in __contains__(self, key); 4069 False; 4070 """"""; -> 4071 hash(key); 4072 try:; 4073 return key in self._engine. TypeError: unhashable type: 'list'; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041:954,layers,layers,954,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1497#issuecomment-729598041,1,['layers'],['layers']
Performance," (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2084,cache,cached,2084,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," 2D DatetimeArray; 1202 result = np.asarray(self._values[key]); -> 1203 disallow_ndim_indexing(result); 1204 return result; 1206 if not isinstance(self.index, MultiIndex):. File ~\AppData\Roaming\Python\Python311\site-packages\pandas\core\indexers\utils.py:341, in disallow_ndim_indexing(result); 333 """"""; 334 Helper function to disallow multi-dimensional indexing on 1D Series/Index.; 335 ; (...); 338 in GH#30588.; 339 """"""; 340 if np.ndim(result) > 1:; --> 341 raise ValueError(; 342 ""Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer ""; 343 ""supported. Convert to a numpy array before indexing instead.""; 344 ). ValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.; ```. ### Versions. <details>. ```-----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; annoy NA; anyio NA; asttokens NA; attr 22.1.0; autograd NA; autograd_gamma NA; babel 2.11.0; backcall 0.2.0; bbknn 1.6.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; brotli NA; certifi 2024.02.02; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.6; entrypoints 0.4; executing 0.8.3; fastjsonschema NA; formulaic 1.0.1; future 0.18.3; gseapy 1.1.2; h5py 3.9.0; hypergeom_ufunc NA; idna 3.4; igraph 0.11.5; interface_meta 1.3.0; invgauss_ufunc NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; lifelines 0.28.0; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.59.1; nume",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3086:6484,bottleneck,bottleneck,6484,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3086,1,['bottleneck'],['bottleneck']
Performance," = IProgress(min=0, max=1); 99 pbar.value = 1. NameError: name 'IProgress' is not defined. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); <ipython-input-5-ec5b1e8cd660> in <module>; ----> 1 sc.datasets.moignard15(). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/datasets/__init__.py in moignard15(); 108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filename, return_ext=True); --> 491 is_present = check_datafile_present_and_download(; 492 filename,; 493 backup_url=backup_url,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in check_datafile_present_and_download(path, backup_url); 745 path.parent.mkdir(parents=True); 746 ; --> 747 download(backup_url, path); 748 return True; 749 . ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in download(url, path); 722 ; 723 path.parent.mkdir(parents=True, exist_ok=True); --> 724 with tqdm(unit='B', unit_scale=True, miniters=1, desc=path.na",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:1509,cache,cache,1509,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['cache'],['cache']
Performance," CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata). # Then run on a machine with 16 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata). # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); print((adata8.uns['neighbor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:2002,cache,cache,2002,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['cache'],['cache']
Performance," File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/python3.12/site-packages/dask/threaded.py"", line 90 in get; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 662 in compute; File ""<venv>/lib/python3.12/site-packages/dask/base.py"", line 376 in compute; File ""~/Dev/scanpy/tests/test_utils.py"", line 243 in test_is_constant_dask; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 159 in pytest_pyfunc_call; File ""<venv>/lib/python3.12/site-packages/pluggy/_callers.py"", line 103 in _multicall; File ""<venv>/lib/python3.12/site-packages/pluggy/_manager.py"", line 120 in _hookexec; File ""<venv>/lib/python3.12/site-packages/pluggy/_hooks.py"", line 513 in __call__; File ""<venv>/lib/python3.12/site-packages/_pytest/python.py"", line 1627 in runtest; File ""<venv>/lib/python3.12/site-packages/_pytest/runner.py"", line 17",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:4389,queue,queue,4389,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['queue'],['queue']
Performance," I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipywidgets 8.1.2; isoduration NA; jedi 0.19.1; jinja2 3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:1073,cache,cache,1073,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['cache'],['cache']
Performance," It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None and max_cells is None else X > 0, axis=0; ^^^^^; TypeError: '>' not supported between instances of 'CSRDataset' and 'int'; ```. ### Versions. <details>. ```; Matplotlib is building the font cache; this may take a moment.; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 10.2.0; colorama 0.4.6; cycler 0.12.1; cython_runtime NA; dateutil 2.8.2; h5py 3.10.0; igraph 0.11.4; joblib 1.3.2; kiwisolver 1.4.5; leidenalg 0.10.2; llvmlite 0.42.0; matplotlib 3.8.3; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numpy 1.26.4; packaging 23.2; pandas 2.2.1; psutil 5.9.8; pyparsing 3.1.1; pytz 2024.1; scipy 1.12.0; session_info 1.0.0; six 1.16.0; sklearn 1.4.1.post1; texttable 1.7.0; threadpoolctl 3.3.0; typing_extensions NA; wcwidth 0.2.13; -----; Python 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]; Linux-5.4.0-165-generic-x86_64-with-glibc2.31; -----; Session information updated at 2024-03-05 10:07; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:2191,cache,cache,2191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['cache'],['cache']
Performance," ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:1605,cache,cache,1605,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,3,['cache'],['cache']
Performance," exists on the main branch of scanpy. ### What happened?. I have a bunch of matrix.mtx, barcode.tsv and genes.tsv. When I want to read in the files with:; `data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:1235,cache,cache,1235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance," f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:562) var_names=var_names,; [563](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:563) make_unique=make_unique,; [564](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:564) cache=cache,; [565](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:565) cache_compression=cache_compression,; [566](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:4367,cache,cache,4367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance," files I get an error (code and error attached).; I've tried it on a file for which this command previously worked (but now it no longer does) as well as trying to merge two loom files, which also gives me this error. So I believe it's to do with a package version rather than the files themselves, but I cannot seem to figure out which packages are responsible and which versions I would need instead.; Any help would be greatly appreciated!. My code:; adata_vel = scv.utils.merge(adata, adatal). This is my error:; <img width=""844"" alt=""Screenshot 2023-03-09 at 11 43 11"" src=""https://user-images.githubusercontent.com/85882727/224000508-fb6a0e9a-c49d-4186-a531-88233f3cfc82.png"">. #### Versions. -----; anndata 0.8.0; scanpy 1.9.3; -----; OpenSSL 20.0.1; PIL 8.2.0; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; anndata2ri 1.0.6; annoy NA; anyio NA; appnope 0.1.2; asttokens NA; astunparse 1.6.3; attr 21.4.0; babel 2.9.0; backcall 0.2.0; backports NA; boto3 1.26.7; botocore 1.29.7; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cryptography 3.4.7; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; debugpy 1.5.1; decorator 5.0.6; dot_parser NA; dunamai 1.6.0; executing 0.8.2; fbpca NA; flatbuffers NA; fsspec 0.7.4; gast 0.5.3; get_version 3.5; google NA; gprofiler 1.0.0; h5py 3.7.0; idna 2.10; igraph 0.10.2; importlib_resources NA; intervaltree NA; ipykernel 6.8.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.2; jmespath 1.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.7.0; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.7.1; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numpy 1.22.0; opt_einsum v3.3.0; packaging 20.9; pandas 1.2.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2443:1311,bottleneck,bottleneck,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2443,1,['bottleneck'],['bottleneck']
Performance," in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2003,cache,cached,2003,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance," intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1667,load,loading,1667,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance," last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1651,cache,cache,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,2,['cache'],['cache']
Performance," loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.rst, tutorials.rst, usage-principles.rst; Error in github_url('scanpy._settings.Scanpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1828,load,loading,1828,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance," make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I am running the Scrublet function to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1021,cache,cache,1021,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance," me. I expected infinity loops and unreachable code, but it turned out to be correct (:. `inf` is there for the sparse case `zero_center=False` and a gene with zero variance but finite mean. Here is the example (slightly modified from the new `tests/test_scaling.py`), with the four cases for genes `(mean==0,mean!=0) x (var==0,var!=0)`:; ```; X = csr_matrix([[-1,2,0,0],[1,2,4,0],[0,2,2,0]]); X = sc.pp.scale(Xtest, copy=True, zero_center=False); X; ```; If `std[std == 0] = eps` (`eps!=0`) is only in the dense path, I get: `array([[-1., inf, 0., 0.], [ 1., inf, 2., 0.], [ 0., inf, 1., 0.]])`; if `std[std == 0] = 1` is before the sparse/dense split, I get: `array([[-1., 2., 0., 0.], [ 1., 2., 2., 0.], [ 0., 2., 1., 0.]])`; if `std[std == 0] = 1e-12` is before the sparse/dense split, I get: `array([[-1., 2.e+12, 0., 0.], [ 1., 2.e+12, 2., 0.], [ 0., 2.e+12, 1., 0.]])`. This suggests, that `0/0` in a sparse setting remains `0` (I guess thats what you see); it makes sense for an efficient sparse matrix implementation, as the `0` is not even represented in the sparse data, so scaling with anything is optimized away. If it were not, it should probably yield `nan` and not `0`.; But if you have something finite with zero variance, you get an explicit `<finite>/0=inf`. [This IS an edge case, and probably never the case in real expression data, but still the behaviour should be consistent and well defined.]; Now, if you have the statement `std[std == 0] = eps` before the sparse/dense split, the `inf` is caught in both cases. The change from `eps=1e-12` to `eps=1` only makes the values keep their original values without zero centering, instead of having these values multiplied by the arbitrary `1e12`. I read the intent for this behaviour into the Note in the docs ""Variables (genes) that do not display any variation (are constant across all observations) are retained"". Setting them to zero makes no sense to me without zero centering. With zero centering setting the values to zero i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221:1282,optimiz,optimized,1282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1160#issuecomment-622613221,1,['optimiz'],['optimized']
Performance," message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {};",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1464,cache,cache,1464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['cache'],['cache']
Performance," or package error. I am getting Segmentation fault error with sc.pp.calculate_qc_metrics with my data. I tried running this on eg. data, and have the same error. I think this might be related to numba issue, but not sure. I did ran python debugger on the script, also attaching the output i got. ; [scanpylog.txt](https://github.com/theislab/scanpy/files/4567012/scanpylog.txt). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; #!/usr/bin/env python; import os, sys ; import scanpy as sc; import scanpy.external as sce; import scipy as sp; import numpy as np; import pandas as pd; os.getcwd(). #sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); #sc.logging.print_versions() #this also give segmentation fault error . #1102 external eg file ; #curl -o pbmc_1k_v2_filtered_feature_bc_matrix.h5 -O http://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v2/pbmc_1k_v2_filtered_feature_bc_matrix.h5. #load file; ext_AD = sc.read_10x_h5('/home/pjb40/scratch/KimCarla_Timeseries_scRNAseq_lung_cancer_organoids_hbc03856_scrathDir/data/TimeSeries_epithelial_ScanpyNotebook/ext_data/pbmc_1k_v2_filtered_feature_bc_matrix.h5', gex_only = True). ext_AD.var_names_make_unique(). print(ext_AD). sc.pp.calculate_qc_metrics(ext_AD, inplace=True). print (ext_AD). ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""timeseriesScanpy.py"", line 65, in <module>; sc.pp.calculate_qc_metrics(ext_AD, inplace=True); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 274, in calculate_qc_metrics; parallel=parallel; File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line 102, in describe_obs; proportions = top_segment_proportions(X, percent_top, parallel); File ""/home/pjb40/jupytervenv/lib/python3.7/site-packages/scanpy/preprocessing/_qc.py"", line",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1193:1090,load,load,1090,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1193,1,['load'],['load']
Performance," parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There is an error ` <class 'inspect._empty'>`; - The rest looks good to me, except for the superficial stylistic remarks above.; ```; Signature: sc.pp.neighbors(adata:anndata.base.AnnData, n_neighbors:int=15, n_pcs:Union[int, NoneType]=None, use_rep:Union[str, NoneTy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:3198,optimiz,optimization,3198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance, py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5795,cache,cached-property,5795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cached-property']
Performance," read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:2449,race condition,race condition,2449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance," same output. The below example suggests that this is not the case. . ### Minimal code sample. ```python; adata_sub = sc.read_h5ad(""your_favourite_object.h5ad""); n_genes = 1491; for i in range(10):; sc.pp.highly_variable_genes(adata_sub, n_top_genes=n_genes). unique_genes = list(adata_sub.var['highly_variable'][adata_sub.var['highly_variable'] == True].index). if i == 0:; all_unique = list(set(unique_genes)); print(f""total {len(all_unique)} unique genes""); else:; all_unique = list(set(all_unique+unique_genes)); print(f""total {len(all_unique)} unique genes""); ```. ### Error output. ```pytb; total 1491 unique genes; total 1814 unique genes; total 2042 unique genes; total 2163 unique genes; total 2237 unique genes; total 2305 unique genes; total 2356 unique genes; total 2401 unique genes; total 2437 unique genes; total 2453 unique genes; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 8.4.0; asciitree NA; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.02.1; dateutil 2.8.1; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; django 4.1.3; entrypoints 0.4; executing 0.8.3; fasteners 0.18; fsspec 2023.4.0; google NA; h5py 3.6.0; igraph 0.10.6; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; jupyter_server 1.13.5; kiwisolver 1.2.0; kneed 0.8.3; leidenalg 0.10.1; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mishalpy NA; mpl_toolkits NA; mpmath 1.2.1; msgpack 1.0.2; natsort 8.3.1; nbinom_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numcodecs 0.11.0; numexpr 2.8.1; numpy 1.21.6; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.3; parso 0.8.3; pickleshare 0.7.5; pkg_resources NA; plotly 5.6.0; prompt_toolkit 3.0.20; psutil 5.8.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrenc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2579:1444,bottleneck,bottleneck,1444,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2579,1,['bottleneck'],['bottleneck']
Performance," sc.pl.dotplot(adata, markers, groupby='bulk_labels', return_fig=True); dp.add_totals(size=1.2)\; .legend(color_title='log(UMI count+1)', width=1.6, show_size_legend=True)\; .style(cmap='Blues', dot_edge_color='black', dot_edge_lw=1, size_exponent=1.5)\; .show(); ```; All objects have consistent functions for `legend`, to set up titles and width, `style()` to set visual parameters specific to each plot like colormap, edge color, linewidth. `swap_axes` to transpose the figure, `add_dendrogram` with options to change the with of the dendrogran and `add_total` tho show a bar plot of the total number of cells per category. Also includes options to sort the categories. ![image](https://user-images.githubusercontent.com/4964309/81702505-a9d5e400-946b-11ea-823b-018f5dadac84.png). Here is description of changes:; **all figures**:; * Set a title to the image. ; * Pass an `axe` where to plot the image.; * Return a dictionary of axes for further manipulation; * using `return_fig` the plot object can be used to adjust the proportions to the legend and other visual aspects can be fine tuned.; * a bar plot with the totals per category can be added. This will align at the left of the image if the categories are plotted in rows or at the top if they are plotted in columns.; * legend can be removed; * `groupby` can be a list of categories. . **dotplot**; * Improved the colorbar and size legend for dotplots. Now the colorbar and size have titles, which can be modified using the `colorbar_title` and `size_title` arguments. They also align at the bottom of the image and do not shrink if the dotplot image is smaller.; * Plot genes in rows and categories in columns (swap_axes).; * Using the DotPlot object the dot_edge_color and line width can be set up, a grid added as well as several other features; * `sc.pl.rank_genes_groups_dotplot` can now plot `pvals` and `log fold changes`. **matrixplot**; * added title for colorbar and positioned as in dotplot; * `sc.pl.rank_genes_groups_matrixplo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1210:2826,tune,tuned,2826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1210,1,['tune'],['tuned']
Performance," scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; with plt.style.context(""seaborn-white""):; fig, ax = plt.subplots(figsize=(8,8)); sc.pl.umap(adata_clr, color=['id_jira'], ax=ax, s=200 ); fig.suptitle(""SOLEXASEQ-1672/1680 | Aitchison Distance [THIS SHOULD BE SUPTITLE]"", fontsize=15); ax.set_title(""SOLEXASEQ-1672/1680 | Aitchison Distance [THIS SHOULD BE TITLE]"", fontsize=15). ```; ![image](https://user-images.githubusercontent.com/9061708/235007925-b7e1c2ae-924c-4fb3-b840-b2d12c15725f.png). #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.3; -----; Bio 1.79; PIL 9.0.1; PyQt5 NA; adjustText NA; appnope 0.1.2; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; cachecontrol 0.12.10; certifi 2022.12.07; cffi 1.15.0; charset_normalizer 2.0.11; colorama 0.4.4; comm 0.1.1; compositional 2022.8.31; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.4; decorator 5.1.1; defusedxml 0.7.1; ensemble_networkx 2023.1.23; entrypoints 0.4; ete3 3.1.2; executing 0.8.2; fastcluster 1.1.26; fontTools 4.29.1; h5py 3.7.0; hdmedians NA; hive_networkx 2021.05.18; hypergeom_ufunc NA; idna 3.3; igraph 0.10.2; ipykernel 6.19.4; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.23.4; kiwisolver 1.3.2; leidenalg 0.9.1; llvmlite 0.38.0; lxml 4.7.1; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; matplotlib_venn 0.11.6; mpl_toolkits NA; msgpack 1.0.3; natsort 8.1.0; nbinom_ufunc NA; networkx 2.6.3; numba 0.55.1; numpy 1.21.5; packaging 21.3; palettable 3.3.0; pandas 1.4.0; parso 0.8.3; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.3.0; prompt_toolkit 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2477:1147,cache,cachecontrol,1147,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2477,1,['cache'],['cachecontrol']
Performance, scipy=1.11.2; - seaborn=0.13.2; - seaborn-base=0.13.2; - send2trash=1.8.2; - session-info=1.0.0; - setuptools=68.1.2; - simplejson=3.19.2; - six=1.16.0; - snappy=1.1.10; - sniffio=1.3.0; - soupsieve=2.3.2.post1; - stack_data=0.6.2; - statsmodels=0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astun,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:14694,cache,cachetools,14694,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cachetools']
Performance," the data (points belonging to one clusters should be connected). Any graph clustering algorithm respects that, even spectral clustering. So, I'm not a big fan of trying 5 clustering algorithms to produce sensible results. Either a given representation of the data clusters clearly or it doesn't. If it doesn't, Louvain clustering just gives you one possible, representative partitioning of the data. But there are many others that are equally meaningful. Similar for other graph clustering algorithms. Now, running Louvain clustering on a fully connected graph is prohibitive computationally (memory and CPU time wise).; > Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary k neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. That's just speculation to me. I never saw convincing benchmarks. No one claims that ""discounting all distance information gives a more accurate result"". It's just that it's computationally cheaper. I acknowledge that a ""non-fixed-degree knn graph"" varying say, between 5 and 100, would be computationally tractable and would carry information about the sampling density of the data in the given representation. This information is only indirectly available in the fixed-degree knn graph (more loops etc. in high-density regions). I never investigated this as I never saw fundamental results on such a non-fixed-degree knn graph. As it's also hard to benchmark this, I'd be afraid of getting into this if one doesn't have the time to get the fundamentals right. I want to note that even in the context of diffusion processes, we managed to obtain meaningful results with kNN graphs in practice. And this c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777:1323,perform,perform,1323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416725777,1,['perform'],['perform']
Performance," to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = No",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1073,cache,cache,1073,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,3,['cache'],['cache']
Performance," var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'hvg', 'log1p', 'pca', 'neighbors', 'umap', 'leiden', 'leiden_colors'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'distances', 'connectivities'. ```. Ive tried to check whether the data is maybe different or something, but i dont see anything that could be causing these differences, could you please help trying to figure out why the leiden clustering suddenly produces different results? . ### Minimal code sample. ```python; sc.pp.neighbors(adata, n_pcs = 30, n_neighbors = 20); sc.tl.umap(adata); sc.tl.leiden(adata, resolution = 0.2) ; sc.pl.umap(adata, color='leiden'); ```. ### Error output. _No response_. ### Versions. <details>. ```; sc.logging.print_versions(); -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 9.4.0; PyQt5 NA; adjustText 1.0.4; asttokens NA; atomicwrites 1.4.1; bottleneck 1.3.5; brotli NA; bs4 4.12.2; certifi 2024.02.02; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.2.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.8; executing 2.0.1; gseapy 1.1.2; h5py 3.9.0; html5lib 1.1; idna 3.4; igraph 0.11.3; ipykernel 6.29.2; jedi 0.19.1; jinja2 3.1.2; joblib 1.3.2; kiwisolver 1.4.4; leidenalg 0.10.2; llvmlite 0.42.0; lxml 5.1.0; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; matplotlib_inline 0.1.6; mkl 2.4.1; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.4; numpy 1.24.3; packaging 23.1; pandas 2.0.3; parso 0.8.3; patsy 0.5.3; pickleshare 0.7.5; platformdirs 3.10.0; prompt_toolkit 3.0.42; psutil 5.9.0; pure_eval 0.2.2; pyarrow 11.0.0; pycparser 2.21; pydeseq2 0.4.7; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pynndescent ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956:2914,bottleneck,bottleneck,2914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956,1,['bottleneck'],['bottleneck']
Performance," we even want relative expression counts?; 2. What assumptions do downstream methods have on the distribution of expression values. For the first question: relative gene expression values ignore differences in cell sizes/number of molecules in the cell. There are some molecules whose numbers scale with the size of the cell, and others that don't (e.g., many housekeeping genes). Choosing relative over absolute expression values to compare gene expression across cells would be helpful to compare expression of those genes that scale with size, but not the others.... so there's not really a perfect answer here. Thus, removing all effects of total counts may not be the desirable outcome. Secondly, many downstream methods assume normally distributed expression data (e.g., DE methods like: t-tests, limma, MAST, or several batch correction/data integration methods). Log transformation is used as a variance stabilization to approximate a normal distribution (quite often poorly, but better than without). This leads to many methods performing better with log transformation. IMO, the ideal approach is probably something like scVI, GLMPCA, or scTransform, where you fit a model directly to the count data and use the residuals to describe the data. This would address both steps of normalization and variance stabilization at the same time. If we have a good model to describe the data, the residuals should quantify the biological variance + normally distributed noise. Overall, I would use other normalization approaches than CPM, and use log-transformation with anything that uses size factors that scale per-cell expression values. . Note also that the effect described in the second paper you mention (from Aaron Lun) will mainly be relevant when you have biased distributions of sequencing depth between two samples that you are comparing. If the size factors are similarly distributed between both conditions, then the DE effect will not be so dramatic (as far as I understood it anyway).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643:1626,perform,performing,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-678119643,1,['perform'],['performing']
Performance," when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1041,cache,cache,1041,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['cache'],['cache']
Performance,![Screenshot from 2024-04-04 12-00-02](https://github.com/scverse/scanpy/assets/37635888/adfce8bd-34ac-44c7-9530-43b3a73577e8). I have some concerns about the performance of the no numba version for larger datasets. So it might be better to either switch to the numba kernel for larger datasets or take the compile hit for small datasets,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2036739880:159,perform,performance,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2036739880,1,['perform'],['performance']
Performance,"## Feature type; <!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. ## Request; <!-- Please describe your wishes below: -->; The `scanpy.pp.subsample` function has been really useful for iteration, experimentation, and tutorials. However, I've found that sometimes I don't want to (or simply can't) read the entire `AnnData` object into memory before subsampling. As such, it would be nice to instantiate an `AnnData` object in backed mode, then subsample that directly. ### Current behavior:. ```python; import scanpy as sc. adata = sc.read_h5ad(""matrix/scatlas.h5ad"", backed=""r""); adata_sample = sc.pp.subsample(adata, fraction=0.01, copy=True); ```; yields:; ```; ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`.; ```; ### Workaround:; A workaround I've found is by creating a boolean array and using boolean indexing to grab a random subset of rows from the `AnnData` object:. ```python; import numpy as np. FRACTION = 0.01. np.random.seed(0); random_bool = np.random.choice(; [True, False], size=adata.shape[0], p=[FRACTION, 1 - FRACTION]; ). adata_sample = adata[random_bool, :]; adata_sample_mem = adata_sample.to_memory(); ```. It's easy enough but would be nice to have within `scanpy` itself!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2495:1151,load,load,1151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2495,1,['load'],['load']
Performance,"### Minimal code sample (that we can copy&paste without having any data). when I follow your tutorial on https://nbisweden.github.io/workshop-scRNAseq/labs/compiled/scanpy/scanpy_03_integration.html; to the ; ```python; cdata = sc.external.pp.mnn_correct(alldata['covid_1'],alldata['covid_15'],alldata['covid_17'],; alldata['ctrl_5'],alldata['ctrl_13'],alldata['ctrl_14'], ; svd_dim = 50, batch_key = 'sample', save_raw = True, var_subset = var_genes); ```; line, then it said; `[1] 1764091 illegal hardware instruction (core dumped`. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 8.2.0; bottleneck 1.3.2; cffi 1.14.5; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; fsspec 0.9.0; h5py 2.10.0; igraph 0.9.8; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.36.0; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 8.0.0; numba 0.53.1; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; pkg_resources NA; psutil 5.8.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; six 1.15.0; sklearn 0.24.1; sphinxcontrib NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 5.4.1; zope NA; -----; Python 3.8.8 (default, Apr 13 2021, 19:58:26) [GCC 7.3.0]; Linux-5.11.0-38-generic-x86_64-with-glibc2.10; 64 logical CPU cores; -----. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2044:628,bottleneck,bottleneck,628,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2044,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. (extracted from #3167). Utilizing the murine hematopoietic progenitors from [Nestorowa et al., 2016](https://doi.org/10.1182/blood-2016-05-716480), as well as the regev_lab_cell_cycle_genes.txt, one issue is apparent. Currently the code doesn’t produce the expected number of bins of equal or approximately equal size. Bin 24 is empty when n_bins = 25.; ![current_hist](https://github.com/user-attachments/assets/35e5d1a4-fdd0-406e-a2f9-1b53efacc8fa). ### The current ranking system code within score_genes(); ```py; n_items = int(np.round(len(obs_avg) / (n_bins - 1))); obs_cut = obs_avg.rank(method=""min"") // n_items; ```. ### The modified code in #3167; ```py; obs_avg.sort_values(ascending=True, inplace=True); n_items = int(np.ceil(len(obs_avg) / (n_bins))); rank = np.repeat(np.arange(n_bins), n_items)[:len(obs_avg)]; obs_cut = pd.Series(rank, index=obs_avg.index); ```. The modified code performs as expected producing 25 bins containing approximately equal number of genes. The last bin can have up to 24 less than expected because the total number of genes is not perfectly divisible by 25.; ![modified_hist](https://github.com/user-attachments/assets/6ced8eec-b56b-4642-a33c-71acaee98369). ### Minimal code sample. ```python; TODO; ```. ### Error output. _No response_. ### Versions. <details>. ```; Python 3.10.12 ; scanpy==1.10.2 anndata==0.10.8 umap==0.5.6 numpy==1.26.4 scipy==1.14.0 pandas==2.2.2 scikit-learn==1.5.0 statsmodels==0.14.2 igraph==0.11.6 louvain==0.8.2 pynndescent==0.5.13; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3168:1185,perform,performs,1185,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3168,1,['perform'],['performs']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi!. I am new to scanpy and I am facing some trouble reading my data in an appropriate way. I noticed that anndata objects in memory require roughly 4x the space they require on disk, so working with large datasets (>50GB on disk) is prohibitive in most scenarios. The USP of h5 files, however, is that you can index and slice them on disk as if they were in memory. This way I could greatly reduce the data size before loading it into memory. However, when I attempt to filter on a backed anndata object, I encounter a TypeError. The case of gene filtering should be just a column-sum, comparing it against a threshold and then saving it as a boolean index mask. It seems like the case that the data is backed and not in memory - which should be the default when dealing with h5 files - is not considered in the scanpy API. Am I simply missing something here?. ### Minimal code sample. ```python; from urllib.request import urlretrieve; import scanpy as sc. # We are downloading a small dataset here, 43MB. url = ""https://datasets.cellxgene.cziscience.com/7fb8b010-50bd-4238-a466-7c598f16d061.h5ad""; filename = ""testfile.h5ad"". urlretrieve(url, filename). adata = sc.read_h5ad(filename, backed=""r+""). sc.pp.filter_genes(adata, min_cells=100); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""/home/ubuntu/test_scanpy.py"", line 11, in <module>; sc.pp.filter_genes(adata, min_cells=100); File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 237, in filter_genes; filter_genes(; File ""/mnt/storage/anaconda3/envs/scanpy/lib/python3.12/site-packages/scanpy/preprocessing/_simple.py"", line 258, in filter_genes; X if min_cells is None ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2894:709,load,loading,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2894,1,['load'],['loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I recently started using scanpy and python. I am running into the following error. Any help is much appreciated. ![image](https://github.com/user-attachments/assets/76e1c619-a764-45ac-a475-4bad342854b1). ### Minimal code sample. ```python; sc.pl.umap(adata,color =[""leiden""]); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.6; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; get_annotations NA; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 1.5.3; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215:754,bottleneck,bottleneck,754,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I have a bunch of matrix.mtx, barcode.tsv and genes.tsv. When I want to read in the files with:; `data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True)`. I get the following error:; `FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'`. The thing is that the file exist here:; ![kép](https://github.com/user-attachments/assets/a3ee8f51-833d-4adb-ab9f-f6ff5b19387f). I have changed the *genes.tsv.gz file's name to *features.tsv.gz but still got the same error. Here is the full error log:; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], [line 1](vscode-notebook-cell:?execution_count=62&line=1); ----> [1](vscode-notebook-cell:?execution_count=62&line=1) data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); [2](vscode-notebook-cell:?execution_count=62&line=2) data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); [77](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:77) @wraps(fn); [78](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:78) def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; [79](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/Pip",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:478,cache,cache,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. In #3048 we started raising errors for functions that don’t support backed mode, but seems like a tutorial used `dendrogram` in backed mode: https://scverse-tutorials.readthedocs.io/en/latest/notebooks/scverse_data_backed.html. ![grafik](https://github.com/user-attachments/assets/0317c570-0af8-4c5e-8f3f-7831335763af). That was probably a mistake and the data just got loaded to memory, but since `dendrogram` can be reimplemented using `.get.aggregate`, we should do that!. ### Minimal code sample. ```python; import scanpy as sc. adata = sc.datasets.pbmc3k(); adata.filename = ""test.h5ad""; sc.pl.dotplot(adata, [""FCN1""], groupby=""index"", dendrogram=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NotImplementedError Traceback (most recent call last); Cell In[44], line 1; ----> 1 sc.pl.dotplot(mdata[""rna""], var_names=[""CD2""], groupby=""leiden"", figsize=(10, 3), dendrogram=True, swap_axes=True). File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~/.local/share/hatch/env/virtual/scverse-tutorials/_YRPCeuX/basic-scrna/lib/python3.12/site-packages/scanpy/plotting/_dotplot.py:1046, in dotplot(adata, var_names, groupby, use_raw, log, num_categories, expression_cutoff, mean_only_expressed, cmap, dot_max, dot_min, standard_scale",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3199:659,load,loaded,659,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3199,1,['load'],['loaded']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Loading data in backed mode, I get an AxisError when trying to calculate QC metrics. Problem has happened on three different datasets but doesn't happen when I read the data into memory. ### Minimal code sample. ```python; sc.datasets.pbmc3k(); pbmc = sc.read_h5ad('data/pbmc3k_raw.h5ad', backed = 'r+'); pbmc.var['mt'] = pbmc.var_names.str.startswith('MT-'); pbmc.var['ribo'] = pbmc.var_names.str.startswith((""RPS"", ""RPL"")); sc.pp.calculate_qc_metrics(pbmc, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; AxisError Traceback (most recent call last); Cell In[8], line 3; 1 pbmc.var['mt'] = pbmc.var_names.str.startswith('MT-'); 2 pbmc.var['ribo'] = pbmc.var_names.str.startswith((""RPS"", ""RPL"")); ----> 3 sc.pp.calculate_qc_metrics(pbmc, qc_vars=['mt', 'ribo'], percent_top=None, log1p=False, inplace=True). File ~/miniconda3/envs/parse_sepsis/lib/python3.12/site-packages/scanpy/preprocessing/_qc.py:315, in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel); 312 if isinstance(qc_vars, str):; 313 qc_vars = [qc_vars]; --> 315 obs_metrics = describe_obs(; 316 adata,; 317 expr_type=expr_type,; 318 var_type=var_type,; 319 qc_vars=qc_vars,; 320 percent_top=percent_top,; 321 inplace=inplace,; 322 X=X,; 323 log1p=log1p,; 324 ); 325 var_metrics = describe_var(; 326 adata,; 327 expr_type=expr_type,; (...); 331 log1p=log1p,; 332 ); 334 if not inplace:. File ~/miniconda3/envs/parse_sepsis/lib/python3.12/site-packages/scanpy/preprocessing/_qc.py:109, in describe_obs(adata, expr_type, var_type, qc_vars, percent_top, lay",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3004:289,Load,Loading,289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3004,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. When i previously performed leiden clustering on my data, the shape of the UMAP changed, as expected. . However, when i now try to reproduce my results, I suddenly am only able to get the leiden clustering that follows the distribution of the unclustered umap . Unclustered UMAP; ![UMAP_ADvsCT_3-18-2024](https://github.com/scverse/scanpy/assets/127406679/2fed0a0c-b20e-425a-a99f-7b1c61f64242). Clustered UMAP: ; ![UMAP_ADvsCT](https://github.com/scverse/scanpy/assets/127406679/a1272099-fde4-4025-ad61-392d1342cd0c). the dataset with which i produced the clustered UMAP: ; ```; adata3; Out[505]: ; AnnData object with n_obs × n_vars = 13243 × 10850; obs: 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'sample', 'group', 'disease_status', 'leiden'; var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'disease_status_colors', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'sample_colors', 'umap'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'; obsp: 'connectivities', 'distances'; ```. the dataset with which i produced the unclustered UMAP: ; ```; adata; Out[518]: ; AnnData object with n_obs × n_vars = 13243 × 10850; obs: 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'pct_counts_hb', 'percent_mt2', 'n_counts', 'sample', 'group', 'disease_status', 'leiden'; var: 'gene_ids', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'pct_dro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956:307,perform,performed,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956,1,['perform'],['performed']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I just copied the code of official example, and changed the path to my own documents. But it seems that it doesn't work. One folder of the total containing ""barcodes"", ""features"" and ""matrix"" has been attached below and the entire raw data comes from here: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139324. ### Minimal code sample. ```python; import scanpy as sc. path = r'RAW'; adata = sc.read_10x_mtx('RAW/HD PBMC_1/',var_names='gene_symbols', cache=True) ; adata.var_names_make_unique(); sc.pp.filter_cells(adata, min_genes=200) ; sc.pp.filter_genes(adata, min_cells=3); ```. ### Error output. ```pytb; IndexError: index (2444) out of range; ```. ### Versions. <details>. ```. ```. </details>. [HD PBMC_1.zip](https://github.com/scverse/scanpy/files/13404294/HD.PBMC_1.zip); ![2](https://github.com/scverse/scanpy/assets/147734739/0db8909d-84e4-41c5-8beb-09480d7adc3a)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2759:749,cache,cache,749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2759,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I tried to import scanpy and got an error. ### Minimal code sample. ```python; import scanpy as sc; ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""C:\Users\zacha\PycharmProjects\CellAssign\pipeline.py"", line 2, in <module>; import scanpy as sc; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\__init__.py"", line 6, in <module>; from ._utils import check_versions; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\scanpy\_utils\__init__.py"", line 21, in <module>; from anndata import AnnData, __version__ as anndata_version; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\__init__.py"", line 7, in <module>; from ._core.anndata import AnnData; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\anndata\_core\anndata.py"", line 17, in <module>; import h5py; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\__init__.py"", line 33, in <module>; from . import version; File ""C:\Users\zacha\.conda\envs\CellAssign\lib\site-packages\h5py\version.py"", line 15, in <module>; from . import h5 as _h5; File ""h5py\h5.pyx"", line 1, in init h5py.h5; ImportError: DLL load failed while importing defs: The specified procedure could not be found.; ```. ### Versions. <details>. ```. ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2542:1456,load,load,1456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2542,1,['load'],['load']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I'm trying to use `sc.pl.spatial` with the dataset that is available on 10X Visium with the sample ID `CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma`. I can open and do some basic QC just fine, but when I try to plot, I get the error `TypeError: can't multiply sequence by non-int of type 'float`. ### Minimal code sample. ```python; import scanpy as sc; import anndata as an; import pandas as pd; import numpy as np; import matplotlib as mpl; import matplotlib.pyplot as plt; import seaborn as sns; import scanorama. sc.set_figure_params(facecolor=""white"", figsize=(8, 8)); sc.settings.verbosity = 3. # Loading dataset; adata = sc.read_visium(; path=r""\external"",; count_file=""CytAssist_FFPE_Human_Lung_Squamous_Cell_Carcinoma_filtered_feature_bc_matrix.h5"",; load_images=True,; source_image_path=r""\spatial"",; ). adata.var_names_make_unique(). # Quality control; adata.var[""mito""] = adata.var_names.str.startswith(""MT-""); sc.pp.calculate_qc_metrics(; adata, qc_vars=[""mito""], percent_top=None, log1p=False, inplace=True; ); sc.pl.spatial(adata); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""\scanpy\plotting\_tools\scatterplots.py"", line 1002, in spatial; File ""\plotting\_tools\scatterplots.py"", line 391, in embedding; # if user did not set alpha, set alpha to 0.7; File ""\scanpy\plotting\_utils.py"", line 1107, in circles; if scale_factor != 1.0:; TypeError: can't multiply sequence by non-int of type 'float'; ```; The json file on the spatial folder with the scale factors is as follows:. ```json; {; ""regist_target_img_scalef"": 0.16836435,; ""tissue_hires_scalef"": 0.056121446,; ""tissue_lowres_scalef"": 0.016836435,; ""fiducial_diamet",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2778:899,Load,Loading,899,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2778,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:327,load,load,327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,3,"['cache', 'load']","['cache', 'load']"
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Installation using pip & installation from github repository. ### Minimal code sample. ```python; pip install scanpy; ```. ### Error output. ```pytb; Installing collected packages: tbb, distlib, asciitree, stdlib-list, setuptools-scm, pbr, numcodecs, nodeenv, natsort, igraph, identify, filelock, fasteners, docutils, cfgv, array-api-compat, accessible-pygments, zarr, virtualenv, sphinx, session-info, pytest-nunit, pytest-mock, profimp, mdit-py-plugins, leidenalg, sphinxext-opengraph, sphinx-design, sphinx-copybutton, sphinx-autodoc-typehints, scanpydoc, pynndescent, pydata-sphinx-theme, pre-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryorn",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:957,cache,cache,957,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. RGBA colors that are provided as tuples or lists in `adata.uns[f""{key}_colors""]` are converted to numpy arrays when saving and loading the adata. This leads plots (e.g. `pl.pca` or `pl.umap`) to run into an error (see below). ### Minimal code sample. ```python; adata = sc.datasets.pbmc3k_processed(); adata = adata[adata.obs[""louvain""].isin([""Dendritic cells"",""Megakaryocytes""])].copy(); adata.uns[""louvain_colors""] = [[1, 0, 0, 1], [0, 0, 1, 1]] # or [(1, 0, 0, 1), (0, 0, 1, 1)]; sc.pl.pca(adata, color=""louvain""). adata.write(""./data/pbmc3k_processed_rgba.h5ad""). adata = sc.read(""./data/pbmc3k_processed_rgba.h5ad""); print(type(adata.uns[""louvain_colors""][0])) # --> numpy.ndarray; sc.pl.pca(adata, color=""louvain""); ```. ### Error output. ```pytb; ValueError Traceback (most recent call last); ...; 8 adata = sc.read(""./data/pbmc3k_processed_rgba.h5ad""); 9 print(type(adata.uns[""louvain_colors""][0])) # --> numpy.ndarray; ---> 10 sc.pl.pca(adata, color=""louvain""). File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:893, in pca(adata, annotate_var_explained, show, return_fig, save, **kwargs); 845 """"""\; 846 Scatter plot in PCA coordinates.; 847 ; (...); 890 pp.pca; 891 """"""; 892 if not annotate_var_explained:; --> 893 return embedding(; 894 adata, 'pca', show=show, return_fig=return_fig, save=save, **kwargs; 895 ); 896 else:; 897 if 'pca' not in adata.obsm.keys() and 'X_pca' not in adata.obsm.keys():. File ~/opt/anaconda3/envs/sc_test/lib/python3.11/site-packages/scanpy/plotting/_tools/scatterplots.py:267, in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, neighbors_key, arrows, arrows_kwds, g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2730:418,load,loading,418,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2730,1,['load'],['loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydev",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:965,cache,cache,965,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,2,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. When running phenograph from the external module, and specifying `method='leiden'`, the output says that: . Running Louvain modularity optimization ; After 1 runs, maximum modularity is Q = 0.794615; After 16 runs, maximum modularity is Q = 0.796318; Louvain completed 36 runs in 2.867233991622925 seconds. `Louvain` in the above example should be changed to `Leiden`. . ### Minimal code sample. ```python; from scanpy import external. df = pd.DataFrame(np.zeros(1000,10)); phenograph = external.tl.phenograph ; cluster_ph = phenograph(df.values, k=60, method='leiden')[0]; ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.4; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; executing 1.0.0; functions NA; google NA; h5py 3.9.0; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 8.0.2; jedi 0.18.1; jinja2 3.1.2; joblib 1.3.2; jupyter_server 1.18.1; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.40.1rc1; markupsafe 2.1.1; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numpy 1.24.4; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; plotly 5.14.1; prompt_toolkit 3.0.31; psutil 5.9.2; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 9.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.2.1; scipy 1.11.2; seaborn 0.12.2; session_info 1.0.0; six 1.16.0; sklearn 1.3.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2653:426,optimiz,optimization,426,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2653,1,['optimiz'],['optimization']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. According to the `pp.neighbors()` [docs we have](https://github.com/scverse/scanpy/blob/4642cf8e2e51b257371792cb4fcb9611c0a81123/scanpy/neighbors/__init__.py#L96):; ```; knn; If `True`, use a hard threshold to restrict the number of neighbors to; `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian; Kernel to assign low weights to neighbors more distant than the; `n_neighbors` nearest neighbor.; ```. However, the adjacency represented by `adata.uns['neighbors']['connectivities_key']` shows many more neighbors than `n_neighbors` when `knn=True`. ### Minimal code sample. ```python; import urllib.request; import scanpy as sc. # load the data; h5_data = ""https://datasets.cellxgene.cziscience.com/6ff309fa-e9f6-405d-b24e-3c35528f154e.h5ad""; urllib.request.urlretrieve(h5_data, ""/tmp/data.h5ad"") ; adata = sc.read_h5ad(""/tmp/data.h5ad""). # compute the adjacency thresholded at k=10; k=10; sc.pp.neighbors(adata, n_neighbors=k, n_pcs=40, random_state=42,knn=True); adjacency = (adata.obsp[adata.uns['neighbors']['connectivities_key']].todense() > 0).astype(np.int32); print(f""adjacency matrix (k={k}) shape: {adjacency.shape}""). # check to see if we got a threshold; max_neighbors = np.max(adjacency.sum(axis=0)); print(f""Max neighbors={max_neighbors}""); ```. ### Error output. ```pytb; adjacency matrix (k=10) shape: (1011, 1011); Max neighbors=91; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.9.8; -----; Bio 1.83; MOODS NA; PIL 10.2.0; absl NA; anyio NA; argcomplete NA; arrow 1.3.0; asttokens NA; astunparse 1.6.3; attr 23.2.0; attrs 23.2.0; babel 2.14.0; biothings_client 0.3.1; bpnetlite 0.6.0; cattr NA; cattrs NA; certifi 2024.02.02; cffi 1.16.0; charset_normal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3014:941,load,load,941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3014,1,['load'],['load']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hi! I am not sure if this is a bug... ; Every time I rescale the `pbmc3k_processed` matrix using it as input in the scanpy `sc.pp.scale` function, I get a very slightly different matrix in output, enough to generate a different UMAP with each run. But if I rewrite it using numpy in a simple function called `my_scale_function` it outputs the exact same matrix as the input, generating the same UMAP down the line... Could someone explain to me what is happening?; (Note: The matrix is not sparse). ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; def my_scale_function(X, clip=False):; mean, var = mean_var(X, axis=0); X -= mean; std = np.sqrt(var); std[std == 0] = 1; X /= std; if clip:; X = np.clip(X, -10, 10); return np.matrix(X). ### Scanpy scale vs my_scale_function; mtx = adata.X; from scipy.sparse import issparse; print(""mtx is parse="" + str(issparse(np.matrix(mtx))) + ""\n""); print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(mtx); print((mtx == mtx_rescaled).all()); print(""Rescaled with scanpy:""); mtx_rescaled = sc.pp.scale(mtx, zero_center=True, max_value=None, copy=True); print(str((np.matrix(mtx) == mtx_rescaled).all()) + ""\n""); print(""\nOriginal matrix:""); print(mtx); print(""\nMatrix rescaled with scanpy:""); print(mtx_rescaled); ```. ### Error ou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629:871,Load,Loading,871,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629,1,['Load'],['Loading']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The test suite keeps failing with a segfault on the `python=3.9` build. I haven't been able to reproduce locally. Interestingly, I haven't seen it error when I rerun the check. It looks like this always happens during the call to `nn_approx`. ### Minimal code sample. ```python; NA; ```. ### Error output. ```pytb; platform linux -- Python 3.9.18, pytest-8.0.1, pluggy-1.4.0 -- /opt/hostedtoolcache/Python/3.9.18/x64/bin/python; cachedir: .pytest_cache; rootdir: /home/vsts/work/1/s; configfile: pyproject.toml; testpaths: scanpy; plugins: nunit-1.0.6, mock-3.12.0; [1mcollecting ... [0mcollected 1474 items. scanpy/_utils/compute/is_constant.py::scanpy._utils.compute.is_constant.is_constant [32mPASSED[0m[32m [ 0%][0m; scanpy/datasets/_ebi_expression_atlas.py::scanpy.datasets._ebi_expression_atlas.ebi_expression_atlas [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pl.py::scanpy.external.pl.phate [33mSKIPPED[0m (needs modul...)[32m [ 0%][0m; scanpy/external/pp/_bbknn.py::scanpy.external.pp._bbknn.bbknn [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_harmony_integrate.py::scanpy.external.pp._harmony_integrate.harmony_integrate [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_hashsolo.py::scanpy.external.pp._hashsolo.hashsolo [33mSKIPPED[0m[32m [ 0%][0m; scanpy/external/pp/_magic.py::scanpy.external.pp._magic.magic [32mPASSED[0m[32m [ 0%][0m; scanpy/external/pp/_scanorama_integrate.py::scanpy.external.pp._scanorama_integrate.scanorama_integrate Fatal Python error: Illegal instruction. Thread 0x00007f00347c4640 (most recent call first):; File ""/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/threading.py"", line 316 in wait; File ""/opt/hostedtoolcache/Python/3.9.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2866:720,cache,cachedir,720,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2866,1,['cache'],['cachedir']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Tried to run this function:; sc.tl.leiden(test, resolution = 0.1, restrict_to = ('leiden', ['5'])). and instead it is subsetting cluster 5 into over 400 new subsets, even with my resolution set to 0.1. I've also tried different resolutions and none of them work, it ignores the resolution altogether. ; ![leiden](https://github.com/scverse/scanpy/assets/88872118/7fa7114e-d8ae-4e91-94b6-ece1f9505594). ### Minimal code sample. ```python; sc.tl.leiden(test, resolution = 0.1, restrict_to = ('leiden', ['5'])); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.3; scanpy 1.9.6; -----; PIL 10.0.1; appnope 0.1.2; asttokens NA; attr 23.1.0; bottleneck 1.3.5; brotli NA; celltypist 1.6.2; certifi 2023.11.17; cffi 1.16.0; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2022.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; dill 0.3.7; docrep 0.3.2; entrypoints 0.4; exceptiongroup 1.2.0; executing 0.8.3; fsspec 2023.10.0; h5py 3.7.0; idna 3.4; igraph 0.10.8; inflect NA; ipykernel 6.28.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.3; joblib 1.3.2; jupyter_server 1.23.4; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.59.0; numexpr 2.8.7; numpy 1.26.3; omnipath 1.0.8; packaging 23.1; pandas 2.1.4; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prompt_toolkit 3.0.43; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pyda",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2906:964,bottleneck,bottleneck,964,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2906,1,['bottleneck'],['bottleneck']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. `sc.tl.ingest` uses `pkg_version('anndata')`, which errors out using the latest version of `anndata`. ### Minimal code sample. ```python; from scanpy._compat import pkg_version; pkg_version(""anndata""); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); ----> 1 pkg_version(""anndata""). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/scanpy/_compat.py:80, in pkg_version(package); 76 @cache; 77 def pkg_version(package):; 78 from importlib.metadata import version as v; ---> 80 return version.parse(v(package)). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:52, in parse(version); 43 def parse(version: str) -> ""Version"":; 44 """"""Parse the given version string.; 45 ; 46 >>> parse('1.0.dev1'); ref='/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:0'>0</a>;32m (...); 50 :raises InvalidVersion: When the version string is not a valid version.; 51 """"""; ---> 52 return Version(version). File /opt/saturncloud/envs/saturn/lib/python3.9/site-packages/packaging/version.py:195, in Version.__init__(self, version); 184 """"""Initialize a Version object.; 185 ; 186 :param version:; ...; --> 195 match = self._regex.search(version); 196 if not match:; 197 raise InvalidVersion(f""Invalid version: '{version}'""). TypeError: expected string or bytes-like object; ```. ### Versions. <details>. ```; -----; anndata 0.10.6; scanpy 1.10.0; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cairo 1.23.0; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; colorlog NA; comm 0.1.4; cupy 12.2.0; cupy_backends NA; cupyx NA; c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2978:790,cache,cache,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2978,1,['cache'],['cache']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. The exception happened when try to run scanpy `highly_variable_genes` with sparse dataset loaded in backed mode. ### Minimal code sample. ```python; # read backed; adata = anndata.read_h5ad(file_path, backed='r'); X = adata.raw.X if adata.raw is not None else adata.X; # dataset must be sparse there; print(issparse(X[0])); # calculate dispersions; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, inplace=False); ```; True; ```. ### Error output. ```pytb; loop of ufunc does not support argument 0 of type SparseDataset which has no callable expm1 method!; ```. goes from https://github.com/scverse/scanpy/blob/bc349b999be62196aa51b59db6e2daa37f428322/scanpy/preprocessing/_highly_variable_genes.py#L206. ### Versions. <details>. ```; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.3.0; appnope 0.1.3; asttokens NA; backcall 0.2.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.3; decorator 5.1.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; igraph 0.10.2; ipykernel 6.17.1; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.0; llvmlite 0.39.1; louvain 0.8.0; matplotlib 3.6.2; mpl_toolkits NA; natsort 8.2.0; numba 0.56.4; numpy 1.23.5; packaging 21.3; pandas 1.2.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.5.4; plotly 5.11.0; prompt_toolkit 3.0.33; psutil 5.9.4; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.8.0; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.13.0; pyparsing 3.0.9; pytz 2022.6; scipy 1.9.3; session_info 1.0.0; setuptools 62.3.2; sitecustomize NA; six 1.16.0; sklearn 1.1.3; stack_data 0.6.1; texttable 1.6.6; threadpoolctl 3.1.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2764:381,load,loaded,381,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2764,1,['load'],['loaded']
Performance,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. Hello, . I cannot produce normal looking paga plot. Whether I am using my own data or datasets provided through scanpy, the output for PAGA looks weird. . Please see below. Is this something to do with figure margins/axes?. Thanks in advance. ![Screenshot 2023-09-13 at 20 48 43](https://github.com/scverse/scanpy/assets/40166451/c7313231-a2dc-49cb-a3df-21e97d86d278). ### Minimal code sample. ```python; adata2 = sc.datasets.pbmc3k_processed(); sc.tl.paga(adata2, groups='louvain'); sc.pl.paga(adata2); ```. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.5; -----; PIL 8.0.1; backcall 0.2.0; bottleneck 1.3.7; cellrank 1.5.1; cffi 1.15.1; cloudpickle 2.2.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.5.0; dateutil 2.8.2; decorator 5.1.1; docrep 0.3.2; google NA; h5py 3.8.0; igraph 0.10.4; importlib_resources NA; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 3.0.3; joblib 1.2.0; kiwisolver 1.3.0; leidenalg 0.9.1; llvmlite 0.34.0; lz4 3.1.10; markupsafe 2.1.3; matplotlib 3.7.1; mpl_toolkits NA; natsort 8.3.1; numba 0.51.2; numexpr 2.8.5; numpy 1.23.5; packaging 23.1; pandas 1.5.3; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; progressbar 4.2.0; prompt_toolkit 3.0.8; psutil 5.7.2; ptyprocess 0.6.0; pygam 0.8.0; pygments 2.7.2; pygpcca 1.0.4; pyparsing 2.4.7; python_utils NA; pytz 2020.1; ruamel NA; scipy 1.10.1; scvelo 0.2.5; seaborn 0.11.0; session_info 1.0.0; six 1.15.0; sklearn 1.2.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tblib 1.7.0; texttable 1.6.7; threadpoolctl 2.1.0; tlz 0.12.1; toolz 0.11.1; tornado 6.1; tqdm 4.50.2; traitlets 5.0.5; typ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2665:933,bottleneck,bottleneck,933,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2665,1,['bottleneck'],['bottleneck']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. #### Summary; Integration of the `polars` and `fast_matrix_market` libraries into Scanpy's data loading functions, specifically `scanpy.read_10x_mtx` and `scanpy.read_mtx`. This will improve the loading speed of `.mtx` and `.csv` files, which is crucial for handling large-scale single-cell datasets more efficiently. #### The problem; The current data loading mechanisms in Scanpy, while effective for small to medium datasets, could be substantially optimized for speed when dealing with larger datasets. #### Expected Impact; - Reduced loading times; - Improving the user experience; - Enhanced scalability. #### Code snipped. ```; import fast_matrix_market; import os; import scanpy as sc; import scipy as sp. def read_10x_faster(; path: str; )-> sc.AnnData:; """"""; Read a sparse matrix in Matrix Market format and two CSV files with gene and cell metadata; into an AnnData object.; ; Args:; path: Path to the directory containing the matrix.mtx, genes.tsv, and barcodes.tsv files.; ; Returns:; An AnnData object with the matrix, gene metadata, and cell metadata. """"""; mtx_file = os.path.join(path, ""matrix.mtx""); gene_info = os.path.join(path, ""genes.tsv""); cell_metadata = os.path.join(path, ""barcodes.tsv""); ; # Read the .mtx file into a sparse matrix using the fast_matrix_market package (faster than scanpy, uses multiprocessing); mtx = fast_matrix_market.mmread(mtx_file). # Convert the sparse matrix to a CSR matrix; # Otherwise you will not be able to use it with scanpy; if isinstance(mtx, sp.sparse.coo.coo_matrix):; mtx = mtx.tocsr(); ; # Create an AnnData object; adata = sc.AnnData(X=mtx.T). # Polars is faster than pandas reading csv files; # Read the gene names and cell names into the AnnData object; adata.var = pl.read_csv(gene_info, separator= '\t', has_header=False).to_pandas(); ; # Read the cell names and cell met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2846:258,load,loading,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2846,6,"['load', 'optimiz', 'scalab']","['loading', 'optimized', 'scalability']"
Performance,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation; 2. Use already existing mean-var; 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3226:601,perform,performance,601,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226,1,['perform'],['performance']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hi, thanks for your efforts for maintaining this wonderful framework! I was wondering if there is an implementaion of Seurat FindConservedMarkers function in Scanpy, which can perform DGE analysis per group(e.g datasets) then combine them. It could be extreme useful especially when batch effects exist.; Thanks in advance!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3117:338,perform,perform,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3117,1,['perform'],['perform']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization?. My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :; replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while; Individual y might have replicate 1 in library 1 but replicate 2 in library 4.; so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3309:255,perform,perform,255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309,1,['perform'],['perform']
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. I am using `regress_out` and it is painfully slow. Even on a system where I set `n_jobs=36` and `sc.settings.n_jobs = 36`, each core of which has 36Gib of memory, I find that . ```; sc.pp.regress_out(adata, ['total_counts', ], n_jobs=n_jobs); ```. is practically unusable. At the moment that calculation is at `985` minutes. Looking at `htop` while the memory is certainly allocated (`191 Gib / 1.48Tb`), it _feels_ like setting `n_jobs` at all actually hinders performance.... I've checked the [source code](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L580) so I know that [`n_jobs`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L631C5-L631C55); should be set correctly. ```python; n_jobs = sett.n_jobs if n_jobs is None else n_jobs # NOTE: sett.n_jobs defaults to 1. # ... res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks); ```. Looking at [`_regress_out_chunk`](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L692), there really doesn't seem to be anything necessarily bottlenecking the performance except for either setting the [chunk length](https://github.com/scverse/scanpy/blob/master/scanpy/preprocessing/_simple.py#L662C5-L662C14) in `regress_out`, or just the size of the dataset... ```python; len_chunk = np.ceil(min(1000, X.shape[1]) / n_jobs).astype(int); ```. Am I missing something?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781:624,perform,performance,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781,3,"['bottleneck', 'perform']","['bottlenecking', 'performance']"
Performance,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. If I run scanpy.external.pp.dca with default parameters, do I still need to perform normalization, scaling, and log1p afterward? How can I obtain the non-normalized values?. ```py; scanpy.external.pp.dca(adata, mode='denoise', ae_type='nb-conddisp', normalize_per_cell=True, scale=True, log1p=True, hidden_size=(64, 32, 64), hidden_dropout=0.0, batchnorm=True, activation='relu', init='glorot_uniform', network_kwds=mappingproxy({}), epochs=300, reduce_lr=10, early_stop=15, batch_size=32, optimizer='RMSprop', random_state=0, threads=None, learning_rate=None, verbose=False, training_kwds=mappingproxy({}), return_model=False, return_info=False, copy=False); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2626:238,perform,perform,238,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2626,2,"['optimiz', 'perform']","['optimizer', 'perform']"
Performance,### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. currently `pp.scale` with a `mask_obs` with a sparse matrix and with `zero_center== False` takes a really long time to update the sparse matrix. This also takes up a lot of memory because of the parity calculations. I would suggest a numba kernel that just swaps out the data. This works really well for rapids-singlecell and greatly improves performance and reduces the memory overhead.; I would open a PR with this kernel. ------; Performance for 90k cells and 25k genes:; without mask:; CPU 645 ms | GPU 37 ms | 20x; with mask:; CPU 22 s | GPU 50 ms | 460x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2941:505,perform,performance,505,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2941,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,### What kind of feature would you like to request?. New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?. ### Please describe your wishes. I'm currently implementing a function that takes in an anndata and then subsamples a given representation using https://github.com/jmschrei/apricot. This generally serves the purpose of semi-optimally picking a reduced number of points that's still representative of the latent space. . Is this sth within the scope of scanpy? When it's done it wouldn't be too much effort to polish it up for a PR. The dependency load seems fairly low.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:594,load,load,594,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['load'],['load']
Performance,### What kind of feature would you like to request?. Other?. ### Please describe your wishes. - maybe use [`cache.mkdir()`](https://docs.pytest.org/en/7.1.x/reference/reference.html#std-fixture-cache) for persistent files (data); - set directory to read only in CI during test run,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2555:108,cache,cache,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2555,2,['cache'],['cache']
Performance,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. Currently, `score_genes()`/`score_genes_cell_cycle()` do not have any examples and specifically no documentation of what the input transformation of the data should be, just a link to a (very old) notebook. The notebook says ""Log-transformation of data and scaling should always be performed before scoring"" but I suspect many users (like me) have missed this (especially the scaling step!). It would be great if this was stated in the function documentation with an example.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2909:376,perform,performed,376,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2909,1,['perform'],['performed']
Performance,"### What kind of feature would you like to request?. Other?. ### Please describe your wishes. There are two levels of shared parameters that people could benefit from:. 1. Multiple Notebooks that work with the same data and could benefit from shared configuration, e.g. labels and color maps defined for a certain axis/annotation; 2. Plotting using the same labels, color map or so. This could be achieved using object oriented plotting (todo issue number). Having shared configuration files could be achieved either by direct support in the plotting functions (`x='cell type'`) or by adding a convenience function that loads a config object.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2767:620,load,loads,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2767,1,['load'],['loads']
Performance,"### 📄 `_make_forest_dict()` in `scanpy/neighbors/__init__.py`. 📈 Performance improved by **`77%`** (**`0.77x` faster**). ⏱️ Runtime went down from **`5670.84μs`** to **`3195.82μs`**; ### Explanation and details. I have used `numpy.array` and `numpy.concatenate` for your sizes and dat object which are much faster than `numpy.fromiter` and assignation respectively, especially when dealing with a large dataset. The sizes of your data_list are computed only once and used where needed. Which results in runtime improvements compared to previous code, where data sizes were computed multiple times in different parts of the code. ### Correctness verification. The new optimized code was tested for correctness. The results are listed below. #### ✅ 8 Passed − 🌀 Generated Regression Tests; <details>; <summary>(click to show generated tests)</summary>. ```python; # imports; import numpy as np; import pytest. # function to test; # (The function definition is omitted as it was provided in the original prompt). # helper class to create mock trees with properties; class MockTree:; def __init__(self, hyperplanes, offsets, children, indices):; self.hyperplanes = np.array(hyperplanes); self.offsets = np.array(offsets); self.children = np.array(children); self.indices = np.array(indices). # unit tests. # Test with a single tree with one-dimensional properties; def test_single_tree_one_dimensional():; tree = MockTree(hyperplanes=[1, 2], offsets=[3], children=[4, 5], indices=[6, 7]); forest = [tree]; result = _make_forest_dict(forest); assert result[""hyperplanes""][""start""][0] == 0; assert result[""offsets""][""start""][0] == 0; assert np.array_equal(result[""hyperplanes""][""data""], tree.hyperplanes); assert np.array_equal(result[""offsets""][""data""], tree.offsets). # Test with multiple trees with two-dimensional properties; def test_multiple_trees_two_dimensional():; tree1 = MockTree(hyperplanes=[[1, 2], [3, 4]], offsets=[5, 6], children=[[7, 8], [9, 10]], indices=[[11, 12], [13, 14]]); tree2 = Moc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2971:65,Perform,Performance,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2971,2,"['Perform', 'optimiz']","['Performance', 'optimized']"
Performance,"(0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-normalizer=3.3.2; - colorama=0.4.6; - colorcet=3.1.0; - colorful=0.5.6; - comm=0.2.2; - contourpy=1.2.1; - cycler=0.12.1; - debugpy=1.8.1; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.8; - dnspython=2.6.1; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.2.0; - executing=2.0.1; - filelock=3.14.0; - fonttools=4.53.0; - fqdn=1.5.1; - freetype=2.12.1; - get-annotations=0.1.2; - gffpandas=1.2.2; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.5; - h11=0.14.0; - h2=4.1.0; - h5py=3.11.0; - hdf5=1.14.3; - hpack=4.0.0; - httpcore=1.0.5; - httpx=0.27.0; - hyperframe=6.0.1; - icu=73.2; - idna=3.7; - igraph=0.10.12; - importlib-metadata=7.1.0; - importlib_metadata=7.1.0; - importlib_resources=6.4.0; - ipykernel=6.29.4; - ipython=8.25.0; - isoduration=20.11.0; - jedi=0.19.1; - jinja2=3.1.4; - joblib=1.4.2; - json5=0.9.25; - jsonpointer=2.4; - jsonschema=4.22.0; - jsonschema-specifications=2023",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:3169,cache,cached-property,3169,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cached-property']
Performance,"(= 0.40.0-2),; debconf (= 1.5.79),; debhelper (= 13.5.2),; debianutils (= 5.5-1),; dh-autoreconf (= 20),; dh-python (= 5.20211105),; dh-strip-nondeterminism (= 1.12.0-2),; diffutils (= 1:3.7-5),; dmsetup (= 2:1.02.175-2.1),; docutils-common (= 0.17.1+dfsg-2),; dpkg (= 1.20.9),; dpkg-dev (= 1.20.9),; dwz (= 0.14-1),; file (= 1:5.39-3),; findutils (= 4.8.0-1),; flit (= 3.0.0-1),; fontconfig (= 2.13.1-4.2),; fontconfig-config (= 2.13.1-4.2),; fonts-font-awesome (= 5.0.10+really4.7.0~dfsg-4.1),; fonts-lato (= 2.0-2.1),; fonts-lyx (= 2.3.6-1),; g++ (= 4:11.2.0-2),; g++-11 (= 11.2.0-10),; gcc (= 4:11.2.0-2),; gcc-11 (= 11.2.0-10),; gcc-11-base (= 11.2.0-10),; gettext (= 0.21-4),; gettext-base (= 0.21-4),; gir1.2-atk-1.0 (= 2.36.0-2),; gir1.2-freedesktop (= 1.70.0-2),; gir1.2-gdkpixbuf-2.0 (= 2.42.6+dfsg-2),; gir1.2-glib-2.0 (= 1.70.0-2),; gir1.2-gtk-3.0 (= 3.24.30-3),; gir1.2-harfbuzz-0.0 (= 2.7.4-1),; gir1.2-pango-1.0 (= 1.48.10+ds1-1),; grep (= 3.7-1),; groff-base (= 1.22.4-7),; gtk-update-icon-cache (= 3.24.30-3),; gzip (= 1.10-4),; hicolor-icon-theme (= 0.17-2),; hostname (= 3.23),; imagemagick (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6-common (= 8:6.9.11.60+dfsg-1.3),; imagemagick-6.q16 (= 8:6.9.11.60+dfsg-1.3),; init-system-helpers (= 1.60),; intltool-debian (= 0.35.0+20060710.5),; libacl1 (= 2.3.1-1),; libaec0 (= 1.0.6-1),; libamd2 (= 1:5.10.1+dfsg-2),; libaom3 (= 3.2.0-1),; libapparmor1 (= 3.0.3-5),; libarchive-zip-perl (= 1.68-1),; libargon2-1 (= 0~20171227-0.2),; libarpack2 (= 3.8.0-1),; libasan6 (= 11.2.0-10),; libatk-bridge2.0-0 (= 2.38.0-2),; libatk1.0-0 (= 2.36.0-2),; libatk1.0-data (= 2.36.0-2),; libatlas3-base (= 3.10.3-11),; libatomic1 (= 11.2.0-10),; libatspi2.0-0 (= 2.42.0-2),; libattr1 (= 1:2.5.1-1),; libaudit-common (= 1:3.0.6-1),; libaudit1 (= 1:3.0.6-1),; libavahi-client3 (= 0.8-5),; libavahi-common-data (= 0.8-5),; libavahi-common3 (= 0.8-5),; libbinutils (= 2.37-8),; libblas3 (= 3.10.0-1),; libblkid1 (= 2.37.2-4),; libblosc1 (= 1.21.1+ds1-1),; libbr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:2863,cache,cache,2863,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['cache'],['cache']
Performance,"(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend); 10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21515,cache,cache,21515,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"(f[k]); 420 else: # Base case; --> 421 d[k] = read_attribute(f[k]); 422 ; 423 d[""raw""] = _read_raw(f, as_sparse, rdasp). /broad/software/free/Linux/redhat_7_x86_64/pkgs/anaconda3_2020.07/lib/python3.8/functools.py in wrapper(*args, **kw); 873 '1 positional argument'); 874 ; --> 875 return dispatch(args[0].__class__)(*args, **kw); 876 ; 877 funcname = getattr(func, '__name__', 'singledispatch function'). ~/.local/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 181 else:; 182 parent = _get_parent(elem); --> 183 raise AnnDataReadError(; 184 f""Above error raised while reading key {elem.name!r} of ""; 185 f""type {type(elem)} from {parent}."". AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; ```. #### Versions. The following is my colleague's package version list (the one that isn't working). <details>. -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.4; -----; PIL 7.2.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2022.7.1; dateutil 2.8.1; decorator 4.4.2; fsspec 2022.01.0; google NA; h5py 3.6.0; igraph 0.9.9; ipykernel 5.3.2; ipython_genutils 0.2.0; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; leidenalg 0.8.0; llvmlite 0.38.1; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.5.2; mpl_toolkits NA; natsort 8.1.0; numba 0.55.2; numexpr 2.7.1; numpy 1.21.6; packaging 21.3; pandas 1.4.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.4; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:3047,bottleneck,bottleneck,3047,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['bottleneck'],['bottleneck']
Performance,"(most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1345,cache,cached-property,1345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['cache'],['cached-property']
Performance,")); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 numeric_only=numeric_only); 10957 ; 10958 return set_function_name(stat_func, name, cls). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds); 3613 # dispatch to ExtensionArray interface; 3614 if isinstance(delegate, ExtensionArray):; -> 3615 return delegate._reduce(name, skipna=skipna, **kwds); 3616 elif is_datetime64_dtype(delegate):; 3617 # use DatetimeIndex implementation to handle skipna correctly. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in _reduce(self, name, axis, skipna, **kwargs); 2179 msg = 'Categorical cannot perform the operation {op}'; 2180 raise TypeError(msg.format(op=name)); -> 2181 return func(**kwargs); 2182 ; 2183 def min(self, numeric_only=None, **kwargs):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in max(self, numeric_only, **kwargs); 2222 max : the maximum of this `Categorical`; 2223 """"""; -> 2224 self.check_for_ordered('max'); 2225 if numeric_only:; 2226 good = self._codes != -1. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in check_for_ordered(self, op); 1517 raise TypeError(""Categorical is not ordered for operation {op}\n""; 1518 ""you can use .as_ordered() to change the ""; -> 1519 ""Categorical to an ordered one\n"".format(op=op)); 1520 ; 1521 def _values_for_argsort(self):. TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one; ```. I was confused for two reason",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:3771,perform,perform,3771,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['perform'],['perform']
Performance,"); 128 if key in f:; 129 del f[key]; --> 130 _write_method(type(value))(f, key, value, *args, **kwargs); 131 ; 132 . ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 210 except Exception as e:; 211 parent = _get_parent(elem); --> 212 raise type(e)(; 213 f""{e}\n\n""; 214 f""Above error raised while writing key {key!r} of {type(elem)}"". RuntimeError: Unable to create link (name already exists). Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'gene_name' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'var' of <class 'h5py._hl.files.File'> from /. ```. Am I doing something wrong with this data? Or should I be using a different command?. #### Versions. <details>. anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.2.0; anndata2ri 1.0.6; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; backports NA; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; dunamai 1.6.0; fsspec 0.9.0; get_version 3.5; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.35.0; markupsafe 1.1.1; matplotlib 3.3.4; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.52.0; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.4; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; rpy2 3.4.5; samalg 0.8.6;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:6533,bottleneck,bottleneck,6533,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,1,['bottleneck'],['bottleneck']
Performance,"* `struct_dict['k']` should return a simple 1D ndarray, not a `StructDict`; * `struct_dict[['a', 'b']]` should return a proper `StructDict` including all additional fields like `_keys`. both problems are visible in this test: https://travis-ci.org/theislab/scanpy/jobs/227216146#L224. but a test should be added for the second point once the first point is fixed. ---. i would have fixed it, but i don’t have the slightest idea what all the “multicolumn” fields are for. also i don’t believe in too many caches and private fields, they get out of sync too easily. recomputing tiny things is fast, e.g. `self._keys` could simply be replaced with `self.dtype.names`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/14:504,cache,caches,504,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/14,1,['cache'],['caches']
Performance,"**I installed the newest versions of JAVA and VC+++, didn't work.; Here also attaches the information of install scapy[leiden]:**; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:164,cache,cached,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,10,['cache'],['cached']
Performance,"**The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**; ```; sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean; ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**; Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905:919,perform,performed,919,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905,1,['perform'],['performed']
Performance,"**Update** Note sure if it is a problem with our script. However, if colors are not found, can it use default color-map and let us plot after raising a warning?. Log is following. ~Working on a PR.~ This issue is for reference. Could not upload source file since it depends on data file which are huge. ```; (Py36) pragati@wasabi-simons ~/Work/scanpy_exp $ python planaria.py ; scanpy==1.3.2+4.g7c9fb1a anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:02:39.15); Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 158, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('grey80', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/axes/_axes.py"", line 4210, in scatter; colors = mcolors.to_rgba_array(c); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 259, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 160, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/home/pragati/Py36/lib/python3.6/site-packages/matplotlib/colors.py"", line 204, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'grey80'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home/pragati/Py36/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne; return plot_scatter(adata, basis='tsne', **k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286:822,cache,cache,822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286,1,['cache'],['cache']
Performance,*; Collecting scanpy[leiden]; Using cached scanpy-1.7.2-py3-none-any.whl (10.3 MB); Collecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.w,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1122,cache,cached,1122,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,", but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1632,cache,cache,1632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,2,['cache'],['cache']
Performance,", in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, esca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21362,cache,cache,21362,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,", n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 221 ) # 0 is not a valid value for rapids, unlike original umap; 222 X_contiguous = np.ascontiguousarray(X, dtype=np.float32); --> 223 umap = UMAP(; 224 n_neighbors=n_neighbors,; 225 n_components=n_components,. ~/miniconda3/envs/rapids-0.20/lib/python3.8/site-packages/cuml/internals/api_decorators.py in inner_f(*args, **kwargs); 792 kwargs.update({k: arg for k, arg in zip(sig.parameters, args)}); 793 ; --> 794 return func(**kwargs); 795 ; 796 # Set this flag to prevent auto adding this decorator twice. cuml/manifold/umap.pyx in cuml.manifold.umap.UMAP.__init__(). TypeError: %d format: a number is required, not str; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; 2f7ece400a652629565c523b34ee61b04afa385c NA; PIL 8.1.2; absl NA; anndata 0.7.6; anyio NA; astunparse 1.6.3; attr 20.3.0; babel 2.9.0; backcall 0.2.0; brotli 1.0.9; cachetools 4.2.2; cellrank 1.3.1; certifi 2020.12.05; cffi 1.14.4; chardet 4.0.0; click 7.1.2; cloudpickle 1.6.0; colorama 0.4.4; cudf 0.20.0a+294.gfbb9a988fa; cugraph 0.20.0a+65.g924f6782.dirty; cuml 0.20.0a+110.gab47f2e11; cupy 9.0.0; cupy_backends NA; cupyx NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dask_cuda 0+unknown; dask_cudf 0.20.0a+294.gfbb9a988fa; dateutil 2.8.1; decorator 4.4.2; distributed 2021.04.0; docrep 0.3.2; fastrlock 0.5; flatbuffers NA; fsspec 2021.04.0; future_fstrings NA; gast NA; get_version 2.1; google NA; h5py 3.1.0; heapdict NA; idna 2.10; igraph 0.9.1; ipykernel 5.4.3; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; jinja2 2.11.2; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.2.2; jupyterlab_server 2.1.2; keras_preprocessing 1.1.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.4.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; nbclassic NA; nb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1837:2182,cache,cachetools,2182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1837,1,['cache'],['cachetools']
Performance,", tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 1299 # some axes don't allow reindexing with dups; 1300 if not allow_dups:; -> 1301 self.axes[axis]._can_reindex(indexer); 1302 ; 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer); 3475 # trying to reindex on an axis with duplicates; 3476 if not self._index_as_unique and len(indexer):; -> 3477 raise ValueError(""cannot reindex from a duplicate axis""); 3478 ; 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis; ```; Loading a single h5 file works and produces expected output:; ```; a = sc.read_10x_h5('./a.h5', gex_only = True); a; AnnData object with n_obs × n_vars = 7474 × 31053; var: 'gene_ids', 'feature_types', 'genome'; ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:4485,Load,Loading,4485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,1,['Load'],['Loading']
Performance,", values_key). ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_default_colors_for_categorical_obs(adata, value_to_plot); 468 ); 469 ; --> 470 _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length]); 471 ; 472 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/scanpy/plotting/_utils.py in _set_colors_for_categorical_obs(adata, value_to_plot, palette); 428 colors_list = [to_hex(next(cc)['color']) for x in range(len(categories))]; 429 ; --> 430 adata.uns[value_to_plot + '_colors'] = colors_list; 431 ; 432 . ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/compat/_overloaded_dict.py in __setitem__(self, key, value); 104 self.overloaded[key].set(value); 105 else:; --> 106 self.data[key] = value; 107 ; 108 def __delitem__(self, key):. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in __setitem__(self, idx, value); 32 stacklevel=2,; 33 ); ---> 34 with self._update() as container:; 35 container[idx] = value; 36 . ~/miniconda3/envs/csp/lib/python3.8/contextlib.py in __enter__(self); 111 del self.args, self.kwds, self.func; 112 try:; --> 113 return next(self.gen); 114 except StopIteration:; 115 raise RuntimeError(""generator didn't yield"") from None. ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/views.py in _update(self); 38 def _update(self):; 39 adata_view, attr_name, keys = self._view_args; ---> 40 new = adata_view.copy(); 41 attr = getattr(new, attr_name); 42 container = reduce(lambda d, k: d[k], keys, attr). ~/miniconda3/envs/csp/lib/python3.8/site-packages/anndata/_core/anndata.py in copy(self, filename); 1526 ; 1527 if filename is None:; -> 1528 raise ValueError(; 1529 ""To copy an AnnData object in backed mode, ""; 1530 ""pass a filename: `.copy(filename='myfilename.h5ad')`. "". ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`. ```. #### Versions. Scanpy: 1.8.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2401:4486,load,load,4486,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2401,1,['load'],['load']
Performance,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar!. ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357:350,tune,tuned,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778671357,1,['tune'],['tuned']
Performance,"- [ X] I have checked that this issue has not already been reported.; - [ X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40); sc.tl.umap(adata); sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP']); sc.tl.leiden(adata); sc.pl.umap(adata, color=['leiden', 'CST3', 'NKG7']). ```. ```pytb; The output does not match the clustering of the PBMC3k tutorial. I've downloaded the PBMC notebook from github and my output has clusters 4 and 5 switched., see my output below. How can I rectify this?; ```. #### Versions. <details>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.0.1; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.02.1; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; executing 0.8.3; fsspec 2022.02.0; google NA; h5py 3.6.0; igraph 0.10.4; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.13.5; kiwisolver 1.3.2; leidenalg 0.9.1; llvmlite 0.38.0; markupsafe 2.0.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.3.1; nbinom_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.1; numpy 1.21.5; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; pkg_resources NA; plotly 5.6.0; prompt_toolkit 3.0.20; psutil 5.8.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5.8; pyparsing 3.0.4; pythoncom NA; pytz 2021.3; pywintypes NA; ruamel NA; scipy 1.7.3; seaborn 0.11.2; session_info 1.0.0; setuptools 61.2.0; six",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2471:873,bottleneck,bottleneck,873,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2471,1,['bottleneck'],['bottleneck']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; #Perform a clustering for scran normalization in clusters; adata_pp = adata.copy(); sc.pp.normalize_per_cell(adata_pp, counts_per_cell_after=1e6); sc.pp.log1p(adata_pp); sc.pp.pca(adata_pp, n_comps=15); sc.pp.neighbors(adata_pp); sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5); ```. ```pytb; ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-785c54721f17> in <module>; 8 sc.pp.pca(adata_pp, n_comps=15); 9 sc.pp.neighbors(adata_pp); ---> 10 sc.tl.louvain(adata_pp, key_added='groups', resolution=0.5). ~\anaconda3\lib\site-packages\scanpy\tools\_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, neighbors_key, obsp, copy); 135 weights = None; 136 if flavor == 'vtraag':; --> 137 import louvain; 138 if partition_type is None:; 139 partition_type = louvain.RBConfigurationVertexPartition. ModuleNotFoundError: No module named 'louvain'; ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1566:500,Perform,Perform,500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566,1,['Perform'],['Perform']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### This is the code I'm using which was copied from the tutorial:. ```python; import scanpy as sc; adata = sc.datasets.paul15(); sc.pp.log1p(adata); sc.pp.neighbors(adata, n_neighbors=20, use_rep='X', method='gauss'); sc.tl.diffmap(adata); sc.tl.dpt(adata, n_branchings=1, n_dcs=10); sc.pl.diffmap(adata, color=['dpt_pseudotime', 'dpt_groups', 'paul15_clusters']); ```; The results I got are:; ![image](https://user-images.githubusercontent.com/33322882/168080076-f6767970-0b3b-4623-ab25-e9e38201e6ef.png). Which is totally different than in the tutorial:; ![image](https://user-images.githubusercontent.com/33322882/168081015-cddf77eb-7b83-4802-8890-3b0d87a5755d.png). Can anyone run into this problem ever? Thanks for your help. #### Versions. <details>; -----. anndata 0.8.0. scanpy 1.9.1. -----. PIL 8.4.0. backcall 0.2.0. beta_ufunc NA. binom_ufunc NA. bottleneck 1.3.2. cffi 1.15.0. colorama 0.4.4. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. debugpy 1.5.1. decorator 4.4.2. defusedxml 0.6.0. google NA. h5py 3.6.0. ipykernel 6.4.1. ipython_genutils 0.2.0. jedi 0.18.1. joblib 0.17.0. kiwisolver 1.3.1. llvmlite 0.38.0. ... Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]. Linux-3.16.0-11-amd64-x86_64-with-glibc2.19. -----. Session information updated at 2022-05-12 14:59. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2254:1273,bottleneck,bottleneck,1273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2254,1,['bottleneck'],['bottleneck']
Performance,"- [ ] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Hi Scanpy,; I noticed the error was kind of mnnpy related and after checking the issues there and updating two suggested packages, still getting the error. . ```python; adata_mnn = adata.copy(); adata_list = [adata_mnn[adata_mnn.obs['sample'] == i] for i in adata_mnn.obs['sample'].unique()]; adata_mnn, _, _ = sc.external.pp.mnn_correct(*adata_list, batch_key=""sample""); ```. ```pytb; Performing cosine normalization...; Starting MNN correct iteration. Reference batch: 0; Step 1 of 4: processing batch 1; Looking for MNNs...; Computing correction vectors...; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-49-f894e9f745f6> in <module>; ----> 1 adata_mnn, _, _ = sc.external.pp.mnn_correct(*adata_list, batch_key=""sample""). /projects/da_workspace/Users/amoussavi/Software/Anaconda_python3/lib/python3.7/site-packages/scanpy/external/pp/_mnn_correct.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_concatenate, save_raw, n_jobs, *datas, **kwargs); 152 save_raw=save_raw,; 153 n_jobs=n_jobs,; --> 154 **kwargs,; 155 ); 156 return datas, mnn_list, angle_list. /Anaconda_python3/lib/python3.7/site-packages/mnnpy/mnn.py in mnn_correct(var_index, var_subset, batch_key, index_unique, batch_categories, k, sigma, cos_norm_in, cos_norm_out, svd_dim, var_adj, compute_angle, mnn_order, svd_mode, do_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1367:874,Perform,Performing,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1367,1,['Perform'],['Performing']
Performance,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data); Hi, . I'm processing the `loom` object from the Cao et al 2020 (Dataset [GSE156793](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE156793)), so I can save it as an `anndata` for downstream analyses. Below are the steps I'm taking to do this: . - Load modules. ```python; import anndata; import numpy as np; import pandas as pd; import scanpy as sc; from scipy.sparse import csr_matrix, csc_matrix; ```; - Read loom object. Takes ~ 4 hrs. . ```python; gex_matrix = sc.read_loom('GSE156793_S3_gene_count.loom'); gex_matrix; ```; - Read in metadata ; ```python; gex_metadata = pd.read_csv('GSE156793_S1_metadata_cells.txt.gz', sep = ',', index_col = 0); gex_matrix.obs = gex_metadata; gex_matrix.obs; ```; - Transform to `CSR` matrix; ```python; gex_matrix.X = csr_matrix(gex_matrix.X); gex_matrix.X; ```; - Save object; ```python; gex_matrix.write('GSE156793_GEX_ctl210604.raw.h5ad'); ```. However, I get the following error. Any ideas what this may be related to? . ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_series(group, key, series, dataset_kwargs); 269 if series.dtype == object: # Assuming it’s string; --> 270 group.create_dataset(; 271 key,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:741,Load,Load,741,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['Load'],['Load']
Performance,"- [x] Additional function parameters / changed functionality / changed defaults?. At the moment when we are plotting data points in e.g., `sc.pl.umap()` with `color='covariate'` we determine the plotting order in two ways:; 1. if `'covariate'` is continuous the highest values are plotted on top, to showcase the peaks of the distribution;; 2. if `'covariate'` is a categorical variable, the order of `adata.obs_names` is used (i believe). As we often concatenate datasets after integration or loading from multiple sources, covariates we plot are usually not randomly ordered here. I think the first case is fine (and it can be turned off), but we should probably not be doing case 2. Instead, it would be good if the default was to plot in a random order unless the covariate is ordered internally (I believe this is already taken into account, but not sure). I have come across this issue several times now, and we're not solving this in a good way imo. Fabian has mentioned this to me several times as well. What do you think @fidelram @ivirshup ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263:494,load,loading,494,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [ ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample . ```python; import scanpy as sc; import numpy as np; import pandas as pd; adata = sc.datasets.pbmc68k_reduced(); sc.pl.pca_loadings(adata, components = '1,2,3'). ```. ```pytb; ValueError: Axis limits cannot be NaN or Inf; ```; **Note**: recalculating pca solves the problem . #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.8.0.dev78+gc488909a; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.5; appnope 0.1.0; attr 19.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.5.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2.20.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; idna 2.10; igraph 0.8.3; ipykernel 5.3.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; jinja2 2.11.2; joblib 0.16.0; jsonschema 3.2.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; llvmlite 0.33.0+1.g022ab0f; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.2.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.7; numba 0.50.1; numexpr 2.7.1; numpy 1.18.5; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pvectorc NA; pygments 2.6.1; pyparsing 2.4.7; pyrsistent NA; pytz 2020.1; scanpy 1.8.0.dev78+gc488909a; scipy 1.5.0; seaborn 0.10.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; sphinxcontrib NA; statsmodels 0.11.1; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.3; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 2.1.5; notebook 6.0.3; -----; Python 3.8.3 (default, Jul 2 2020, 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1816:698,bottleneck,bottleneck,698,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1816,1,['bottleneck'],['bottleneck']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am following all the steps from the PBMC3k tutorial (https://scanpy.readthedocs.io/en/latest/tutorials/pbmc3k.html).; I ran the wilcoxon method for finding marker genes and saved the object to a .h5ad file, after which I reloaded the object from the file and the log1p dict appears to be empty for some reason. I checked the object before saving it to the file and it had the 'base' key whereas after loading it back it was missing, causing the error below. . ### Minimal code sample (that we can copy&paste without having any data). ```python; adata = sc.read(results_file); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5); result = adata.uns['rank_genes_groups']; groups = result['names'].dtype.names; pd.DataFrame(; {group + '_' + key[:1]: result[key][group]; for group in groups for key in ['names', 'pvals']}).head(5); sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'); sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20); ```. ```pytb; KeyError Traceback (most recent call last); Input In [57], in <cell line: 1>(); ----> 1 sc.tl.rank_genes_groups(adata, 'leiden', groups=['0'], reference='1', method='wilcoxon'); 2 sc.pl.rank_genes_groups(adata, groups=['0'], n_genes=20). File /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scanpy/tools/_rank_genes_groups.py:590, in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds); 580 adata.uns[key_added] = {}; 581 adata.uns[key_added]['params'] = dict(; 582 groupby=groupby,; 583 reference=reference,; (...); 587 corr_method=corr_method,; 588 ); --> 590 test_obj = _RankGenes(adata, groups_order, groupby, reference, use_raw, la",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2497:632,load,loading,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Trying to run through Pearson residual but when trying to perform 'Plot quality control metrics' produced an error saying it could not find keys in adata.obs or in data.var_names. ### Minimal code sample (that we can copy&paste without having any data). ```python; # adata_control = sc.read_csv('/Users/csb/Desktop/SevenBridge_custom reference remapping_2022.7.21/Sample_sample_Control_WTA/Control_new.csv'); adata_control.uns[""name""] = ""zfish_Control"". for adata in adata_control:; adata.var_names_make_unique(); print(adata.uns[""name""], "": data shape"", adata.shape); sc.pp.filter_cells(adata, min_genes=100); sc.pp.filter_genes(adata, min_cells=100). for adata in adata_control:; adata.var['mt'] = adata.var_names.str.startswith('mt-'); sc.pp.calculate_qc_metrics(; adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True. for adata in adata_control:; print(adata.uns[""name""], "":""); sc.pl.violin(; adata,; [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],; jitter =0.4,; multi_panel=True,; ); ```. ```pytb; zfish_Control :; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /var/folders/bh/yzgycbkj1jn1bd9p52t5s8s40000gn/T/ipykernel_17867/842143982.py in <module>; 1 for adata in adata_control:; 2 print(adata.uns[""name""], "":""); ----> 3 sc.pl.violin(; 4 adata,; 5 [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/plotting/_anndata.py in violin(adata, keys, groupby, log, use_raw, stripplot, jitter, size, layer, scale, order, multi_panel, xlabel, ylabel, rotation, show, save, ax, **kwds); 779 ); 780 else:; --> 781 obs_df = get.obs_df(adata, keys=keys, layer=layer, use_raw=use_raw); 782 if groupby is None:; 783 obs_tidy ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305:287,perform,perform,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305,1,['perform'],['perform']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Hi,. My understanding of the ""groups"" argument in sc.tl.rank_genes_groups is that it subsets the data and then performs the differential expression testing. I.e. if I have clusters 1 to 10, and I set groups=[1,2], the output will give me the genes differentially expressed in cluster 1 as compared to cluster 2 (and 2 vs 1).; However, the current function still compares to all other clusters (see below). ; Is that the intention? If so, we should update the readthedocs I think.; If this is a bug, let's try to fix it :) I can take a look myself in that case. Just wanted to check if I misinterpreted the readthedocs. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc. # load data; adata = sc.datasets.pbmc68k_reduced(); # cluster; sc.tl.leiden(adata, key_added=""clusters"", resolution=0.5); print(""Clusters:"", sorted(set(adata.obs[""clusters""]))); # do test with groups=""all""; sc.tl.rank_genes_groups(adata, groupby=""clusters"", groups=""all""); # store results, sorting genes by logfc; genes_cluster_0_vs_all = [; (name, logfc); for logfc, name in sorted(; zip(; adata.uns[""rank_genes_groups""][""logfoldchanges""][""0""],; adata.uns[""rank_genes_groups""][""names""][""0""],; ),; reverse=True,; ); ]; # do test with groups=[""0"",""1""], i.e. only a subset of the clusters; sc.tl.rank_genes_groups(adata, groupby=""clusters"", groups=[""0"", ""1""]); # store result; genes_cluster_0_vs_1 = [; (name, logfc); for logfc, name in sorted(; zip(; adata.uns[""rank_genes_groups""][""logfoldchanges""][""0""],; adata.uns[""rank_genes_groups""][""names""][""0""],; ),; reverse=True,; ); ]; # print top 5 genes and logfcs for both,; # they're the same and should not be; print(""Top genes cluster 0 versus all:\n"", genes_cluster_0_vs_all[:5]); print(""Top genes ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1519:340,perform,performs,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1519,2,"['load', 'perform']","['load', 'performs']"
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am not 100% this is a bug, so please correct me if I'm doing something wrong... I am using scanpy with a very large scRNAseq dataset (SEA-AD, the sparse h5ad is ~35GB); When I use the read_h5ad file, even when running in backed mode, I get an out of memory exception. I expect it to be due to the fact that parts of the dataset are still read to memory, even in backed mode.; As you can see in the stack trace below, eventually anndata's `read_sparse()` function is called (in `_io/specs/methods.py`; But this method has the following implementation in the latest version:; ```python; def read_sparse(elem):; return SparseDataset(elem).to_memory(); ```; Thus, loading (part of) the dataset to memory. Like I said, I am not sure whether this is a bug, or supposed to happen. But to me it seems odd that backed mode still loads large portions of the dataset to memory. ### Workaround. For my own project, I got around this issue, by removing the call to `.to_memory()` within the source of anndata. I am not sure whether this breaks any other functionality, but I can use the dataset the way I need it right now. ### Minimal code sample (that we can copy&paste without having any data); (this cannot be run without any data, because the problem is that it fails with a big dataset. I included an aws download command to the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:891,load,loading,891,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['load'],['loading']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. To reproduce this issue:; 1. download the public 10x dataset here (https://cf.10xgenomics.com/samples/cell-exp/2.1.0/hgmm_12k/hgmm_12k_raw_gene_bc_matrices_h5.h5) ; 2. run the following. ```python; import scanpy as sc. adata_human = sc.read_10x_h5('hgmm_12k_raw_gene_bc_matrices_h5.h5', genome='hg19'); adata_mouse = sc.read_10x_h5('hgmm_12k_raw_gene_bc_matrices_h5.h5', genome='mm10'). assert (adata_human.X != adata_mouse.X).sum() > 0, 'these count matrices are equal'; ```. which produces the assertion error. We see that the loaded data is the same regardless of `'genome'` argument. A look at the file itself shows this is not the case (notice the number of gene names, which are different for hg19 and mm10):. ![image](https://user-images.githubusercontent.com/10214815/165848884-0ef5c172-83f9-4ead-9687-0acadb496e87.png). #### Versions. Also I think I can say confidently that this was working fine as of scanpy 1.8.1. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.1.0; appnope 0.1.2; backcall 0.2.0; cached_property 1.5.2; cellbender NA; cffi 1.14.5; colorcet 3.0.0; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 5.0.9; fontTools 4.33.3; h5py 3.2.0; igraph 0.9.10; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.10; llvmlite 0.38.0; lxml 4.8.0; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; numba 0.55.1; numexpr 2.7.3; numpy 1.19.2; packaging 20.9; pandas 1.2.3; param 1.12.1; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.0; pynndescent 0.5.6; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.1; seaborn 0.11.2; session_info 1.0.0; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246:758,load,loaded,758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246,1,['load'],['loaded']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; ---. Hello, I am trying to run `sc.pp.highly_variable_genes` with `flavor='seurat_v3'` on some data, but it is giving me an error. The data has been run through Kallisto Bustools and is being loaded in as an .h5ad file; it should be raw count data. . ### Minimal code sample. ```python; zf_48 = anndata.read_h5ad(""data.h5ad""); zf_48.var[""gene_ids""] = zf_48.var.index.values. t2g = pd.read_csv(""transcripts_to_genes_no_rm.txt"", header=None, names=[""tid"", ""gene_id"", ""gene_name""], sep=""\t""); t2g.index = t2g.gene_id; t2g = t2g.loc[~t2g.index.duplicated(keep='first')]; zf_48.var[""gene_name""] = zf_48.var.gene_ids.map(t2g[""gene_name""]); zf_48.var.index = zf_48.var[""gene_name""] . zf_48.var_names_make_unique(); sc.pp.filter_cells(zf_48, min_genes=550); sc.pp.filter_genes(zf_48, min_cells=10); zf_48; #AnnData object with n_obs × n_vars = 887 × 13180; # obs: 'n_genes'; # var: 'gene_name', 'gene_ids', 'n_cells'. zf_48.var['mt'] = zf_48.var_names.str.startswith('mt-') ; sc.pp.calculate_qc_metrics(zf_48, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1); ```; Here is the error:; ```pytb; ValueError Traceback (most recent call last); <ipython-input-170-37cd37b7326e> in <module>; 16 zf_48 = zf_48[zf_48.obs.n_genes_by_counts < 2500, :]; 17 zf_48 = zf_48[zf_48.obs.pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 41",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:334,load,loaded,334,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['load'],['loaded']
Performance,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; ---; Hello, I am trying to save my .h5ad files to my desktop from scanpy, but adata.write is giving me an error. The data has been run through Kallisto Bustools and is being loaded in as an .h5ad file, but the error appears even if I try to save it immediately after loading it in, without any modifications. Unfortunately, the error does not happen when I try it on other data, such as the pbmc3k data. ### Minimal code sample . ```python; zf_48 = anndata.read_h5ad(""/Users/julius/Desktop/48hpf_adata.h5ad""); zf_48.var[""gene_ids""] = zf_48.var.index.values. t2g = pd.read_csv(""/Users/julius/Desktop/zf_transcripts_to_genes_no_rm.txt"", header=None, names=[""tid"", ""gene_id"", ""gene_name""], sep=""\t""); t2g.index = t2g.gene_id; t2g = t2g.loc[~t2g.index.duplicated(keep='first')]; zf_48.var[""gene_name""] = zf_48.var.gene_ids.map(t2g[""gene_name""]); zf_48.var.index = zf_48.var[""gene_name""] . zf_48.write_h5ad(""/Users/julius/Desktop/zf_48.h5ad""); ```; Here is the error:; ```pytb; RuntimeError Traceback (most recent call last); ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, key, val, *args, **kwargs); 208 try:; --> 209 return func(elem, key, val, *args, **kwargs); 210 except Exception as e:. ~/opt/anaconda3/lib/python3.8/site-packages/anndata/_io/h5ad.py in write_array(f, key, value, dataset_kwargs); 184 value = _to_hdf5_vlen_strings(value); --> 185 f.create_dataset(key, data=value, **dataset_kwargs); 186 . ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds); 138 if name is not None:; --> 139 self[name] = dset; 140 return dset. ~/opt/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py in __setitem__(self, name, obj); 372 if isinstance(obj, HLObject):; --> 373 h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl); 374 . h5py/_objects.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1982:316,load,loaded,316,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1982,2,['load'],"['loaded', 'loading']"
Performance,"-------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did no",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1626,cache,cache,1626,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,2,['cache'],['cache']
Performance,"--------------; TypeError Traceback (most recent call last); /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>; 3 sc.pp.pca(mat_all); 4 sc.pp.neighbors(mat_all); ----> 5 sc.tl.umap(mat_all); 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",; 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key); 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200; 193 n_epochs = default_epochs if maxiter is None else maxiter; --> 194 X_umap = simplicial_set_embedding(; 195 X,; 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'; ```. And the versions I've been running:; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWeb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460:1311,Bottleneck,Bottleneck,1311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579#issuecomment-1062410460,1,['Bottleneck'],['Bottleneck']
Performance,------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1231,load,loading,1231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Un",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4197,cache,cached,4197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: py,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1749,cache,cached,1749,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files there:; ```; ls /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/; barcodes.tsv features.tsv matr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1800,cache,cache,1800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"-> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex; ```. </details>. I don't get an error from this on master, but I do get these warnings. ```; *c* argument looks like a single numeric RGB or RGBA sequence, whi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:4428,Cache,CachedProperty,4428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['Cache'],['CachedProperty']
Performance,. </details>. Luke's environment: MacOS Ventura 13.4.1. Intel MacBook pro. <details>; <summary> Luke's failing env </summary>. ```; # packages in environment at /Users/luke.zappia/miniconda3/envs/scanpy-dev:; #; # Name Version Build Channel; anndata 0.10.6 pypi_0 pypi; array-api-compat 1.4.1 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 h10d778d_5 conda-forge; ca-certificates 2024.2.2 h8857fd0_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.0 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.3.0 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.49.0 pypi_0 pypi; fsspec 2024.2.0 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.3.2 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h73e2aa4_0 conda-forge; libffi 3.4.2 h0d85af4_5 conda-forge; libsqlite 3.45.2 h92b6c6a_0 conda-forge; libzlib 1.2.13 h8a1eda9_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.3 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4 h93d8f39_2 conda-forge; networkx 3.2.1 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.0 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd75f5a5_0 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.2.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 conda-forge; platformdirs 4.2.0 pypi_0 pypi; pluggy 1.4.0 pypi_0 pypi; pre-commit 3.7.0 pypi_0 pypi; profimp 0.1.0 pypi_0 pypi; pynndescent 0.5.11 pypi_0 pypi; pyparsing 3.1.2 pypi_0 pypi; pytest 8.1.1 pypi_0 pypi; pytest-cov 4.1.0 pypi_0 pypi; pytest-m,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:33112,load,loader,33112,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,"... Hello, I hope I'm in the correct place. I was wondering, in Spatial Transcriptomics analyses, after loading data `adata = sc.read_visium`, where are the spot co-ordinates stored?. Example; spot x y; 1x17x20	17	20	; 1x17x21	17	21",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2253:104,load,loading,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2253,1,['load'],['loading']
Performance,.0 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.21.5); Requirement already satisfied: seaborn in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.11.2); Requirement already satisfied: tqdm in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.62.3); Requirement already satisfied: natsort in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (7.1.1); Requirement already satisfied: networkx>=2.3 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (2.6.3); Requirement already satisfied: importlib-metadata>=0.7 in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (4.8.2); Requirement already satisfied: joblib in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (1.1.0); Requirement already satisfied: sinfo in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.3.4); Requirement already satisfied: patsy in c:\users\charles\anaconda3\lib\site-packages (from scanpy) (0.5.2); Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Requirement already satisfied: six in c:\users\charles\anaconda3\lib\site-packages (from h5py>=2.10.0->scanpy) (1.16.0); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from importlib-metadata>=0.7->scanpy) (3.7.0); Requirement already satisfied: cycler>=0.10 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (0.11.0); Requirement already satisfied: pyparsing>=2.2.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (3.0.4); Requirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:2644,cache,cached,2644,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,.0; plotly 5.17.0; pluggy 1.3.0; ply 3.11; pooch 1.7.0; prometheus-client 0.17.1; prompt-toolkit 3.0.39; Protego 0.3.0; psutil 5.9.5; ptyprocess 0.7.0; PuLP 2.7.0; pure-eval 0.2.2; py-cpuinfo 9.0.0; pyarrow 13.0.0; pyasn1 0.5.0; pyasn1-modules 0.3.0; pycodestyle 2.10.0; pycosat 0.6.6; pycparser 2.21; pyct 0.4.6; pycurl 7.45.1; pydantic 1.10.13; pydeseq2 0.4.1; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0.3; pyflakes 3.0.1; Pygments 2.16.1; PyJWT 2.8.0; pylint 2.17.5; pylint-venv 3.0.2; pyls-spyder 0.4.0; pynndescent 0.5.10; pyodbc 4.0.39; pyOpenSSL 23.2.0; pyparsing 3.1.1; PyQt5-sip 12.11.0; PySocks 1.7.1; pytest 7.4.2; python-dateutil 2.8.2; python-dotenv 1.0.0; python-json-logger 2.0.7; python-lsp-black 1.3.0; python-lsp-jsonrpc 1.1.2; python-lsp-server 1.7.2; python-slugify 8.0.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz_comms 3.0.0; PyWavelets 1.4.1; pyxdg 0.28; PyYAML 6.0.1; pyzmq 25.1.1; QDarkStyle 3.1; qstylizer 0.2.2; QtAwesome 1.2.3; qtconsole 5.4.4; QtPy 2.4.0; queuelib 1.6.2; referencing 0.30.2; regex 2023.10.3; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; reretry 0.11.8; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:8619,queue,queuelib,8619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['queue'],['queuelib']
Performance,".1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2306,cache,cached-property,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached', 'cached-property']"
Performance,".index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:1761,cache,cache,1761,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"/T/sphinx-err-qbzn5se8.log, if you want to report the issue to the developers.; Please also report this if it was a user error, so that a better error message can be provided next time.; A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!; make: *** [html] Error 2; ```. </details>. <details>; <summary> contents of the referenced log file </summary>. ```python; # Sphinx version: 4.1.0; # Python version: 3.8.10 (CPython); # Docutils version: 0.16 release; # Jinja2 version: 2.11.2; # Last messages:; # reading sources... [ 2%] dev/documentation; # reading sources... [ 2%] dev/external-tools; # reading sources... [ 3%] dev/getting-set-up; # reading sources... [ 3%] dev/index; # reading sources... [ 3%] dev/release; # reading sources... [ 4%] dev/testing; # reading sources... [ 4%] dev/versioning; # reading sources... [ 4%] ecosystem; # reading sources... [ 5%] external; # reading sources... [ 5%] generated/classes/scanpy.pl.DotPlot; # Loaded extensions:; # sphinx.ext.mathjax (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/mathjax.py; # sphinxcontrib.applehelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/applehelp/__init__.py; # sphinxcontrib.devhelp (1.0.2) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/devhelp/__init__.py; # sphinxcontrib.htmlhelp (2.0.0) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/htmlhelp/__init__.py; # sphinxcontrib.serializinghtml (1.1.5) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/serializinghtml/__init__.py; # sphinxcontrib.qthelp (1.0.3) from /usr/local/lib/python3.8/site-packages/sphinxcontrib/qthelp/__init__.py; # alabaster (0.7.12) from /usr/local/lib/python3.8/site-packages/alabaster/__init__.py; # sphinx.ext.autodoc.preserve_defaults (1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/autodoc/preserve_defaults.py; # sphinx.ext.autodoc.type_comment (4.1.0) from /usr/local/lib/python3.8/site-packages/sphinx/ext/aut",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557:1508,Load,Loaded,1508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946#issuecomment-877995557,1,['Load'],['Loaded']
Performance,"/install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/base.py"", line 21, in <module>; from scipy.sparse.sputils import IndexMixin; ImportError: cannot import name 'IndexMixin'; ```. ```bash; #pip3 install works any better?; pip3 install scanpy --target $PYTHONPATH --upgrade; pip3 install scanpy-scripts --target $PYTHONPATH --upgrade",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:1406,load,load,1406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,1,['load'],['load']
Performance,"/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command /cluster/software/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/scratch/tmp/pip-build-g14",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:1952,cache,cached,1952,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,"0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 3: As you recommend, I do `!pip uninstall tables` and `conda install -c conda-forge pytables`, then; ```python; import tables # pass. import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_15024/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 19 from numpy import random; 20 from scipy import sparse; ---> 21 from anndata import AnnData, __version__ as anndata_version; 22 from textwrap import dedent; 23 f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:3476,load,load,3476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,0.0; - mdurl==0.1.2; - ml-dtypes==0.3.2; - namex==0.0.8; - opt-einsum==3.3.0; - optree==0.11.0; - rich==13.7.1; - tensorboard==2.16.2; - tensorboard-data-server==0.7.2; - tensorflow==2.16.1; - tensorflow-io-gcs-filesystem==0.37.0; - termcolor==2.4.0; - werkzeug==3.0.3; - wrapt==1.16.0; ```. The virtual environment on my laptop (successful case):; ```; channels:; - pytorch; - bioconda; - conda-forge; dependencies:; - adjusttext=1.0.4; - anndata=0.10.5.post1; - anyio=3.7.1; - aom=3.5.0; - appnope=0.1.3; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.4.1; - arrow=1.2.3; - asttokens=2.2.1; - async-lru=2.0.4; - attrs=23.1.0; - babel=2.12.1; - backcall=0.2.0; - backports=1.0; - backports.functools_lru_cache=1.6.5; - beautifulsoup4=4.12.2; - bleach=6.0.0; - blosc=1.21.4; - brotli=1.0.9; - brotli-bin=1.0.9; - brotli-python=1.0.9; - bzip2=1.0.8; - c-ares=1.19.1; - c-blosc2=2.10.2; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - cairo=1.18.0; - certifi=2024.6.2; - cffi=1.15.1; - charset-normalizer=3.2.0; - colorama=0.4.6; - colorcet=3.0.1; - colorful=0.5.4; - comm=0.1.4; - contourpy=1.1.0; - cryptography=41.0.4; - cycler=0.11.0; - dav1d=1.2.1; - debugpy=1.6.8; - decorator=5.1.1; - defusedxml=0.7.1; - dill=0.3.7; - dnspython=2.4.2; - entrypoints=0.4; - et_xmlfile=1.1.0; - exceptiongroup=1.1.3; - executing=1.2.0; - expat=2.5.0; - ffmpeg=6.0.0; - filelock=3.12.2; - font-ttf-dejavu-sans-mono=2.37; - font-ttf-inconsolata=3.000; - font-ttf-source-code-pro=2.038; - font-ttf-ubuntu=0.83; - fontconfig=2.14.2; - fonts-conda-ecosystem=1; - fonts-conda-forge=1; - fonttools=4.42.1; - fqdn=1.5.1; - freetype=2.12.1; - fribidi=1.0.10; - get-annotations=0.1.2; - gettext=0.21.1; - gffutils=0.13; - glpk=5.0; - gmp=6.3.0; - gmpy2=2.1.2; - gnutls=3.7.8; - graphite2=1.3.13; - h11=0.14.0; - h2=4.1.0; - h5py=3.9.0; - harfbuzz=7.3.0; - hdf5=1.14.1; - hpack=4.0.0; - httpcore=0.18.0; - hyperfra,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:9258,cache,cached-property,9258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['cache'],['cached-property']
Performance,0.14.0; - stdlib-list=0.10.0; - svt-av1=1.6.0; - sympy=1.12; - tbb=2021.11.0; - tenacity=8.2.3; - terminado=0.17.1; - texttable=1.7.0; - threadpoolctl=3.2.0; - tinycss2=1.2.1; - tk=8.6.12; - tomli=2.0.1; - torchvision=0.15.2; - tornado=6.3.3; - traitlets=5.9.0; - typing_extensions=4.8.0; - typing_utils=0.1.0; - tzdata=2023c; - umap-learn=0.5.5; - uri-template=1.3.0; - wcwidth=0.2.6; - webcolors=1.13; - webencodings=0.5.1; - websocket-client=1.6.2; - wheel=0.41.2; - x264=1!164.3095; - x265=3.5; - xlrd=1.2.0; - xorg-libxau=1.0.11; - xorg-libxdmcp=1.1.3; - xz=5.2.6; - yaml=0.2.5; - zeromq=4.3.4; - zipp=3.16.2; - zlib=1.2.13; - zlib-ng=2.0.7; - zstd=1.5.2; - pip:; - absl-py==1.4.0; - astunparse==1.6.3; - bcbio-gff==0.7.0; - biopython==1.81; - cachetools==5.3.1; - click==8.1.7; - flatbuffers==23.5.26; - gast==0.4.0; - geoparse==2.0.3; - gffpandas==1.2.0; - google-auth==2.22.0; - google-auth-oauthlib==1.0.0; - google-pasta==0.2.0; - grpcio==1.57.0; - imageio==2.34.1; - keras==2.13.1; - lazy-loader==0.4; - libclang==16.0.6; - louvain==0.8.2; - markdown==3.4.4; - numpy==1.24.3; - oauthlib==3.2.2; - opt-einsum==3.3.0; - protobuf==4.24.1; - pyasn1==0.5.0; - pyasn1-modules==0.3.0; - requests-oauthlib==1.3.1; - rsa==4.9; - scikit-image==0.24.0; - tensorboard==2.13.0; - tensorboard-data-server==0.7.1; - tensorflow==2.13.0; - tensorflow-estimator==2.13.0; - tensorflow-macos==2.13.0; - termcolor==2.3.0; - tifffile==2024.6.18; - tqdm==4.66.1; - typing-extensions==4.5.0; - urllib3==1.26.16; - werkzeug==2.3.7; - wrapt==1.15.0; ```. ### Minimal code sample. ```python; sc.pp.scrublet(adata); ```. ### Error output. _No response_. ### Versions. <details>. ```; # Successful case; -----; anndata 0.10.5.post1; scanpy 1.10.1; -----; PIL 9.4.0; astunparse 1.6.3; cffi 1.15.1; colorama 0.4.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; defusedxml 0.7.1; dill 0.3.7; gmpy2 2.1.2; google NA; h5py 3.9.0; igraph 0.11.3; joblib 1.3.2; kiwisolver 1.4.4; legacy_api_wrap NA; leidenalg 0.10.2; llvm,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:14945,load,loader,14945,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['load'],['loader']
Performance,"0972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2284,cache,cache,2284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,1.1.1o-h166bdaf_0. Proceed ([y]/n)? y. Downloading and Extracting Packages; keyutils-1.6.1 | 115 KB | ############################################################################# | 100% ; h5py-3.6.0 | 1.4 MB | ############################################################################# | 100% ; cached_property-1.5. | 11 KB | ############################################################################# | 100% ; c-ares-1.18.1 | 113 KB | ############################################################################# | 100% ; anndata-0.8.0 | 151 KB | ############################################################################# | 100% ; libev-4.33 | 104 KB | ############################################################################# | 100% ; libnghttp2-1.46.0 | 680 KB | ############################################################################# | 100% ; libcurl-7.82.0 | 342 KB | ############################################################################# | 100% ; libssh2-1.10.0 | 233 KB | ############################################################################# | 100% ; cached-property-1.5. | 4 KB | ############################################################################# | 100% ; openssl-1.1.1o | 2.1 MB | ############################################################################# | 100% ; certifi-2022.5.18.1 | 150 KB | ############################################################################# | 100% ; hdf5-1.12.1 | 3.5 MB | ############################################################################# | 100% ; libedit-3.1.20191231 | 121 KB | ############################################################################# | 100% ; ca-certificates-2022 | 144 KB | ############################################################################# | 100% ; krb5-1.19.3 | 1.4 MB | ############################################################################# | 100% ; Preparing transaction: done; Verifying transaction: done; Executing transaction: done. ```. Any ideas??,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:3184,cache,cached-property-,3184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['cache'],['cached-property-']
Performance,"1.4.1 is out, are we sure this is as scalable as it was before and not a backwards breaking change. If yes, we can merge immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487026612:37,scalab,scalable,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487026612,1,['scalab'],['scalable']
Performance,"108 filename = settings.datasetdir / 'moignard15/nbt.3154-S3.xlsx'; 109 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 110 adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url); 111 # filter out 4 genes as in Haghverdi et al. (2016); 112 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 92 filename = Path(filename) # allow passing strings; 93 if is_valid_filename(filename):; ---> 94 return _read(; 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 489 else:; 490 ext = is_valid_filename(filename, return_ext=True); --> 491 is_present = check_datafile_present_and_download(; 492 filename,; 493 backup_url=backup_url,. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in check_datafile_present_and_download(path, backup_url); 745 path.parent.mkdir(parents=True); 746 ; --> 747 download(backup_url, path); 748 return True; 749 . ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/scanpy/readwrite.py in download(url, path); 722 ; 723 path.parent.mkdir(parents=True, exist_ok=True); --> 724 with tqdm(unit='B', unit_scale=True, miniters=1, desc=path.name) as t:; 725 def update_to(b=1, bsize=1, tsize=None):; 726 if tsize is not None:. ~/anaconda3/envs/test_scanpy/lib/python3.8/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs); 206 unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1; 207 total = self.total * unit_scale if self.total else self.total; --> 208 self.container = self.status_printer(; 209 se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1130:1914,cache,cache,1914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1130,1,['cache'],['cache']
Performance,10x Genomics dataset loading feature,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1264:21,load,loading,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1264,1,['load'],['loading']
Performance,2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1379,cache,cached,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"2. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1816,cache,cache,1816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"200 np.concatenate([doublet_latent_rep, np.log(doublet_lib_size)], axis=1); 201 ); 202 doublet_adata.obs[LABELS_KEY] = ""doublet""; --> 204 full_adata = latent_adata.concatenate(doublet_adata); 205 cls.setup_anndata(full_adata, labels_key=LABELS_KEY); 206 return cls(full_adata, **classifier_kwargs). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/anndata/_core/anndata.py:1808, in AnnData.concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1799 pat = rf""-({'|'.join(batch_categories)})$""; 1800 out.var = merge_dataframes(; 1801 [a.var for a in all_adatas],; 1802 out.var_names,; 1803 partial(merge_outer, batch_keys=batch_categories, merge=merge_same),; 1804 ); 1805 out.var = out.var.iloc[; 1806 :,; 1807 (; -> 1808 out.var.columns.str.extract(pat, expand=False); 1809 .fillna(""""); 1810 .argsort(kind=""stable""); 1811 ),; 1812 ]; 1814 return out. File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/accessor.py:224, in CachedAccessor.__get__(self, obj, cls); 221 if obj is None:; 222 # we're accessing the attribute of the class, i.e., Dataset.geo; 223 return self._accessor; --> 224 accessor_obj = self._accessor(obj); 225 # Replace the property with the accessor object. Inspired by:; 226 # https://www.pydanny.com/cached-property.html; 227 # We need to use object.__setattr__ because we overwrite __setattr__ on; 228 # NDFrame; 229 object.__setattr__(obj, self._name, accessor_obj). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:181, in StringMethods.__init__(self, data); 178 def __init__(self, data) -> None:; 179 from pandas.core.arrays.string_ import StringDtype; --> 181 self._inferred_dtype = self._validate(data); 182 self._is_categorical = is_categorical_dtype(data.dtype); 183 self._is_string = isinstance(data.dtype, StringDtype). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:235, in StringMethods._validate(data); 23",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2474:1876,Cache,CachedAccessor,1876,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2474,1,['Cache'],['CachedAccessor']
Performance,"3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1846,cache,cached,1846,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2939,cache,cached,2939,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from import,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1528,cache,cached,1528,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"30 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; batchglm v0.7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.1.0; keras 2.9.0; kiwisolver 1.4.3; llvmlite 0.38.1; magic 3.0.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbinom_ufunc NA; numba 0.55.2; numexpr 2.8.1; numpy 1.22.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.2; parso 0.8.3; patsy 0.5.2; pcurve NA; pexpect 4.8.0; phate 1.0.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.29; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pydev_ipython NA;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:10709,bottleneck,bottleneck,10709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['bottleneck'],['bottleneck']
Performance,"30 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242922 5.806298 7.895355; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598463; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; means dispersions dispersions_norm; index ; CEP128 0.151130 5.858001 7.996479; DOK3 0.272308 5.838402 7.958147; ARVCF 0.129909 5.807068 7.896862; YPEL2 0.242923 5.806298 7.895356; UBE2D4 0.254622 5.778868 7.841706; FAM210B 0.266598 5.724431 7.735234; CTB-113I20.2 0.126570 5.654503 7.598464; GBGT1 0.177501 5.604167 7.500014; LRRIQ3 0.098048 5.437717 7.174459; MTIF2 0.220279 5.371215 7.044389; ```. To generate seurat_hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:3558,Load,Load,3558,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,2,"['Load', 'load']","['Load', 'load']"
Performance,"74 return index.get_loc(indexer) # int; 75 elif isinstance(indexer, (Sequence, np.ndarray, pd.Index, spmatrix, np.matrix)):; 76 if hasattr(indexer, ""shape"") and (. /software/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2646 return self._engine.get_loc(key); 2647 except KeyError:; -> 2648 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2649 indexer = self.get_indexer([key], method=method, tolerance=tolerance); 2650 if indexer.ndim > 1 or indexer.size > 1:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'SPP1'. ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.4.0; backcall 0.1.0; bottleneck 1.2.1; cffi 1.11.5; cloudpickle 0.5.3; colorama 0.3.9; cycler 0.10.0; cython_runtime NA; cytoolz 0.9.0.1; dask 0.17.5; dateutil 2.7.3; decorator 4.3.0; fa2 NA; flaskext NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.9.7; ipykernel 4.8.2; ipython_genutils 0.2.0; ipywidgets 7.2.1; jedi 0.12.0; joblib 0.13.2; kiwisolver 1.0.1; legacy_api_wrap 1.2; leidenalg 0.8.8; llvmlite 0.34.0; louvain 0.6.1; lxml NA; matplotlib 3.3.4; mpl_toolkits NA; natsort 6.0.0; networkx 2.5.1; numba 0.51.2; numexpr 2.6.5; numpy 1.19.5; packaging 21.0; pandas 1.1.5; parso 0.2.0; pexpect 4.5.0; pickleshare 0.7.4; pkg_resources NA; prompt_toolkit 1.0.15; psutil 5.4.5; ptyprocess 0.5.2; pycparser 2.18; pygments 2.2.0; pynndescent 0.5.0; pyparsing 2.2.0; pytz 2018.4; ruamel NA; scipy 1.4.1; scvelo 0.2.4; setuptools_scm NA; simplegeneric NA; six 1.11.0; sklearn 0.24.2; sphinxcontrib NA; storemagic NA; tables 3.4.3; texttable 1.6.2; toolz 0.9.0; tornado 5.0.2; tqdm 4.32.1; traitlets 4.3.2; typing_extensions",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2018:4281,bottleneck,bottleneck,4281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2018,1,['bottleneck'],['bottleneck']
Performance,"79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:2729,race condition,race condition,2729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/matrix.mtx.gz. ```; But I have 10x files there:; ```; ls /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/cellranger/MUC13974/count_matrices/filtered_feature_bc_matrix/; barcodes.tsv features.tsv matrix.mtx",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:2142,cache,cache,2142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,2,['cache'],['cache']
Performance,"972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1972,cache,cache,1972,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://clou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2128,cache,cache,2128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE20097",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2440,cache,cache,2440,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"9c3ff4c_1 conda-forge; zipp 3.16.2 pyhd8ed1ab_0 conda-forge. </p>; </details> . 2. If I create an environment and install scanpy and pytorch (GPU) from conda, then different runs are not reproducible:; ```; conda create -n scanpy_test2; conda install -c conda-forge scanpy leidenalg scvi-tools; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; The packages in this environment:. <details><summary>Packages</summary>; <p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9295,cache,cached-property,9295,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,":; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m); ); dblX = pos * adata.X; # TODO: Downsample total counts; srcs = np.sort(combos.reshape(n_doublets, 2), axis=1); obs = pd.DataFrame(srcs, columns=[""src1"", ""src2""]); var = pd.DataFrame(index=adata.var_names); return sc.AnnData(dblX, obs=obs, var=var). # Load data. # http: // cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_10k_v3/pbmc_10k_v3_filtered_feature_bc_matrix.h5; pbmc = sc.read_10x_h5(""./data/10x/pbmc_10k_v3_filtered_feature_bc_matrix.h5""); pbmc.var[""gene_symbols""] = pbmc.var.index; pbmc.var.set_index(""gene_ids"", inplace=True). dblt = simulate_doublets(pbmc); dblt.var[""gene_symbols""] = pbmc.var[""gene_symbols""]. pbmc.raw = pbmc; dblt.raw = dblt. pbmc = preprocess(pbmc); dblt = preprocess(dblt). sc.pp.pca(pbmc); pca_update(dblt, pbmc). umap = UMAP(); pbmc.obsm[""X_umap""] = umap.fit_transform(pbmc.obsm[""X_pca""]); dblt.obsm[""X_umap""] = umap.transform(dblt.obsm[""X_pca""]). sc.tl.embedding_density(pbmc, ""umap""); sc.tl.embedding_density(dblt, ""umap""); ```; </details>. <details> ; <summary> Getting setup for datashader plots (much shorter) : </summary>. Make dataframe:. ```python; pbmcdf = pd.DataFrame(pbmc.obsm[""X_umap""], columns=[""x"", ""y""]) # Real data; dbltdf = pd.DataFrame(dblt.obsm[""X_umap""], columns=[""x"", ""y""]) # Simulated doublets. pb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:2255,Load,Load,2255,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['Load'],['Load']
Performance,"; ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_k",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:1826,load,load,1826,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['load'],['load']
Performance,"; 1092 # A collection of keys; -> 1093 keyarr, indexer = self._get_listlike_indexer(key, axis); 1094 return self.obj._reindex_with_indexers(; 1095 {axis: [keyarr, indexer]}, copy=True, allow_dups=True. <path>/lib/python3.9/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis); 1312 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr); 1313 ; -> 1314 self._validate_read_indexer(keyarr, indexer, axis); 1315 ; 1316 if needs_i8_conversion(ax.dtype) or isinstance(. <path>/lib/python3.9/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis); 1375 ; 1376 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique()); -> 1377 raise KeyError(f""{not_found} not in index""); 1378 ; 1379 . KeyError: ""['GGAACCCTCTCCCAGC-batch1'] not in index""; ```. #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.4.0; annoy NA; anyio NA; attr 21.2.0; autoreload NA; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; brotli NA; certifi 2021.10.08; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fastjsonschema NA; fsspec 2021.08.1; h5py 3.3.0; idna 3.2; igraph 0.10.2; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.1.2; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.23.3; jupyterlab_server 2.8.2; kiwisolver 1.3.1; leidenalg 0.9.0; llvmlite 0.37.0; markupsafe 2.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.2.0; nbclassic 0.4.8; nbformat 5.7.0; nbinom_ufunc NA; notebook_shim NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.11.0; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pv",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2377:4724,bottleneck,bottleneck,4724,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2377,1,['bottleneck'],['bottleneck']
Performance,"; 5336730508589856979 --> 8881403918513157720. 5898621639535744825((any)); 8881403918513157720 --> 5898621639535744825. 2373763162411159295[""(1, 1)""]; 2513425685193572888 --> 2373763162411159295. 1659302467096852217((any)); 2373763162411159295 --> 1659302467096852217. 7195453449900658805[""(0, 0)""]; 6263727941369393084 --> 7195453449900658805. 7976077601232067203((any-\naggregate)); 7195453449900658805 --> 7976077601232067203. 687812693798660380[""(0, 1)""]; 7256567839680908872 --> 687812693798660380; 687812693798660380 --> 7976077601232067203. 3901936098833081796[""(1, 0)""]; 5898621639535744825 --> 3901936098833081796; 3901936098833081796 --> 7976077601232067203. 8795010127805778162[""(1, 1)""]; 1659302467096852217 --> 8795010127805778162; 8795010127805778162 --> 7976077601232067203. 1203378416021505679[""()""]; 7976077601232067203 --> 1203378416021505679; 9179805111332178500((invert)). 1203378416021505679 --> 9179805111332178500; 5169565091578776769[""()""]; 9179805111332178500 --> 5169565091578776769; 814146044537405006((and)); 5169565091578776769 --> 814146044537405006. 1050532709569538834[""()""]; 814146044537405006 --> 1050532709569538834; ```. I *am* of course using `map_blocks`. If we really wanted, I assume we could still replace sequences of two operations like. ```mermaid; flowchart LR. step0[""(0, 0)""] --> op0((signbit)) --> step1[""(0, 0)""] --> op1((any)) --> step2[""(0, 0)""]; ```. with individual operations, but I’m not sure if that’s worth the code readability problems. Smells of premature optimization. <details>; <summary>mean_var graph</summary>. ```mermaid; flowchart LR. step000[""(0, 0)""] --> op000((mean_\nchunk)) --> step001[""(0, 0)""] --> op00((mean_agg-\naggregate)) --> step00[""0""]; step100[""(1, 0)""] --> op100((mean_\nchunk)) --> step101[""(1, 0)""] --> op00. step010[""(0, 1)""] --> op010((mean_\nchunk)) --> step011[""(0, 1)""] --> op10((mean_agg-\naggregate)) --> step10[""1""]; step110[""(1, 1)""] --> op110((mean_\nchunk)) --> step111[""(1, 1)""] --> op10; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2621#issuecomment-1753182156:2458,optimiz,optimization,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2621#issuecomment-1753182156,1,['optimiz'],['optimization']
Performance,"; Package matplotlib conflicts for:; scanpy -> matplotlib[version='3.0.*|>=2.2']; Package scikit-learn conflicts for:; scanpy -> scikit-learn[version='>=0.21.2']; Package natsort conflicts for:; scanpy -> natsort; Package openssl conflicts for:; python=3.7 -> openssl[version='>=1.0.2o,<1.0.3a|>=1.0.2p,<1.0.3a|>=1.1.1a,<1.1.2a|>=1.1.1b,<1.1.2a|>=1.1.1c,<1.1.2a|>=1.1.1d,<1.1.2a']; Package importlib_metadata conflicts for:; scanpy -> importlib_metadata[version='>=0.7']; ```. I can import `scanpy` by opening Python 3 interpreter from the terminal by running `python`. ```; Python 3.7.5 (default, Oct 25 2019, 15:51:11); [GCC 7.3.0] :: Anaconda, Inc. on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import scanpy as sc # this works; ```. Check the `PATH`:. ```; ['', '/home/tsundoku/anaconda3/lib/python37.zip', '/home/tsundoku/anaconda3/lib/python3.7', '/home/tsundoku/anaconda3/lib/python3.7/lib-dynload', '/home/tsundoku/.local/lib/python3.7/site-packages', '/home/tsundoku/anaconda3/lib/python3.7/site-packages']; ```. But it fails to load from `reticulate`. ```; library(reticulate); repl_python(); ```. ```; import pandas as pd; import scanpy as sc; ```. ```; ModuleNotFoundError: No module named 'scanpy'; ```. Check the `PATH`:. ```; import sys; sys.path; ```. ```; ['', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/bin', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python36.zip', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/lib-dynload', '/home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages', '/home/tsundoku/R/x86_64-pc-linux-gnu-library/3.6/reticulate/python']; ```. Okay so `scanpy` is installed but the `PATH` are different. Not sure why `py_install()` doesn't work. I guess the alternative is including the those paths for reticulate but not sure at the moment how to do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452:14523,load,load,14523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575281452,1,['load'],['load']
Performance,"; sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5) #these are default values; adata.raw = adata #save raw data before processing values and further filtering; adata = adata[:, adata.var.highly_variable] #filter highly variable; sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt']) #Regress out effects of total counts per cell and the percentage of mitochondrial genes expressed; sc.pp.scale(adata, max_value=10) #scale each gene to unit variance; sc.tl.pca(adata, svd_solver='arpack'); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=20); sc.tl.umap(adata); return adata. adata = sc.read_csv(""./myfile.csv"", first_column_names=True); adata = pp(adata); ```. My computer is Mac book Intel i5. Thanks!; #### Versions. <details>. -----; anndata 0.8.0; scanpy 1.9.1; -----; OpenSSL 22.0.0; PIL 9.2.0; PyObjCTools NA; absl NA; appnope 0.1.2; astunparse 1.6.3; attr 21.4.0; backcall 0.2.0; bcrypt 3.2.0; beta_ufunc NA; binom_ufunc NA; boto3 1.24.28; botocore 1.27.28; bottleneck 1.3.5; brotli NA; certifi 2022.09.24; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; chex 0.1.5; cloudpickle 2.0.0; colorama 0.4.5; contextlib2 NA; cryptography 37.0.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; deprecate 0.3.2; dill 0.3.4; docrep 0.3.2; entrypoints 0.4; etils 0.8.0; flax 0.6.1; fsspec 2022.7.1; google NA; graphviz 0.20; h5py 3.7.0; idna 3.4; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jax 0.3.23; jaxlib 0.3.22; jedi 0.18.1; jinja2 2.11.3; jmespath 0.10.0; joblib 1.1.1; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.8.10; llvmlite 0.39.1; louvain 0.8.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; matplotlib_inline 0.1.6; ml_collections NA; mpl_toolkits NA; msgpack 1.0.3; mudata 0.2.0; multipledispatch 0.6.0; natsort 8.1.0; nbinom_ufunc NA; numba 0.56.3; numexpr 2.8.3; numpy 1.22.4; numpyro 0.10.1; opt_einsum v3.3.0; optax 0.1.3; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2359:2485,bottleneck,bottleneck,2485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2359,1,['bottleneck'],['bottleneck']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; .../usr/local/lib/python3.6/site-packages/joblib/externals/loky/backend/semaphore_tracker.py:198: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown; len(cache)). <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; ...gene_trends = d.palantir.presults.compute_gene_trends(pr_res, ; ...: d.imp_df.iloc[:, 0:1000], ['RG']) ; ...: . ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ...Segmentation fault (core dumped); ```; it made me out of the python environment.; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > ...; Scanpy version: 1.4.3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1223:272,cache,cache,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1223,1,['cache'],['cache']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; I have upgraded to scanpy 1.4.6 in a conda environment. Since then I cannot load the package into python, as it gives me the following error: `AttributeError: module 'cairo' has no attribute 'version_info'`. <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/__init__.py"", line 38, in <module>; from . import plotting as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/__init__.py"", line 1, in <module>; from ._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot, dendrogram, correlation_matrix; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/scanpy/plotting/_anndata.py"", line 16, in <module>; from matplotlib import pyplot as pl; File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 2349, in <module>; switch_backend(rcParams[""backend""]); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/__init__.py"", line 833, in __getitem__; plt.switch_backend(rcsetup._auto_backend_sentinel); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 204, in switch_backend; switch_backend(candidate); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/site-packages/matplotlib/pyplot.py"", line 221, in switch_backend; backend_mod = importlib.import_module(backend_name); File ""/home/tsztank/.miniconda3/envs/brandeis/lib/python3.7/importlib/__init__.py"", line 127, in import_module; return _bootstrap._gcd_import(name[level:], package, leve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166:150,load,load,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166,1,['load'],['load']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; I've had hard time in figuring this out. This is not a problem of scanpy directly but apparently is related to [scikit-learn 0.21 series](https://github.com/scikit-learn/scikit-learn/issues/14485) which is a dependency of latest scanpy version (1.4.6). Also related to [this comment in pytorch](https://github.com/pytorch/pytorch/issues/2575#issuecomment-523657178). My issue is that I'm using, in addition to scanpy, another library performing a dl_import with static TLS. ; So if I issue; ```python; import scanpy as sc; import graph_tool.all as gt; ```; I get. ```python; ImportError: dlopen: cannot load any more object with static TLS ; ```; error and I'm not able to use the second library. Reversing the order of the imports raises the same error and I'm not able to use `scanpy`. The issue is solved installing scikit-learn 0.20.4 (the last of 0.20 series). What are the exact scikit-learn 0.21.2 dependecies in scanpy?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:508,perform,performing,508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,2,"['load', 'perform']","['load', 'performing']"
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; Loading data using `adata = sc.datasets.visium_sge('V1_Human_Lymph_Node')` with Anndata<0.7rc1 leads to error `'AnnData' object has no attribute 'is_view'`.; The reason is that the function name changed in version 0.7rc1 from `isview` -> `is_view`. I propose two possible solutions:; **Solution A**: Change requirements to `anndata>=0.7rc1`; **Solution B**: Add function to anndata:; ```python; def isview(self):; return self.is_view(); ```; I think solution B is preferable as it provides back-compatibility of anndata. ---; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; pip install git+https://github.com/theislab/scanpy.git@spatial; import scanpy as sc; adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'); ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Variable names are not unique. To make them unique, call `.var_names_make_unique`.; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-2-59eff31dcd22> in <module>; 1 get_ipython().system('pip install git+https://github.com/theislab/scanpy.git@spatial'); 2 import scanpy as sc; ----> 3 adata = sc.datasets.visium_sge('V1_Human_Lymph_Node'). /opt/conda/lib/python3.7/site-packages/scanpy/datasets/__init__.py in visium_sge(sample_id); 368 ; 369 # read h5 file; --> 370 adata = read_10x_h5(files['counts']); 371 adata.var_names_make_unique(); 372 . /opt/conda/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_h5(filename, genome, gex_only); 169 if gex_only:; 170 adata = adata[:, list(map(lambda x: x == 'Gene Expression', adata.var['feature_types']))]; --> 171 if adata.is_view:; 172 return adata.copy(); 173 else:. AttributeError: 'AnnData' object has no attribute 'is_view'; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; >",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1027:74,Load,Loading,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1027,1,['Load'],['Loading']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; The bug is just like the title of issue, _AttributeError: module 'scanpy' has no attribute 'anndata'_, for I just wanna to load a h5ad file from Tabula-Muris dataset; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc. data = sc.anndata.read_h5ad(''tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'); ```; <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; Traceback (most recent call last):. File ""<ipython-input-91-67a79760d7cb>"", line 1, in <module>; sc.anndata.read_h5ad('tabula-muris-senis-facs-processed-official-annotations-Bladder.h5ad'). AttributeError: module 'scanpy' has no attribute 'anndata'; ```; #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; > scanpy==1.4.3 anndata==0.6.19 umap==0.3.10 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.1 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1322:197,load,load,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1322,1,['load'],['load']
Performance,"<!-- Please give a clear and concise description of what the bug is: -->; scanpy.pp.recipe_seurat and scanpy.pp.recipe_zheng17 indicate that they expect non log-transformed data. This leads both functions to do by default the highly variable gene (HVG) selection on non log-transformed data. This seems contrary to the scanpy and seurat clustering tutorials, which perform HVG selection after log-transform. It also seems contrary to the new function scanpy.pp.highly_variable_genes which expects log-transformed inputs. scanpy version : 1.5.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251:365,perform,perform,365,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251,1,['perform'],['perform']
Performance,"<!-- What kind of feature would you like to request? -->. It would be very useful for the GPU data science and research community if Scanpy were able to perform end to end workflows on the GPU, using either Cupy, CuDF or both. An initial iteration of this feature could include simply swapping out the numpy imports for cupy. . - [X] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177:153,perform,perform,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177,1,['perform'],['perform']
Performance,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [x] Other?. <!-- Please describe your wishes below: -->; ... I'm adapting ScanPy for my compositional data analysis (CoDA) methodologies in the same vein as https://github.com/scverse/scanpy/issues/2475 . In this, I would like to perform Aitchison PCA but I'd also like to keep my Anndata object in counts form instead of creating another one. . For example: . ```python; import numpy as np; import pandas as pd; from typing import Union. def clr(x:Union[np.ndarray, pd.Series], multiplicative_replacement:Union[None,str,float,int]=""auto"") -> Union[np.ndarray, pd.Series]:; """"""; http://scikit-bio.org/docs/latest/generated/skbio.stats.composition.clr.html#skbio.stats.composition.clr; """"""; assert np.all(x >= 0); if multiplicative_replacement == ""auto"":; if np.any(x == 0):; multiplicative_replacement = 1/(len(x)**2); if multiplicative_replacement is None:; multiplicative_replacement = 0; x = x.copy() + multiplicative_replacement; x = x/x.sum(); log_x = np.log(x); geometric_mean = log_x.mean(); return log_x - geometric_mean. # Add CLR to layers; adata.layers[""clr""] = adata.to_df().apply(clr, axis=1) # Not the fastest way but just to show example. sc.tl.pca(adata, svd_solver='arpack', layer=""clr"") # -> Return addata object where PCA is performed on CLR. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2476:642,perform,perform,642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2476,2,['perform'],"['perform', 'performed']"
Performance,"<!-- What kind of feature would you like to request? -->; - [ x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; In function read_10x_mtx there could be an option to search for non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:970,cache,cache,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; As more and more technologies allow multimodal characterization of single cells it could be useful to exploit some functionalities of scanpy's toolkit to perform, at least, some rough integrative analysis. Assuming we have to modalities on different layers (say RNA and ATAC), one could create two knn graphs for both layers and use `leidenalg.find_partition_multiplex` to perform a joint call of partitions handling the two (or more) graphs as a multiplex. I have tested myself this approach, described in [leidenalg documentation](https://leidenalg.readthedocs.io/en/latest/multiplex.html), it works and it is highly configurable. ; We can take care of the implementation of enhancement (as `leiden_multiplex()` function?), I just want to be sure that it is not already on the development roadmap and that it is ok to have it into scanpy and not as an external tool.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107:623,perform,perform,623,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107,2,['perform'],['perform']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2762; - [x] Tests included; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: dev process change. All changes were automatic, except for:. - removing unused imports or replacing them with `__all__`. Much more uncontroversial than AnnData as scanpy’s public modules were more well defined from the start. There were no ambiguous cases except for `sc.get` re-exporting `""_check_mask""`, `""_get_obs_rep""`, and `""_set_obs_rep""`. But since those aren’t documented, we can decide over their fate at a later date.; - Fixing circular imports like `sc.{pp,tl}.pca`. I only needed to create a `neighbors/_doc.py` file for shared neighbors/tools doc parts, and put the `pca` import in `tools` in `__getattr__` (supported since 3.7); - replacing some `with open(p) as f: x = json.loads(f.read())`s with `x = json.loads(Path(p).read_text())`. All in all surprisingly easy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2761:1149,load,loads,1149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2761,2,['load'],['loads']
Performance,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2969; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Seems like this code is super performance sensitive: Having a Python implementation of `getrandbits` in 8572ecb1b38616f98f2af6462aa4fe5a3a8871ae resulted in a slowdown:. | Change | Before [0d4554b4] | After [1b2d9dd5] | Ratio | Benchmark (Parameter) |; |----------|----------------------|---------------------|---------|------------------------------------------|; | + | 15.2±0.03ms |	31.7±0.1ms |	2.09 | preprocessing.time_highly_variable_genes |,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3041:516,perform,performance,516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3041,1,['perform'],['performance']
Performance,<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #2973 ; - [ ] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. Visium HD stores its coordinates in a `.parquet` file. This loads said file.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992:547,load,loads,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992,1,['load'],['loads']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Closes #3051; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [ ] Release notes not necessary because:. TODO:. - [x] release notes; - [x] some added text explaining things; - [x] run internet tests, implement caching for datasets. Optional:. 1. continue to not run the internet tests in CI. A side effect of this PR is that our tests get less flaky by not running the flaky `ebi_expression_atlas` doctest; 2. run internet tests in CI; 1. add caching to CI; 2. make sure the dataset functions don’t download already-downloaded data; 3. validate cached data instead; 4. run the internet tests (with caching) in CI. ## [Rendered](https://icb-scanpy--3060.com.readthedocs.build/en/3060/api/datasets.html)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3060:926,cache,cached,926,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3060,1,['cache'],['cached']
Performance,"<!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this; 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already!; - [x] Tests included or not required because:; <!-- Only check the following box if you did not include release notes -->; - [x] Release notes not necessary because: edited",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3306:905,load,load,905,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306,1,['load'],['load']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->. In our analyses we wanted to try SCTransform normalization instead of default log-norm. I have done it quite crudely, but it works: I run SCT in Seurat and dump the counts on disk to load in scanpy.; While verifying that this approach worked, we encountered slight inconsistencies between clustering using (1) vanilla log-norm scanpy (2) SCT imported scanpy and (3) SCT in Seurat.; After investigation, it appears that vanilla scanpy sometimes better picks up some clusters than SCT+scanpy, despite the latter having more relevant genes in its HVG list. Here is the investigation: https://github.com/mxposed/notebooks/blob/master/sct-scanpy.ipynb. And here are the main questions that remain:; 1. Why Vanilla scanpy could resolve those populations, despite operating on less marker HVGs?; 2. What is the difference between kNN graph construction and clustering between Seurat and scanpy?; 3. How to be sure we did not undercluster and miss some smaller cell populations?. I would be glad for any feedback or input, and of course if someone knows the answers, that's great!. Best wishes,; Nick. PS. Thank you for scanpy!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531:360,load,load,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531,1,['load'],['load']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ... Hi,. I notice that even if I set n_jobs=1 in sc.pp.regress_out, all cpus are utilized. This also happens if I set n_jobs to other numbers. Basically there isn't a noticeable difference in cpu usage no matter what number of n_jobs I set. I'm using CentOS6.8 on a machine with 28 physical cores and hyper threading on (appears as 56 cores in the os). Is this an intended behavior, or just my installation? I understand that some numpy functions are naturally multi-threaded because of the setup of BLAS libraries. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1396:638,multi-thread,multi-threaded,638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1396,1,['multi-thread'],['multi-threaded']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; ...; Since I'm writing an extension that performs some analysis on neighbor graph I'm a bit confused about the current status of scanpy's way to store that information. Will connectivity matrix always be stored into `adata.obsp` making `adata.uns['neighbors']['connectivities']` deprecated? I always need to access the connectivity sparse matrix, as far as I understand the path will be. ```python; conn_key = adata.uns[neighbor_key][f'{neighbor_key}_connectivities']; adata.obsp[neighbor_key][conn_key]; ```. except when no `neighbor_key` is given and matrix is `adata.obsp['connectivities']`, correct?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1176:218,perform,performs,218,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1176,1,['perform'],['performs']
Performance,"<!--; ⚠ If you need help using Scanpy, please ask in https://scanpy.discourse.group/ instead ⚠; If you want to know about design decisions and the like, please ask below:; -->; Hi, ; I am little confused about the parameter in pl.ump, use_raw=False. ; When you set raw=False, it takes normalized, log transformed but not corrected gene expression, while when you set user_raw=True, it takes scaled and corrected gene expression. What does corrected gene expression means here? . From tutorial it reads as below:; ""As we set the .raw attribute of adata, the previous plots showed the “raw” (normalized, logarithmized, but uncorrected) gene expression. You can also plot the scaled and corrected gene expression by explicitly stating that you don’t want to use .raw."". Trying to get some clarification on my results, in case, where i performed DE with t-test, and get top 5 genes, When i want to look them in clusters and plot ump, I do not see them with pl.ump with user_raw=True but can see them with user_raw=False. ; Any clarification will be great. . thanks, ; Preeti ; ...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1266:832,perform,performed,832,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266,1,['perform'],['performed']
Performance,<details>; <summary>pip list</summary>. ```; anndata 0.7.8; asttokens 2.0.5; bcrypt 3.2.0; Bottleneck 1.3.2; brotlipy 0.7.0; cached-property 1.5.2; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.12; chart-studio 1.1.0; click 8.0.4; cmake 3.22.2; colorama 0.4.4; conda 4.11.0; conda-package-handling 1.7.3; cryptography 36.0.1; cycler 0.11.0; Cython 0.29.20; devtools 0.8.0; dunamai 1.9.0; executing 0.8.2; fa2 0.3.5; Fabric 1.6.1; fonttools 4.29.1; get_version 3.5.4; h5py 3.6.0; idna 3.3; igraph 0.9.9; install 1.3.5; joblib 1.1.0; kiwisolver 1.3.2; legacy-api-wrap 1.2; llvmlite 0.38.0; loom 0.0.18; loompy 3.0.6; mamba 0.15.3; matplotlib 3.5.1; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; MulticoreTSNE 0.1; natsort 8.1.0; networkx 2.6.3; numba 0.55.1; numexpr 2.8.1; numpy 1.21.2; numpy-groupies 0.9.14; opt-einsum 3.3.0; packaging 21.3; pandas 1.4.1; paramiko 2.9.2; patsy 0.5.2; Pillow 9.0.1; pip 21.2.4; plotly 5.6.0; pycosat 0.6.3; pycparser 2.21; PyNaCl 1.5.0; pynndescent 0.5.6; pyOpenSSL 22.0.0; pyparsing 3.0.7; PyQt5 5.12.3; PyQt5_sip 4.19.18; PyQtChart 5.12; PyQtWebEngine 5.12.1; pyro-api 0.1.2; pyro-ppl 1.8.0; pysam 0.18.0; PySocks 1.7.1; python-dateutil 2.8.2; pytz 2021.3; requests 2.27.1; retrying 1.3.3; ruamel-yaml-conda 0.15.80; scanpy 1.7.0rc1; scikit-learn 1.0.2; scipy 1.7.3; seaborn 0.11.2; setuptools 58.0.4; sinfo 0.3.4; six 1.16.0; statsmodels 0.13.2; stdlib-list 0.8.0; tables 3.7.0; tenacity 8.0.1; texttable 1.6.4; threadpoolctl 3.1.0; torch 1.10.2; tornado 6.1; tqdm 4.62.3; umap-learn 0.4.6; unicodedata2 14.0.0; urllib3 1.26.8; velocyto 0.17.17; wheel 0.37.1; xlrd 1.2.0; ```. </details>,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318:91,Bottleneck,Bottleneck,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2169#issuecomment-1062402318,2,"['Bottleneck', 'cache']","['Bottleneck', 'cached-property']"
Performance,"= False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex. ```. </details>. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; backcall 0.2.0; bb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:4727,Cache,CachedProperty,4727,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['Cache'],['CachedProperty']
Performance,"= sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2085,cache,cached-property,2085,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['cache'],['cached-property']
Performance,"> . Hi Alex,. I basically followed the discussion thread in Seurat: remove the header of the tissue_positions.csv, and change this file name to tissue_positions_list.csv. Based on the discussion of another thread, the scanpy authors are maintaining squidpy for the spatial transcriptomics part. If you use the squidpy function to load the data, there's no such issue (they considered the versions in their code). Best,; Changfeng",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2391#issuecomment-1412756118:330,load,load,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2391#issuecomment-1412756118,1,['load'],['load']
Performance,"> @LuckyMD matrix multiplication on sparse matrices is actually a pretty efficient version of breadth first search, as [used by graphBLAS](http://arxiv.org/abs/1606.05790v2). Hmm... i had no idea it was more efficient than using queues, but Figure 10 suggests otherwise. Matrix multiplication definitely seems easier. > do you want the first step neighbors to have the same weight as the second step neighbors?. You could keep these separate by binarizing the adjacency matrix before doing the multiplication. The neighbors that are only reachable via the Nth-hop should always have a 1 in the N-th matrix product that way.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531:229,queue,queues,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1383#issuecomment-704331531,1,['queue'],['queues']
Performance,"> @flyingsheep I can assure you, that's the normal case in academic HPC systems. I agree that this is a huge and common problem in many HPC systems. I usually install conda and R packages to non-home directories with bigger space to avoid issues on servers. One can fill up hundreds of MB by just installing a single package e.g. human genome from Bioconductor 😄 . > Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. There is a user home and the cache is `~/.cache` and $XDG_CACHE_HOME is undefined (at least in my case). Some pip wheel files are there for example. . Although it's painful to work in such systems, I believe it's user's responsibility to fix this. One idea might be to print a warning when the cache directory is created for the first time along with the path itself to inform the user about where files are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878:413,cache,cache,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476797878,5,['cache'],['cache']
Performance,"> Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. we could overcome this by simply updating anndata in the test then. > @cache is new in 3.8, but the implementation is:. what do you suggest to do? use your implementation or implement this wrapper?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705:109,cache,cached,109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1053622705,2,['cache'],"['cache', 'cached']"
Performance,"> And scipy is also some 100 MB right?. Scipy is actually under `~/.cache` on my mac, ¯\\\_(ツ)_/¯. > Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. > miniconda is somewhere else for me by default, and it contains everything. I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. > You'd not notice it much, because datasets are just being re-downloaded on demand. So the compute nodes on this HPC have limited internet connectivity. One of the use cases I'd had for adding the expression atlas was to be able to easily try a method across a bunch of test datasets. If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. > My favorite command line interfaces have the ability to query options and set options globally by writing to a config file. I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804:68,cache,cache,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478212804,4,['cache'],"['cache', 'cached']"
Performance,"> By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:824,optimiz,optimization,824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088,1,['optimiz'],['optimization']
Performance,"> Can we keep the docs on what exactly is happening + how to troubleshoot somewhere in this doc? This means things like: How to tag + build locally, twine check, list contents of distributed file etc. Sure, as we agreed on in person, I’ll just add a section to the end of the document.; If the build process or package structure aren’t touched, doing things manually isn’t necessary. > We should also automate some checks to avoid broken releases. As we agreed in person: Let’s postpone this. E.g. don't allow this except on specific branches + probably turn on merge queue so we know only commits that pass tests + doc builds get to those branches. This PR automatically does `twine check`, which is enough improvement over “trust the person doing the release to do that” to be worth the change, even if it wasn’t for the added convenience!. /edit: all addressed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678:568,queue,queue,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2720#issuecomment-1785549678,1,['queue'],['queue']
Performance,"> Concatenating obsm without touching uns puts the object in an unstable state somehow from diffmap point of view. Sure, but this should only ever effect `diffmap`. . Arguably it also puts the object in an unstable state from a PCA point of view since there's no promise that observation loadings correspond to the variable loadings. I don't think users should have the expectation that meaning is preserved by concatenation, but I'm not sure if this is something people would believe. > I'm not entirely sure. Less experienced users might concatenate things and plot a UMAP without running sc.tl.umap on the new concatenated object and see some super weird things. Have users reported that this is confusing?. > It'd be cool to print a warning in such cases somehow, that concatenated obsms are not compatible or so. I think a note in the docstring for concatenation should be sufficient. My expectation is that it's much more common for our users to be familiar with what similar methods (like `np.concatenate` and `pd.concat`) do, and to have the right expectations about this. Bioconductor's `SummarizedExperiment` classes also do not warn about this, and concatenate along their `reducedDims`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183:288,load,loadings,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1021#issuecomment-582736183,2,['load'],['loadings']
Performance,"> Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. I'll try and do that soon. For now, I'll focus on providing you with the benchmarks you requested!. > * Datasets size (one small, one large (>50k cells)); > * Implicit centering, densifying centering, no centering; > * single threaded, multi-threaded <---------. I could not find a `n_jobs` argument in `scanpy.pp.pca`. Can you elaborate a little on the single threaded, multi-threaded bit?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273:485,multi-thread,multi-threaded,485,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589512273,2,['multi-thread'],['multi-threaded']
Performance,"> Greatest advantage at first sight for me: scanpy.api.AnnData is now anndata.AnnData. to be fair, this was a simple consequence of adding intersphinx and would have been possible without the rest. > Also, you don't seem to have to mingle around with autodoc anymore, which seems a good thing... yes, but by now i added another, less invasive hack to get the parameter doc style the way you want them. it would be nice to have numpydoc-style parameter rendering as a separate extension or sphinx option. :skull_and_crossbones: the hack is also not finished, as in its current form, it’ll break docstrings with indentation (code blocks, lists, …). optimally the hack would be rewritten as a sphinx extension that can be loaded after the others. (it’s only a hack because it piggypacks on another extension just to ensure it runs last). > it's going to be a lot of work to rewrite all the docstrings... We don’t lose anything if we do it gradually: Unconverted Docstrings just render as they do now. > there might be some danger of introducing bugs as one needs to rewrite the function headers. i don’t think it’s possible to get bugs this way: we’re just adding type annotations, we don’t change the defaults or the order or anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230:719,load,loaded,719,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/119#issuecomment-379833230,1,['load'],['loaded']
Performance,"> Has CITE-seq support in scanpy been abandoned? Would you recommend muon instead? https://muon-tutorials.readthedocs.io/en/latest/cite-seq/1-CITE-seq-PBMC-5k.html. You can still use the CLR transformation function provided in the previous posts to transform the data and perform the rest of the analysis steps using scanpy. Alternatively, you can use the clr transform function from scikit-bio (http://scikit-bio.org/docs/0.4.1/generated/generated/skbio.stats.composition.clr.html). Either way, you will need to convert the adata to pandas dataframe using the `to_df()` function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336:272,perform,perform,272,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-1054761336,1,['perform'],['perform']
Performance,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; > ; > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring.""; > ; > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257:682,perform,performed,682,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1466032257,1,['perform'],['performed']
Performance,"> Hey @ywen1407!; > ; > The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though.; > ; > Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here... Thanks for the explanation. I tried concatenating all samples with inner join and it actually went well! The overall number of genes do drop from 45K to around 20K but after preprosessing, the clustering looks OK.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229:671,optimiz,optimize,671,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-699114229,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"> I actually meant recreate the counts by reloading the data object ;). I guess I think about this because I want the h5ad file to include absolutely everything, so that it can be simply used as a single file distribute the ""full dataset"" (with annotations, noisy genes, raw and normalized expression, cell/gene representations etc.). Imagine you upload a single h5ad file to GEO when you publish something and you're done without thinking about how much the users can ""go back"" from the h5ad file. Otherwise yeah, it's possible to either unnormalize things or load the original data file. > we would normally regard this as background noise anyway, no?. This depends on how the filtering is done I think. Some people keep only protein coding genes in adata.X, which makes adata.raw even more important since all non-coding gene expression goes to adata.raw. Or miro/ribo genes are filtered out sometimes, which might be needed later on e.g. to redo qc etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442:561,load,load,561,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1798#issuecomment-819938442,1,['load'],['load']
Performance,"> I could not find a n_jobs argument in scanpy.pp.pca. Can you elaborate a little on the single threaded, multi-threaded bit?. The blas library used by numpy is multithreaded by default. You can change this by setting an environment variable. This might have to happen before numpy is imported. Here's how you'd do that:. ```python; import os; os.environ[""MKL_NUM_THREADS""] = ""1"" # If you're using MKL blas; os.environ[""OPENBLAS_NUM_THREADS""] = ""1"" # If you're using open blas; ```. Using sc.datasets.pbmc3k:. <details>; <summary> Single threaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 4.36 s, sys: 57.2 ms, total: 4.42 s; Wall time: 4.43 s. %time sc.pp.pca(pbmc) ; CPU times: user 15.7 s, sys: 127 ms, total: 15.8 s; Wall time: 15.8 s; ```. </details>. <details>; <summary> Multithreaded </summary>. ```python; %time sc.pp.pca(pbmc, pca_sparse=True) ; CPU times: user 28.9 s, sys: 5.44 s, total: 34.4 s; Wall time: 2.39 s. %time sc.pp.pca(pbmc) ; CPU times: user 1min 37s, sys: 23.6 s, total: 2min 1s; Wall time: 9.92 s; ```. </details>. > I noticed that if zero_center=False then TruncatedSVD does not accept the svd_solver argument and defaults to the randomized solver. Good catch! I'm pretty sure that should be passed the solver.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438:106,multi-thread,multi-threaded,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589921438,1,['multi-thread'],['multi-threaded']
Performance,> I had the same problem when I loaded sample data from a csv as a data frame and assigned it to adata.obs = df; > […](#). I meet the same problem when I try replace the adata.obs with annother pandas dataframe,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157:32,load,loaded,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-1019247157,1,['load'],['loaded']
Performance,"> I know exactly that in PCA I can interpret a component based on its rank (and/or variance contribution). Ah, I meant more specifically that it may be easier to biologically interpret an ICA. > That would say I should try as many decompositions as possible to see when I get a good result. I'm a little unsure of your meaning here. Do you mean decompositions like decomposition techniques? If so, I don't think this is the right conclusion. I think it means: probably PCA for clustering, probably NMF for finding gene modules. I would also suspect something which finds sparser variable loadings like ICA or NMF could be more robust for cross dataset classification. If you mean, if the results are unstable how do we know which to trust – I did ask that question. I think it's the usual: have a validation dataset, maybe some ensemble/ robustness method, or do some sort of enrichment. It's an open question, but a lot of our analysis pipeline is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033:588,load,loadings,588,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560313033,1,['load'],['loadings']
Performance,"> I saw some of the github automated tests test are failing now, but I don't really understand the error messages tbh ;) Are they even related to the execution of the code provided by this PR?. yeah also don't understand them, it might be @cache in py 3.7 has issues? will investigate next week and report back!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610:240,cache,cache,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050003610,1,['cache'],['cache']
Performance,"> I think it's really good to record the key added. I have a couple questions about the quality score.; > ; > * Should it still be called a modularity score if we aren't using the modularity quality function?. If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term `scaled modularity` anymore. > * Do you have some use cases for recording the modularity score? My impression was that it may not have much interpretable meaning, especially between different graphs.; > . To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). > Also, should this stuff be mirrored to `leiden`?. It's done.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529235054:406,optimiz,optimization,406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529235054,1,['optimiz'],['optimization']
Performance,"> I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized?. You can see where the materialization occurs by looking for references to `materialize_as_ndarray` in the existing code. For example, in `filter_genes`: https://github.com/theislab/scanpy/blob/master/scanpy/preprocessing/_simple.py#L215, where the gene subset of materialized as an ndarray, then used to subset the anndata. Contrast this to the optimized version where the materialize step is not needed, and the data remains a dask array throughout the `filter_genes` method: https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L18. > I think this can be worked around in AnnData side in many cases. That would be great. > Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. On the GPU questions, these all sound like promising avenues, but I haven't looked into any of them.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037:548,optimiz,optimized,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-555940037,3,"['bottleneck', 'optimiz', 'perform']","['bottlenecks', 'optimized', 'performance']"
Performance,"> I'm not sure i fully understand the point of caching. So you store the exact output of all the computations of a function so that it can be rerun exactly? How big do those objects become?. We've had problems in the past when running notebooks on different computers (by having different distros or just using the server) or just updating a library produced different results in terms of embedding/clustering... The other benefit is that if analyzing the data in multiple stages (or multiple times), you'd have to either store the adata object after each stage and then load it for the next one. Or just run it from scratch, which can take some time. Not to mention a forgotten parameter which affects reproducibility. The caching makes this convenient - just run the notebook. We only store the attributes generated by each function, therefore the size depends on what you cache and the dimensionality of the data. For ~8k cells, PCA takes upto 8MB (if I remember correctly).; Currently, there's no compression scheme in place, but I have it on my todo list.; The other thing would be to add more control to user during runtime about what needs to be cached.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523:571,load,load,571,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947#issuecomment-562544523,3,"['cache', 'load']","['cache', 'cached', 'load']"
Performance,"> I'm not sure what t-SNE implementation is currently used in scanpy, but would it make sense to switch it to openTSNE? It's a Cython re-implementation of FIt-SNE, it's available on conda and should be very easy to depend on. We use `MulticoreTSNE` if it's installed, but fall back to `sklearn`. > As far as I understand the scanpy architecture, it builds a kNN graph and then runs downstream analysis. Right now, we tend to use a connectivity graph built by UMAP, but are working on making this more generic. We're thinking about allowing the UMAP embedding to be generated on graphs we provide as well. > 1. switch scanpy to using openTSNE for tSNE, using already constructed kNN graph. I think I'd like to see this. That package is much more actively maintained than our current backend, and looks interesting. I would like it if the TSNE was flexible about the graph that was used. I'm not sure that I'll get to this, but a PR would be welcome. I'd have to see some performance/ results before thinking about changing the defaults, or whether this would go into a major or minor version change. > 2. add tSNE support for ingest using openTSNE functionality. @Koncopd do you have any thoughts on this?. > 3. change default tSNE parameters (n_iter, learning rate, initialization) following openTSNE defaults. Again, I'd have to think about backwards compatibility. Maybe this could start as a `sc.tl.opentsne` function?. > 4. add some tSNE ""recipes"". I'd be interested in this. Skimming that paper now, I really like the idea of showing regions of uncertainty for projection would be very useful. I'd be interested in how these ""recipes"" could be wrapped in a function.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395:970,perform,performance,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1233#issuecomment-631235395,1,['perform'],['performance']
Performance,"> IIRC, it's discussed in more detail in Malte's paper:; > ; > > ; > ; > In the same way that cellular count data can be normalized to make them comparable between cells, gene counts can be scaled to improve comparisons between genes. Gene normalization constitutes scaling gene counts to have zero mean and unit variance (z scores). This scaling has the effect that all genes are weighted equally for downstream analysis. There is currently no consensus on whether or not to perform normalization over genes. While the popular Seurat tutorials (Butler et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0020)) generally apply gene scaling, the authors of the Slingshot method opt against scaling over genes in their tutorial (Street et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0125)). The preference between the two choices revolves around whether all genes should be weighted equally for downstream analysis, or whether the magnitude of expression of a gene is an informative proxy for the importance of the gene. In order to retain as much biological information as possible from the data, we opt to refrain from scaling over genes in this tutorial.; > ; > https://www.embopress.org/doi/full/10.15252/msb.20188746; > ; > Since there has been no new development on this topic, we cited Malte and also opted not to scale. This is also discussed by Malte himself in the issue that was cited above.; > ; > I cannot comment on spatial data itself and make confident statements here. Thanks a lot; so is there a conclusion or recommendation whether scale or not on spatial data? @ivirshup @Zethson",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034435734:476,perform,perform,476,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034435734,1,['perform'],['perform']
Performance,"> If another flavor is used, we do not record or print anything about quality, so it's ok. But there is a chance that user supplies a partition type from Louvain/Leiden that is not using modularity optimization (e.g. CPM or Surprize). In this case, we do not print the term scaled modularity anymore. By default we don't use modularity as the quality function, we use the `RBConfigurationVertexPartition`. I believe that we should get a quality score from the leiden and louvain packages regardless of quality function though. Could we use the term `quality_score` and also store `quality_function` used in params like: `.uns[key_added][""params""][""quality_function""] = partition_type.__name__`?. > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :). A couple follow up points on this and @LuckyMD's points. * I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; * I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529329056:198,optimiz,optimization,198,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529329056,1,['optimiz'],['optimization']
Performance,"> In https://github.com/theislab/anndata/issues/311 I found a case where converting arrays of numerics to strings creates a bug when assigning to AnnData obsm with DataFrames with a RangeIndex. In that case, I understand there's a desire to avoid ambiguity in positional vs label indexing, but that issue was solved in pandas with the .loc and .iloc conventions. Why not carry that forward?. Also, the index of a dataframe in `obsm` must match `obs_names`. . > An alternative would be to explicitly return a categorical from the clustering function, i.e. rather than ensuring that the clustering returns an array of str, ensure that it returns a categorical where the categories are ints. We do explicitly return a categorical, it's just a categorical of strings. There are a few reasons for this:. * Some plotting libraries (more-so when the methods were written) don't respect that a categorical array with numeric values is categorical. `seaborn` has some very weird behavior around this.; * When subclustering within a cluster, the sub-cluster is named like: `""{previous}-{sub}""`. You can't do that with a numeric value. By just always using string categoricals we can be consistent about the type resulting from `sc.tl.leiden` this way.; * Unclear what the advantage of using integer values for clustering names would be. Performance should be the same since they are categoricals. ----------------------. About `plt.scatter(adata.X[:,0], adata.X[:,1], c=adata.obs['louvain'])`, I think most of the scientific python ecosystem would like it if `c` could be categorical, and that it would mean categorical palette would be used. There's even an issue by our own @flying-sheep about it https://github.com/matplotlib/matplotlib/issues/6214, but it sounds like it's not gonna happen anytime soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678:1327,Perform,Performance,1327,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-582728678,1,['Perform'],['Performance']
Performance,"> In particular, I'm wondering if there might be a jax implementation as I'm a bit more keen on that as a dependency. I don't have any plans to switch from PyTorch to JAX. I did evaluate JAX when I started the project, but it wasn't mature enough back then. > I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. I'm not super clear on the semantics of the graphs obtained from UMAP. They might differ somewhat from the ones obtained from PyMDE. > Would this be the right way to retrieve the graphs for the object, or is distortions not the right field?. That's not quite right. Assuming that `mde` was constructed from `preserve_neighbors`, try this:. ```python3. weights = mde.distortion_function.weights.cpu().numpy(); edges = mde.edges.cpu().numpy(); n_items = mde.n_items. graph = pymde.Graph.from_edges(edges, weights, n_items).adjacency_matrix; ```. (API docs for `Graph` here: https://pymde.org/api/index.html#pymde.Graph. In the Graph class, distances/weights are used interchangeably.). I'll just mention however that with PyMDE, the weights and edges don't fully determine the embedding. The weights are parameters to distortion functions, which convey the extent to which two items are similar or dissimilar. Roughly speaking positive weights mean items are similar and should be close together, and negative weights mean that they're dissimilar and shouldn't be close (but need not be far). More details here:https: //pymde.org/mde/index.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262:305,perform,perform,305,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062222262,1,['perform'],['perform']
Performance,"> Is there anything like [clustree](https://github.com/lazappi/clustree) in python that integrates nicely with scanpy?. I have resorted to writing a small Rscript that takes a saved adata.h5ad file as input, loads it using `reticulate`, runs Clustree, saves it. I then run the script from a notebook using `invoke.run` from the `invoke` package as a function in a notebook and load the output figure as an image in the notebook. Here is the script I use in case it helps:. ```R. suppressPackageStartupMessages({; library(reticulate); library(SingleCellExperiment); library(glue); library(clustree); }); sc <- import(""scanpy""). args <- commandArgs(trailingOnly = TRUE); H5AD_PATH = args[1]; OUT_PATH = args[2]. print(glue(""H5AD_PATH: {H5AD_PATH}"")); print(glue(""OUT_PATH: {OUT_PATH}"")). load_adata = function(h5ad_path) {; adata <- sc$read_h5ad(h5ad_path). return(adata); }. count_clusterings = function(adata){; # Ryan suggests:; # length(grep(""leiden"",names(adata$obs))). clusterings = c(); for (x in adata$obs_keys()){; if (startsWith(x, ""leiden"")){; clusterings = append(clusterings, x); }; }; ; return(length(clusterings)); }. set_fig_dimensions = function(num_clusterings){; width = 10; height = (0.6 * num_clusterings); ; if (height < 8){; height = 8; }; ; png(width = width, height = height); options(repr.plot.width = width, repr.plot.height = height); ; return(list(width=width,height=height)); }. adata = load_adata(h5ad_path=H5AD_PATH). dims = set_fig_dimensions(num_clusterings = count_clusterings(adata)); # dims. # options(repr.plot.width = 10, repr.plot.height = 10). g = clustree(; x=adata$obs,; prefix=""leiden_"",; # suffix = NULL,; # metadata = NULL,; # count_filter = 0,; # prop_filter = 0.1,; # layout = ""sugiyama"",; # layout = ""tree"",; # use_core_edges = FALSE,; # highlight_core = FALSE,; # node_colour = prefix,; # node_colour_aggr = NULL,; # node_size = ""size"",; # node_size_aggr = NULL,; # node_size_range = c(4, 15),; # node_alpha = 1,; # node_alpha_aggr = NULL,; # node_text_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409:208,load,loads,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-785309409,2,['load'],"['load', 'loads']"
Performance,"> It doesn't have to be a TIFF image - in my experience slide scanners save JPEG images internally, so there is no value in converting that to TIFF. . This is interesting to know. > Also, it would be cool to use sc.pl.spatial for other technologies - say to overlay single cell spatial over the microscopy image image. . Can you elaborate on this? What tipe of technology and plot do you have in mind?. > I am wondering if you could add support for a fullres slot with size factor 1 and explain which variables need to be set for it to work in the tutorial. Mmh, I still think that the added value for looking at the fullres instead of the png in the context of overlaying spots to image is very little. In the hires png, even when cropping, the underlying resulting image is still quite good. Maybe not enough for analysis purpose, but for visualization should do the job no? I'm interested to hear your thoughts on this. The reason for not supporting it in the same way it's done now is that the image can be quite big (several GBs for fluorescent visium for instance) and so maybe it's not a good idea to load it in the anndata. I'll think about including a small section on this in the tutorial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980:1108,load,load,1108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-703211980,1,['load'],['load']
Performance,"> Just concatenate the datasets first and then use Combat. Something like:; > ; > ```; > adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); > sc.pp.combat(adata_merge, batch='sample'); > ```; > ; > Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy. Thanks for your early reply and kind suggestion~",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924:370,perform,performs,370,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527754924,1,['perform'],['performs']
Performance,"> Just to clarify, Jan started this PR because we were explicitly asked by some of the Scanpy core developers to prepare it for the core library. . I see, my comments weren't really directed at anyone in particular -- I know we are all trying to do good work and it's great that you all have thought a lot about this particular normalization -> dim. red. problem. > We view it basically as ""scTransform done right"". And scTransform is already published and is being used. Sure, but my point is that the analytic Pearson residuals method hasn't been peer-reviewed, and while the results in your preprint appear promising there are still questions that remain; e.g., how does it compare to deviance residuals? What is the effect on datasets that do not have so many cell types, i.e, ""continuous"" datasets? What happens when looking at metrics that aren't qualitative evaluation of t-SNE embeddings?. > One option would be to hold this PR until our paper is formally accepted... That makes sense to me, or just put it in external for now, or write generic methods for ""residuals"" that includes analytic, deviance, etc, with deviance as default (and as flavors?)? I'm not sure what is appropriate here, and some guidelines from the core scanpy team would be appreciated. For example, most people I know use the `""seurat_v3""` flavor of HVG selection, but it's not the default. It makes sense to me to change defaults as more information becomes available about performance/popularity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817:1456,perform,performance,1456,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-798687817,1,['perform'],['performance']
Performance,> Let's update the notebook as well. Would be great to understand performance difference before merging + get rid of the horrible densifying operation in `dask.ipynb`. On it: https://github.com/scverse/scanpy-tutorials/pull/137,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999:66,perform,performance,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263#issuecomment-2385462999,1,['perform'],['performance']
Performance,"> Moving 10x reading functions to anndata. I haven't worked much with h5py or tables, is it time-consuming to refactor these functions? It seems like moving to anndata is the most straightforward solution at least logically to me. > scanpy as a requirement. I like scanpy, but the only thing we really *require* in scvi is the data loading part. A user could take their scvi outputs and go use Seurat if that makes them happy. And then like the data loading functions are simple enough that we could just implement them ourselves. I'm sure a lot of people are currently doing this, which inspired the idea to have a standalone package. > Splitting off new modules. Your questions are very valid. I don't really have good answers for them. I could just see a standalone package being widely used and community driven, especially if there is some scanpy backing + maybe optional dependencies/functionality to get your objects ready for R analysis pipelines.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365:332,load,loading,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-680188365,2,['load'],['loading']
Performance,"> My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html); > ; > @Zethson do you have a good citation for this?. Here's the English version of the reply:. Thank you very much for your authoritative answer! You mentioned that in some benchmarks, performing the densifying scale transform didn't show significant performance improvements. I also noticed that sc-best-practices adopts a similar workflow. However, I have a further question: if the step of adding this densifying scale transform is included, would it negatively impact the overall performance? For example, would it reduce the training or inference speed? Or would the impact be negligible?. Thank you again for taking the time to answer my questions! Your opinions are very insightful and helpful to me. I look forward to your further guidance!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485:87,perform,performance,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034431485,4,['perform'],"['performance', 'performing']"
Performance,"> One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. @ktpolanski Thanks for the suggestion. Can you clarify which implementation you used where this is implemented?. I think one can essentially convert the variance of Pearson residuals into a p-value using a chi-square test. Then instead of the fixed number of HVG genes one could use some p-value cutoff. We have not experimented with this approach at all though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246:39,perform,perform,39,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-874558246,1,['perform'],['perform']
Performance,"> Scipy is actually under ~/.cache on my mac, ¯\\_(ツ)_/¯. Sorry, I was too terse here: What I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:29,cache,cache,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,6,['cache'],"['cache', 'cached']"
Performance,"> So it might be better to either switch to the numba kernel for larger datasets or take the compile hit for small datasets. The compiled versions should get cached, so it's a one time cost per install. No?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346:158,cache,cached,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2042381346,1,['cache'],['cached']
Performance,"> Some pip wheel files are there for example. And scipy is also some 100 MB right?. > Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. That's exactly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My pe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:431,cache,cached,431,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],"['cache', 'cached']"
Performance,"> The internals will very likely change over the coming months/year. Good to know, thanks! Any hints about what will change here? In particular, I'm wondering if there might be a `jax` implementation as I'm a bit more keen on that as a dependency. > But most embedding problems (including all problems specified using the preserve_neighbors function, which is the most commonly used recipe) have associated weighted graphs. I'd be interested in seeing how these graphs perform compared to the ones we get from UMAP. Would this be the right way to retrieve the graphs for the object, or is `distortions` not the right field?. ```python; from scipy import sparse. weights = mde.distortions().cpu().numpy(); edges = mde.edges.cpu().numpy(). graph = sparse.coo_matrix((weights, (edges[:, 0], edges[:, 1])), shape=(mde.n_items, mde.n_items)); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274:469,perform,perform,469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1062106274,1,['perform'],['perform']
Performance,"> To be able to reproduce and help, it is a big aid for us if you can supply a code sample that we can run: that is, with some dummy data (the datasets scanpy readily supplies are great for that), and the error/unexpected behaviour you get. Can you show such an example, with data? It is not immediately clear to me what specific you are trying to add or construct; I'm not sure whether basically the dataframe gets destroyed by the operation you intend to perform, or whether it is the violin plot failing (if the dataframe is crooked, it would be this to be fixed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546:457,perform,perform,457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3005#issuecomment-2066797546,1,['perform'],['perform']
Performance,"> We could wrap it in a function that checks the number of cells and only compiles this to faster code when necessary. So that's what this PR would replace. The reason I thought this could be replaced is that `numba` now allows on-disk cacheing of parallelized functions. This means that the function would only have to be compiled once per install. That cache only get's invalidated if function's source code get's modified, so this shouldn't cause too much pain for development testing times. I've added a note to the documentation mentioning this, so I think it's fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715:236,cache,cacheing,236,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534371715,2,['cache'],"['cache', 'cacheing']"
Performance,"> Well, so essentially, this PR reversed what I did quite some time ago to speed up the CI... Exactly! With the crucial difference that after the first build, binary packages for everything are being cached by pip, *and* now we don’t have to install conda every build. The one you linked to was just this one initial build. The real numbers are now:. &nbsp; | Runtime | Total | Link; --- | --- | --- | ---; Before (conda) | 4m47s | 8m33s | https://travis-ci.org/theislab/scanpy/builds/454438531 ; After (pip) | 3m34s | 5m 2s | https://travis-ci.org/theislab/scanpy/builds/456855724. > But, let's leave it like this. Hopefully, at some point, we'll have a less hackish way than the previous conda install script of dealing with this. The time is now, wheee!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200:200,cache,cached,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439811200,1,['cache'],['cached']
Performance,"> You can actually already do this by changing the order of your categorical, e.g.:; > ; > ```python; > adata = sc.datasets.pbmc3k_processed(); > # plot default:; > sc.pl.umap(adata, color=""louvain""); > # reorder categories alphabetically; > adata.obs.louvain = adata.obs.louvain.cat.reorder_categories(; > sorted(adata.obs.louvain.cat.categories); > ); > # plot with new category order:; > sc.pl.umap(adata, color=""louvain""); > ```; > ; > Which gives: <img alt=""Screenshot 2022-09-24 at 19 07 31"" width=""390"" src=""https://user-images.githubusercontent.com/32548783/192110283-af0d14c5-0d79-4ecd-96ff-c079f5743887.png"">. Thanks for your replay. Here, I changed the order of categorical as below:. # 0. loading data; adata = sc.datasets.pbmc3k_processed(); # 1. plot default:; sc.pl.umap(adata, color=""louvain""). # 2. show the default order of categories:; adata.obs['louvain'].cat.categories; # **Index(['CD4 T cells', 'CD14+ Monocytes', 'B cells', 'CD8 T cells', 'NK cells', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], dtype='object')**. # 3. reorder categories as customize; adata.obs['batch_3rd'].cat.reorder_categories(['CD8 T cells', 'CD4 T cells', 'B cells', 'NK cells', 'CD14+ Monocytes', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], inplace=True, ordered=True); adata.obs['batch_3rd'].cat.categories; # **Index(['CD8 T cells', 'CD4 T cells', 'B cells', 'NK cells', 'CD14+ Monocytes', 'FCGR3A+ Monocytes', 'Dendritic cells', 'Megakaryocytes'], dtype='object')**. # 4. plot with new category order:; sc.pl.umap(adata, color=""louvain"")",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257082217:701,load,loading,701,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2290#issuecomment-1257082217,1,['load'],['loading']
Performance,"> absolute numbers of cells expressing a gene is similar between clusters as a use case. I am aware that this is a bit of a niche problem, and I am not particularly happy with domino plots as a solution either. I have no better vehicle to discuss this than opening an issue :( Hopefully this inspires the next person who deals with this problem. As to the question, maybe sticking to this example will help me explain:. I am looking at the expression of Hb9/Mnx in my whole-body dataset. I notice from the feature scatter that it seems to be somewhat expressed in clusters 0, 2, and 18. Wanting to be sure, I look at the dotplot. The dotplot tells me that there is a greater proportion of cells in cluster 18 that express it, compared to 0 and 2. The dotplot might make me believe that Hb9 is a marker for cluster 18, and if I do an in-situ hybridisation, these are the cells I would be staining. However, the truth is that the vast majority of cells that express Hb9 are actually in clusters 0 and 2, different cell types than 18. The number of cells in each cluster correlates with the number of cells in the organism, so if I performed the in-situ I would get lots of cells that I could mistakenly all identify as cluster 18. Does this make more sense?. EDIT: I am not advocating for domino plots to be part of ScanPy. I am simply trying to start a discussion, and trying to see if there was an easy fix that I missed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889:1129,perform,performed,1129,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2107#issuecomment-1017354889,1,['perform'],['performed']
Performance,"> also as I asked before: why go away from dataclasses?. I don't think that switching away from data classes removed any meaningful functionality here, but having to use `default_factory`, `InitVar`, and/or `__post_init__` would add more complexity. I don't think that there being some internal data classes is important here, especially since it's not user visible and may change at any time anyways. I have a few ideas for ways to change the implementation to add more methods, none of which are compatible with `Aggregate` being a data class. * One path forward just removes the class entirely, since it doesn't do much now; * The other uses a number of cached properties, which I don't think make a ton of sense to use with dataclasses. Is there some functionality the data class was adding that I'm missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2590#issuecomment-1887297037:657,cache,cached,657,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2590#issuecomment-1887297037,1,['cache'],['cached']
Performance,"> only working on genes. technically it could work on continuos covariates as well, should I add that option?. Sure. I think it would make sense to mimic the API of `gearys_c` as much as possible here. > I think it could be worth it to add a row wise normalization of the weights (standard in pysal). What would this entail? I wonder if this is best left up to the user, who can just chose what values to pass in?. > should consider to skip permutation entirely as well. Yeah, I'm not sure if we need this at the moment. I wonder if calculating p-values should even be a separate method? I would like to understand more about how p-values are calculated, and what you're getting out of this. . For instance, I would assume it's not appropriate to calculate a p-value for gene expression using a nearest neighbor network based on gene expression. --------------------. Side note, on performance. So right now it looks like computing one permutation is fairly fast (which makes sense). Most of the time comes from the permutation testing. After that, most of the time looks like it's coming from `adata.obs_vector`, which is a bit slow especially if you're getting many genes with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288:882,perform,performance,882,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-799062288,1,['perform'],['performance']
Performance,"> sklearn has also had a PR on this topic out for a long time and it just does not seem to budge. Allowing sparse support for PCA doesn't seem to be high on their priority list(?). I've read that situation as that particular PR being stalled, but it's also just for the random solver. I think sklearn would really like to have this feature. I think there's support for this from the community (where the referenced comment is yours):. > The perfect implementation of implicit data centering must be solver agnostic, allowing any matrix-free sparse PCA and SVD solver from scipy and scikit to be used. E.g., adding support to call any matrix-free scikit SVD/PCA solver in #12794 (comment) would make it perfect PR for implicit data centering. Do you think you could make a PR with this to sklearn? I'd like to see the response it gets, and judge based on that. My preference would be for this to go there, but I'm very open to having this in our codebase until it's in a `sklearn` release. > what's the best way of sharing the reproducing jupyter notebook with you?. Ha, that's actually a difficult question. I'm not quite sure, zip file should be fine. Thanks for sharing!. Ideally what I'd like from a benchmark of performance would be time and memory usage for the product of these conditions:. * Datasets size (one small, one large (>50k cells)); * Implicit centering, densifying centering, no centering; * single threaded, multi-threaded. I'd also lean towards making this the default for sparse data. But to do that, I will need to look a little closer at correctness. For that, could you show the average residual from a few runs (with different seeds) for all output values between implicit vs explicit centering?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984:1216,perform,performance,1216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-589500984,2,"['multi-thread', 'perform']","['multi-threaded', 'performance']"
Performance,"> the main hpc I'm on 1gb of space where appdirs would put these files. That's a misconfigured server, not a normal case. We should use appdirs as default, catch a IOError on write, and send a nice message like. > Error: Cannot write to your cache directory. Please make sure there's space in {cache_dir!r} or override the cache directory by setting one of the $SCANPY_CACHE_DIR or $XDG_CACHE_DIR environment variables. All linux-based systems should set $XDG_CACHE_DIR if there's a better place than ~/.cache for such files.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808:242,cache,cache,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476675808,3,['cache'],['cache']
Performance,"> then I'd say NearMiss and related are straightforward and scalable (just need to compute a kmeans whcih is really fast). For sampling from datasets, I would want to go with either extremely straightforward or something that has been shown to work. Maybe we could start with use provided labels to downsample by?. > reshuflling is performed. Reshuffling meaning that the order is changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364:60,scalab,scalable,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054247364,2,"['perform', 'scalab']","['performed', 'scalable']"
Performance,"> though I definitely see a lot of toolkits using something like ~/.{toolkitname} on my mac. . There’s two possible reasons: 1.: We’re talking about something from the 80’s like SSH or BASH. They earned their right to things their way because they’re older than the standards they’re not following. Or 2.: Whoever designed this didn’t do their research and just hacked in first thing that came to mind. This is not an excuse. MacOS knows about `~/Library/Caches` and to clean it out when disk space gets scarce. The same applies to Linux’ and Windows’ respective canonical cache directories. Each OS has that place specifically to place things like those datasets there. > For example, the main hpc I'm on only gives me about 1gb of space where appdirs would put these files. Ouch. Seems like it’s time to file a bug report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476558610:455,Cache,Caches,455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476558610,2,"['Cache', 'cache']","['Caches', 'cache']"
Performance,">> Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:; >; > I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. > cc @jakirkham @pentschev. FWIW made sure to cc us on the issues that you opened. Though please cc us on other ones. Should add I think CuPy devs will want to keep their sparse implementation in pretty close alignment with SciPy's. So I don't think CuPy's sparse will solve any issues that SciPy's sparse does not also solve. However things that CuPy does not implement that SciPy does implement, are likely in scope. Though no idea where these sit in the priority queue ATM.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557644893:857,queue,queue,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557644893,1,['queue'],['queue']
Performance,">> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. **The problem is the error in importing gene names both when using id and when using symbolic labeling. All genes have the same name. if you use `anndata=0.10.3` instead of `anndata=0.10.4`, then everything works correctly.**. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; import matplotlib; import seaborn as sns. path='<path_to_files>'. adata = sc.read_10x_mtx(; path, ; var_names='gene_symbols', ; cache=True). adata.var_names_make_unique(). adata.var; ```. ### Error output. ```pycon; >>> # then anndata=0.10.4; >>> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. ### Expected. ```pycon; >>> # then anndata=0.10.3; >>> print(adata.var); gene_ids feature_types; 4933401J01Rik ENSMUSG00000102693 Gene Expression; Gm26206 ENSMUSG00000064842 Gene Expression; Xkr4 ENSMUSG00000051951 Gene Expression; Gm18956 ENSMUSG00000102851 Gene Expression; Gm37180 ENSMUSG0000010",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:7176,cache,cache,7176,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cache']
Performance,">A couple follow up points on this and @LuckyMD's points:; > I don't actually know how different the quality score can be for different solutions. Any chance you have some stats on quality scores from multiple clusterings? I'm mostly wondering if ""good"" clusterings are associated with high quality scores.; > I think if a user sees a value like ""quality"" they could ascribe more meaning to it than it deserves. I think we should add some docs about what it means, and how to interpret it. I'm not sure I entirely understand your point. The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""? . I have looked at some stats for communities in protein-protein interaction networks, and the quality of the communities can change dramatically with louvain output there (could find the link if you think it's relevant). However, modularity as a score is fairly degenerate toward the optimal score and therefore the value often doesn't change that much between the optimized partitions. I'm not sure how this is on the knn graphs though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195:579,optimiz,optimized,579,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529377195,2,['optimiz'],['optimized']
Performance,"@Celine-075, I'm about this statement:. > When i previously performed leiden clustering on my data, the shape of the UMAP changed, as expected. This is not expected unless you recompute UMAP. What do you mean by clustered UMAP?. > So here you see that part of cluster 1 is actually added to cluster 2 (which also make sense when looking at the expression profiles of those groups). I'm not sure I can see that, since it's not obvious which point in the first plot corresponds to a point in the other plot. I think a [confusion matrix](https://scanpy.readthedocs.io/en/stable/generated/scanpy.metrics.confusion_matrix.html) (or using the same UMAP layout) would be a more appropriate way to compare the clusterings here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034421743:60,perform,performed,60,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034421743,1,['perform'],['performed']
Performance,"@Intron7 I think the aim here is indeed to not keep anything in VRAM anyway. In the code/functions I propose here, the data is only transiently stored in device memory for calculation and the resulting output is always transfered back to host once finished. Moreover, I also think that loading a huge mtx file with a 4Go GPU is not impossible. From what I understood rmm should allow oversubscription on host RAM using the following command:; ```python; rmm.reinitialize(managed_memory=True); cp.cuda.set_allocator(rmm.rmm_cupy_allocator); ```. I had a look at your code and GPU accelerated preprocessing functions would be also welcome in scanpy in my opinion! I feel that `scale` and `regress_out` could benefit from such speedup for example.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794:286,load,loading,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1106490794,1,['load'],['loading']
Performance,"@Intron7 this was surprisingly hard to get right. Unfortunately, there are now a few more checks and some `hstack`ing. Do those tank the performance?. /edit: I benchmarked some, this is better than what we had before",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2756#issuecomment-1816509533:137,perform,performance,137,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2756#issuecomment-1816509533,1,['perform'],['performance']
Performance,"@Koncopd ; hi, sc.pp.neighbors doesn't have hsnw which has superior performance from what I've seen in my data and literature https://arxiv.org/abs/1603.09320",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394:68,perform,performance,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318#issuecomment-658987394,1,['perform'],['performance']
Performance,"@Koncopd Hi, Thank you for the pointer. It seems to be a problem caused by pytables package. But I still couldn't import tables after installing and uninstalling pytables packages for many times. And I'm in Windows system.; (base) C:\Users\yuhong>python; ```; (base) C:\Users\yuhong>conda list | grep pytables; pytables 3.6.1 py37h14417ae_3 conda-forge ; Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tables; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232:790,load,load,790,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468#issuecomment-716168232,1,['load'],['load']
Performance,"@LuckyMD . Thank you very much for pointing me in the right direction!! I was able to merge the my anndata objects using ```anndata.concatenate()``` than I created the trajectory analysis for the identified clusters using ```sc.pl.paga```. As explained in my initial post, I have data from 12 samples and 3 treatments. When I created my anndata object, I labeled the batch categories using the following command:. ```adata_merged = adata_ESTRUS.concatenate(adata_DIESTRUS, adata_PROESTRUS,batch_categories=['ESTRUS','DIESTRUS','PROESTRUS'])```. Now, I would like choose a cell type (one specific identified clusters) to perform trajectory analysis according to treatment (in my case batch_category). Do you have any suggestions on how to do that?. Thanks again!!. Joao",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859#issuecomment-565168151:620,perform,perform,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859#issuecomment-565168151,1,['perform'],['perform']
Performance,"@LuckyMD, I think you can get the docker environment travis uses. * [Docker image for travis python env](https://hub.docker.com/r/travisci/ci-python); * [Guide on running it](https://andy-carter.com/blog/setting-up-travis-locally-with-docker-to-test-continuous-integration). I did this a couple years ago, but I know travis has changed a bunch since then. Another good first step would be to figure out if it only fails on the first build, and if caches are being used in any way. Also, do the builds ever fail for forks? I don't think they've been failing [for me](https://travis-ci.org/ivirshup/scanpy/builds).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933:447,cache,caches,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478823933,1,['cache'],['caches']
Performance,@MichaelPeibo Could you install the version 1.4.5.post1? It is not available in conda and with 1.4.4.post1 I'm getting the same error. Thanks!. ```; conda search -c bioconda scanpy; Loading channels: done; # Name Version Build Channel ; scanpy 1.3.1 py36_0 bioconda ; scanpy 1.3.2 py36_0 bioconda ; scanpy 1.3.3 py36_0 bioconda ; scanpy 1.3.4 py36_0 bioconda ; scanpy 1.3.5 py36_0 bioconda ; scanpy 1.3.6 py36_0 bioconda ; scanpy 1.3.7 py36_0 bioconda ; scanpy 1.4 py_0 bioconda ; scanpy 1.4.1 py_0 bioconda ; scanpy 1.4.2 py_0 bioconda ; scanpy 1.4.3 py_0 bioconda ; scanpy 1.4.4 py_0 bioconda ; scanpy 1.4.4 py_1 bioconda ; scanpy 1.4.4.post1 py_0 bioconda ; scanpy 1.4.4.post1 py_1 bioconda ; scanpy 1.4.4.post1 py_2 bioconda ; ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828:182,Load,Loading,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/942#issuecomment-577681828,1,['Load'],['Loading']
Performance,"@Mr-Milk Could you elaborate on why you think this functionality belongs within scanpy? Recently, we've been thinking a bit about what should go in vs. what should not, and most of the recent/upcoming additions are around either performance (dask) or vendoring tools that are either essential + in need of some love (maybe bbknn) or tools that we once relied on, but are no longer maintained and we need to bring in to the package to ensure continuity (scrublet). . Your tool seems great, well-maintained, and has a clean API so I am not sure what it would add for either project to have it in `scanpy`. But I am open to be convinced!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352465866:229,perform,performance,229,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2444#issuecomment-2352465866,1,['perform'],['performance']
Performance,"@adamgayoso, I have a question regarding the implementation of Seurat v3 HVG and am not sure if this is the correct thread (it's probably not). My question is regarding the final step where the function reports, variances_norm or norm_gene_var. Based on the description here, https://www.overleaf.com/project/5e7e320564f7d4000175d082, the norm_gene_var function computes the variance of the transformed values assuming that the mean of the zscores is 0. I guess my question is, post clipping values to a maximum, I think the mean of the transformed values might not be 0 anymore so if you were just to perform, var(transformed values), it will not equal the same value as variances_norm equation for the sparse approach. Reading through the referenced paper provided (Stuart 2019) its not clear whether they perform the variance of zscores post clipping, or with the assumption that mean zscore is 0 preclipping. . If this is not relevant, please feel free to ask me to delete this comment.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161:602,perform,perform,602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/993#issuecomment-1040462161,2,['perform'],['perform']
Performance,"@adamgayoso, have you had any thoughts here about how we manage the `key` for variable loadings?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157:87,load,loadings,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-1081974157,1,['load'],['loadings']
Performance,"@andrea-tango ; Really awesome!; I am also wondering to find some parameters to tune in scanpy's `rank_genes_groups` like in Seurat. . Because I found there is some difference in makers by scanpy's default(using `wilcoxon` ) and Seurat's default parameters` only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25`. (Seurat's default method is wilcoxon), in this case, I can find interesting markers calculated by Seurat but not in Scanpy's. However, when I tried scanpy's `logreg` method, I found many overlap DEGs between two calculations, aka, scanpy's `logreg` and Seurat's `wilcox`. Have you ever came into similar results?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/460#issuecomment-471241654:80,tune,tune,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/460#issuecomment-471241654,1,['tune'],['tune']
Performance,"@ashish615 after doing some benchmarking myself I found out that your solution for `axis=1` is under performing compared to `axis=0` for larger arrays. I think that is because of the memory access pattern you choose. I rewrote the function with that in mind. I'll again make a PR to you, because for some reason you disallow us from making changes to your PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3099#issuecomment-2191349887:101,perform,performing,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3099#issuecomment-2191349887,1,['perform'],['performing']
Performance,"@atarashansky I'll have time to give a little more detailed notes this weekend. * Have you looked at submitting this in a PR to sklearn? I think they would be better at evaluating the stability. I'd be happy to help with this if you want.; * At this point, I think PCA should go into its own file. Could you move the `pca` function and your sparse on into a `scanpy/preprocessing/_pca.py` file?; * From the stability and performance checks, I think this could be similar enough make it the default. I think this should just be the default behaviour for when: 1) the data matrix is sparse 2) `zero_center=True` 3) `svd_solver` is `arpack` or `lobpcg`. Any objections to this @Koncopd, @flying-sheep, @falexwolf?. -----------------------------. @Koncopd, I don't think I've looked over your implementation much. Is it similar to the stalled sklearn PR? If so, do you have a sense of why the `sklearn` PR for the randomized solver is stalled?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817:421,perform,performance,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-592268817,1,['perform'],['performance']
Performance,"@atarashansky, the performance is looking very very good:. ```python; import scanpy as sc; import numpy as np; from sklearn.datasets import fetch_20newsgroups_vectorized. X = fetch_20newsgroups_vectorized(""all"").data; # 18846 x 130107 csr_matrix. a = sc.AnnData(X, dtype=np.float64); %time implicit = sc.pp.pca(a, pca_sparse=True, dtype=np.float64, copy=True); # CPU times: user 34.8 s, sys: 5.52 s, total: 40.3 s; # Wall time: 2.93 s; # Peak memory (including dataset) is about 770 MB; %time explicit = sc.pp.pca(a, pca_sparse=False, dtype=np.float64, copy=True); # CPU times: user 55min 37s, sys: 1min 50s, total: 57min 28s; # Wall time: 7min 43s; # Peak memory is about 36 GB. assert np.allclose(implicit.obsm[""X_pca""], explicit.obsm[""X_pca""]); assert np.allclose(implicit.varm[""PCs""], explicit.varm[""PCs""]); ```. But the variance and explained variance ratio are still off. Why not calculate them the same way sklearn does?. Also, any thoughts on making a PR there?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303:19,perform,performance,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-593738303,1,['perform'],['performance']
Performance,"@chris-rands, that section of the Seurat tutorial also ends in these points:. > * We encourage users to repeat downstream analyses with a different number of PCs (10, 15, or even 50!). As you will observe, the results often do not differ dramatically.; > * We advise users to err on the higher side when choosing this parameter. For example, performing downstream analyses with only 5 PCs does significantly and adversely affect results. > Can anyone explain/show literature on if/why the scanpy default of 50 PCs works well?. I think there is enough to show that PCA works well. I'm not sure if I can show you a paper that says either choosing a high cutoff, or using jackstraw/ elbow plots gives better downstream results. I'd note that the [cited paper](https://www.cell.com/fulltext/S0092-8674(15)00549-8) for the Seurat tutorial doesn't seem to evaluate this. ---------------. @wolf5996, I'm not sure I agree with your point that lower PCs are more likely to contain non biological variability. I don't think that a component which explains more variability in the dataset would necessarily represent biological variability. As an example, if we have a dataset with two evenly sized batches, and a rare cell type which makes up ~1% of the population, wouldn't a PC representing the batch explain much more variability than a PC corresponding to the rare cell type?. Anecdotally, I can say batch effects can show up in high principal components.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073:342,perform,performing,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822286073,1,['perform'],['performing']
Performance,"@coh-racng I would like to add that for your specific intention the best way is to load the `plot_scatter` function that accepts `basis` as parameter and works well with layers. The code should be:; ```PYTHON; from scanpy._plotting.scatterplots import plot_scatter`; plot_scatter(adata, basis='<name>'....); ```. @ivirshup, @falexwolf I think we should add `plot_scatter` to the API maybe renaming it `plot_embedding` to help users like @coh-racng. Currently we have two different ways to make scatter plots: One for embeddings (`plot_scatter`) and other more generic for obs and vars (`sc.pl.scatter`) that accepts `x` and `y` parameters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114:83,load,load,83,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/762#issuecomment-517618114,1,['load'],['load']
Performance,"@dawe In all benchmarks that I did maybe a bit less than a year ago, the `python-louvain` didn't seem to produce satisfying results... But maybe I did something stupid. I'll reevaluate this, thanks, Davide! PS: One can easily switch between implementations; simply pass `flavor='taynaud'` to `sc.tl.louvain` and you'll use `python-louvain`. See [here](https://github.com/theislab/scanpy/blob/5299c6caaec6402513f1e0442186350787177d2c/scanpy/tools/louvain.py#L118-L125). However, I removed this from the docs as I was not so satisfied with it... @flying-sheep the only thing where `igraph` is used in Scanpy is for graph drawing, where it's incredibly faster than `networkx` (completely forget about `networkx` in this respect); the performant `louvain` implementation is due to the `louvain` package, which simply uses `igraph`'s graph data structures",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215:731,perform,performant,731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/97#issuecomment-370393215,1,['perform'],['performant']
Performance,"@dburkhardt Sorry for the late response to this. I agree that the space of single cell 'omics analysis tools is essentially the wild west, where every tool should be viewed critically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:776,perform,performs,776,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['perform'],['performs']
Performance,"@falexwolf ; In your Bioinformatics paper ""destiny: diffusion maps for large-scale single-cell data in R"", you show how to determine the optimal Gaussian kernel width and the plot of The Eigenvalues of the first 100 diffusion components. Could you tell us how to perform it with scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-396115524:263,perform,perform,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-396115524,1,['perform'],['perform']
Performance,"@falexwolf @willtownes @LuckyMD Valentine Svensson suggests that zero inflation does not exist in droplet protocols, but that log-transforming data could be responsible for the apparent zero inflation. Further, the high number of zeros can be accurately modeled with a non-zero-inflated model: https://www.nature.com/articles/s41587-019-0379-5. Since GLM-PCA doesn’t model zero inflation, it’s probably a really good base for distance calculations in scanpy in cases where its performance is sufficient. [From the paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6):. > The multinomial model adequately describes negative control data, and there is no need to model zero inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723:477,perform,performance,477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-592476723,1,['perform'],['performance']
Performance,"@falexwolf I try to answer where I can. I should probably have clarified a bit above. I would argue that most real data DE tests benefit from accounting for technical covariates. For example, you should probably not perform batch correction on your data and then do a wilcoxon rank sum test, but instead take the normalized (and log transformed) data or the raw counts and include a batch covariate in the test. This also holds for technical covariates that describe the complexity of the data (such as size factors or n_genes). Often these factors are not sufficiently accounted for by simple normalization techniques (especially for plate-based data), and are thus included in the DE testing framework. This is done in MAST (and MAST performs better with this `detRate` covariate in the Soneson & Robinson paper you cite above), and it is also done in a recent negative binomial DE test from [Mayer et al, Nature 2018](http://www.nature.com/doifinder/10.1038/nature25999). When you are not able to fit the background variability in your model, you will have a lower sensitivity. Accounting for covariates is obviously not possible with t-tests or wilcoxon rank sum tests. Hence my statement about lower sensitivity. They did perform comparatively well in the DE method comparison, which is why I'd argue that they're useful for first pass exploratory applications (and marker gene detection when you don't want to use more fancy approaches like [this](https://www.biorxiv.org/content/early/2018/11/05/463265)). However, if you can account for technical covariates, that's probably a good approach to use. Also, according to the comparison paper you mention, there are not more false positives when using MAST or limma compared to t-tests or Wilcoxon rank sum tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088:216,perform,perform,216,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447865088,3,['perform'],"['perform', 'performs']"
Performance,"@falexwolf, @flying-sheep . Just to recap what's left to be resolved here. * I'll reset the default value of `datasetdir` to the current value ""./data""; * Related, how about `datasetdir` instead of `datasetsdir`? It matches more to `cachedir` and `figdir`. Also, by analogy, it's ""potato sack"" not ""potatoes sack"" so ""dataset directory"" sounds more natural that ""datasets directory"" to me.; * `ebi_expression_atlas` vs `ebi_sc_expression_atlas`; * Potentially adding a class for settings right now?; * I think this becomes more important if `datasetdir` is documented. I bet people will set it with a `str` instead of a `Path` and that'll break things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478843888:233,cache,cachedir,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478843888,1,['cache'],['cachedir']
Performance,"@fidelram . * Cool. I think I had gotten confused about some of the labelling on there which I'm going to blame jet lag for 😊; * Using the `inline` backend with ~300k cells, it takes about 5 seconds per plot for me. Since I'm trying to improve the experience of sitting with a biologist figuring out labels for clusters, this is a little slow once you get to 5 or more plots – especially when you just want to update one. From `%prun`, it looks like about half of `matplotlib`'s plotting time is spent figuring out where to put points, and the extents of the plot, so I figured that could be a good target for optimization. I've looked into copying the plot after layout, but before coloring, but I'm not sure how feasible that is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991:610,optimiz,optimization,610,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427228991,1,['optimiz'],['optimization']
Performance,"@fidelram Can you change the resolution of the non-weighted version to reproduce a clustering similar to the weighted case? Weighting can change the scale of the resolution parameter. I would assume clustering on a weighted, fully connected network would be a lot more computationally expensive. I'm curious if it's worth the additional cost. It may also perform worse as you are putting more emphasis on the euclidean distances between transcriptomes than you may want (are the values more important or only the order of the distances?).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415853701:355,perform,perform,355,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415853701,1,['perform'],['perform']
Performance,"@fidelram So based on that could you say that the non-weighted method performs better for cluster 10 (PNEC/Brush cluster) as it is identified in this partition, but merged with other cells in cluster 3 in the weighted partition?. I guess it might take a more thorough analysis to make those types of conclusions though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416207545:70,perform,performs,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416207545,1,['perform'],['performs']
Performance,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158:338,perform,performs,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-424548158,1,['perform'],['performs']
Performance,"@fidelram, really excited to try this out! I've got a couple questions about this PR:. * On that grid of plots @falexwolf posted, it looks like there a shared set of colorbars for two genes. Is this a coincidence (i.e. both genes happen to be expressed on a similar scale), is the colorscale being generated on the range of both genes, or is something else going on?; * When making a set of plots on the same coordinates (different genes on the same UMAP coordinates), have you found any way to reduce computational load? I'd like to think there are repeated computations (like layout) some memoization could speed up, but haven't figured out how.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062:516,load,load,516,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426852062,1,['load'],['load']
Performance,@flying-sheep @gokceneraslan great! I agree it's hard to compare these algorithms as the performance of an imputation strategy often depends on the downstream use case. I'm looking forward to checking out the countae preprint. I find the [scVI](https://github.com/YosefLab/scVI) benchmark of imputation methods to be useful for now.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111:89,perform,performance,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367680111,1,['perform'],['performance']
Performance,"@flying-sheep As always, thank you for your thorough thoughts on the topic! And as always, my ""hacking-numerics"" perspective likely is not a path that is long term sustainable. With what I wrote at the very beginning of this thread, I simply wanted to express that I thought that we shouldn't transition quickly and immediately; for the cosmetic reasons and for the reason of staying away from creating entry hurdles. I still don't think that scanpy needs to precede major packages like numpy and many others in adapting type annotations. But, in essence, I trust you and if you want to push this further I'm fine if scanpy becomes somewhat a field of experimentation for how to deal with type annotations in scientific and numerics-centered software. . @ivirshup Thank you very much for your remarks, too! I agree with your concerns and examples, but wouldn't have been able to summarize them as neatly. *Conclusion:* @flying-sheep if you feel you have bandwidth for improving the cosmetics (thanks for what you did already, also the PR to ipython) that lead to more homogeneous docstrings (I'd say: `Union[a, b]` → `a, b`), of course, please go ahead. If people make PRs with old-school docstrings and without type annotations, I'd still not trouble them, for now. When we have converged on new docstrings and canonical type annotations so that at least people who really know what they're doing (@ivirshup) don't feel things are ambiguous anymore (say in a year), we can start to rigorously ask for them. PS: Thanks for the hints about Jedi etc. @flying-sheep. But likely, I'll keep playing around and reading documentation of packages using shift-tab in jupyter and develop using emacs relatively plain (there were times when I worked with quite some extensions, but these days, I'm back to almost plain for performance reasons - I know that's probably not smart, but anyways)...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798:1812,perform,performance,1812,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441472798,1,['perform'],['performance']
Performance,@flying-sheep can you cite a reference for scImpute and countae outperforming MAGIC? I'd be curious to learn which hyperparameter optimization methods and performance measures were used in the benchmark.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135:130,optimiz,optimization,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/45#issuecomment-367378135,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"@flyingsheep I can assure you, that's the normal case in academic HPC; systems. On Tue, Mar 26, 2019 at 3:37 PM Philipp A. <notifications@github.com> wrote:. > the main hpc I'm on 1gb of space where appdirs would put these files; >; > That's a misconfigured server, not a normal case. We should use appdirs as; > default, catch a IOError on write, and send a nice message like; >; > Your cache directory is full. Please make sure there's space in; > {cache_dir} or override the cache directory by setting the; > $SCANPY_CACHE_DIR environment variable.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/558#issuecomment-476675808>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAS-TQPrmr3LWdmwNL5O6XPnRdSAcl_1ks5vajC0gaJpZM4cKXC7>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167:388,cache,cache,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476677167,2,['cache'],['cache']
Performance,"@gatocor Thanks, for me it worked with `pip install --pre numba`, ; terminal output:; ```; Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2); Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0); Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1); Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0); ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version?. Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344:628,load,load,628,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652#issuecomment-789673344,1,['load'],['load']
Performance,"@gokceneraslan Hey, sorry for my long silence on this. I've been using @Hoohm's [https://github.com/Hoohm/CITE-seq-Count](CITE-seq-Count) for ADT/HTO tag counting which produces (in recent versions) a 10X v3 style `mtx` directory for both reads and UMIs. In some cases, I'll load these in as their own AnnData object with reads and counts as different `layers` which is helpful in computing per-cell or per-tag ""sequencing saturation"" and other metrics involving both reads and counts. This is especially helpful for investigating some pilot experiments (lipid tags, cholesterol tags, etc.) we've been doing. However, most of the time I'll just load the tags matrix in as a pandas dataframe and run them through a demuxing function that'll modify `adata.obs`. A couple challenges/ideas to consider:. * at our facility, we're typically building the same Illumina i7 index (`ATTACTCG`) into all tag libraries. This leads to some tricky situations when using a NovaSeq for sequencing since the multiple tag libraries (with disjoint sets of tags) may be run on the same sequencing flowcell lane. This results in a single set of FASTQ files and thus a single barcode-tag matrix for all tag libraries on that lane. Therefore, the mapping between transcriptome AnnData objects <-> tag library matrices is not always 1-to-1.; * in my experience, HTO libraries have a large variance in quality, so for the most part I've been using the transcriptome as my ""ground truth"" as to what is a cell. However, I imagine others use HTOs to ""rescue"" cells that were not called by their pipeline of choice (and I hope to do this once I build enough trust in the data). In that case, one would want to intersect the HTO classifications with the raw cell-gene matrix.; * not all tags are antibody based, so I'd vote for naming all related functions `*hashtags()`. I'd therefore vote for something like the following design:; ```{python}; # htos is a AnnData object; htos = sc.read_hashtags(filename) . # classify_hashtags a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900:275,load,load,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-543387900,2,['load'],['load']
Performance,"@gokceneraslan Totally agree it's the user's responsibility. I would say that it's the devs responsibility to make it as easy as possible for the user. How about printing the absolute path of the data's destination on download?. @flying-sheep Would there necessarily be an error if space ran out? I could probably fit a few datasets in 2gb. From your previous depiction, I thought the older ones would just be deleted, right? If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. Also here's the [docs](https://opus.nci.org.au/display/Help/Filesystems+User+Guide#FilesystemsUserGuide-DiskQuotaPolicy) for my HPCs filesystem. I don't have an `XDG_CACHE_HOME` variable set when I log in. I'm also not sure scanpy fits the app model. When I look in my `~/Library/Caches/` I see things like Illustrator, VSCode, and Slack. When I think about example datasets that are available through scientific computing packages I think of:. * `scikit-learn` – `~/scikit_learn_data`; * `seaborn` – `~/seaborn-data`; * `NLTK` – `~/nltk_data`; * `keras` and `tensorflow` – `~/.keras/datasets`; * `conda` – `~/miniconda3/`; * `intake` – `~/.intake/cache/` (specifically for caching feature); * CRAN and bioconductor data packages – same place as packages I think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448:934,Cache,Caches,934,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476943448,2,"['Cache', 'cache']","['Caches', 'cache']"
Performance,"@gokceneraslan since they are largely the same thing (just a different optimization strategy), do we even need to keep both?. Otherwise, I think I'd prefer them to be separate functions, so you don't get argument interactions. For example, the `partition_type` argument has to be a type from the same package as the method, otherwise there are segfaults.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/570#issuecomment-478211254:71,optimiz,optimization,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/570#issuecomment-478211254,1,['optimiz'],['optimization']
Performance,"@grst I don't think `leiden` is the issue here, but `pynndescent`. My guess is this is going to have to do with the CPU that gives different results being much older using a different instruction set that the other intel processors. This could be triggered by either use of any parallelism at all or `pynndescent` being pretty liberal with the use of `numba`'s `fastmath`, and different CPUs having different features. Do you get the same graph out of `pynndescent` if you are make the computation single threaded? If not, we may be able to look at the assembly to see which feature sets give different results. It's possible these can be turned off with a flag. But we may then have to consider what kind of a performance hit we'd be willing to take for exact reproducibility on discontinued chip sets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078:711,perform,performance,711,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2014#issuecomment-946679078,1,['perform'],['performance']
Performance,"@grst Thanks it seems logical, but,. It is mentioned in Seurat Pbmc3k example that best resolution parameter is 0.6-1.2 , but you used less and get more clusters. May be because i didn't explore random seed in leiden. ; In louvain and leiden we usually optimize 'modularity' value, what if we just calculate modularity values for different resolution instead of optimizing for given resolution and then for resolution where 'modularity' is maximum, we optimized 'modalarity'. Is this ok ? But i also think that 'modularity' increases when we have small number of clusters. Any suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498158306:253,optimiz,optimize,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498158306,3,['optimiz'],"['optimize', 'optimized', 'optimizing']"
Performance,"@ivirshup . > Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. The quality score returned by RBConfigurationVertexPartition is unscaled modularity, so it's something like `41726.23`. So how would one interpret that? I don't think there is any point in reporting raw RBConfigurationVertexPartition quality value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977:171,optimiz,optimization,171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529929977,2,['optimiz'],"['optimization', 'optimized']"
Performance,"@ivirshup For what you want we need to look into bokeh or plotly as they are optimized to render thousands of points quickly. I think that matplotlib is not going to be a solution here. Last week I played a bit with Dash and I found that is quite easy to set up an app to quickly explore gene expression. Maybe this is something that we can further develop. Currently, it uses matplotlib but I also tried it with plotly with decent results. Here is a very crude but functional demo using human lung airway data from *Plasschaert et a. Nature. 2018. “A Single-Cell Atlas of the Airway Epithelium Reveals the CFTR-Rich Pulmonary Ionocyte.”* https://doi.org/10.1038/s41586-018-0394-6.: https://demo-scexplorer.herokuapp.com/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-427291301:77,optimiz,optimized,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-427291301,1,['optimiz'],['optimized']
Performance,@ivirshup I think the benchmarks have shown satisfactory performance of this PR. Should we move on to polishing the code organization?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662:57,perform,performance,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-591647662,1,['perform'],['performance']
Performance,"@ivirshup I think writing a file for uploading it to the web, for read caches, and for for checkpoints of a pipeline has different requirements. I think a `h5ad_compression` or even `hdf5_compression` setting could have its place, but separately from the `cache_compression`. We’ll have to think about naming though. Maybe we want to namespace our settings like matplotlib’s rcparams?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481:71,cache,caches,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847#issuecomment-532191481,1,['cache'],['caches']
Performance,"@ivirshup any way to force Azure to clear its cache or use a different runner? The “invalid instruction” error here probably comes from using a binary wheel compiled for a newer CPU. /edit: wow, 9 attempts. Maybe just dropping Python 3.8 will get us there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417:46,cache,cache,46,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2605#issuecomment-1761383417,1,['cache'],['cache']
Performance,"@ivirshup is it possible that Travis has cached pbmc3k and that's what's causing the error? I really don't have it running pytest locally either. . Also as far as the code review -- I understand code is duplicated, but this code does not really fit in the existing implementation because it works a bit differently and requires raw data. Let me know how you'd like to address this. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412:41,cache,cached,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1182#issuecomment-619321412,1,['cache'],['cached']
Performance,"@ivirshup, thanks! But from my understanding, now PAGA pie chart becomes super fragile (?) and now there's no solution to it? Only solutions I saw in these discussions is the new function cluster_fates in theislab/cellrank#25 but we still have to wait right?. And I found another bug when I want to plot the pathway analysis. Usually it's fine but recently I could not perform it with the same data input. Does anyone know how to deal with it?. ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-44-72c504b15b2e> in <module>; 17 title='{} path'.format(descr),; 18 return_data=True,; ---> 19 show=False); 20 data.to_csv(""C:/Users/Lin/write/paga_path_{}.csv"".format(descr)); 21 pl.savefig(""C:/Users/Lin/figures/paga_path_KTC.pdf""). ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in paga_path(adata, nodes, keys, use_raw, annotations, color_map, color_maps_annotations, palette_groups, n_avg, groups_key, xlim, title, left_margin, ytick_fontsize, title_fontsize, show_node_names, show_yticks, show_colorbar, legend_fontsize, legend_fontweight, normalize_to_zero_one, as_heatmap, return_data, show, save, ax); 1037 if n_avg > 1:; 1038 old_len_x = len(x); -> 1039 x = moving_average(x); 1040 if ikey == 0:; 1041 for key in annotations:. ~\Miniconda3\envs\project\lib\site-packages\scanpy\plotting\_tools\paga.py in moving_average(a); 980 ; 981 def moving_average(a):; --> 982 return _sc_utils.moving_average(a, n_avg); 983 ; 984 ax = pl.gca() if ax is None else ax. ~\Miniconda3\envs\project\lib\site-packages\scanpy\_utils.py in moving_average(a, n); 374 An array view storing the moving average.; 375 """"""; --> 376 ret = np.cumsum(a, dtype=float); 377 ret[n:] = ret[n:] - ret[:-n]; 378 return ret[n - 1:] / n. <__array_function__ internals> in cumsum(*args, **kwargs). ~\Miniconda3\envs\project\lib\site-packages\numpy\core\fromnumeric.py in cumsum(a, axis, dtype, out); 2421 ; 2422 """"""; ->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967:369,perform,perform,369,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1168#issuecomment-615878967,1,['perform'],['perform']
Performance,"@ivirshup:; * If I am looking at the right example, the two genes have a different scale. There are 6 plots, but only two genes being plotted repeatedly. This is the same behaviour as before. However, now you can pass `vmin` and `vmax` to have the same scale in all plots that are quantitative.; * The point coordinates (e.g. tSNE or UMAP) are already precomputed, what changes in each plot is the order in which to plot the points because by default `sort_order=True`. The matplotlib scatter function takes care of the layout, but I don't see how this can be further optimized. Do you have any idea? Plotting large numbers of cells does not seem particularly slow for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/244#issuecomment-426894394:568,optimiz,optimized,568,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/244#issuecomment-426894394,1,['optimiz'],['optimized']
Performance,"@karenlawwc ; For the test.h5ad that you’ve saved using adata.write, I think you want to load it with sc.read_h5ad() rather than sc.read_10x_h5(), since the saved test file will be in AnnData h5ad format",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1255636170:89,load,load,89,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1255636170,1,['load'],['load']
Performance,"@michalk8 thanks for the extensive recommendations!. I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this?. I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here?. --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be goo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635:312,load,load,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-754352635,1,['load'],['load']
Performance,"@mxposed It may be worth noting that scanpy's sc.pp.highly_variable_genes takes an argument `flavor` which defaults to the original [2015 Seurat paper](https://www.nature.com/articles/nbt.3192). To Obtain the same set of Highly Variable Genes as produced by modern versions of Seurat [2019 Stuart et al. paper](https://www.sciencedirect.com/science/article/pii/S0092867419305598), it is necessary to pass 'seurat_v3' for this value. You will need to install scikit-misc for this method to work:; ```sh; pip install --user scikit-misc; ```; But there is another wrinkle... the seurat3 algorithm needs count data. therefore it is necessary to rearrange the normalization in scanpy:; ```py; # find the highly variable genes...; # Since we are using seurat_v3 as the flavor,; # we have to do this before normalization; sc.pp.highly_variable_genes(sc96, flavor='seurat_v3', ; n_top_genes=2000). # Normalize and log transform (over all genes); sc.pp.normalize_total(sc96, target_sum=1e4); sc.pp.log1p(sc96). # it is necessary to do the Normalization before selecting; # to just the highly variable genes else our normalization ; # for reads will only be counting the subset. # now select the subset; sc96 = sc96[:,sc96.var.highly_variable]; ```; With these steps scanpy selects the exact same set of HGV and the Normalized log1p data in scanpy `sc96.X` is equal to `sc96$RNA@data)[VariableFeatures(object=sc96),]` in Seurat to about 6 decimal places in my dataset. And thanks for sharing your notebook link, I am trying to perform a similar comparison.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692:1517,perform,perform,1517,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1531#issuecomment-1079775692,1,['perform'],['perform']
Performance,"@quasiben As far as I know Cusparse is being used under Cupy currently for a lot of the operations. I’m not quite sure why those slicing strategies aren’t supported yet. I just figured maybe they were less trivial than the others and weren’t immediately needed so they were pushed off to future feature requests. . The issue #2360 I can’t imagine is too hard- I imagine the output array the size of the selection list could be allocated and a Cuda kernel scheduled to write the selected entries in parallel. I’m not as sure about the other issue, but what Dask is trying to do seems more like an API compatibility issue than one of performance/compute.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727:632,perform,performance,632,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618719727,1,['perform'],['performance']
Performance,"@sjfleming I am currently facing the same issue. I was able to load the h5 output with this input function you stated here https://lightrun.com/answers/broadinstitute-cellbender-read_10x_h5-error-in-scanpy-191. But after further analysis and I wanted to save the adata object with the write function to h5ad format, I am not able to read that saved h5ad object with scanpy again with error ; **test.h5ad contains more than one genome. For legacy 10x h5 files you must specify the genome if more than one is present. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp']**. For some reason, the genome ""GRCh38"" is not showing up in the available genomes options. And when I tried to use command test = sc.read_10x_h5 ('test.h5ad', genome = ""GRCh38), this error shows up again ; **Could not find genome 'GRCh38' in 'test.h5ad'. Available genomes are: ['X', 'layers', 'obs', 'obsm', 'obsp', 'raw', 'uns', 'var', 'varm', 'varp'].** . Do you know if this error is related to this pull request? and is there any fix to it so that I can save processed h5ad after cellbender and able to read it again? Thank you very much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051:63,load,load,63,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2246#issuecomment-1247444051,1,['load'],['load']
Performance,"@tomwhite OK, I added this to the release notes (https://github.com/theislab/scanpy/commit/cee23dc13cf2b77d8e23ee0f91eb55fac0e35ed8, sorry confounded with some style change); it would be nice to have a link to your performance benchmarks... Let me know when we should announce it on twitter. I'm also happy to retweet...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889:215,perform,performance,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/371#issuecomment-456647889,1,['perform'],['performance']
Performance,"A list for keeping track of things that we might change when breaking backwards compat at some point:. - [ ] merge sparse pca https://github.com/theislab/scanpy/pull/1066; - [x] merge https://github.com/theislab/scanpy/pull/1111; - [ ] merge #572; - [ ] make `t-test` or `wilxocon` the default of `tl.rank_genes_groups`; - [ ] set the cachdir default to `user_cache_dir(…)`, `~/.scanpy/cache/` or `~/.cache/`; - [ ] stationary states in DPT: https://github.com/theislab/scanpy/blob/b11b4abe5e16053c010e57b2dd3a27396a4b0cf2/scanpy/neighbors/__init__.py#L853-L857 thanks to @Marius1311 for pointing it out!; - [ ] rename `log2fc` or similarly: #446; - [ ] add `inplace` functionality where easily possible, that's not a simple renaming; a function that has `inplace` in it, should only return the annotation if `inplace=False`; the `copy` functions return the whole `adata`, which we don't want...; - [ ] rename `n_comps` to `n_components` everywhere; - [ ] consider merging https://github.com/theislab/scanpy/pull/403; - [ ] replace default pca solver with 'arpack'; - [ ] change default solver in logreg solver in rank_genes_groups to lbfgs; - [x] merge #621; - [ ] make `pp.highly_variable_genes` return a df instead of a recarray...; - [ ] Transition away from positional APIs: #464 (actually backwards compatible through decorator!). anndata:; - [ ] merge https://github.com/theislab/anndata/pull/130 and fix Scanpy tests",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/453:386,cache,cache,386,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/453,2,['cache'],['cache']
Performance,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way.; I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/279#issuecomment-426898375:43,optimiz,optimizes,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426898375,1,['optimiz'],['optimizes']
Performance,"According to the PBMC 3k tutorial, which I consider as the 'best practice' tutorial for scanpy, regressing out the fraction of mitochondrial reads and the number of detected genes is recommended as a 'standard processing step'. . Having analysed two different datasets, I am so sure anymore if this is a good idea. **number of detected genes**; I loaded these datasets into scanpy and processed them according to the 3k PBMC tutorial: ; * [Savas et al., 2018](https://doi.org/10.1038/s41591-018-0078-7), ~6k cells, CD3+ T cells, BRCA; * [Lambrechts et al., 2018](https://doi.org/10.1038/s41591-018-0096-5), ~32k cells, whole tissue NSCLC. Regress-out seems to perfectly do its jobs on the *Savas et al.* dataset, that contains closely related cell types (1st row of figure): The 2nd PC is confounded by the number of detected genes and this effect is reduced. . On the *Lambrechts et al* dataset, that contains all kinds of cells (cancer, stromal, immune), this looks differently: Neither of the first 2 PCs seems to be related to the number of detected genes and it actually seems to me that I am 'loosing' information by applying `regress_out` (everything is now a single blob). . **cell cycle**; The lower part of the plot shows regress out applied to the cell cycle (following [the scanpy tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb) and the 'alternative approach' described in the [seurat vignette](https://satijalab.org/seurat/cell_cycle_vignette.html#assign-cell-cycle-scores), i.e. I regressed out the difference between the G2M and S phase scores):; ```; adata.obs[""cell_cycle_diff""] = adata.obs[""S_score""] - adata.obs[""G2M_score""]; sc.pp.regress_out(adata, ['cell_cycle_diff']); ```; Like that, the differences between dividing and non-dividing cells should be preserved. ; Again, in the *Savas* dataset, after regressing out the cell cycle effects, G1 is correctly separated from G2M/S. In *Lambrechts*, there is no cle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/526:347,load,loaded,347,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/526,1,['load'],['loaded']
Performance,Add pip cache to CI,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1620:8,cache,cache,8,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1620,1,['cache'],['cache']
Performance,"Addendum: different errors are generated depending on which axis is first sliced. The data set I'm loading is a dense matrix. ```; >>> data.X.dtype; dtype('<f4'); >>> data[:,0][0,:]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 669, in __init__; self._init_as_view(X, oidx, vidx); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 694, in _init_as_view; uns_new = deepcopy(self._adata_ref._uns); File ""/usr/lib/python3.6/copy.py"", line 180, in deepcopy; y = _reconstruct(x, memo, *rv); File ""/usr/lib/python3.6/copy.py"", line 307, in _reconstruct; y[key] = value; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 444, in __setitem__; _init_actual_AnnData(adata_view); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 367, in _init_actual_AnnData; adata_view._init_as_actual(adata_view.copy()); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 880, in _init_as_actual; self._check_dimensions(); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1879, in _check_dimensions; .format(self._n_obs, self._obs.shape[0])); ValueError: Observations annot. `obs` must have number of rows of `X` (1), but has 2638 rows.; >>> data[0,:][:,0]; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1303, in __getitem__; return self._getitem_view(index); File ""/cellxgene/venv/lib/python3.6/site-packages/anndata/base.py"", line 1307, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/cellxg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600:99,load,loading,99,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/332#issuecomment-433745600,1,['load'],['loading']
Performance,"Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default.; 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865:328,perform,performance,328,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865,1,['perform'],['performance']
Performance,"After @ivirshup's pytables PR (#2064) we started having issues with loading h5 files with scalar datasets, such as those created by CellBender (https://github.com/broadinstitute/CellBender/issues/128). It is currently not an issue for the 10X h5 files for now since they don't have any scalars, however it'd be good to just handle scalars as well as arrays for two reasons, 1) to fix the cellbender file loading problem 2) to fix potential problems we might end up having if 10X h5 format includes scalar datasets. I am not an HDF/h5py/tables person though, so please review carefully :) (although it's a tiny PR) Also let me know what you think, @sjfleming!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2344:68,load,loading,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2344,2,['load'],['loading']
Performance,"After removing the graphs and loading the loom file into scanpy with the now empty graphs slot, is there a way to manually add it back in? For example, before removing the graphs attribute, I call as.matrix() and saved it as a CSV (probably a better way to do this to maintain the sparse property). I can now read this CSV back into Python (e.g. with pandas), but what is the correct way to reload it into the resulting AnnData object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-653220911:30,load,loading,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-653220911,1,['load'],['loading']
Performance,"Ah yes - sorry I missed the context manager bit from earlier in this thread. I tried running this earlier today using the script in theislab/scanpy_usage#17, and for 130K cells NN took 53s unoptimized vs 35s optimized (32 cores). (Not a proportionate speedup, but still worthwhile.) The UMAP speedup shown in that script is significant too. I see that there is a UMAP issue to discuss this further.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913#issuecomment-553420798:208,optimiz,optimized,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913#issuecomment-553420798,1,['optimiz'],['optimized']
Performance,"Ah, I think I see what you're asking now. At the moment, I don't think we have a function for that. But this should be fairly straightforward to work around. Something like this should work:. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from sklearn.metrics import pairwise_distances; import seaborn as sns. def groupby_mean(adata, groupby):; grouped = adata.obs.groupby(groupby); results = np.zeros((grouped.ngroups, adata.n_vars), dtype=np.float64). for idx, indices in enumerate(grouped.indices.values()):; results[idx] = np.ravel(adata.X[indices].mean(axis=0)). return pd.DataFrame(results, columns=adata.var_names, index=grouped.groups.keys()). # Loading data; pbmc_full = sc.datasets.pbmc3k_processed().raw.to_adata(); pbmc_small = sc.datasets.pbmc68k_reduced().raw.to_adata(); var_intersect = pbmc_full.var_names.intersection(pbmc_small.var_names). # Calculate mean expression per cell type; full_means = groupby_mean(pbmc_full[:, var_intersect], ""louvain""); small_means = groupby_mean(pbmc_small[:, var_intersect], ""louvain""). # Correlation distance between celltypes; corr_mtx = pd.DataFrame(; pairwise_distances(full_means, small_means, metric=""correlation""),; index= full_means.index,; columns=small_means.index,; ); ```. Is this more of what you were thinking?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537:680,Load,Loading,680,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1760#issuecomment-807905537,1,['Load'],['Loading']
Performance,"Ah, okay... so you sample based on how representative a cell is of its neighbours, and then you use that weight to calculated PCA, marker genes, and perform visualizations. Is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494336456:149,perform,perform,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494336456,1,['perform'],['perform']
Performance,"Ahh... this makes sense. And this makes it a bit dangerous as well. I would generally compute HVGs after batch correction.. and batch correction generally takes log-normalized data, so the data you have before performing this function will be log-normalized most of the time. I assume this is true not just for me. Maybe log=False should be the default? Or at least a warning should be output I feel.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208:210,perform,performing,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/172#issuecomment-398721208,1,['perform'],['performing']
Performance,"An example with real data:. ![counts](https://user-images.githubusercontent.com/20436557/89998524-f957c600-dc8d-11ea-9036-a5d165d6bad5.png). Code:; ```py; # Load the PBMC 3k data; adata = sc.read_10x_mtx(; os.path.join(; save_path, ""filtered_gene_bc_matrices/hg19/""; ), # the directory with the `.mtx` file; var_names=""gene_symbols"", # use gene symbols for the variable names (variables-axis index); ); adata.var_names_make_unique(). # Get counts; adata.obs[""n_counts""] = adata.X.sum(axis=1).A1; sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4); adata.obs[""n_counts_normalized""] = adata.X.sum(axis=1).A1; sc.pp.log1p(adata); adata.obs[""n_counts_normalized_log""] = adata.X.sum(axis=1).A1. # Dim reduction; sc.tl.pca(adata, svd_solver=""arpack""); sc.pp.neighbors(adata); sc.tl.umap(adata); sc.pl.umap(adata, color=[""n_counts"", ""n_counts_normalized"", ""n_counts_normalized_log""]); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269:157,Load,Load,157,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364#issuecomment-672762269,1,['Load'],['Load']
Performance,"Any ideas what's going on here? I can't do a ""pip3.6 install scanpy"" on our linux cluster:. <details>. ```; Collecting scanpy; Using cached scanpy-0.4.3.tar.gz; Requirement already up-to-date: anndata>=0.5 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: matplotlib==2.0.0 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: pandas>=0.21 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scipy in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: seaborn in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: psutil in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: h5py in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:133,cache,cached,133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546:65,load,loading,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-496960546,1,['load'],['loading']
Performance,"Anybody experience something similar? . I'm attempting to regress out cell cycle gene information from a single cell dataset. ```; # ; # # Part of the error message that probably matters most; # . Crashed Thread: 0. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 00000001039d0000-00000001039d1000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. # ; # ; # ; ```. Any ideas on what the problem could be?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/194:709,multi-thread,multi-threaded,709,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/194,1,['multi-thread'],['multi-threaded']
Performance,"Arguments for and against converting values in `downsample_counts`:. If we don't convert dtypes back to what they originally were, there's a slight performance boost since we don't have to have two copies. I we return an array of integers we run into trouble downstream with functions that aren't tested with integer arrays. Issues from this have been opened a few times, so when I wrote this I thought it might be worth just maintaining the input type. I'm not sure I agree with that now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197:148,perform,performance,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865#issuecomment-552292197,1,['perform'],['performance']
Performance,"As can be seen with the KL divergence values in the above table, while the output of Intel optimized t-SNE is different, it is equivalent in quality.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122440284:91,optimiz,optimized,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122440284,1,['optimiz'],['optimized']
Performance,"As the clustering is an optimization of the modularity I would argue it makes little sense to use modularity to evaluate the clustering again. Especially at different resolutions, the modularity values you obtain are not really comparable (as the resolution parameter is introduced to not optimize pure modularity and get the same result you would otherwise get at resolution 1). A comparison between knn-graph modularity at the same resolution would tell you how inherently modular the graph is. Is that what you want to know? Or what does an 'improved' graph look like to you?. I agree with Alex that using the silhouette coefficient wouldn't be much more informative. It would then just be an assessment of the approach of using a KNN graph and modularity optimization. And as that approach has been shown to work quite well, evaluating it based on something that works less well (clustering in the feature space directly) feels a bit uninformative. I would go with what you suggested: evaluating based on marker gene expression. In the end the graph is a tool to describe the biology, so any graph structure means little without it. . If you want to evaluate how well the graph represents the biology, maybe the best way forward would be to infer cell-type labels (or use a dataset with labels) and look at the normalized mutual information between clusters and the labels. The clusters would have to be obtained in the same way for each graph (e.g. modularity optimization at a fixed resolution). Depending on how specific the labels are you will get a different result though ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942:24,optimiz,optimization,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409960942,4,['optimiz'],"['optimization', 'optimize']"
Performance,At the moment I am saving the adata and I am generating the plots loading it. In this way the coordinates are always the same.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801:66,load,loading,66,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1363#issuecomment-678040801,1,['load'],['loading']
Performance,"Awesome, thank you!. Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797:608,perform,performance,608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441478797,2,"['bottleneck', 'perform']","['bottleneck', 'performance']"
Performance,"Awesome, thanks everyone. @ivirshup I added something to the release notes in the latest commit. I hope the formatting is okay -- let me know if there's some better way to do it. @LuckyMD I've seen your benchmarking preprint and admire the work! For the current API, I'm currently mooching off of tutorials made by others: one which is simpler and one (included in the scanpy tutorials) that is a little more advanced: https://github.com/brianhie/scanorama#full-tutorial. Should this get merged and included in the scanpy API, I promise I'll make a new notebook-based tutorial (probably in Google Colab) that shows off the new API and include a link to it from the Scanorama GitHub README.md. I also agree with shortening the default embedding to `'X_scanorama'` and have done that in the latest commit. @falexwolf Happy to make any changes to the tests if you think that will boost performance, if you'd like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954:883,perform,performance,883,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665719954,1,['perform'],['performance']
Performance,"B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ); 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%); 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%); 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%); 	 257.750.810.841 instructions:u # 2,44 insn per cycle; 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%); 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%); 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%); 	; 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ); ```. - this PR:. ```console; $ git switch hvg_PR_numba; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ); 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%); 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%); 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%); 	 373.047.679.552 instructions:u # 2,19 insn per cycle; 	 # 0,11 stalled cycles per insn ( +- 9,57% ) (83,34%); 	 67.830.590.839 branches:u # 1,093 G/sec ( +- 9,58% ) (83,35%); 	 1.702.825.180 branch-misses:u # 4,56% of all branches ( +- 9,56% ) (83,28%); 	; 	 7,1623 +- 0,0560 seconds time elapsed ( +- 0,78% ); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266:1375,Perform,Performance,1375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266,1,['Perform'],['Performance']
Performance,Backport PR #2248 on branch 1.9.x (Fix legacy 10x loader when more than one genome exists),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2342:50,load,loader,50,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2342,1,['load'],['loader']
Performance,Backport PR #2248: Fix legacy 10x loader when more than one genome exists,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2342:34,load,loader,34,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2342,1,['load'],['loader']
Performance,Backport PR #3177 on branch 1.10.x (Cache data for subsequent test runs),MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3179:36,Cache,Cache,36,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3179,1,['Cache'],['Cache']
Performance,Backport PR #3177: Cache data for subsequent test runs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3179:19,Cache,Cache,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3179,1,['Cache'],['Cache']
Performance,"By default, scanpy took the expression data saved at adata.raw if that is not available it took the data from adata.X. If you are loading the expression data from csv or txt file, try to save adata.raw = data, before slicing for HVGs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-1770949492:130,load,loading,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-1770949492,1,['load'],['loading']
Performance,"C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")); C:\Users\laurenh\anaconda3\lib\site-packages\seaborn\_core.py:1303: UserWarning: Vertical orientation ignored with only `x` specified.; warnings.warn(single_var_warning.format(""Vertical"", ""x"")). #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; anndata 0.7.5; backcall 0.2.0; bottleneck 1.3.2; cairo 1.20.0; cffi 1.14.3; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2.30.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.1; joblib 0.17.0; kiwisolver 1.3.0; legacy_api_wrap 0.0.0; llvmlite 0.34.0; louvain 0.7.0; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.1.1; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; pandas 1.1.3; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.1; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.6.0; scipy 1.5.2; seaborn 0.11.0; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.0.4; traitlets 5.0.5; typing_exten",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1742:2297,bottleneck,bottleneck,2297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1742,1,['bottleneck'],['bottleneck']
Performance,"CC @flying-sheep this is an untested as I don't have a windows machine handy to trigger the platform-int-size problem. I'm also somewhat guessing at the fix! From looking at the scanpy source, I don't think that changing the `dtype` of `ns` to a platform consistent and wider `int` will do anything catastrophic to performance or alter the logic in the alg in which it's used as it seems to be a simple index. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732:315,perform,performance,315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1359#issuecomment-670421732,1,['perform'],['performance']
Performance,Cache data for subsequent test runs,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3177:0,Cache,Cache,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3177,1,['Cache'],['Cache']
Performance,"Cache datasets so notebook tests can run without requiring an external server, since they cover realistic use cases and a good amount of the API.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/717:0,Cache,Cache,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/717,1,['Cache'],['Cache']
Performance,Can I load it if it's not in `zarr` format.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2375620692:6,load,load,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2375620692,1,['load'],['load']
Performance,Cannot load EBI Expression Datasets,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2449:7,load,load,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2449,1,['load'],['load']
Performance,"Cool ! ; in order to load legacy h5, I had to freeze scanpy==1.8.2; now I included this fix in a scanpy fork. I hope it gets merged soon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2248#issuecomment-1127793416:21,load,load,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2248#issuecomment-1127793416,1,['load'],['load']
Performance,"Copying using the `copy` module is a bit ill defined for `AnnData` objects currently. This has to do with some internals of how we do views of arrays. In general I'd recommend doing copies via `adata.copy()`, which performs a deep copy. But it looks like there might be another problem with the PCA not being exactly reproducible. After a fair amount of checking that it was exactly reproducible, it looks like we forgot to actually pass the random seed... There has been fixed, and there will be a bug-fix release soon (#1240). This still does not fix the issue of reproducibility if you've made a shallow copy of a AnnData view with `copy`. I'll have to look into this a bit more.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443:215,perform,performs,215,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1239#issuecomment-631951443,1,['perform'],['performs']
Performance,Could not use Scanpy 0.4 to load AnnData file generated by v0.2.8,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56,1,['load'],['load']
Performance,"Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805:52,load,loadings,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805,1,['load'],['loadings']
Performance,"DENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=us",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:986,cache,cache,986,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,1,['cache'],['cache']
Performance,"Dear @wangjiawen2013,. what is the interest behind your question? Do you have many datasets with very few cells?; Scanpy itself can easily work with very small datasets, but you should always be aware of statistical limitations when performing statistical tests etc on very few cells.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672:233,perform,performing,233,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815287672,1,['perform'],['performing']
Performance,"Dear both, . correlation matrices are available now. Following our usual split into tools and plotting, you can call . `sc.tl.correlation_matrix(adata,name_list, n_genes=20, annotation_key=None, method='pearson')`. for correlation matrix calculation. ; I have left out a few parameters because I wrote the function actually to conveniently plot results from DE testing, but the basic functionality is the following: . _adata_ is the usual AnnData object you are working with. ; _name_list_ is a string containing gene names and should be specified. ; _n_genes_ cuts the name_list if the number specified is smaller then the length of the list, so set this high enough if you want to work with large data ; _annotation_key_ allows you to specify a string that works as the key in the AnnData object where results are stored. By default, the key is ""Correlation_matrix"". The method basically wraps the pd.DataFrame.corr method, which allows you to specify the correlation method ('pearson', 'spearman', 'kendall'). . I use it for smaller data so it has not been optimized for performance (yet), but I tested the method for 3k cells and 600 genes and ended up with a runtime of ~8 seconds. I hope that is conveniently fast enough for you (if not let us know). . After calling the tool, you can plot correlation matrices (using a wrapper for seaborn heatmap) by calling. `sc.pl.correlation_matrix(adata, annotation_key=None)`. This function searches basically only the AnnData annotation (again, if no key specified, ""Correlation_matrix"" is the default). Hope this does the job!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662:1060,optimiz,optimized,1060,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72#issuecomment-361891662,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,Defer loading of umap to speed up import,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704:6,load,loading,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704,1,['load'],['loading']
Performance,"Definitely been an abstract todo for a while. Tracking for 1.9. Some questions:. * What about methods where more than one element is added to the AnnData? E.g. for PCA we also add the variable loadings to `varm`; * How do these parameters get tracked in the `uns` metadata? Currently the key added there is largely fixed, but maybe it should vary too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904:193,load,loadings,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1861#issuecomment-867334904,1,['load'],['loadings']
Performance,Do not use the cached rp_forest,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1558:15,cache,cached,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1558,1,['cache'],['cached']
Performance,"Don't know what this ""review"" process is , but basically it is ready. . De: ""Lukas Heumos"" ***@***.***> ; À: ""theislab/scanpy"" ***@***.***> ; Cc: ""Yves Le Feuvre"" ***@***.***>, ""Mention"" ***@***.***> ; Envoyé: Jeudi 6 Janvier 2022 20:11:35 ; Objet: Re: [theislab/scanpy] Pca loadings n points patch (PR #2075) . [ https://github.com/Yves33 | @Yves33 ] is this ready for review? . — ; Reply to this email directly, [ https://github.com/theislab/scanpy/pull/2075#issuecomment-1006847333 | view it on GitHub ] , or [ https://github.com/notifications/unsubscribe-auth/ACEYIQUT75OTZC3MUGVAT3DUUXSOPANCNFSM5JWG2IZQ | unsubscribe ] . ; Triage notifications on the go with GitHub Mobile for [ https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675 | iOS ] or [ https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub | Android ] . ; You are receiving this because you were mentioned. Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047:275,load,loadings,275,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075#issuecomment-1007917047,1,['load'],['loadings']
Performance,"E.g. if your original input matrix has 1,000,000 number of cells and 100; genes. You don't want to process all rows, so you can perform either; uniform sampling or weighted sampling on the data. I have performed; weighted sampling and sampled e.g. only 1,000 rows then each rows will have; a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>; wrote:. > I don't quite understand what sampled data with weights on the rows are.; > How do you weight individual cells in a dataset? What do weights like this; > mean?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494098188:128,perform,perform,128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494098188,2,['perform'],"['perform', 'performed']"
Performance,"Every time I build the docs locally, they do a complete rebuild. This is painfully slow (especially with our examples that run on each build) and really discourages editing the docs. This is happening because the sphinx sees the config being modified. There are two causes of this:. * The version being set dynamically – at each commit the version string changes.; * `scanpydoc.elegant_typehints` sets some properties of the config after it's loaded. E.g.:. ```; updating environment: [config changed ('typehints_formatter')] 317 added, 0 changed, 0 removed; ```. ### Solution. Version being set dynamically does really add that much value for us, so I just removed that part of the version string. `scanpydoc.elegant_typehints` does make the doc-strings nicer, but it is not worth a five minute build to update the docs. Ideally it can be implemented in a way that doesn't make sphinx think the config has changed, but I am disabling it until then.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2199:443,load,loaded,443,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2199,1,['load'],['loaded']
Performance,"Finally, we could solve this elegantly without sacrificing a scalable design, as shown in the [tutorial](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). Also, Scanpy is accepted in Genome Biology and will soon be published. Merry Christmas! :); Alex",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-353766971:61,scalab,scalable,61,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-353766971,1,['scalab'],['scalable']
Performance,Fix legacy 10x loader when more than one genome exists,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2248:15,load,loader,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2248,1,['load'],['loader']
Performance,"Fixes #1697. I'll leave any major backwards compatibility changes (i.e. updating to use python 3.7+ features) to the future. CI took a really long time to install the dependencies on the first run. Hopefully that will be cached? Not sure what triggers a saved cache. At the moment it looks like building `louvain` is taking a while, which we should actually just get around to deprecating.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1897:221,cache,cached,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1897,2,['cache'],"['cache', 'cached']"
Performance,"Fixes #1806. New behaviour for Moran's I and Geary's C. If one of the variables passed has constant values, the score for that variable is `nan` and the function warns the user about this. Previously, the presence of this variable would silently fail, corrupting the other outputs as well. Adds a new utility `is_constant` to check if values in an array are constant. * Could have less code repetition, since now there's some logic that is applied to any case which is 2d, but the conditional isn't structured this way.; * Performance hit pretty minor, as computing the metric itself is expensive.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1891:523,Perform,Performance,523,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1891,1,['Perform'],['Performance']
Performance,"Following on discussion from #316, I've renamed a number of arguments and metrics. Additionally I've optimized a bit for memory usage. Changes to naming can be summarized as follows:. | current | proposed |; | ------- | -------- |; |`total_features_by_{expr_values}` | `n_{var_type}_by_{expr_type}`|; |`total_{expr_values}` | `total_{expr_type}`|; |`pct_{expr_values}_in_top_{n}_features` | `pct_{expr_type}_in_top_{n}_{var_type}`|; |`total_{expr_values}_{feature_control}` | `total_{expr_type}_{qc_var}`|; |`pct_{expr_values}_{feature_control}` | `pct_{expr_type}_{qc_var}`|; | | |; |`total_{expr_values}` | `total_{expr_type}`|; |`mean_{expr_values}` | `mean_{expr_type}`|; |`n_cells_by_{expr_values}` | `n_cells_by_{expr_type}`|; |`pct_dropout_by_{expr_values}` | `pct_dropout_by_{expr_type}`|. I went with `qc_vars` over `control_vars` on the recommendation of a lab mate, since they are presumably variables which are important for quality control, but were not necessarily controlled.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/358:101,optimiz,optimized,101,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/358,1,['optimiz'],['optimized']
Performance,"From a gene matrix, tsne and cluster .csv files obtained from cell ranger output I was able to load these into scanpy and display a tsne plot that look exactly like the output of cellranger cloupe file. This is great thanks!. ![screen shot 2018-12-18 at 14 32 39](https://user-images.githubusercontent.com/39877296/50134113-eaaae700-02d1-11e9-96db-8c2a3393724b.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/399#issuecomment-448102220:95,load,load,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/399#issuecomment-448102220,1,['load'],['load']
Performance,"From the methods of the paper mentioned by @wangjiawen2013:. > our results were not sensitive to the default values of nPC_max. which reinforces my thinking that overshooting the number of PCs isn't a problem for typical clustering and visualization purposes. For interpreting the variable loadings, some selection might be helpful. I'd definitely be interested in having methods like these for use with other latent variable methods. Also that MCV paper's Figure 2b should probably have the APOE axis share a scale, maybe by removing the cell that has ~twice the APOE log expression of any others. I'd be interested in seeing how different the plots look after that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707:290,load,loadings,290,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-559334707,1,['load'],['loadings']
Performance,Get errors when performing sc.pp.highly_variable_genes!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:16,perform,performing,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['performing']
Performance,Getting errors when loading loom files,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247:20,load,loading,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247,1,['load'],['loading']
Performance,"Goal:. Add `dask` use-cases to the scanpy benchmarks so we can understand performance changes. . Nice links:. 1. Example benchmark: https://github.com/scverse/scanpy/blob/main/benchmarks/benchmarks/preprocessing_counts.py; 2. Project we use for benchmarking: https://asv.readthedocs.io/projects/asv-runner/en/latest/index.html; 3. Dask local cluster: https://distributed.dask.org/en/stable/api.html#cluster; 4. Using scanpy and dask: https://scanpy.readthedocs.io/en/stable/tutorials/experimental/dask.html. NOTE: this `read_elem_as_dask` function in the notebook is with anndata 0.11 i.e., `pip install --pre anndata`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519:74,perform,performance,74,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3013#issuecomment-2419644519,1,['perform'],['performance']
Performance,"Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. . <img width=""749"" alt=""Screenshot 2019-07-27 at 12 49 49"" src=""https://user-images.githubusercontent.com/37718031/61993507-06971800-b06d-11e9-815e-acf667f818a5.png"">; <img width=""730"" alt=""Screenshot 2019-07-27 at 12 41 58"" src=""https://user-images.githubusercontent.com/37718031/61993416-edda3280-b06b-11e9-9a4f-7d4a1259cd47.png"">. This happens in the first loop to load all the datasets. If I run only one dataset the same error `(unsupported operand type(s) for +: 'int' and 'str')` showed up when I plot some data quality summary plots:. For instance:; `p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac')`; `adata = adata[adata.obs['mt_frac'] < 0.2]; print('Number of cells after MT filter: {:d}'.format(adata.n_obs))`; `sc.pp.filter_cells(adata, min_genes = 700); print('Number of cells after gene filter: {:d}'.format(adata.n_obs))`. I am using data generated by 10x V3 and CellRanger v3.0.2. I really do not know where the problem is. . I really appreciate any advice/help to solve this issue. Thanks in advance",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/751:194,load,load,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751,2,['load'],['load']
Performance,"Great work! . Python (64bit) throws a memory error when projecting ~1 million cells into 3D UMAP with 40 features (PCs). I suppose that matrix products are super big, but I'm performing it on 256G RAM.; Is there a way to decrease memory usage?; I would appreciate your advice!; Thanks ahead!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/710:175,perform,performing,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/710,1,['perform'],['performing']
Performance,"HI everyone, . I have the excat same issue, which prevents me from performing further analysis. ; What I did : ; - dropna(), still boolean values, which poses the same error again (boolean values are NANs appearently); - fillna(0) : replaced all NAN values with 0, but this poses a problem later in the analysis when i lognormalize the data (log(0) = inf).; How do you guys deal with these sorts of problems with your data ? . I don't think the mt colum should contain boolean values... (cf. screeshot); Please correct me if i am wrong, and thank you in advance for your help. ![Screenshot from 2021-12-13 17-17-56](https://user-images.githubusercontent.com/45742503/145848639-6d7c6ee6-a38f-4c48-b38a-c8339984e360.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183:67,perform,performing,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-992636183,1,['perform'],['performing']
Performance,"Have you rebooted the python after updating anndata? Can you paste the exact error you're seeing? Are you allowed to share the object you're having trouble loading?. OP's error is bizarre, e.g. the last part seems to be pointing to an empty line. Like the package was updated but the python was not restarted. I can't recreate the exact one he's seeing, but I've managed to get other disjoint errors along those lines by updating anndata to 0.8.0 in a second terminal while the python in question is still running.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185:156,load,loading,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2297#issuecomment-1450229185,1,['load'],['loading']
Performance,"Hej again,. I found a solution to my problem. If I read my object enabling the cache, I do not need to have it backed, because the huge use of memory when I generate the plots does not happen anymore. However I like the idea of having backed data, and it would be nice to understand why it did not work. Maybe it will be useful with larger datasets. Cheers,; Samuele",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440#issuecomment-456429967:79,cache,cache,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440#issuecomment-456429967,1,['cache'],['cache']
Performance,"Hello - I am trying to use scanpy to load visium ST data. The tissue_positions.csv file is located in the spatial folder - I renamed it to tissue_positions_list.csv per the expectation of scanpy but I continue to get the following error. I have confirmed that the folder structure and pathing is correct. Can you think of what else could be the issue with it not reading/finding this file? . ```py; >>> import os; >>> # p = os.path.join( ""path to outs location""); >>> print(p); ""path to outs location""; >>> print(os.path.exists(p)); True; >>> ad = sc.read_visium(p); ```. ```pytb; Traceback (most recent call last):. Cell In[6], line 1; ad = sc.read_visium(p). File ~\anaconda3\lib\site-packages\scanpy\readwrite.py:390 in read_visium; raise OSError(f""Could not find '{f}'""). OSError: Could not find 'path to outs location\spatial\tissue_positions_list.csv'; ```. ### Session/scanpy info:; Software versions; Python 3.10.9 64bit [MSC v.1916 64 bit (AMD64)]; IPython 8.10.0; OS Windows 10 10.0.22621 SP0; scanpy 1.9.3; Sun May 14 14:59:57 2023 Eastern Daylight Time",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2488:37,load,load,37,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2488,1,['load'],['load']
Performance,"Hello @davidhbrann ,; Sorry for the late response.; I tried again without typing the `--user` in the Anaconda Powershell. Please see below. Step1: install without force. Didn't work. Proceed to Step2.; ```python; (base) C:\WINDOWS\system32>conda activate Python38; (Python38) C:\WINDOWS\system32>pip install scikit-misc; Requirement already satisfied: scikit-misc in c:\users\park_lab\appdata\roaming\python\python38\site-packages (0.1.4); Requirement already satisfied: numpy in c:\users\park_lab\anaconda3\envs\python38\lib\site-packages (from scikit-misc) (1.20.3); ```; Step2: force install.; ```python; (Python38) C:\WINDOWS\system32>pip install scikit-misc --force; Collecting scikit-misc; Using cached scikit_misc-0.1.4-cp38-cp38-win_amd64.whl (142 kB); Collecting numpy; Downloading numpy-1.21.5-cp38-cp38-win_amd64.whl (14.0 MB); |████████████████████████████████| 14.0 MB 3.3 MB/s; Installing collected packages: numpy, scikit-misc; Attempting uninstall: numpy; Found existing installation: numpy 1.20.3; Uninstalling numpy-1.20.3:; Successfully uninstalled numpy-1.20.3; ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:702,cache,cached,702,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['cache'],['cached']
Performance,"Hello @ivirshup , Thanks for your reply. I figured that this might be an issue due to the anndata being read in backed mode. Although the file is large (7 gb in .h5ad format, and as soon as it gets read in memory, it blows up to 28 gb), but for now I have utilized a larger machine for performing my eda, and converted the sparse matrix to a dense one . Thanks for your clarification !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147#issuecomment-1053044750:286,perform,performing,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147#issuecomment-1053044750,1,['perform'],['performing']
Performance,"Hello @ivirshup thanks for this!. Quick question (still very new to python). Upon following your suggestion I get this error:; AttributeError: module 'scanpy.api.tl' has no attribute '_utils'. I then proceeded to install utils (pip install utils), and then; import utils. But still doesn't work. I assume it's because I'm not loading it correctly into the environment for scanpy to use but I don't know how?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562:326,load,loading,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/769#issuecomment-519061562,1,['load'],['loading']
Performance,"Hello world!; I've read in many papers that when performing a re-clustering of some populations, like T cells or B cells, prior to the step of integration and so on, they re-calculate the HVGs but excluding the TCR- or BCR-related genes, because they are donor-specific, especially when talking about BCR. Can you help me how to remove the TCR- or BCR-related genes before computing the HVGs selection, but without removing them from the .var of the anndata, since I want to evaluate their expression during the step of cell annotation?. The code that I use to calculate the HVGs is the following:; sc.pp.highly_variable_genes(adata,; n_top_genes = 4000, flavor = ""seurat_v3"",; layer = ""raw"", batch_key = 'sample_id',; subset = False). Thanks a lot!; Paolo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2895:49,perform,performing,49,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2895,1,['perform'],['performing']
Performance,"Hello, . I am using scanpy rank genes groups, and rank genes group filter for differential expression analysis after using a classifier. I often receive errors because statistics cannot be calculated on these types of low count groups. The workaround I have found is to drop these cells from the adata object, and then continue with differential expression. Is there an existing solution for this that is better? Could we consider adding this as a flag to the function call? What I have in mind is a flag like ""ignore_low = True"". The flag would operate by taking the passed adata object, applying the 2 cell filtration internally, and performing differential expression as normal on this internal object. It would then append the relevant uns categories to the original adata object before exiting. The threshold could even be passable to make this more general. . What do we think? Is this too niche for this scale of a repository? In principle, I think that forcing these observations to be dropped is not best practice.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3118:636,perform,performing,636,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3118,1,['perform'],['performing']
Performance,"Hello, . I have managed to get my Seurat object converted into Loom and then read into Scanpy. Now my main objective is to use the clusters identified using Seurat in order to create a PAGA trajectory map. I was able to do a similar thing for Seurat -> Monocle by integrating the Seurat clusters and allow Monocle to perform a trajectory analysis on them. . I have the following Scanpy object:; ![scanpy_adata](https://user-images.githubusercontent.com/11708268/58907732-7a075380-86d4-11e9-9f2a-4c539ea58c80.png). All the cluster information along with cell ids are present in the obs part of the Scanpy object. Is there anyway to use that information in order to perform a PAGA trajectory analysis?. Thank you,; Behram",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680:317,perform,perform,317,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680,2,['perform'],['perform']
Performance,"Hello,. It would appear that `louvain-igraph` has been obsoleted in favour of `leidenalg`, and the author makes a [persuasive case](https://arxiv.org/abs/1810.08473) as to the superiority of the new approach. To my untrained eye, the algorithm is conceptually similar to the Louvain modification used by Seurat, but introduces an extra collapsed network refinement step. it should be easy to support this in Scanpy - the syntax appears to be identical to the old `louvain` innards, and I was able to construct a very minimal dummy function for testing by taking the key bits of `sc.tl.louvain()` and replacing `louvain.` with `leidenalg.`:. ```py; import leidenalg; import numpy as np; import pandas as pd; from scanpy import utils; from natsort import natsorted. def leiden(adata, use_weights=False, resolution=1, iterations=-1):; 	g = utils.get_igraph_from_adjacency(adata.uns['neighbors']['connectivities'], directed=True); 	weights = None; 	if use_weights:; 		weights = np.array(g.es[""weight""]).astype(np.float64); 	part = leidenalg.find_partition(; 		g, leidenalg.RBConfigurationVertexPartition, ; 		resolution_parameter = resolution, weights = weights, ; 		n_iterations = iterations,; 	); 	groups = np.array(part.membership); 	adata.obs['louvain'] = pd.Categorical(; 		values=groups.astype('U'),; 		categories=natsorted(np.unique(groups).astype('U')),; 	); ```. As such, replacing any `louvain.` with `leidenalg.` in `sc.tl.louvain()` would do most of the work. Probably the only new thing that would need support would the the `n_iterations` parameter in `leidenalg.find_partition()`. The default value is 2, positive values control how many passes of the algorithm are performed. -1 just makes it run until it fails to improve the clustering.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/350:1677,perform,performed,1677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/350,1,['perform'],['performed']
Performance,"Hello,; For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:; - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)).; - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381:673,perform,performed,673,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599#issuecomment-1465898381,1,['perform'],['performed']
Performance,"Hello,; it's me again, really thanks for your kindly reply before.; when I analyze my own data using `sc.tl.dpt` with default `n_branches`, it worked well, but when I set `n_branches` more than 0, it occurred an error:; ```no root cell found, no computation of pseudotime; --> To enable computation of pseudotime, pass the index or expression vector; of a root cell. Either add; adata.add['iroot'] = root_cell_index; or (robust to subsampling); adata.var['xroot'] = adata.X[root_cell_index, :]; where ""root_cell_index"" is the integer index of the root cell, or; adata.var['xroot'] = adata[root_cell_name, :].X; where ""root_cell_name"" is the name (a string) of the root cell.; perform Diffusion Pseudotime analysis; using ""X_pca"" for building graph; using stored data graph with n_neighbors = 30 and spectrum; [ 1. 0.9944264293 0.9934666753 0.9925051928 0.9899699688; 0.9893597364 0.9855745435 0.9840251803 0.981688261 0.9806631804]; detect 1 branching; do not consider groups with less than 2742 points for splitting; branching 1: split group 0; WARNING: detected group with only [] cells. ValueError Traceback (most recent call last); <ipython-input-3-b1749d943ac4> in <module>(); ----> 1 get_ipython().run_cell_magic('time', '', 'sc.tl.dpt(adata_corrected,n_jobs=48,n_pcs=30,allow_kendall_tau_shift=False,n_branchings=1)\nsc.logging.print_memory_usage()'). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py in run_cell_magic(self, magic_name, line, cell); 2113 magic_arg_s = self.var_expand(line, stack_depth); 2114 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /pu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33:676,perform,perform,676,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33,1,['perform'],['perform']
Performance,"Here are some updates:; - `_fuzzy_simplicial_set` from umap has been freshly exposed in the nightly version of cuml 22.06 (stable should be there in the coming weeks), so I did a quick implementation and now have a fully accelerated sc.pp.neighbors!; - I also used this opportunity to introduce `read_mtx_gpu` function, which includes a dask_cudf backend for out of vram memory mtx reading. I performed a speed comparison on a 100.000 cells dataset, running full simple pipeline from loading the mtx until UMAP/leiden:. ![image](https://user-images.githubusercontent.com/27488782/170506738-39eb95ac-9340-4790-ad0d-36ac07575b5f.png). The GPU accelerated code shows a 13X speedup compared to CPU based functions (tested on 12 CPU cores system)!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110:393,perform,performed,393,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1533#issuecomment-1138619110,2,"['load', 'perform']","['loading', 'performed']"
Performance,"Here's an example with the newest numba and llvmlite. . I noticed with fewer cells it works. . ```python; import scanpy as sc; import anndata; import numpy as np. a = anndata.AnnData(np.random.poisson(size=(4000, 5000))); b = anndata.AnnData(np.random.poisson(size=(10000, 5000))). sc.external.pp.mnn_correct(a, b); ```. ```; Performing cosine normalization...; Starting MNN correct iteration. Reference batch: 0; Step 1 of 1: processing batch 1; Looking for MNNs...; Computing correction vectors...; /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""compute_correction"" failed type inference due to: non-precise type pyobject; [1] During: typing of argument at /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py (107). File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File "".pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /data/yosef2/users/adamgayoso/.pyenv/versions/3.7.3/envs/anndata_test/lib/python3.7/site-pac",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200:326,Perform,Performing,326,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/974#issuecomment-572849200,1,['Perform'],['Performing']
Performance,"Hey @giovp,; thanks for going over the PR once more - I'm sorry about the problem with the docs, anything I can do here? I am not very experienced with readthedocs.. > I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Yes, thanks for catching that! Seems I just forgot to `git add _recipe.py`. Regarding the name of the output field: I decided to call it `X_pearson_residuals_pca` as it is the data in `X` after Pearson residuals plus PCA. I thought that adds some clarity to how that PCA was obtained. . On the other hand, if one were to apply Pearson residuals and PCA ""manually"" in sequence and with default settings, one would get an `adata` with `X` holding the Pearson residuals and `obs['X_pca']` holding the PCA results.. that is also how the `recipe_weinreb17()` returns its PCA. So maybe it would be cleaner the way you suggested. Same goes for the `adata.uns['pearson_residuals_pca']` field btw, which I would then rename to `adata.uns['pca']`. I will make a quick commit including that change!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567:388,perform,performed,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890959567,1,['perform'],['performed']
Performance,"Hey @kchl5 and @vitkl,. Muon (`mu.read_10x_h5()`) should load it correctly if the `feature_types` value for the gRNAs is different from the one for the genes. As they are missing, I assume it is. Moreover, just in case you're interested, splitting by `feature_types` is even [a feature](https://github.com/scverse/mudata/blob/4d3b5f4e6039b4a31519584db5461a5809741dce/mudata/_core/mudata.py#L88) of the `MuData` initialiser, so running . ```py; adata = sc.read_10x_h5(h5file, gex_only=False); mdata = MuData(adata); ```. should also work, and this is roughly what muon does. (Thanks for tagging me, @adamgayoso!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2398#issuecomment-1386019022:57,load,load,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2398#issuecomment-1386019022,1,['load'],['load']
Performance,"Hey @ywen1407!. The ideal case is that you don't pre-filter the gene sets before concatenating. Then, if you have aligned both sets of samples to the same genome, everything should be fine and you can filter out genes afterwards. Otherwise an outer join would only assume all values you filtered out were 0, which is probably not the way forward. That's why the only decent option you really have is an inner join. I assume you should have the unfiltered objects somewhere though. Regarding memory use: ComBat is something we (actually, this was thanks to @Marius1311) just re-implemented from python and R code that was flying around. We do not generally optimize methods that were published elsewhere. How much RAM are you using that it's crashing? I think Marius even made ComBat usable for sparse matrices, so it's already using less memory than it was before. 38K cells doesn't sound like something that would require more than 16GB RAM. I can run datsasets with 50k locally. You can of course always try other batch correction/data integration methods that are less memory intensive such as BBKNN or scVI. We tested scalability of data integration tools (also BBKNN and ComBat memory use) here: https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2. However, ComBat is one of the least memory intensive methods out there... so maybe there is little room for optimization here...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414:656,optimiz,optimize,656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1431#issuecomment-698818414,3,"['optimiz', 'scalab']","['optimization', 'optimize', 'scalability']"
Performance,"Hey all!; Sorry for the delay, I finally went over the comments of @ivirshup. Thanks again for the feedback! I think I could address everything, except:. - the issue of how exactly we should select HVGs with simple batch correction. @adamgayoso and @gokceneraslan (and maybe @dkobak ?) might have an opinion here as well. See [thread](https://github.com/theislab/scanpy/pull/1715#discussion_r774980182).; - how to best cache raw data to save time while testing. I proposed a solution but not sure if it is a good-style solution, maybe have another look! See [thread](https://github.com/theislab/scanpy/pull/1715/#discussion_r774915501). Btw, I've also posted a tutorial for PRs a while back (https://github.com/theislab/scanpy-tutorials/pull/43) - any comments to that?. Hope you enjoy your Christmas holidays!; Best,; Jan",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467:419,cache,cache,419,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1000800467,1,['cache'],['cache']
Performance,Hey! I looked at multiplex louvain a bit a few years ago (and put it in a grant that didn't get funded in the end ^^)... i guess one of the difficult things to actually using this is tuning the inter layer weight. I reckon this should actually be regarded as a new approach to multi-modal data integration. And it would require quite a bit of parameter tuning to understand how these edge weights need to be tuned. Hence I'm not sure if we just want to add it like this...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504:408,tune,tuned,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-828389504,1,['tune'],['tuned']
Performance,"Hey! 😄 . I'd in principle happy if we move the default `scanpy.settings.cachedir` from `./cache/` to `appdirs.user_cache_dir()`. . However, if then any Scanpy installation breaks, as _the main hpc I'm on 1gb of space where appdirs would put these files_, I would probably not make this the default, but choose something like `~/cache-scanpy/`, that is, a visible directory in home (if we really want, `~/.scanpy/` is also fine). Under https://scanpy.readthedocs.io/en/latest/api/index.html#settings, we could also talk about other alternatives. I second Isaac's concern. Like many others, I'm computing on AWS these days and there, the canonical way of making data locally accessible is via EBS volumes. Hence, I'm used to setting the cachedir to that mount point with a visible name, knowing that this can hold a lot of data. I'd manually clean it if something that I don't use often takes too much space. So, I fear that `appdirs.user_cache_dir()` is not smart enough to figure out locations on a Linux system that hold the size of data that we're typically talking about. It would for sure be the right solution for laptops and work stations etc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843:72,cache,cachedir,72,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476588843,4,['cache'],"['cache', 'cache-scanpy', 'cachedir']"
Performance,"Hey!. Here's the downsample function I wrote to downsample count matrices. Now the function is also loaded via the api. Best,. Malte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/100:100,load,loaded,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/100,1,['load'],['loaded']
Performance,"Hey!; I just updated to latest master branch and I can no longer load scanpy. `import scanpy as sc` gives me the error:; ``` ---------------------------------------------------------------------------; PackageNotFoundError Traceback (most recent call last); <ipython-input-1-0074c9bc0b31> in <module>; ----> 1 import scanpy as sc. ~/new_scanpy/scanpy/scanpy/__init__.py in <module>; 25 __version__ = get_versions()['version']; 26 ; ---> 27 check_versions(); 28 del get_versions, check_versions; 29 . ~/new_scanpy/scanpy/scanpy/utils.py in check_versions(); 38 ; 39 anndata_version = version(""anndata""); ---> 40 umap_version = version(""umap-learn""); 41 ; 42 if anndata_version < LooseVersion('0.6.10'):. ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in version(package); 103 ""Version"" metadata key.; 104 """"""; --> 105 return distribution(package).version; 106 ; 107 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in distribution(package); 84 :return: A ``Distribution`` instance (or subclass thereof).; 85 """"""; ---> 86 return Distribution.from_name(package); 87 ; 88 . ~/anaconda3/envs/sc-tutorial/lib/python3.6/site-packages/importlib_metadata/api.py in from_name(cls, name); 50 return resolved; 51 else:; ---> 52 raise PackageNotFoundError(name); 53 ; 54 @staticmethod. PackageNotFoundError: umap-learn ; ```. I have `umap-learn` 0.3.9 installed. . Scanpy version: 1.4.3+115.g1aecabf; Anndata version: 0.6.22rc1. It seems to work with umap-learn 0.3.8, scanpy 1.4.3+105.gc748b35. and anndata 0.6.22.post1+1.g8dcc3cd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/739:65,load,load,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/739,1,['load'],['load']
Performance,"Hey, sorry for being slow here. upon looking into this again, it is the case that `read_10x_mtx` has to make strong assumptions on the files being generated by Cell Ranger. This is also reflected in the filenames this software outputs. Is there a widely used processing pipeline which does not adhere to this file naming?; If yes, scanpy should indeed be able to deal with this;; If no, custom workflows would actually be more reliably dealt with by using a small custom reading script as suggested by @flying-sheep above:. > Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:; > ; > https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341; > ; > Took me 3 minutes:; > ; > ```python; > samples = []; > for sample in range(1, 10):; > s = read(; > path / f'{sample}.matrix.mtx',; > cache=cache,; > cache_compression=cache_compression,; > ).T; > genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); > s.var_names = genes[0]; > s.var['gene_symbols'] = genes[1].values; > s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; > samples.append(s); > adata = AnnData.concatenate(samples); > ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694:1106,cache,cache,1106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-1759283694,2,['cache'],['cache']
Performance,"Hey, thanks for the description - yes your example dataset would be very helpful - if you could post a small code snippet here which generates this dataset and shows the specific steps you perform that would be great. If you cannot produce the dataset in a script, you could also send a link to the dataset (if its public or synthetic, making sure you're allowed to share) :). In both cases, sending a script here we can run too is a great help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191785768:189,perform,perform,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191785768,1,['perform'],['perform']
Performance,"Heya,. I have been trying to get scanpy loaded and a simple example up and running. . I tried following the "" Clustering 3K PBMCs Following a Seurat Tutorial"" by trying to execute the following code:. ```py; import numpy as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(inde",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734:40,load,loaded,40,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:; ```py; ad = sc.read_h5ad('scdataset.h5ad', backed='r+'); ad2 = sc.read_h5ad('scdataset.h5ad'); ```; and; ```py; random_genes = list(ad.var_names.to_series().sample(100)); ```; this works perfectly:; ```py; sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42); ```; but, this:; ```py; sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42); ```; yields the following error:; ```pytb; -----------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-113-9cb28e089b25> in <module>; ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 90 else:; 91 obs_avg = pd.Series(; ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes; 93 ; 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims); 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims); 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims); --> 951 avg = _divide_by_count(tot, cnt, out=out); 952 ; 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out); 216 else:; 217 if out is None:; --> 218 return a.dtype.type(a / b); 219 else:; 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence.; ```. thanks; Mark",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/883:67,load,load,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883,1,['load'],['load']
Performance,Hi . We have purchased the Nadia dolomite machine which is the automated version of dropseq. I am using the dropseqpipe pipeline for demultiplexing and generate count matrix. Analysis with Seurat is fine but how do you load a count matrix in scanpy?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/366:219,load,load,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366,1,['load'],['load']
Performance,"Hi @ALL,; I want that the object of annData to save the normalized expression matrix that exclude the scaling matrix to perform the pyscenic regulon analysis.but the code adata.to_df().to_csv(EXP_MTX_QC_FNAME) just save the scaling matrix that had the negative number in but just normalized d matrix.so i merely want to export the normlized matrix data and then imported to the pyscenic to analyize. So how can i do for this request?; Any advice would be appreciated.; Best,; hanhuihong",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1650:120,perform,perform,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650,1,['perform'],['perform']
Performance,"Hi @Khalid-Usman,. Regressing out should indeed be performed before highly variable gene selection. This was not in the original scRNA-seq tutorials from Seurat and Scanpy though. If you're interested in a current best-practices tutorial (based on scanpy, but also including R tools), you can find it [here](https://www.github.com/theislab/single-cell-tutorial). The reason it might not have been done on all genes initially is for speed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662:51,perform,performed,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/707#issuecomment-505387662,1,['perform'],['performed']
Performance,"Hi @LuckyMD ,. Thanks so much for getting back to me this quickly. I just want to clarify that I am not running this analysis with the built-in 10x data set, I have followed the tutorial as seen on the link in the report, which says: ""The data consist in 3k PBMCs from a Healthy Donor and are freely available from 10x Genomics"". I have downloaded the file from the following URL, as seen in the tutorial:. http://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz. This is also the same URL found on this link, directly from 10x:. https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k. The 10x summary [here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_web_summary.html) mentions LYZ as one of the most differentially expressed genes, yet it is missed by the sample analysis as performed in the Scanpy tutorial. As both use the exact same count matrix as a source, there are two possibilities here as far as I can see: either the thresholds and filtering parameters in the tutorial are inaccurate and miss important marker genes, or there is a bug that drops these genes. My question is which of the following is true. From your answer I would assume it's the former, in which case maybe a disclaimer pointing this out would be helpful in the tutorial page? I think, as it stands, the average user would assume important marker genes such as LYZ would not be missed by even a rough analysis of a PBMC data set. For reference, the [tutorial](https://satijalab.org/seurat/v3.2/pbmc3k_tutorial.html) which the Scanpy one is apparently based on finds LYZ as a very important contributor to the first principal component.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053:865,perform,performed,865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665580053,1,['perform'],['performed']
Performance,"Hi @LuckyMD - thanks for your reply! Yeah that makes sense. I'm performing these corrections using a subset of highly variable genes, so I guess to ""make up"" for the loss of ""true"" HVGs in the new subclusters of cells I could select a higher number of HVGs to perform the original alignment? As well as maybe using a larger number of components for downstream applications from the low-dimensional embedding outputted by the original alignment. Does that make sense to you?. One more question - when performing differential gene expression analysis, what is your preferred pipeline/method when using aligned datasets? I generally do not perform the correction on the gene expression matrix when aligning, and I think doing DE with corrected matrices is not as common. So maybe other methods that use batch as a covariate would be preferable (e.g. diffxpy or others?) Would really appreciate any suggestions here!. PS. many congratulations on the benchmarking integration paper in Nature Methods - excellent work and very useful resource for the field!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766:64,perform,performing,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2162#issuecomment-1061085766,4,['perform'],"['perform', 'performing']"
Performance,"Hi @LuckyMD ; Thank you for the fast reply. Yes to FastMNN, as I understand from using align_cds – when you specify discretely what you want to remove e.g. sample-sample variation it calls FastMNN from batchelor. Thanks for the recommendation – I will check out Scanorama, been meaning to read the review on integration techniques. . > you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. Ahh okay, I misunderstood the process then – my understanding was that some of the mnn correction would be carried over when performing velocity analysis. I will check out the scvelo forum for info on comparing samples. . Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916:795,perform,performing,795,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-735661916,1,['perform'],['performing']
Performance,"Hi @LuckyMD,. Thank you for your detailed reply! Your explanation of the issue with batch correction in scRNA-seq data is very straightforward.; In kBET paper, the performance of ComBat in simple batch correction scenarios is impressive. (I have once used kBET, but found it very slow.). In addition, with your help, I have solved the problem of using scran's `findMarkers()` function:; ```; tmp_cluster=adata.obs['leiden'].astype(int); ```; ```; %%R -i tmp_cluster -i adata -o tmp_allMarkers; tmp_allMarkers<-scran::findMarkers(adata,clusters=tmp_cluster,block=adata$batch,direction=""up""); tmp_allMarkers<-as.list(tmp_allMarkers); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216:164,perform,performance,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-503083216,1,['perform'],['performance']
Performance,"Hi @Olivia117,. Let's see if I can help. I think there are a few misunderstandings here. It appears that you are mixing the `adata.var['highly_variable']` approach with the `adata.obsm['X_geneset1']` approach Alex suggested. Firstly, there is a typo in Alex' code above. It should read:; ```; adata.obsm['X_geneset1'] = adata[:,['gene1', 'gene2', 'gene3', 'gene4']].X; sc.pp.neighbors(adata, use_rep='X_geneset1'); ```; I believe. Your error is due to this typo. The command is interpreting `'Map7d1'` as a cell index rather than a gene index. However, there are also a few other things.; 1. `adata.var['highly_variable']` takes a boolean list, so you should assign e.g., `[True, True, False, False]` if you are interested in only the first two genes out of a total of 4 genes in the dataset. This can be trivially extended to select your Gene1, Gene,... Gene500 that you are interested in. When using this approach you will need to run `sc.pp.pca(adata, svd_solver='arpack', use_highly_variable=True)` and `sc.pp.neighbors(adata)` before clustering with louvain or leiden. This approach subsets to your genes of interest, then performs PCA on this gene subset, and builds a KNN graph based on Euclidean distances in this PCA space, which is then used for clustering.; 2. If you don't want to use the route via PCA, you need to assign to `adata.obsm` as Alex suggests (with my typo correction above). Even if you do not have anything in `adata.obsm`, it should still work. If you want to put something in `adata.obsm`, just run `sc.pp.pca(adata, svd_solver='arpack')` and you will see `adata.obsm['X_pca']` appear. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089:1128,perform,performs,1128,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/510#issuecomment-487980089,1,['perform'],['performs']
Performance,"Hi @adamgayoso , thanks for the comment, your raised very fair points. I disagree on couple of them but I think it's a very healthy discussion: . > then it does belong in scanpy more formally I think. In that sense, it sets a strange precedent about what belongs inside the main scanpy, versus external. the discussion on whether to include this in `scanpy.external` or `scanpy.core` was carried out here: https://github.com/berenslab/umi-normalization/issues/1 , two key take home messages from that were (imho):; - the simplicity of the method, in terms of codebase, and its scalability makes it suitable to be hosted in `core`.; - it is not strictly a new method, but has several connections with previous [sctransform](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) and [glm-pca](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6) (also, not sure on what basis you said that ""`glm-pca` is supposed to be better"", would be genuinely curious to see some evaluations). > This is just a general comment, but is it a bit rushed to include the analytic pearson residuals method in the main scanpy module given that the method has only been described in a preprint?. I also disagree about peer-review being a gold standard about legitimacy of the method: I find it a bit unusual in light of the ever-lasting discussion of peer-review flaws in academia, and I personally use non-peer-reviewed computational tools all the time. Beside that, I think you raise 2 very important points here (that are possibly flaws on Scanpy side):; - Should there be a transparent and more thorough vetting process about what is added in Scanpy, especially if it's something fundamental like normalization? (imho yes); - Should this process be somehow formalized, e.g. a common issue title like `[new method] My new method` ?. With such a system in place, I think it would have enabled you (and others) to express your disagreement at earlier stage (as you wouldn't poss",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115:577,scalab,scalability,577,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-799276115,1,['scalab'],['scalability']
Performance,"Hi @brianhie,. It's great that you're contributing to Scanpy to make the interoperability even easier (I guess it was already quite good given you built on `AnnData`). We have been evaluating data integration methods and in which Scanorama performed quite well (you may have seen the [preprint](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2)). One aspect that would make it even easier to use the tool that we were missing in the comparison is a small tutorial. The example in the function docstring is already very helpful, but do you think it would be possible to add a quite jupyter notebook in this direction? This is obviously a request outside of this PR. On the topic of the PR, I wonder if `adata.obsm['X_pca_scanorama']` is a good default name for the generated embedding, and not just `adata.obsm['X_scanorama']` as the standard user may not have delved into the methodology as much.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723:240,perform,performed,240,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1332#issuecomment-665592723,1,['perform'],['performed']
Performance,"Hi @fidelram ,. Thanks for the response.; I think either or both would be great. Excuse my ignorance, what's the efficient to interact with scanpy, I am guessing `annData` ? If `annData` I am again guessing that the object can be efficiently made given the CSR/CSC sparse matrix or is there already a support to import other binary matrix formats ?. Currently alevin dumps [EDS](https://github.com/COMBINE-lab/EDS) (a binary matrix format), and I wrote a small Rust library to convert it to other formats (h5, csv, mtx) and found EDS is faster to load and uses less memory, at least in R. We have a support of EDS in R world through Mike Love's awesome `tximport` package. Since scanpy provides great support and efficient implementation of various single-cell analyses for the python world, I'd love to make EDS import and alevin interaction for downstream processing as efficient as possible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764:547,load,load,547,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-538028764,1,['load'],['load']
Performance,"Hi @flying-sheep , I’m using Scanpy on an HPC system, and even though the administrator updated it to the latest version, I'm still encountering the same error. -----; anndata 0.9.2; scanpy 1.10.2; -----; PIL 9.5.0; asciitree NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; bottleneck 1.3.6; cffi 1.15.0; cloudpickle 2.2.1; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.2; dask 2024.5.2; dateutil 2.9.0.post0; debugpy 1.5.1; decorator 4.4.2; defusedxml 0.7.1; dill 0.3.8; dot_parser NA; entrypoints 0.4; executing 0.8.3; fasteners 0.18; google NA; h5py 3.8.0; igraph 0.10.8; ipykernel 6.9.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.1.2; joblib 1.4.0; jupyter_server 1.18.1; kiwisolver 1.4.2; legacy_api_wrap NA; leidenalg 0.10.1; llvmlite 0.42.0; louvain 0.8.2; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.6.0; mpl_toolkits NA; msgpack 1.0.5; natsort 8.4.0; numba 0.59.0; numcodecs 0.12.1; numexpr 2.8.4; numpy 1.23.5; packaging 21.3; pandas 2.1.0; parso 0.8.3; patsy 0.5.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.23.0; prompt_toolkit 3.0.20; psutil 5.9.1; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 16.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydot 1.4.2; pygments 2.16.1; pynvml NA; pyparsing 3.0.9; pytz 2022.1; ruamel NA; scipy 1.11.2; seaborn 0.13.2; session_info 1.0.0; setuptools 61.2.0; six 1.16.0; sklearn 1.3.2; sphinxcontrib NA; stack_data 0.2.0; statsmodels 0.14.0; tblib 2.0.0; texttable 1.6.7; threadpoolctl 2.2.0; tlz 0.12.2; toolz 0.11.2; torch 2.2.0+cu121; torchgen NA; tornado 6.1; tqdm 4.63.0; traitlets 5.1.1; typing_extensions NA; wcwidth 0.2.5; xxhash NA; yaml 6.0; zarr 2.15.0; zipp NA; zmq 22.3.0; zoneinfo NA; zope NA; -----; IPython 8.4.0; jupyter_client 7.1.2; jupyter_core 4.10.0; jupyterlab 3.4.4; notebook 6.4.12; -----; Python 3.9.12 (main, Apr 5 2022, 06:56:58) [GCC 7.5.0]; Linux-3.10.0-1160.99.1.e",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344:278,bottleneck,bottleneck,278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3215#issuecomment-2330378344,1,['bottleneck'],['bottleneck']
Performance,"Hi @flying-sheep @ilan-gold ,; Based on our previous discussion, we observed that applying and then removing a patch while fixing the seed causes the t-SNE output to change. In our experiment, we used 1.3 million data points to run t-SNE and compared the results of the patched and unpatched versions by examining the KL Divergence from both runs. The results are summarized in the table below. . In the above code use **USE_FIRST_N_CELLS** to set number of records and use sc.tl.tsne(adata, n_pcs=tsne_n_pcs, **use_fast_tsne=False**) to run optimized run with latest commit. You can get KL divergence numbers by logging [kl_divergence_](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). ![image](https://github.com/scverse/scanpy/assets/1059402/ffef81b0-b0bf-461e-8ad3-b7ce9ba4c361)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265:542,optimiz,optimized,542,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2122306265,1,['optimiz'],['optimized']
Performance,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:; ```; adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var); adata_raw.write(my_file); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938:620,load,loaded,620,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499111938,1,['load'],['loaded']
Performance,"Hi @ilan-gold,. Regarding your thought in this [comment](https://github.com/scverse/scanpy/pull/3061#issuecomment-2134651481), we can enable or disable Intel optimization from outside the code. However, users might not be aware of how to use this feature. Instead, if we add it to scanpy directly, all scanpy users will know the same option available. If we agree with the option discussed in this [comment](https://github.com/scverse/scanpy/pull/3061#issuecomment-2114783668 ), I can proceed with updating the t-SNE file.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2136745993:158,optimiz,optimization,158,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2136745993,1,['optimiz'],['optimization']
Performance,"Hi @k3yavi, I would be great to have a tutorial in which an alevin generated matrix is loaded into scanpy. Your suggestion is to have such a tutorial hosted by scanpy or you plan to add it to your list of tutorials?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856#issuecomment-537518323:87,load,loaded,87,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856#issuecomment-537518323,1,['load'],['loaded']
Performance,"Hi @o0stsou0o ,; Could it be that you have `igraph` loaded somewhere while you were uninstalling? Not sure why you can't remove `igraph` otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/807#issuecomment-640070940:52,load,loaded,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807#issuecomment-640070940,1,['load'],['loaded']
Performance,"Hi @preetida,. I think this question is more directed towards the `single-cell-tutorial` github [here](github.com/theislab/single-cell-tutorial). I assume that's where you got the above sentence from. In case you haven't done so already, you can check out the accompanying paper with that tutorial [here](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746). In general whatever you store in `adata.raw` is what is used when you set `use_raw=True`. In that tutorial I have stored log-normalized data in `adata.raw.X` and I store log-normalized and batch corrected data in `adata.X`. Thus, you are plotting two different versions of the data when you set `use_raw` differently. In general, if you set up your `adata.raw` as I did in the tutorial, it is advisable to plot with `use_raw=False`, but when you perform a DE test, you shouldn't use the corrected data stored in `adata.X`, so the default is `use_raw=True`. I hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245:811,perform,perform,811,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1266#issuecomment-639506245,1,['perform'],['perform']
Performance,"Hi @r-reeves,; Maybe this is indeed a separate issue. `mnnpy` is indeed working on the gene expression matrix, and not on a low dimensional embedding like `FastMNN` (which is what I assume you might have been using?). You could try [Scanorama](https://github.com/brianhie/scanorama) which is a method similar to FastMNN, using a sped up algorithm and no iterative merging of batches, but a method they call ""panoramic stitching"". It has performed quite well in our [benchmark of data integration methods](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2), and is in the scanpy ecosystem and therefore should work seamlessly in a Scanpy workflow. All of this being said, you will only get an integrated graph structure with this for scvelo, which may help a little, but won't remove the batch effect for RNA velocity calculation. scvelo doesn't currently have any batch removal in its pipeline as it is quite difficult to add as it works directly from the normalized count data and fits a model to these. @VolkerBergen has been thinking a bit about how to perform batch correction in an scvelo model, maybe he could chime in, or you could post an issue in the scvelo repo.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157:437,perform,performed,437,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1289#issuecomment-734426157,2,['perform'],"['perform', 'performed']"
Performance,"Hi Alex,; I managed to get the log working by using your function to convert to AnnData rather than mine. (adata = sc.AnnData(x)). However, coloring the plots still does not work. I get the following error.; TypeError: object of type 'numpy.int64' has no len(). You can reproduce the error by the following; ### Load Data; x = pd.read_csv('Trial_data.csv', delimiter=',', index_col=0); ### Drop DAPI; x = x.drop(list(x.filter(regex='DAPI.', axis=1)), axis=1); ### Convert to AnnData; adata = sc.AnnData(x); ### Filter cells; sc.pp.filter_cells(adata, min_genes=1); sc.pp.filter_genes(adata, min_cells=1); adata.obs['n_counts'] = adata.X.sum(axis=1); ### Normalize data; sc.pp.log1p(adata); ### PCA; sc.tl.pca(adata, svd_solver='arpack'); sc.pl.pca(adata); sc.pl.pca(adata, color='CD3D'). I also tried it on a different dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-456461004:312,Load,Load,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-456461004,1,['Load'],['Load']
Performance,"Hi Dan, . When you perform the umap calculation using sc.tl.umap, the default matrix used is adata.obsm['X_pca']. Given this, you wouldn't expect the same embedding the way you've done it. if instead you did this. `mapper = umap.UMAP().fit(adata.obsm['X_pca'])` . you'd likely find a very similar embedding to the ones you've shown scanpy producing. As such, I'm guessing there is problem with how you've preprocessed the data, such that the PCA space is not behaving as expected. . why don't you attempt running this notebook "" wget https://github.com/scverse/scanpy-tutorials/raw/master/pbmc3k.ipynb"" with your current installation, and let us know if you can reproduce the tutorial. Then I would suggest adding your data, changing else, and reporting back.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721:19,perform,perform,19,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2386#issuecomment-1364246721,1,['perform'],['perform']
Performance,"Hi I had this problem as well with 1.6.0 it was triggered by scanpy's test code. ```; scanpy.api (unittest.loader._FailedTest) ... ERROR. ======================================================================; ERROR: scanpy.api (unittest.loader._FailedTest); ----------------------------------------------------------------------; ImportError: Failed to import test module: scanpy.api; Traceback (most recent call last):; File ""/usr/lib/python3.9/unittest/loader.py"", line 470, in _find_test_path; package = self._get_module_from_name(name); File ""/usr/lib/python3.9/unittest/loader.py"", line 377, in _get_module_from_name; __import__(name); File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/__init__.py"", line 27, in <module>; from . import pl; File ""/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/api/pl.py"", line 1, in <module>; from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; ImportError: cannot import name 'stacked_violin' from 'scanpy.plotting._anndata' (/<<PKGBUILDDIR>>/.pybuild/cpython3_3.9_scanpy/build/scanpy/plotting/_anndata.py). ----------------------------------------------------------------------; Ran 1 test in 0.000s. ```. I ended up with this patch to get the tests to run successfully.; ```; --- a/scanpy/api/pl.py; +++ b/scanpy/api/pl.py; @@ -1,4 +1,7 @@; -from ..plotting._anndata import scatter, violin, ranking, clustermap, stacked_violin, heatmap, dotplot, matrixplot, tracksplot; +from ..plotting._anndata import scatter, violin, ranking, clustermap, heatmap, tracksplot; +from ..plotting._stacked_violin import stacked_violin; +from ..plotting._dotplot import dotplot; +from ..plotting._matrixplot import matrixplot; ; from ..plotting._preprocessing import filter_genes_dispersion, highly_variable_genes; ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952:107,load,loader,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1397#issuecomment-765003952,4,['load'],['loader']
Performance,"Hi James!. Thank you for the remark! And you're right... several repetitions of the following are consistent:; ![image](https://user-images.githubusercontent.com/16916678/42413998-8527ef8e-822c-11e8-9e45-8aa30f5bb9a7.png). I'm aware of numpy's inplace functionality, and we've discussed it even for log1p... I don't know why we missed using it... Also, I wouldn't have thought that the speedup would be so dramatic, but of course, already for better memory efficiency we should have done it. I'll go through the rest of the toolkit and see whether there is another such striking omission... Regarding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. Thi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:857,bottleneck,bottleneck,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['bottleneck'],['bottleneck']
Performance,"Hi Jorvis! This should be very easy. Use the text file reader:; ```; adata = sc.read_text(filename).transpose(); ```; or use the general purpose reader that writes cache files automatically; ```; adata = sc.read(filename, ext='txt').transpose() # 'tab', 'data', 'tsv' mean the same; ```; see the [API docs](https://scanpy.readthedocs.io/en/latest/api/index.html). The 'tsv' file ending is not yet in the latest release, I just commited that: https://github.com/theislab/scanpy/commit/884c5f8a6a39c43aef27c7398ec9c195b977a3d3. Hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056:164,cache,cache,164,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/65#issuecomment-356956056,1,['cache'],['cache']
Performance,"Hi all! I wanted to make you aware of a caching extension for scanpy and scvelo that @michalk8 and myself have developed called [scachepy](https://github.com/theislab/scachepy) and to kick off a discussion about caching in scanpy. From my point of view, there are currently two main ways to cache your results in scanpy, please correct me if I'm wrong:; - write the AnnData object; - manually write the attributes, e.g. adata.X to file, e.g. pickle. The idea of scachepy is to offer the possibility to cache all fields of an AnnData object associated with a certain function call, e.g. `sc.pp.pca`. It allows you to globally define a caching directory and a backend (default is pickle) that the cached objects will be written to. In the case of PCA, this would amount to calling. ```python; import scachepy; c = scachepy.Cache(<directory>) ; c.pp.pca(adata); ```; where `c.pp.pca` wraps around `sc.pp.pca` but takes additional caching arguments like `force`. So in short, our aim with scachepy is to....; - ...have a flexible and easy to use way to cache variables associated with scanpy/scvelo function calls.; - ... speed up individual steps in a scanpy/scvelo analysis by caching them, without having to save the entire AnnData object; - ... be able to share jupyter notebooks with someone else who can run them on a different machine, possibly on a different OS and yet get the exactly the same results because the critical computations are cached. @michalk8 is the main developer and will be able to tell you much more about it. I would appreciate any input, and would love to discuss caching in scanpy/scvelo.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/947:291,cache,cache,291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/947,6,"['Cache', 'cache']","['Cache', 'cache', 'cached']"
Performance,"Hi all,. I am analysing some 10x samples generated in my lab and I noticed that there could be some problems with your `read_10x_mtx` function. So, I opened 4 times the `pbmc3k` dataset used in your tutorial by using the following lines of code:. `adata = sc.read_10x_mtx(sample, var_names='gene_symbols', cache=True)`; `adata.var_names_make_unique()`; `sc.pp.filter_cells(adata, min_genes=200)`; `sc.pp.filter_genes(adata, min_cells=3)`; `mito_genes = adata.var_names.str.startswith('MT-')`; `adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1`; `adata.obs['n_counts'] = adata.X.sum(axis=1).A1`. I generated the violin plots showing the `n_genes`, `n_counts`, and `percent_mito` for each test and they are different in each test (see attached picture). ![Tests](https://user-images.githubusercontent.com/38785099/72088802-d7b2ec80-3302-11ea-91cd-db9d95698c00.gif). Do you have any suggestions to solve this problem?. Thank you in advance,; Simone",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/977:306,cache,cache,306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977,1,['cache'],['cache']
Performance,"Hi all,. Right now we have two layers in the scanpy API. The top layer consists of the major modules like `pp,pl,tl` as well as the smaller ones like `queries,get,datasets`. In addition, we have some useful functions directly under the scanpy package like `read/read_text/read_mtx` etc. It is obvious that the field is advancing and alternative/better ways to perform fundamental tasks in downstream analysis (e.g. normalization, DE tests, gene selection) are emerging and will continue to emerge. Consequently, this necessitates an expansion of the scanpy API. However, I argue that having flat top-level modules makes it difficult to extend scanpy, while maintaining a reasonable API. . Right now there are two ways to introduce new functionality (assuming that it's not something completely unrelated). 1) add a new flavor/method to an existing function (e.g. `sc.pp.highly_variable_genes`, `sc.tl.rank_genes_groups`) or . 2) add a new function with a shared prefix e.g. `sc.pp.neighbors_tsne` (see https://github.com/theislab/scanpy/pull/1561) or `sc.pp.normalize_pearson_residuals` (see https://github.com/berenslab/umi-normalization/issues/1) or `sc.pp.normalize_pearson_residuals_pca()` (see #1715 ). . Since option 1 is more complicated in terms of managing the arguments (esp. method-specific ones), I believe we tend to switch to option 2 now. But given that we already have many functions with common prefixes and that shifting towards option 2 will likely introduce more functions with long underscored names, top layers will get even flatter and wider. Therefore, I think it's time to consider a third option which is to add another layer which makes the API a tiny bit more hierarchical. Some examples I can think of are:. ```java; sc.read.{adata,csv,text,mtx,excel,loom,h5_10x,mtx_10x,...}; sc.pp.neighbors.{umap,gauss,rapids,tsne}; sc.pp.hvg.{seurat,seurat_v3,dispersion}; sc.pp.norm.{tpm,pearson}; sc.pp.filter.{genes,cells,rank_genes,...}; sc.tl.rank_genes.{logreg,wilcoxon,ttest}; s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1739:360,perform,perform,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1739,1,['perform'],['perform']
Performance,"Hi all,. Sorry I sent a PR(https://github.com/theislab/scanpy/pull/1271) without reading any of these, it's my bad. Some thoughts are as follows:. - I think it's fairly straightforward to check for R dependencies in runtime, please see the PR for more info. - For Travis, I used Ubuntu packages for base R installation and then rest of the R deps are installed by the Travis user in home directory, which is cached. apt-install R installation takes around a minute. This is really hard to reduce, I think. . - After the caching, the installation of sctransform itself take around 15-20sec. This can even be reduced to zero if I check whether it's already installed. See https://travis-ci.org/github/theislab/scanpy/jobs/697070834 for a better breakdown. You can compare this with an existing test run e.g. https://travis-ci.org/github/theislab/scanpy/jobs/696758553. - sctransform test overhead is around 30sec, which can also be reduced. Overall, it adds 4 minutes to the travis test time. I don't know exactly where the remaining difference comes from. - However, if we keep adding more Ubuntu and/or R packages in the scanpy travis, it can get a bit bloated. Even if things are cached, for some reason, there is a 45-50 second cache upload overhead which is not negligible.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553:408,cache,cached,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1068#issuecomment-642835553,3,['cache'],"['cache', 'cached']"
Performance,"Hi everyone,; at the moment, pie charts for paga are a bit brittle, see https://github.com/theislab/cellrank/issues/25.; This pull request is an attempt to fix it. Rather than using `ax.pie`, it's just a bunch of scatterplots with custom markers.; I've tested the performance; ```python; foo = {i: {c.to_hex(cm.viridis(_)): 0.001 for _ in range(255)} for i in range(8)}; sc.pl.paga(adata, color=foo, colorbar=False); ```; ![currect](https://user-images.githubusercontent.com/46717574/77180766-ad339b80-6aca-11ea-9a85-617ad122d140.png). Performancewise, it takes about ~14 seconds to produce the plot with the proposed changes,; ~4s, but I consider that the worst-case scenario.; More importantly, current version doesn't produce a correct plot, see below:; ![buggy](https://user-images.githubusercontent.com/46717574/77180621-7f4e5700-6aca-11ea-8c78-25fbba8f7c98.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1123:264,perform,performance,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1123,2,"['Perform', 'perform']","['Performancewise', 'performance']"
Performance,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together.; But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice?; Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-486916420:170,cache,cache,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-486916420,1,['cache'],['cache']
Performance,Hi negative loadings are also important genes as you can see on the example of `LTB` and `HLA-DRA`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-527409146:12,load,loadings,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527409146,1,['load'],['loadings']
Performance,"Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it.; I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/856:1308,load,loading,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856,1,['load'],['loading']
Performance,"Hi there! Thanks for adding the ingest method to scanpy!; I was wondering what would be the suggested approach when the reference data contains batch effects that should be removed before actually performing the asymmetric integration with a query dataset. I tried to use the neighbors structure returned by BBKNN (which correctly adjusts for batch effects), but the 'metric' object is missing. Here the error:. ```; KeyError Traceback (most recent call last); <ipython-input-22-a805d117788e> in <module>; ----> 1 sc.tl.ingest(adata, adata_ref, obs='time', embedding_method='umap'). /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in ingest(adata, adata_ref, obs, embedding_method, labeling_method, inplace, **kwargs); 115 labeling_method = labeling_method * len(obs); 116 ; --> 117 ing = Ingest(adata_ref); 118 ing.fit(adata); 119 . /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in __init__(self, adata); 268 ; 269 if 'neighbors' in adata.uns:; --> 270 self._init_neighbors(adata); 271 ; 272 if 'X_umap' in adata.obsm:. /opt/conda/lib/python3.7/site-packages/scanpy/tools/_ingest.py in _init_neighbors(self, adata); 229 else:; 230 dist_args = (); --> 231 dist_func = named_distances[adata.uns['neighbors']['params']['metric']]; 232 self._random_init, self._tree_init = make_initialisations(dist_func, dist_args); 233 self._search = make_initialized_nnd_search(dist_func, dist_args). KeyError: 'metric'. ```; I'm running scanpy version 1.4.5.post2. Any help would be highly appreciated!! Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1108:197,perform,performing,197,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1108,1,['perform'],['performing']
Performance,"Hi there, I am having the same issue as above. I have tried the fix that @Xparx has provided but it yields more problems. See the below error which I am now receiving:. ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: float() argument must be a string or a number, not 'csc_matrix'. The above exception was the direct cause of the following exception:. ValueError Traceback (most recent call last); <ipython-input-48-abf5bf78cb77> in <module>; ----> 1 sc.pl.dpt_timeseries(adata_HVG). ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_tools/__init__.py in dpt_timeseries(adata, color_map, show, save, as_heatmap); 159 if as_heatmap:; 160 # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d; --> 161 timeseries_as_heatmap(; 162 adata.X[adata.obs['dpt_order_indices'].values],; 163 var_names=adata.var_names,. ~/.conda/envs/python3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in timeseries_as_heatmap(X, var_names, highlights_x, color_map); 197 _, ax = pl.subplots(figsize=(1.5 * 4, 2 * 4)); 198 ax.imshow(; --> 199 np.array(X, dtype=np.float_),; 200 aspect='auto',; 201 interpolation='nearest',. ValueError: setting an array element with a sequence.; ```. I thought that this might be something to do with the fact that the `np.ones` object is a numpy array instead of a pandas series so I tried substituting this with the line `adata.uns['dpt_changepoints'] = pd.Series(np.ones(adata.obs['dpt_order_indices'].shape[0] - 1))` instead, but this still yielded the same error. Thanks in advance!. Update: I just tried to run this command having used `branching=1' in my analysis and not performing the above correction (even though I know it's inappropriate for my particular system, branching=0 is what I want to use) and it still yielded the same error. As such I think perhaps this could be something independent of the above issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140:1717,perform,performing,1717,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/409#issuecomment-719627140,1,['perform'],['performing']
Performance,"Hi to all, thanks for your interest in glmpca. I have been thinking of doing a python package now that the R package is finished and it would be an honor to have it included in scanpy. Can you give me a sense of how urgently you would need the package (ie what is the typical release cycle)? Also let me note a few caveats about the method:; * It does not handle zero inflation (which ZINB-WAVE does). However, we argue in our paper that despite large numbers of zeros, UMI data are not zero-inflated. We do not make any claim about the appropriateness of the glmpca model for non-UMI data (eg Smart-Seq read counts), which may actually be zero-inflated, although you could certainly run it with eg the negative binomial likelihood.; * glmpca is an alternative to PCA but not necessarily a replacement to PCA. For example, it is at least 10x slower than PCA and we are still working on the big data implementation for sparse matrices (in other words, we assume you can load the data matrix in dense form, which can be limiting).; * We describe a fast approximation to GLM-PCA in the paper which involves transforming raw counts to either Pearson or deviance residuals from a null model then applying standard PCA to that. This approach is just as fast as PCA as long as the null model can be computed in closed-form, which is what we have implemented here: https://github.com/willtownes/scrna2019/blob/master/util/functions.R#L164 . The idea is similar to the sctransform approach used by seurat, but the computation is simpler and faster.; * We also provide a deviance-based gene filtering method which is an alternative to using highly variable genes. This and the residuals functions will be available as an R package on bioconductor. I look forward to collaborating with you all to help make these methods available to a wider community!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230:969,load,load,969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540672230,1,['load'],['load']
Performance,"Hi! Sorry for the very long delay in replying. Indeed, the version in the code doesn't do tie correction. A while ago when originally implementing some changes to this function, we tried using the `scipy.stats.mannwhitneyu`method, but it was significantly slower so we kept the current version instead. If there is a way to improve the performance of scipy version, it might be worth trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/698#issuecomment-528512005:336,perform,performance,336,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/698#issuecomment-528512005,1,['perform'],['performance']
Performance,"Hi! Thanks for the answer. Installing and importing h5py helped. I think I got scanpy to run. However, I am stuck again at reading the .mtx file; ; Since I am new to scanpy I am just following your tutorial. I run the following comand and get the subsequent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:519,cache,cache,519,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,4,['cache'],['cache']
Performance,"Hi! That function is for reading the files output by [cellranger’s mex option](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/matrices). Your files have been renamed by someone in a way we can’t predict, and you should just adapt the little code needed to read them yourself:. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/readwrite.py#L324-L341. Took me 3 minutes:. ```py; samples = []; for sample in range(1, 10):; s = read(; path / f'{sample}.matrix.mtx',; cache=cache,; cache_compression=cache_compression,; ).T; genes = pd.read_csv(path / f'{sample}.genes.tsv', header=None, sep='\t'); s.var_names = genes[0]; s.var['gene_symbols'] = genes[1].values; s.obs_names = pd.read_csv(path / f'{sample}.barcodes.tsv', header=None)[0]; samples.append(s); adata = AnnData.concatenate(samples); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846:548,cache,cache,548,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-545433846,2,['cache'],['cache']
Performance,"Hi!. I recently upgraded to scanpy 1.4 and did not encounter this issue in previous versions. I am trying to generate a heatmap using the following function:; sc.pl.rank_genes_groups_heatmap(adata, n_genes=4, use_raw=True, swap_axes=True). For some reason I am getting white margins on the right and left side which results in misalignment of the colormap identifying my clusters on the bottom and the above heatmap. ![image](https://user-images.githubusercontent.com/7358001/56063815-58c26080-5d3e-11e9-8935-258760c1b0eb.png). I get the same result if I just use sc.pl.heatmap and do a groupby with the clusters. . Any idea how to fix this issue would be most appreciated!. Thanks!!!. Eva. scanpy==1.4 anndata==0.6.19 numpy==1.15.4 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 ; matplotlib == 2.2.3. INSTALLED VERSIONS; ------------------; commit: None; python: 3.7.0.final.0; python-bits: 64; OS: Windows; OS-release: 8.1; machine: AMD64; processor: Intel64 Family 6 Model 69 Stepping 1, GenuineIntel; byteorder: little; LC_ALL: None; LANG: None; LOCALE: None.None. pandas: 0.23.4; pytest: 3.8.0; pip: 19.0.3; setuptools: 40.2.0; Cython: 0.28.5; numpy: 1.15.4; scipy: 1.1.0; pyarrow: None; xarray: None; IPython: 6.5.0; sphinx: 1.7.9; patsy: 0.5.0; dateutil: 2.7.3; pytz: 2018.5; blosc: None; bottleneck: 1.2.1; tables: 3.4.4; numexpr: 2.6.8; feather: None; matplotlib: 2.2.3; openpyxl: 2.5.6; xlrd: 1.1.0; xlwt: 1.3.0; xlsxwriter: 1.1.0; lxml: 4.2.5; bs4: 4.6.3; html5lib: 1.0.1; sqlalchemy: 1.2.11; pymysql: None; psycopg2: None; jinja2: 2.10; s3fs: None; fastparquet: None; pandas_gbq: None; pandas_datareader: None",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/606:1354,bottleneck,bottleneck,1354,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606,1,['bottleneck'],['bottleneck']
Performance,"Hi!; As someone else posted on [stackoverflow](https://stackoverflow.com/questions/54366505/importerror-dll-load-failed-while-file-is-in-working-directory/54441575#54441575), there seem to be problems with the tables dependencies for windows users resulting in the following error when importing scanpy:. ```pytb; >>> import scanpy; ...; File ""C:\Miniconda3\envs\py36\lib\site-packages\scanpy\readwrite.py"", line 9, in; import tables; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables__init__.py"", line 131, in; from .file import File, open_file, copy_file; File ""C:\Miniconda3\envs\py36\lib\site-packages\tables\file.py"", line 35, in; from . import hdf5extension; ImportError: DLL load failed: The specified procedure could not be found.; ```. I've also posted an answer suggestion there. Maybe you could require h5py to have a fixed older version like 2.8 to avoid this problem for other windows users. Downgrading to that version worked for me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/454:108,load,load-failed-while-file-is-in-working-directory,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/454,2,['load'],"['load', 'load-failed-while-file-is-in-working-directory']"
Performance,"Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using ‘cellranger’ software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In ‘Seurat’, I can read the data from my three treatments separated, do quality control, and then integrate them using ‘FindIntegrationAnchors’ and ‘IntegrateData’ functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in ‘Scanpy’ to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that?. In ‘Scanpy’ I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type?. Thank you very much!. Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:156,perform,perform,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,5,['perform'],['perform']
Performance,"Hi, . I'm using your package tl.diffmap in my analysis, and I'm having some difficulties. I have a dataframe I have converted into an anndata object adata. I run the following lines to prepare it for tl.diffmap:. `pp.pca(adata,n_comps=50)`; `pp.neighbors(adata, knn = False, method = 'gauss', n_neighbors = 20)`. I then perform the diffmap:. `tl.diffmap(adata, n_comps = 3)`. and I get the following error:. ![Screen Shot 2019-05-15 at 6 11 47 PM](https://user-images.githubusercontent.com/43049941/58586913-25725d00-822a-11e9-930a-9165efdf60f9.png). I am not sure what I am doing incorrectly here, and I was hoping you could help!. Furthermore, I was wondering why n_comps must be greater than 2?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/668:320,perform,perform,320,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/668,1,['perform'],['perform']
Performance,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,; Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:; > ; > Hi @hejing3283,; > ; > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X.; > ; > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:; > ; > adata_raw.write(my_file); > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695:779,load,loaded,779,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-499126695,1,['load'],['loaded']
Performance,"Hi, ; I'm running Scanpy through Conda on Windows.; I have an issue when I try to import a dataset and set cache = TRUE. ```pytb; ... writing an h5ad cache file to speedup reading next time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:107,cache,cache,107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,7,['cache'],['cache']
Performance,"Hi, @ivirshup . Thanks for the review. I'll address the comments soon.; No, this currently doesn't deal with correctness, tie correction and the other things in the issues, just general structure. But i'll definitely turn to them after this. About the reference thing. Yes, you are right, i can use this approach of course. However, i didn't want to store the same value multiple times, it doesn't make much difference in performance, but still gives uneasy feelings, i would say. upd: oh, i see that `numpy.broadcast_to` creates a view, so it should avoid the problem of storing the same value multiple times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197:422,perform,performance,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1156#issuecomment-614656197,1,['perform'],['performance']
Performance,"Hi, Alex,. Many thanks for your quick reply. I just saw your reply as it is almost 10PM in Singapore now. It is understandable to perform quality control, in-cell normalization and to extract the highly variable genes for ordering. I got your point. For your reply about qPCR, do we need a log normalization? I think a log transform is only required for RNA-Seq data to get a non-skewed normal distribution. As for qPCR data, the delta_Ct value is actually already in a log scale. In the example you have mentioned, there is no call of sc.pp.log1p, either. Instead, we just read the data by ; `adata = sc.read(filename, sheet='dCt_values.txt', backup_url=backup_url)`; and no more processing is applied. As can be found from the original paper, the so-called dCt_value is just defined as HK_Ct - Ct, where HK_Ct is the mean Ct of 4 housing keeping genes on a cell-wise basis. . Besides, in many cases, there may be no UMI data available. In such a case, the normalization per cell for RNA-Seq is actually to compute the FPKM/TPM to compensate for the sequencing depth, right? Usually, the RNA-Seq data in FPKM form is already provided in publications. And then we work on this data to find the highly variable genes. (Just personal understanding. I am new to this field from mechatronics engineering.). Anyway, thanks again for your help. I noticed that there are no examples for pseudo-time ordering with RNA-Seq data. Maybe I can provide one in the near future, as I am working on gene network modeling based on the pseudo-time information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646:130,perform,perform,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/26#issuecomment-312650646,1,['perform'],['perform']
Performance,"Hi, I found that using the sc.tl.rank_genes_groups to perform differential gene expression analysis return the following error. ---; ```python; adata = sc.datasets.pbmc68k_reduced(); sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'); ```. ```pytb; ranking genes; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-92-a8f4e965724c> in <module>; 1 adata = sc.datasets.pbmc68k_reduced(); ----> 2 sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon'). /mnt/data4/weixu/miniconda3_R_4.0/envs/celloracle_env/lib/python3.6/site-packages/scanpy/tools/_rank_genes_groups.py in rank_genes_groups(adata, groupby, use_raw, groups, reference, n_genes, rankby_abs, pts, key_added, copy, method, corr_method, tie_correct, layer, **kwds); 607 for col in test_obj.stats.columns.levels[0]:; 608 adata.uns[key_added][col] = test_obj.stats[col].to_records(; --> 609 index=False, column_dtypes=dtypes[col]; 610 ); 611 . TypeError: to_records() got an unexpected keyword argument 'column_dtypes'; ```; I was wondering that its associate with my pandas version? or other issues?; my pandas version 0.23.4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1478:54,perform,perform,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1478,1,['perform'],['perform']
Performance,"Hi, I have fixed the issue.; It appears that adding, subtracting or dividing numpy.ndarrays with scipy.sparse matrices returns a numpy.matrix. numpy_array /= scipy_sparse_matrix, This command changed the type of numpy_array to numpy.matrix which caused downstream problems. So, you have to transfer the matrix to sparse format again for downstream analysis.; I used the command 'adata.X = scipy.sparse.csr_matrix(adata.X) ' after dividing the measured counts by the size factor.; So, I paste it here as a note of warning when performing this type of operation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456#issuecomment-459623293:526,perform,performing,526,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456#issuecomment-459623293,1,['perform'],['performing']
Performance,"Hi, I tried the snippet, with fastICA and picard, and with a number of cells higher than 30,000, the whitening step cannot be completed. This seems be due to some Lapack limitations. ; `ValueError: Too large work array required -- computation cannot be performed with standard 32-bit LAPACK.`; I don't know how to get around this.... ; Best, ; Chloé",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-540475625:253,perform,performed,253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-540475625,1,['perform'],['performed']
Performance,"Hi, Isaac,. Thank you for your reply. The matrix that I load is the output of; cellranger, we use 10X generate the library. Follow your; recommended tutorial, I can't small size it. Do you have any other; suggestions? The code is: adata = sc.read_10x_mtx(; 'D:/.../.../filtered_feature_bc_matrix/', var_names='gene_symbols',; cache=True) . Thank you so much. Best regards,. Shangyu. Isaac Virshup <notifications@github.com> 于2020年6月5日周五 上午2:31写道：. > The idea behind a self contained example is to give me something that I; > can run on my machine. Ideally you'd be able to put something together with; > randomly generated data that still gave this error. If that's difficult,; > you could keep removing elements from your data until you find the minimal; > object that can reproduce this. Here is a good blog post on how to do this; > <https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports>.; >; > Right now, I'm unable to reproduce the error you're seeing. Do you think; > you could try and create an example you could share with me? This could; > even be sharing your data as an h5ad file.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/1259#issuecomment-639309900>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALYYCBLMIG7FAT7MMJIDC2DRVCNM3ANCNFSM4NOZJRCQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437:56,load,load,56,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1259#issuecomment-640293437,2,"['cache', 'load']","['cache', 'load']"
Performance,"Hi, just to confirm that I tried the new PAGA functions a while ago and the results look very good. (sorry for the delay of the response. I meant to respond to the thread much earlier but got busy doing other stuff.). Now I'm wondering about how to interpret the graph connectivities. An undirected graph does not imply whether two connected clusters are sequential (e.g. progenitors -> newborn neurons -> mature neurons) or on different branches but highly correlated (e.g. neuron subtype 1 vs. neuron subtype 2). . Do you think it's possible to use RNA velocity (http://velocyto.org/) to perform quantitative interference on the directionality of the edges? I have the velocity data but not sure how to mathematically infer edge directions. Maybe I should open a new issue on this or approach you via email? Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/96#issuecomment-393690042:590,perform,perform,590,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/96#issuecomment-393690042,1,['perform'],['perform']
Performance,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway.; There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each?. As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134:662,perform,perform,662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629#issuecomment-781323134,1,['perform'],['perform']
Performance,"Hi, thanks for your ideas and discussion. For me, I think doing scaling is necessary because if the data is not centred to 0, the plane we find based on the covariance matrix may not be the optimized one. The PCA optimization process only works for data with 0 centered I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103829861:190,optimiz,optimized,190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2164#issuecomment-1103829861,2,['optimiz'],"['optimization', 'optimized']"
Performance,"Hi, thanks for your interest in scanpy!. I’ll try to comment on your observations here with your code example:. ```; import scanpy as sc; import numpy as np; ### Loading and preprocessing data; adata = sc.datasets.pbmc3k_processed(). ### Defining scale function; def mean_var(X, axis=0):; mean = np.mean(X, axis=axis, dtype=np.float64); mean_sq = np.multiply(X, X).mean(axis=axis, dtype=np.float64); var = mean_sq - mean**2; # enforce R convention (unbiased estimator) for variance; var *= X.shape[axis] / (X.shape[axis] - 1); return mean, var; ```. As a first note of caution, in your code your function actually modifies the original data matrix, of the scanpy object - which is used again later in the snippet.; → We should create a copy of `X`. Else the code overwrites this object, and ends up comparing an object with itself, while simply using two names for it (this caused your `==` comparisons to evaluate as `True`, but is not what you intend to test).; ```; def my_scale_function(X, clip=False):; # need to make a copy of X; Y = X.copy(); mean, var = mean_var(Y, axis=0); Y -= mean; std = np.sqrt(var); #std[std == 0] = 1; Y /= std; if clip:; Y = np.clip(X, -10, 10); return np.matrix(Y); ```. As a second note of caution, floating point numbers should not be compared with the `==` operator (see for example [here](https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/)). → A more common way would be to use e.g. `np.allclose()` for this purpose. ```; ### Scanpy scale vs my_scale_function. print(""Rescaled with my_scale_function:""); mtx_rescaled = my_scale_function(adata.X). print(""Do a numpy check for closeness of floats:""); print(np.allclose(adata.X, mtx_rescaled)); ```. ```; Do a numpy check for closeness of floats:; False; ```. You can see that this test actually fails. This is because not all genes appear scaled, and your function now actually is doing that.; ```; adata.X.var(0); ```. ```; array([0.9996213 , 0.97964925, 0.29805112, ..., ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273:162,Load,Loading,162,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2629#issuecomment-1708220273,1,['Load'],['Loading']
Performance,"Hi,. Currently using covariates in `sc.tl.rank_genes_groups()` is not implemented. In the Wilcoxon and t-test versions this is also not possible. However, in logistic regression this could be added. As a coarse approximation you could correct for batch using `sc.pp.combat()` and then use the corrected data instead of `adata.raw` (which is the default) to calculate marker genes. However, generally I would not recommend performing statistical analysis on batch-corrected data for other tests. Regarding your `anndata2ri` error... you could also check `adata.var` columns to see if any are categorical, but numeric.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/691#issuecomment-502549720:422,perform,performing,422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/691#issuecomment-502549720,1,['perform'],['performing']
Performance,"Hi,. I am getting an error when loading my loom files, which did not happen before and I am not capable of understanding the error output to try to fix it. . ![screen shot 2018-08-29 at 10 58 23](https://user-images.githubusercontent.com/42487820/44760841-9b527680-ab7b-11e8-9e85-0d0235cee6db.png). Your help will be much appreciated.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/247:32,load,loading,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/247,1,['load'],['loading']
Performance,"Hi,. I can't get the ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:372,queue,queue,372,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,2,"['multi-thread', 'queue']","['multi-threaded', 'queue']"
Performance,"Hi,. I don't think the data is actually different. Only the jitter in the violin plot places the dots in different places (this is an inherent stochastic effect). The underlying value of `n_genes` (and others) is still the same. You can check if `adata.obs['n_counts']` is the same in the 4 objects you load.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/977#issuecomment-572685233:303,load,load,303,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/977#issuecomment-572685233,1,['load'],['load']
Performance,"Hi,. I found there is 'use_rep' for tool.tsne(), but not for tool.umap().; Is there any solution to perform umap on a selected anndata's obsm?. Thanks in advance,; BP",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/689:100,perform,perform,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/689,1,['perform'],['perform']
Performance,"Hi,. I have few queries regarding scanpy. 1. As scanpy is using Louvain Leiden algorithms for clustering which optimize modularity 'Q', so how we can access and print modularity funciton?. 2. Resolution parameter gave us different number of clusters when we iterated between the best suggested 0.6-1.2. So how we know best number of cluster and how we can choose the optimum value of parameter 'resolution'?. Thanks,; Khalid",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670:111,optimiz,optimize,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670,1,['optimiz'],['optimize']
Performance,"Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872:175,perform,perform,175,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872,1,['perform'],['perform']
Performance,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928:47,perform,performed,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669#issuecomment-497118928,1,['perform'],['performed']
Performance,"Hi,. You could just create a new `.obs` variable with the two groups and the perform `sc.tl.rank_genes_groups()` over this variable. For example, you could do something like this:. ```; adata.obs['groups'] = ['group 1' if int(i) < 9 else 'group 2' for i in adata.obs['louvain']]; sc.tl.rank_genes_groups(adata, groupby='groups', key_added='group_DE_results'); ```. as there are only two groups the top-ranked genes for either groups will be the up-regulated genes in that group (and down-regulated in the other group) that are most differentially expressed between the groups. . You should however note that `rank_genes_groups` is not a particularly sensitive test for differential gene expression. While it is good for a quick exploratory analysis, other tools like limma or MAST may give you more DEG results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464:77,perform,perform,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447140464,1,['perform'],['perform']
Performance,"Hi,. `sc.tl.rank_genes_groups()` treats each gene as an independent variable in the test. Thus, the only difference if you were to subset the genes would be that the multiple testing correction would be over fewer genes. You can also do that manually by looking at the `adata.uns['rank_genes']['pvals'][CLUSTER_ID]` and doing the multiple-testing correction yourself over the gene set you care about. However, the p-values of this test are inflated anyway, and therefore they should be used with caution. You should be able to extract your test results of interest by doing something along the lines of this:; ```; CLUST_ID = 0; gene_list = ['Gabrg1', 'Ntrk1', 'Htr1a', 'Plaur', 'Il31ra', 'Gabrg3', 'P2rx3', 'Oprk1', 'P2ry1', 'Cnih3']; gene_mask = [gene in gene_list for gene in adata.uns['rank_genes']['names'][CLUST_ID]]; results = adata.uns['rank_genes']['pvals'][CLUST_ID][gene_mask]; ```. Then you need to perform multiple testing correction over those p-values. And that would be the result you would get from a subsetting. However, multiple-testing over only those values, assumes you will not use the other gene results for anything. If you use the other gene results for something else, then you should just use the results of `sc.tl.rank_genes_groups()` as it is. Also note that `sc.tl.rank_genes_groups()` doesn't really tell you the contribution of genes to the clustering, but it just tells you what genes are characteristic of a cluster in the output. Those aren't the same things. For example, one gene could have been responsible for partitioning the data into 2 parts, but then after subclustering those 2 parts it may not show up as a marker gene in the `sc.tl.rank_genes_groups` results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065:911,perform,perform,911,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/748#issuecomment-515061065,1,['perform'],['perform']
Performance,"Hi,; I couldn't import scanpy due to an error: DLL load failed. I have checked pre-existing issues, but all of them seem to be an h5py issue. My error report seems different from them.; ```; >>> import scanpy as sc; D:\Anaconda\lib\site-packages\dask\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.; data = yaml.load(f.read()) or {}; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\__init__.py"", line 36, in <module>; from . import tools as tl; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\__init__.py"", line 17, in <module>; from ._sim import sim; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\tools\_sim.py"", line 23, in <module>; from .. import _utils, readwrite, logging as logg; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\scanpy\readwrite.py"", line 10, in <module>; import tables; File ""C:\Users\yuhong\AppData\Roaming\Python\Python37\site-packages\tables\__init__.py"", line 99, in <module>; from .utilsextension import (; ImportError: DLL load failed: The specified procedure could not be found. >>> print(sys.version); 3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1468:51,load,load,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1468,7,"['Load', 'load']","['Loader', 'load']"
Performance,"Hi,; I really like using scanpy for our large scale single cell aging project. I am uploading several libraries from different age mice (8 time points) and concatinate them together (see example cmd I used below). I would like to control the color display so that I have a gradient color that changes with age and visualise in umap plot for example sc.pl.umap(adata, color= ['Age','louvain']). How could I do that? Thank you very for your help and for this wonderful package. path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/13_mouse_IL33_8w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_13_mouse_IL33_8w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_13_mouse_IL33_8w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_13_mouse_IL33_8w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_13_mouse_IL33_8w_X.obs['Tissue'] = 'X'; adata_13_mouse_IL33_8w_X.obs['Age'] = '8Weeks'; adata_13_mouse_IL33_8w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_13_mouse_IL33_8w_X, min_genes=250). path = '/home/10XG6_ILC2_thirdrun/cellranger_count_results/1_mouse_IL33_16w_X/outs/filtered_gene_bc_matrices/mm10/'; adata_1_mouse_IL33_16w_X = sc.read(path + 'matrix.mtx', cache=True).transpose(); adata_1_mouse_IL33_16w_X.var_names = np.genfromtxt(path + 'genes.tsv', dtype=str)[:, 1]; adata_1_mouse_IL33_16w_X.obs_names = np.genfromtxt(path + 'barcodes.tsv', dtype=str); adata_1_mouse_IL33_16w_X.obs['Tissue'] = 'X'; adata_1_mouse_IL33_16w_X.obs['Age'] = '16Weeks'; adata_1_mouse_IL33_16w_X.obs['State'] = 'IL33_activated'; sc.pp.filter_cells(adata_1_mouse_IL33_16w_X, min_genes=250). etc............ adata = adata_13_mouse_IL33_8w_X.concatenate([adata_1_mouse_IL33_16w_X, etc ....)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/185:649,cache,cache,649,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/185,2,['cache'],['cache']
Performance,"Hi,; I was reading some mtx file from here: ; https://www.ebi.ac.uk/gxa/sc/experiments/E-HCAD-4/downloads. `adata = sc.read_mtx(""./data/mtx/E-HCAD-4.aggregated_filtered_counts.mtx"")`; `AnnData object with n_obs × n_vars = 25052 × 606606; ` ; `sc.__version__`; `'1.7.1'`. when loading the mtx file the obs and vars are mixed up. ; That happened with another mtx file before. I was wondering if already a fix exists to specify the obs and vars (or switch them if necessary). . Thanks . </details>; ![image](https://user-images.githubusercontent.com/7283790/112545551-a19f4280-8db8-11eb-8e0d-7d56ee0443b5.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1761:276,load,loading,276,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1761,1,['load'],['loading']
Performance,"Hi,; I'm encountering an error when trying to write result file, after perform cell cycle score.; After normalizing, I import cell cycle file and perform the score:. `cc_genes=[gene.strip() for gene in open('[my_cell_cycle_genes]')]; s_genes=[g for g in cc_genes[:43] if g in adata.var_names]; g2m_genes=[g for g in cc_genes[43:] if g in adata.var_names]; sc.tl.score_genes_cell_cycle(adata, s_genes=s_genes, g2m_genes=g2m_genes); `. The field 'phase' of the obs. matrix is of type object:; `adata.obs.phase.dtypes; dtype('O')`. When I write the annData object, I got the error:; `adata.write(results_file); ... storing 'phase' as categorical; TypeError: Categorical is not ordered for operation max; you can use .as_ordered() to change the Categorical to an ordered one`. and now the field 'phase' is categorical:; `adata.obs.phase.dtypes; CategoricalDtype(categories=['G1', 'G2M', 'S'], ordered=False)`. I can modify it as suggested, but it's converted into categorical when writing file again.; Following my version packages:; `sc.logging.print_versions(); scanpy==1.4.2 anndata==0.6.17 umap==0.3.7 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1`. My annData, also on a subset of variables, is too big to attach here, but I could send you by email if you need it. Thanks a lot!; Raffaella",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/645:71,perform,perform,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645,2,['perform'],['perform']
Performance,"Hi,; Scanpy are designed to handle big datasets, while how about the performance on small datasets ? (such as as few as 50 cells from early embryo)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764:69,perform,performance,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764,1,['perform'],['performance']
Performance,"Hi. After I performed ingest, I need to concatenate the two datasets. But when followed the tutorial, used concatenated but this function doesn't;t concatenate the .obsm, therefore the UMAP coordinates are not merged. How did you manage to performed UMAP on the integrated/concatenated dataset?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/985:12,perform,performed,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/985,2,['perform'],['performed']
Performance,"Hi. I am recently transformed Seurat object to scanpy and use it for further pseudotime analysis (PAGA) and it performs really well. . But I have three question here:. 1) I am wondering if anybody here knows how to make PAGA connectivity score heatmap (ref: Popescu et al, 2019, Nature) which shows connections strength between partitions. I've tried dendrogram in scanpy (pl.coorelation.matrix) but we'd like to try more. . 2) And also if anyone knows if we could perform differential expression on the partitions by PAGA to find the marker gene along the potential path?. 3) PAGA generated a pie chart in every partition But does anyone know whether I could acquire the real percentage of the pie representing different Seurat cluster I input?. Thanks in advance for everyone's help!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1133:111,perform,performs,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1133,2,['perform'],"['perform', 'performs']"
Performance,"Hi. I have successfully installed scanpy but ; ImportError Traceback (most recent call last); <ipython-input-5-99fcf407c387> in <module>; ----> 1 import scvelo as scv; 2 import scanpy as sc; 3 import numpy as np. ~/anaconda3/lib/python3.7/site-packages/scvelo/__init__.py in <module>; 14 del version; 15 ; ---> 16 from .read_load import AnnData, read, read_loom, load, read_csv, get_df, DataFrame; 17 from .preprocessing.neighbors import Neighbors; 18 from .tools.run import run_all, test. ~/anaconda3/lib/python3.7/site-packages/scvelo/read_load.py in <module>; 10 from scipy.sparse import issparse; 11 from anndata import AnnData; ---> 12 from scanpy import read, read_loom; 13 ; 14 . ImportError: cannot import name 'read' from 'scanpy' (unknown location). Would you please help me to fix this problem. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1433:363,load,load,363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1433,1,['load'],['load']
Performance,"Hi. Maybe I can help a little as well. Typically batch correction or data integration methods would be used to obtain good clustering of the data, however once differential testing is performed it is still unclear whether the corrected data can or should be used (no batch correction method is perfect and may overcorrect). The standard strategy would be to correct for batch, and any other covariates that you are not interested in for the clustering process. Once you have the clusters, it is standard practice to go back to the raw data and use a differential testing algorithm that allows you to account for batch and other technical covariates in the model (e.g. MAST).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806:184,perform,performed,184,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395726806,1,['perform'],['performed']
Performance,"Hi.; I have a problem to install fa2 (pip install fa2) in windows 10 operating system and I am using python 3.7 version? . Using Conda env . **sc.tl.draw_graph(ds, init_pos='paga'),**; **drawing single-cell graph using layout 'fa'; WARNING: Package 'fa2' is not installed, falling back to layout 'fr'.To use the faster and better ForceAtlas2 layout, install package 'fa2' (`pip install fa2`).**. installation error. Collecting fa2; Using cached fa2-0.3.5.tar.gz (435 kB); Requirement already satisfied: numpy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.18.4); Requirement already satisfied: scipy in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (1.4.1); Requirement already satisfied: tqdm in c:\programdata\miniconda3\envs\jayalal_2_miniconda\lib\site-packages (from fa2) (4.46.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... error; ERROR: Command errored out with exit status 1:; command: 'C:\ProgramData\Miniconda3\envs\Jayalal_2_Miniconda\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""'; __file__='""'""'C:\\Users\\xkalaj\\AppData\\Local\\Temp\\pip-install-golo7r_8\\fa2\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\xkalaj\AppData\Local\Temp\pip-wheel-yjh93oit'; cwd: C:\Users\xkalaj\AppData\Local\Temp\pip-install-golo7r_8\fa2\; Complete output (30 lines):; Installing fa2 package (fastest forceatlas2 python implementation). >>>> Cython is installed?; Yes. >>>> Starting to install!. running bdist_wheel; running build; running build_py; creating build; creating build\lib.win-amd64-3.6; creating build\lib.win-amd64-3.6\fa2; copying fa2\fa2util.py -> build\lib.win-amd64-3.6\fa2; copying fa2\forceatlas2.py -> build\li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1256:438,cache,cached,438,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1256,1,['cache'],['cached']
Performance,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659:307,perform,performed,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284#issuecomment-428513659,2,['perform'],['performed']
Performance,"Hm, you’re right! looks like a Sphinx bug, as even setting it manually doesn’t change things. Maybe pickling the function leads to it having a different object id… . The only ways to fix this:. 1. set the value to a string like `'scanpydoc.elegant_typehints:typehints_formatter'` and implement importing that object in sphinx-autodoc-typehint; 2. make it so sphinx compares function-valued settings in a way that doesn’t bust the cache.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2204#issuecomment-1088546625:430,cache,cache,430,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2204#issuecomment-1088546625,1,['cache'],['cache']
Performance,"Hmm okay, I thought leiden clustering pulls cells with similar expression closer to each other on the UMAP space? By clustered UMAP, i mean the UMAP produced after i performed leiden clustering on it. By unclustered i mean that I just plotted the UMAP without calculating the leiden clusters. . Then I dont know what happened, but when I plotted the UMAP without leiden clustering performed, it had a different shape in the UMAP then after I calculated the leiden clusters. I will check the confusion matrix and come back to it when I have the results. In the meantime I can only post this image where I put both UMAPs next to each other and drew what I meant about part of cluster1 being added to cluster2 after performing the leiden clustering: . ![change-umap](https://github.com/scverse/scanpy/assets/127406679/38399718-5296-4aa0-87a9-4cd5057b4b5d)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034530366:166,perform,performed,166,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2956#issuecomment-2034530366,3,['perform'],"['performed', 'performing']"
Performance,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499770209:57,load,load,57,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499770209,1,['load'],['load']
Performance,"Hopefully I am not too out of date to ask this question. Extending on this discussion, I was wondering how a few of you @bioguy2018 @Khalid-Usman @LuckyMD calculate the Silhouette Scores for your graphs? The simplest way I can think of to extract the vectors required for the calculation will be to use the adjacency matrices as vectors. However, I quickly run into memory issues on large datasets with >= 100K nodes? (Each vector will contain 100K elements) I couldn't even load the matrix into memory to perform any form of dimension reduction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678:475,load,load,475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-1465574678,2,"['load', 'perform']","['load', 'perform']"
Performance,"How were the bulk labels generated then assigned to cells in the pbmc68k dataset? I'm trying to do the same on my data.; Ideally I would like to use my own list to label cells. For example a cell has Gene X and Gene Y, then the 'bulk_label' in .obs is loaded with a string 'Cell Z'. Thank you",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/495:252,load,loaded,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/495,1,['load'],['loaded']
Performance,"How? As said, they’re just for people and IDEs. Scanpy doesn’t use them. It doesn’t throw errors in case something doesn’t fit. We could use https://pypi.org/project/typecheck-decorator/ to throw errors when something is passed that doesn’t fit the annotations. However, doing so has a performance hit and requires flawless annotations (because if the annotations were wrong, that *would* start suddenly throwing errors). I’m just adding type annotations to improve user friendliness by being more clear what functions accept, and because it makes writing documentation easier.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542:286,perform,performance,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441252542,1,['perform'],['performance']
Performance,"Huh weird, it gets detected, but it doesn’t seem to help to call the non-parallel version lol. If I replace the `warn` with a `print`, it’s clear that the correct (non-parallel) function is called from Dask’s thread. Seems like calling numba from a `ThreadPoolExecutor` isn’t supported at all, even if it comes from dask. ```console; $ hatch test tests/test_utils.py::test_is_constant_dask[csr_matrix-0] --capture=no; Numba function called from a non-threadsafe context. Try installing `tbb`.; Numba function called from a non-threadsafe context. Try installing `tbb`. Numba workqueue threading layer is terminating: Concurrent access has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", li",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:617,Concurren,Concurrent,617,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,3,"['Concurren', 'concurren']","['Concurrent', 'concurrently']"
Performance,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682:120,perform,performance,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643#issuecomment-778222682,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"I agree with @LuckyMD about the points regarding covariates. . With respect to two group comparisons without confounding, rank-sum tests have less statitical power than t-tests (https://stats.stackexchange.com/questions/130562/why-is-the-asymptotic-relative-efficiency-of-the-wilcoxon-test-3-pi-compared), disclaimer I haven't checked this proof, I think this is a standard statistics result though, this is also discussed here https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl. I havent run simulations to check how big the influence of the difference in power is on the kind of data we encounter. However, as also pointed out by the second link, violations of the distributional assumptions for t-test impact these results and these violations will be major on scRNAseq. Intuitively I would therefore tend to rank-sum tests. With respect to [diffxpy](https://github.com/theislab/diffxpy): We can account for other noise models in the two-group comparisons by performing model fitting, tutorial [here](https://github.com/theislab/diffxpy_tutorials/blob/master/diffxpy_tutorials/test/single/wald_test.ipynb). The bioarxiv will hopefully be up in the next few weeks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358:1040,perform,performing,1040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447874358,1,['perform'],['performing']
Performance,I also [put this on StackOverflow](https://stackoverflow.com/questions/48326579/unable-to-iterate-over-pandas-dataframe-loaded-from-tabular-data),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/70#issuecomment-358711733:120,load,loaded-from-tabular-data,120,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70#issuecomment-358711733,1,['load'],['loaded-from-tabular-data']
Performance,"I also recently encountered this issue. I've dug into the problem a little bit and for me the cause seems to be that the sc.pp.scale function introduces the NaN values. This occurs for columns which show very little variance and are almost constant. According to the current documentation this should not be the current expected behaviour though and should only (possibly) occur in future versions: . `Variables (genes) that do not display any variation (are constant across all observations) are retained and (for zero_center==True) set to 0 during this operation. In the future, they might be set to NaNs.`. So I'm not sure if this is a bug or if the documentation has not been updated yet. . I've currently circumvented the issue by scaling in sklearn (which retains 0s instead of NaNs) and manually loading the scaled results into my adata object as this is the behaviour I would like for my dataset. In case my example dataset would be helpful let me know then I can share it with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706:803,load,loading,803,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191634706,1,['load'],['loading']
Performance,I am actually booting using the exact same disks so identical OS (Ubuntu 16.04) and BLAS libraries. I am just loading them up with different virtual machines with different numbers of CPUs. In both cases the CPUs are Intel Xeon E5 v3 (Haswell). Have not tried limiting the number of CPUs used by arpack. I didn't know that was something I could do! Do you have a tip on how to do so? I'll look this up and give it a shot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718:110,load,loading,110,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-621186718,1,['load'],['loading']
Performance,"I am also experiencing this issue. Running the following code:; ```; import pandas as pd; import scanpy as sc; import anndata. print(pd.__version__); print(sc.__version__); print(anndata.__version__); adata = sc.datasets.pbmc68k_reduced(); adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:800,cache,cache,800,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,3,['cache'],['cache']
Performance,"I am fairly new with using scanpy, and so I may be performing this incorrectly. I encountered an error when trying to create a backed AnnData object from an h5ad file, and then logarithmizing the data matrix within the object using scanpy.pp.log1p. However I get an error within the AnnData object code because the preprocessing/_simple.py script is not passing a filename in the copy() function. Right now my current workaround is to create the AnnData object as non-backed, do the log1p, and then create a ""filename"" property to the AnnData object afterwards to make it backed for other scanpy functions. ### Example; ```python; import scanpy as sc. dataset_path = ""/path/to/test/data.h5ad"" # Subbing out actual filenames for data; adata = sc.read_h5ad(dataset_path, backed='r'); print(adata) # To ensure there is a backed filepath. adata.raw = sc.pp.log1p(adata, copy=True) # Error is here; ```. #### Error output; ```pytb; # I printed the AnnData object to ensure it was backed; AnnData object with n_obs × n_vars = 4166 × 16852 backed at '/tmp/1b12dde9-1762-7564-8fbd-1b07b750505f.h5ad'; obs: 'cell_type', 'barcode', 'tSNE_1', 'tSNE_2', 'replicate', 'louvain', 'n_genes', 'percent_mito', 'n_counts'; var: 'gene_symbol', 'n_cells'; obsm: 'X_tsne'. # Actual error after calling log1p; Traceback (most recent call last):; File ""log1p_test.cgi"", line 129, in <module>; main(); File ""log1p_test.cgi"", line 81, in main; adata.raw = sc.pp.log1p(adata, copy=True); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/scanpy/preprocessing/_simple.py"", line 292, in log1p; data = data.copy(); File ""/opt/Python-3.7.3/lib/python3.7/site-packages/anndata/_core/anndata.py"", line 1457, in copy; ""To copy an AnnData object in backed mode, ""; ValueError: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`.; ```. #### Versions:; scanpy==1.4.6 anndata==0.7.1 umap==0.3.10 numpy==1.16.3 scipy==1.4.1 pandas==0.24.2 scikit-learn==0.22.2.post1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1153:51,perform,performing,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1153,1,['perform'],['performing']
Performance,"I am following workflow of '_Best-practices in single-cell RNA-seq: a tutorial_' to analyze my single-cell sequencing data sets.; I have calculated the size factor using the scran package and did not perform the batch correction step as I have only one sample. Then, I intended to extract highly variable genes by using the function sc.pp.highly_variable_genes. Unfortunately, I got an error:. > LinAlgError: Last 2 dimensions of the array must be square. <details><summary>Traceback</summary>. ```pytb; LinAlgError Traceback (most recent call last); in ; ----> 1 sc.pp.highly_variable_genes(adata). ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace); 94 X = np.expm1(adata.X) if flavor == 'seurat' else adata.X; 95; ---> 96 mean, var = materialize_as_ndarray(_get_mean_var(X)); 97 # now actually compute the dispersion; 98 mean[mean == 0] = 1e-12 # set entries equal to zero to small value. ~/miniconda3/lib/python3.6/site-packages/scanpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:200,perform,perform,200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['perform']
Performance,"I am trying to load some datasets with `sc.read_h5ad(file_name)`. Frequently, I get the below error. When I re-run the code multiple times or at different times it sometimes works, but often I get the error (using the same code and data). This happens when reading different h5ad datasets (e.g. is not specific to one dataset). At all times there seems to be enough free RAM / similar amount of free RAM. This happens both when using jupyter-notebook and python without jn. Error:; ```pytb; ---------------------------------------------------------------------------; OSError Traceback (most recent call last); ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 155 try:; --> 156 return func(elem, *args, **kwargs); 157 except Exception as e:. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_group(group); 505 if ""h5sparse_format"" in group.attrs: # Backwards compat; --> 506 return SparseDataset(group).to_memory(); 507 . ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/anndata/_core/sparse_dataset.py in to_memory(self); 370 mtx = format_class(self.shape, dtype=self.dtype); --> 371 mtx.data = self.group[""data""][...]; 372 mtx.indices = self.group[""indices""][...]. h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 572 fspace = selection.id; --> 573 self.id.read(mspace, fspace, arr, mtype, dxpl=self._dxpl); 574 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/h5d.pyx in h5py.h5d.DatasetID.read(). h5py/_proxy.pyx in h5py._proxy.dset_rw(). h5py/_proxy.pyx in h5py._proxy.H5PY_H5Dread(). OSError: Can't read data (file read failed: time = Sat Aug 1 13:27:54 2020; , filename = '/path.../filtered_gene_bc_matrices.h5ad', file descriptor = 47, errno = 5, error messag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351:15,load,load,15,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351,1,['load'],['load']
Performance,"I am using a sampling technique, which samples few rows without descreasing; performance. So speed is more than 10X time faster for larger dataset with; similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>; wrote:. > I'm not sure I entirely understand what the weights are based on. I'm; > trying to understand when you would suggest someone use your approach. Why; > do you give one cell a weight of 125? With this type of weight distribution; > you are basically manually changing the marker gene calculation focusing; > nearly only on a single cell. That seems strange to me.; >; > I'm trying to understand the need for scanpy to support weighted; > observations. At the moment I don't see when you would want to differently; > weight the observations... I'm familiar with using weights if I have some; > form of measurement error or uncertainty between samples. I don't really; > see how that holds here. Do you weight the cells based on some kind of; > quality score?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494124913:77,perform,performance,77,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494124913,1,['perform'],['performance']
Performance,"I am wondering about the motivation that went into subtracting the min when performing standardisation of the scale between genes. I find that it leads a misleading visualisation when the genes expressed by all clusters so I am now copying and modifying your function for my work. Do you think it would be justified to remove min subtraction step or make it optional?; Thanks, and please let me know what you think!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1451:76,perform,performing,76,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1451,1,['perform'],['performing']
Performance,"I can do that. I have a pickled object that I can share with you (how?).; Here is how you reproduce the error:; ```; import scanpy.api as sc; import pickle. # Load the object; with open(""example.pkl"",""rb"") as handle:; adata = pickle.load(handle). # Run Scanpy; sc.tl.rank_genes_groups(adata,groupby=""celltype""); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960:159,Load,Load,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/365#issuecomment-440420960,2,"['Load', 'load']","['Load', 'load']"
Performance,"I confirmed that setting the PYTHONHASHSEED environmental variable to 0 did not change the results. The code run below (in jupyter notebook) gave the same results as before while confirming that the PYTHONHASHSEED variable was set to 0 before running the pipeline. ```; # First run on a machine on with 8 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:493,cache,cache,493,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,1,['cache'],['cache']
Performance,"I created a new environment (see below for package details) and there everything works as it should. . <Details>; <summary>Versions in the new working environment</summary>. -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 7.2.0; anndata 0.7.6; anyio NA; argon2 20.1.0; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bottleneck 1.3.2; brotli NA; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.05.0; dateutil 2.8.1; decorator 5.0.9; fsspec 2021.05.0; get_version 2.1; h5py 3.2.1; idna 2.10; igraph 0.9.1; ipykernel 5.5.5; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 3.0.1; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.8.0; jupyterlab_server 2.5.2; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.18; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.9.0; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.2; scipy 1.5.3; seaborn 0.11.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sniffio 1.2.0; socks 1.7.1; sphinxcontrib NA; statsmodels 0.12.2; storemagic NA; tables 3.6.1; tblib 1.7.0; terminado 0.10.0; texttable 1.6.3; tlz 0.11.0; toolz 0.11.1; tornado 6.1; traitlets 5.0.5; typing_extensions NA; urllib3 1.26.4; wcwidth 0.2.5; websocket 0.57.0; yaml 5.4.1; zmq 22.0.3; zope NA; -----; IPython 7.23.1; jupyter_client 6.1.12; jupyter_core 4.7.1; jupyterlab 3.0.16; notebook 6.4.0; -----; Python 3.8.10 (default, May 19 2021, 18:05:58) [GCC 7.3.0]; Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.10; 4 logical CPU cores, x86_64; -----; Session information updated at 2021-05-25 15:50. <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:323,bottleneck,bottleneck,323,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,"I created an adata without using the functions provided by scanpy that; allow you to load single cell data. This kind of conversion is done is done; in that functions, right?. On Tue, Jul 30, 2019, 06:17 Isaac Virshup <notifications@github.com> wrote:. > That'll do it 😄; >; > Do you know how you ended up with non-string indices? Ideally, we would be; > able to prevent that from happening or at least warn the user about it.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/747?email_source=notifications&email_token=ACPDY4T3AAWXADJLTLCIMW3QB66FRA5CNFSM4IG2HWJ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CWRPY#issuecomment-516253887>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACPDY4TBMBQHRYTRFHJCWJTQB66FRANCNFSM4IG2HWJQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-516271609:85,load,load,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516271609,1,['load'],['load']
Performance,"I did notice this warning in later versions of scanpy but only for index of `var` and `obs` not the table columns themselves. The loom file i'm loading contains this variable as an integer int64 type. I simply load the data and convert to categorical. . ```; adata = sc.read_loom(lf); adata.obs.columns = [""cellid"", ""hpf""]; adata.obs[""hpf""] = adata.obs[""hpf""].astype('category'); ```; This does not raise a warning, which seems like it would be hard to catch as I work on the dataframe directly.; Setting a dataframe with an integer index raises a warning as you mentioned. However if this is intended then I can understand this error.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645:144,load,loading,144,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-453877645,2,['load'],"['load', 'loading']"
Performance,"I don't feel well adding a large dataset to the repository. Adding something subsampled and rather low-dimensional would be fine. Alternatively, you could also just add a dataset that is automatically downloaded: as [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L104-L106) or [here](https://github.com/theislab/scanpy/blob/7646c947f632ea7b09fea783e32a017136cfed24/scanpy/datasets/__init__.py#L142-L144). As both travis and readthedocs will cache this, it should be a viable solution that avoids bloating the repository with several MB of data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287:510,cache,cache,510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/207#issuecomment-405508287,1,['cache'],['cache']
Performance,"I encountered the same error (KeyError: 1) when trying to load the .mtx file with scanpy.read_10x_mtx(). After several unsuccessful attempts at renaming the columns and indices in the 'genes.tsv' file in different ways, I found a workaround that worked for me:. 1. Import the .mtx file separately using scanpy.read_mtx().; 2. Convert the imported data to a pandas DataFrame using .to_df().; 3. Manually name the columns and indices using the 'barcodes.tsv' and 'features.tsv' files, respectively. This approach allowed me to bypass the KeyError and successfully load the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888:58,load,load,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053#issuecomment-2133703888,2,['load'],['load']
Performance,"I found a minor bug in this tutorial; [Clustering 3k PBMCs following a Seurat Tutorial](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb). I hope this is the correct venue to post to regarding this. I'm currently going through this to learn how to use scanpy. In the first section; ```; path = './data/pbmc3k_filtered_gene_bc_matrices/hg19/'; adata = sc.read(path + 'matrix.mtx', cache=True).T # transpose the data; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; adata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]; ```. Due to how pandas dataframes indexes this part; ```; genes = pd.read_csv(path + 'genes.tsv', header=None, sep='\t'); adata.var_names = genes[1]; adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ```; does not yield the expected results. As `var_names` becomes the index of `var` adding `genes[0]` will try to merge a data frame with unmatching index resulting in a `NaN` column in `var` for `'gene_ids'`. The solution should be either; ```; genes = genes.set_index(1); adata.var = genes; ```; or; ```; adata.var_names = genes[1]; genes = genes.set_index(1); adata.var['gene_ids'] = genes[0] # add the gene ids as annotation of the variables/genes; ``` . It does probably not have any effect on the tutorial but I thought I'd mention it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/275:428,cache,cache,428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275,1,['cache'],['cache']
Performance,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/279#issuecomment-426950213:279,optimiz,optimization,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279#issuecomment-426950213,1,['optimiz'],['optimization']
Performance,"I guess we can close with this. And sorry, I forgot to answer the above:; > I view my goal here as allowing more representations as input. When I say ""representation"", I mean a feature space representation, which is directly amenable to differentiable mappings, hence optimization and learning. When you say ""graph representation"" that might be a legit notion, too; and one can definitely think about learning different graph representations (by transforming them back to a vector space). But to me, it appears much more reasonable and straight forward to do all the inference on the feature space representation. And one should do it an a way so that the applied metric used to analyze the arising manifold in the learned representation does make sense. Of course, if you work directly with raw data, using different metrics can capture a lot of what you'd otherwise need to learn or preprocess (invariance to scales, ...). Hope this helps a little bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342:268,optimiz,optimization,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-424802342,1,['optimiz'],['optimization']
Performance,I had the same problem when I loaded sample data from a csv as a data frame; and assigned it to adata.obs = df. >,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/747#issuecomment-516289839:30,load,loaded,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/747#issuecomment-516289839,1,['load'],['loaded']
Performance,"I have a dataset with around 400K observations -- I wanted to perform batch correction using sc.pp.combat, but I'm getting out of memory errors after running for a couple hours with > 2 TB of memory. My understanding was that combat used a dense matrix, which requires a lot of memory.; Why is this? Are there suggestions for workarounds here?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1977:62,perform,perform,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977,1,['perform'],['perform']
Performance,"I have a short script which reads a tab file and writes h5 using scanpy. I've found that unless I provide a full path to the write() function or at least a relative one via ""./foo.h5"" it fails. Simplified version:. ```py; adata = sc.read(args.input_file, ext='txt', first_column_names=True).transpose(); adata.write('./test.h5') # this works; adata.write('test2.h5') # this fails; ```. Here's the stack:. ```pytb; WARNING: This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; Traceback (most recent call last):; File ""./convert_gear_group_single_cell_to_hdf5.py"", line 47, in <module>; main(); File ""./convert_gear_group_single_cell_to_hdf5.py"", line 43, in main; adata.write('test2.h5'); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1471, in write; compression=compression, compression_opts=compression_opts); File ""/usr/local/lib/python3.5/dist-packages/anndata/base.py"", line 1513, in _write_h5ad; os.makedirs(os.path.dirname(filename)); File ""/usr/lib/python3.5/os.py"", line 241, in makedirs; mkdir(name, mode); FileNotFoundError: [Errno 2] No such file or directory: ''; _____________________________. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/66:466,cache,cache,466,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/66,2,['cache'],['cache']
Performance,"I have a similar issue to [this comment](https://github.com/theislab/scanpy/issues/1916#issuecomment-927497782). `Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids')`. Switching to `gene_symbols` didn't work. Error messages:; ```; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise V",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:287,cache,cache,287,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,2,['cache'],['cache']
Performance,"I have a similar problem. And this does not seam to be related to any python package version as I have 4 data sets loaded from CellRanger h5 files.; With two of these files the neighbors function works and with two it fails with likely a seg fault as a cpp_abort_hook process takes over. This is REALLY annoying as I also get this problem with a random number of different single cell data sets. I assume there is some issue with a dataset that results in a cpp error.; I do not want to debug that as cpp errors are a pain. Can you guess what the problem might be?; The cpp breaks after the multiprocessor step. The Python process has used my 10 processors to the max for some time, but then fallen back to 100%. So it seams it might be after collecting whatever has been produced in the first multiprocessor step.; Can you tell me what that could be so I can implement a test into my scripts? It also probably would be a good idea if you could implement that test into your package. I'll check the two other links, too. If I do not come back here assume both links were not helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313450128:115,load,loaded,115,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361#issuecomment-1313450128,1,['load'],['loaded']
Performance,"I have been moving between interactive servers not on the queue. `icb-lisa`, `icb-sarah`, and `icb-mona`, and if none of those work, then the older servers `hias`, `sepp`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564:58,queue,queue,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-667944564,1,['queue'],['queue']
Performance,"I haven't performed an in-depth benchmark comparison. But results from a single run of modularity detection on an example (a [Facebook graph](http://konect.uni-koblenz.de/networks/facebook-wosn-links)) is sufficiently revealing I think:; ```; Running Leiden 0.7.0.post1+71.g14ba1e4.dirty; Running igraph 0.8.0; Read graph (n=63731,m=817035), starting community detection.; leidenalg: t=8.048258741036989, m=0.6175825273363675; igraph community_leiden: t=1.159165252931416, m=0.6298702028415605; ```; This is only a relatively small graph, and the difference is likely to be even bigger for larger graphs. Perhaps the `igraph` Leiden algorithm can indeed be the default, with `leidenalg` being an optional choice or something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791:10,perform,performed,10,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586969791,1,['perform'],['performed']
Performance,"I just have scanpy 0.2.7 and am trying to produce bpmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filenam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:647,cache,cache,647,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,1,['cache'],['cache']
Performance,"I just stumbled upon a similar bug using SciKit Learn. It's not ScanPy, but this issue is the only result Google returned when I looked up my error. Here's my crash log:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 000000010ddfb000-000000010ddfd000 [ 8K] r-x/rwx SM=COW /usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python. Application Specific Information:; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff4fb578e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff24844c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff24844a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001104e3f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001104e3527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x00000001104a9b27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x00000001104aeabf array_matrixproduct + 191; 7 org.python.python 	0x000000010de4712e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x000000010dead0e6 call_function + 491; 9 org.python.python 	0x000000010dea5621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x000000010dead866 _PyEval_EvalCodeWithName + 1747; ```. It's not very useful as it's the same as the OP's, but it might help shifting the blame to a common dependency of SciKit Learn and ScanPy (like BLAS having an issue with macOS' Grand Central Dispatch).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214:203,queue,queue,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182#issuecomment-408848214,2,['queue'],['queue']
Performance,"I know this (quite ancient) pull request has been open (#403), but I wasn't sure on its status. I think the consensus was to wait for sklearn to integrate the necessary changes? If that's still the case, then please feel free to remove this PR. Here I make use of scipy's extremely nifty [LinearOperator](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html) class to customize the dot product functions for an input sparse matrix. In this case, the 'custom' dot product performs implicit mean centering. In my benchmarks, performing implicit mean centering in this way does not affect the runtime whatsoever. However, this approach has to use [svds](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html), for which randomized SVD is not implemented. So we have to use 'arpack', which can be significantly slower (but not intractably so.... in my hands, I could still do PCA on datasets of 200k+ cells in minutes, and it sure beats densifying the data, if you want more thorough benchmarks I am happy to generate them!). The way I incorporated this functionality into scanpy/preprocessing/_simple.py might be questionable, and would love any suggestions or advice on how to better integrate this if there is interest in pushing this PR through. Let me know!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066:511,perform,performs,511,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066,2,['perform'],"['performing', 'performs']"
Performance,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html; - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343:823,scalab,scalable,823,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265#issuecomment-423784343,1,['scalab'],['scalable']
Performance,"I like this idea, but think it could be expanded on a bit. I think there are benefits to approaching this through logging. Some advantages of doing this through logging:. * Global record. If a copy is made or an AnnData split, you could figure out which object came from where.; * Control over level of detail. What kind of information is recorded can be customized. Maybe the user wants provenance, but maybe they want performance information. What if tracking was done through logging? Here's a couple quick examples of what I mean:. <details>. <summary>Simple example. Logs `anndata` used, function called, time elapsed </summary>. ```python; from anndata import AnnData; from datetime import datetime; from functools import wraps; from structlog import get_logger; from time import sleep; import uuid. logger = get_logger(). def logged(func):; @wraps(func); def func_wrapper(*args, **kwargs):; call_id = uuid.uuid4() # So we can always match call start with call end; call_start_record = dict(call_id=call_id, called_func=func.__name__); if type(args[0]) is AnnData:; call_start_record[""adata_id""] = id(args[0]); logger.msg(""call"", **call_start_record). t0 = datetime.now(); output = func(*args, **kwargs); dt = datetime.now() - t0. call_finish_record = dict(called_func=func.__name__, elapsed=dt); if type(output) is AnnData:; call_finish_record[""returned_adata_id""] = id(output); logger.msg(""call_finish"", **call_finish_record, call_id=call_id); return output; return func_wrapper. # Usage. @logged; def foo(adata, x, copy=False):; sleep(0.5); if copy: return adata.copy(). import scanpy as sc; pbmcs = sc.datasets.pbmc68k_reduced(). foo(pbmcs, 1); # 2019-02-13 19:27.58 call adata_id=4937049368 call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo; # 2019-02-13 19:27.58 call_finish call_id=UUID('82f3944c-08c1-470a-9d39-03dcabc091a2') called_func=foo elapsed=datetime.timedelta(microseconds=500777); foo(pbmcs, 1, copy=True);; # 2019-02-13 19:28.02 call adata_id=4937049368 cal",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273:420,perform,performance,420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-463117273,1,['perform'],['performance']
Performance,I like your suggestions. Especially the `filter_rank_genes_groups` use makes a lot of sense to me. The one thing I would suggest to take into account is that some of these filtering steps can be done before significance testing and therefore you would not have to perform multiple testing correction on the filtered out genes. This may be quite useful to some. That precludes filtering on p-value though. It also makes a case for filtering already in `rank_genes_groups` rather than in `sc.get`.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770:264,perform,perform,264,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1529#issuecomment-738766770,1,['perform'],['perform']
Performance,"I mean any smFISH or highly-multiplexed protein technology. The plot I have in mind is this:; This visualisation is implemented in our package (in active development - we haven't released yet): https://cell2location.readthedocs.io/en/latest/cell2location.plt.html#cell2location.plt.mapping_video.plot_spatial; ![download-20](https://user-images.githubusercontent.com/22567383/95405951-0ea94380-0911-11eb-84bf-6f712da7875c.png). I agree that the original images can be quite large so it is probably better to not load them by default. However, it is useful to have an option to load. For the Visium data, the utility of using fullres depends on image quality and the goals. Generally, cell diameter in highres images is just 1-4 pixels meaning that a cropped image with, say 10*10 spots will look pixelated and may not be enough to recognise small structures like a gland or a blood vessel, not mentioning cell morphologies or staining (e.g. eosinophils containing red granules).; For single-cell resolution data, it is often useful to zoom in to see if only cells of specific morphology express the gene, like Agt below.; ![download-19](https://user-images.githubusercontent.com/22567383/95405958-12d56100-0911-11eb-9a9b-3a2faa3fa660.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276:512,load,load,512,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1436#issuecomment-705283276,2,['load'],['load']
Performance,"I never get adjustText to work without numerous rounds of parameter optimization, so yeah, I agree.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675:68,optimiz,optimization,68,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1513#issuecomment-839982675,1,['optimiz'],['optimization']
Performance,"I never used something different than `n_neighbors=5` for very small datasets (~1000 cells) up to `n_neighbors=30` for large datasets. However, I rarely change the default `n_neighbors=15` anyways. For very large datasets and in very rare cases I can imagine that it pays off to go up to `n_neighbors=50` or even more, but I never did this... I'd say it doesn't actually make a lot of sense to use the silhouette coefficient for evaluation: the Louvain algorithm optimizes modularity, which you can view as the graph-based version of a silhouette coefficient (""ratio"" of intra-cluster edges versus inter-cluster edges as compared to intra-cluster distances vs. inter-cluster distances in the silhouette coefficient). Once the graph is computed, there is no point in going back to the feature space for the computation of topological properties. In the end, you describe the common workflow. You start with some coarse clustering and recluster the parts of the graph in which you want higher resolution. It's not at all surprising that the clustering by default doesn't agree with marker genes: modularity clustering clusters densely connected partitions of the graph, an information that comes from averaging over all genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223#issuecomment-409829464:463,optimiz,optimizes,463,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223#issuecomment-409829464,1,['optimiz'],['optimizes']
Performance,"I never used spatial data (so far), are they organized as separate `AnnData` objects? If everything that could be integrated is a single `AnnData` then the function would be easy, like. ```python; def leiden_multiplex(adata: Sequence[AnnData], use_computed: bool = False, weights: None):. adj_list = [x.uns['neighbors']['connectivities'] for x in adata]; G_list = [sc._utils.get_igraph_from_adjacency(x) for x in adj_list] #also add the `restrict_to` step. if use_computed:; part_list = [get_partitions_from_adata.obs] or [recalculate_partitions_with_neighbors_params]; # then run the optimizer; else:; membership, improv = la.find_partitions_multiplex(**params). for a in adata:; a.obs['multiplex'] = pd.Categorical(membership). ```; where `adata` is a list of `AnnData` objects, `use_computed` switches between recalculate partitions (`False`) or optimize partitions already calculated (`True`). Weights can be specified to give more or less importance to a specific view. Note that, by default, if set to `None` it is set to a list of ones by `leidenalg`.; Other options, in addition to the usual `copy = False` should be the `leidenalg` type of partitioning (`CPMVertexPartition`, `RBConfigurationVertexPartition`...)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328:585,optimiz,optimizer,585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1107#issuecomment-600076328,2,['optimiz'],"['optimize', 'optimizer']"
Performance,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. ; @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271:677,optimiz,optimize,677,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498046271,1,['optimiz'],['optimize']
Performance,"I reverted the setup.py change which hopefully means that an alrady-compliant setup.py from the flit PR will just pass [the check](https://github.com/theislab/scanpy/actions/runs/610117826) (once it runs, seems to be queued for a while now)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457:217,queue,queued,217,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1689#issuecomment-787846457,1,['queue'],['queued']
Performance,"I see no reason why the possibility shouldn't exist to run the weighted version on the full graph. I'm still curious about the quality of the outcome though. Using protein-protein interaction data, I've noticed that similarity scores perform worse than using network neighbourhoods based on cutoffs to cluster data (this does not have to be the case for scRNA-seq of course). In the latter case you require cells to be each others nearest neighbours to create dense network regions, rather than highly similar transcriptomes based on one calculation of similarity. I would have thought the cutoff approach is more robust to changing similarity metrics as well. It's definitely worth testing this though. Maybe I'm just too skeptical of similarity metrics over all. @fidelram do you have labels on your data where you could verify the quality of those two partitions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676:234,perform,perform,234,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-416161676,1,['perform'],['perform']
Performance,"I tentatively added a benchmark that runs just on `_get_mean_var`. Locally I don’t see any difference though, what’s wrong? Too small data? Numba not set up with correct number of threads?. /edit: also I think the machine is not sufficiently tuned. The original run (before I added the `mean_var` benchmarks) said “No changes in benchmarks.”",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3015#issuecomment-2066327499:242,tune,tuned,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3015#issuecomment-2066327499,1,['tune'],['tuned']
Performance,"I think I've got an example for you, which should be pretty easy for you to play around with in datashader. The example is doublet detection. I'm following the basic outline of the methods which simulate doublets, then project those onto the real data to find which barcode (/cell) the simulated doublets sit next to. Those barcodes are presumed to be doublets. So we'd expect that areas of mostly singlets in the real data would have a lower relative (to the real data) density of points in the simulated. I'm still exploring what the best way to summarize that difference in density is through. Here's an example with some pbmcs from 10x:. <details>; <summary> Setup (loading, simulating, and projecting) </summary>. ```python; import scanpy as sc; import numpy as np; import pandas as pd; from scipy import sparse; from umap import UMAP. from itertools import repeat, chain. # Define functions. def preprocess(adata):; adata.var[""mito""] = adata.var[""gene_symbols""].str.startswith(""MT-""); sc.pp.calculate_qc_metrics(adata, qc_vars=[""mito""], inplace=True); sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000); sc.pp.log1p(adata); return adata. def pca_update(tgt, src, inplace=True):; # TODO: Make sure we know the settings from src; if not inplace:; tgt = tgt.copy(); if sparse.issparse(tgt.X):; X = tgt.X.toarray(); else:; X = tgt.X.copy(); X -= np.asarray(tgt.X.mean(axis=0)); tgt_pca = np.dot(X, src.varm[""PCs""]); tgt.obsm[""X_pca""] = tgt_pca; return tgt. def simulate_doublets(adata, frac=.5):; """"""Simulate doublets from count data.; ; Params; ------; adata; The anndata object to sample from. Must have count data.; frac; Fraction of total cells to simulate.; """"""; m, n = adata.X.shape; n_doublets = int(np.round(m * frac)); pos_idx = np.array(list(chain.from_iterable(map(lambda x: repeat(x, 2), range(n_doublets))))); combos = np.random.randint(0, m, (n_doublets * 2)); pos = sparse.csr_matrix(; (np.ones_like(combos, dtype=adata.X.dtype), (pos_idx, combos)), ; shape=(n_doublets, m);",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384:670,load,loading,670,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/575#issuecomment-481184384,1,['load'],['loading']
Performance,"I think our initially identified bottleneck with using sparse arrays was this here https://github.com/cupy/cupy/issues/2359. The analysis workflows usually have very clear computational bottlenecks, so the translation to GPU should take this into consideration: Is it feasible in terms of available code to keep the array on GPU and actually perform all operations there or will this stay a CPU centric library that deploys particular steps to GPU. In[batchglm / diffxpy](https://github.com/theislab/batchglm) we took the first approach, we build ontop of (a CPU centric scanpy and) deployed GLM fitting to GPU via tensorflow2, we also use estimation code in dask in the same package that we could in principle use with cupy, right now this just sits ontop of numpy. . Happy to be involved with this stuff, I spent some time thinking about this with @quasiben already. I think it is really crucial to figure out where it makes sense to invest time to build pipelines that can be end-to-end be executed on GPU: because of the large number of tools this will not be the entire scanpy tool environment for a long time, so mixed workflows will be necessary. . 1. I would for example restrict all efforts to the submodule `sc.tl` for now because this contains most potential bottlenecks I think that are frequently used. ""end-to-end"" doesnt need to go all the way up to analysis graph leaves, such as plotting, in my opinion, as their is little performance gain there.; 2. Nice to have for non-core functionalities would then be some examples of how GPU-based arrays can be used within anndata so that 3rd parties can modify their tools to directly operate on the GPU array rather then starting to copy arrays. I think this is not really clear for most people right now (I have never done that either) and documenting this properly / improving this would help a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788:33,bottleneck,bottleneck,33,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1177#issuecomment-618890788,5,"['bottleneck', 'perform']","['bottleneck', 'bottlenecks', 'perform', 'performance']"
Performance,"I think that correlation matrix is only in the latest master version. You can install it using:; ```; pip install git+https://github.com/theislab/scanpy.git; ```. Also, be sure to load scanpy as 'import scanpy as sc'. If you use the old method (`import scanpy.api as sc`) it will not work.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206:180,load,load,180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/544#issuecomment-475183206,1,['load'],['load']
Performance,"I think we should introduce a standardized “mask” argument to scanpy functions. This would be a boolean array (or reference to a boolean array in `obs`/ `var`) which masks out certain data entries. This can be thought of as a generalization of how highly variable genes is handled. As an example:. ```python; sc.pp.pca(adata, use_highly_variable=True); ```. Would be equivalent to:. ```python; sc.pp.pca(adata, mask=""highly_variable""); # or; sc.pp.pca(adata, mask=adata.obs[""highly_variable""]); ```. One of the big advantages of making this more widespread is that tasks which previously required using `.raw` or creating new anndata objects will be much easier. Some uses for this change:. ### Plotting. A big one is plotting. Right now if you want to show gene expression for a subset of cells, you have to manually work with the Matplotlib Axes:. ```python; ax = sc.pl.umap(pbmc, show=False); sc.pl.umap(; pbmc[pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells',])],; color=""LDHB"",; ax=ax,; ); ```. If a user could provide a mask, this could be reduced, and would make plotting more than one value possible:. ```python; sc.pl.umap(; pbmc,; color=['LDHB', 'LYZ', 'CD79A’],; mask=pbmc.obs[""louvain""].isin(['CD4 T cells', 'B cells', 'CD8 T cells’,]),; ); ```. ### Other uses. This has come up before in a few contexts:. * Performing normalization on just some variables https://github.com/scverse/scanpy/issues/2142#issuecomment-1046729522; * Selecting a subset of variables for DE tests: https://github.com/scverse/scanpy/issues/1744; * See also https://github.com/scverse/scanpy/issues/748; * Changing use_raw https://github.com/scverse/scanpy/issues/1798#issuecomment-819998988. ## Implementation. I think this could fit quite well into the `sc.get` getter/ validation functions (https://github.com/scverse/scanpy/issues/828#issuecomment-560072919).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234:1337,Perform,Performing,1337,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234,1,['Perform'],['Performing']
Performance,"I thought this would be useful. I recently got a few datasets that were renamed and/or in a different folder structure and I thought it would be good if one could specify that. Something like . ````; def read(folder,mtx_file=None,features_file=None,...):; if mtx_file is not None:; # Load mtx file; else: ; # Fall back to load from folder; ````. Again, thank you so much!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602:284,Load,Load,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882#issuecomment-551408602,2,"['Load', 'load']","['Load', 'load']"
Performance,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699:150,perform,perform,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494314699,1,['perform'],['perform']
Performance,"I updated anndata to 0.8.0 and was not able to load my scanpy 1.8.2 properly. Any ideas?. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_31935/912249142.py in <module>; ----> 1 import scanpy as sc. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2264:47,load,load,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2264,1,['load'],['load']
Performance,"I updated the rank_genes_groups function to output p-values for t-tests and the wilcoxon rank-sum test, as discussed with @falexwolf in #159. ; The changes are outlined below:; - The t-test in the original file used a Welch t-test. I kept this, calculated the relevant degrees of freedom for a Welch test and then extracted the corresponding two-tailed p-value for the t-statistic (score). ; - The Wilcoxon test was originally done in chunks. To get the p-values I had to simplify this approach and use the ranksums function in scipy.stats. This caused me to loop through all of the genes being tested, which was fine for my dataset, but might need to be optimized for larger datasets.; - The adjusted p-values (pvals_adj) were calculated with a standard Bonferroni correction.; - All p-values are outputted and sorted the same way as 'names' or 'scores'. The only difference is that the p-values recarrays use float64 as a datatype to avoid converting a lot of the very small p-values to 0. Hope this is helpful!. Andrés",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/270:655,optimiz,optimized,655,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270,1,['optimiz'],['optimized']
Performance,"I upgraded anndata to 0.8.0 and couldn't load my scanpy 1.8.2 anymore. Error:. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); /tmp/ipykernel_31935/912249142.py in <module>; ----> 1 import scanpy as sc. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/__init__.py in <module>; 12 # (start with settings as several tools are using it); 13 from ._settings import settings, Verbosity; ---> 14 from . import tools as tl; 15 from . import preprocessing as pp; 16 from . import plotting as pl. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/tools/_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ModuleNotFoundError: No module named 'tables'. ```. The messages when updating anndata:; ```; The following packages will be REMOVED:. pytables-3.6.1-py38h9f153d1_1. The following packages will be UPDATED:. anndata 0.7.6-py38h578d9bd_0 --> 0.8.0-py38h578d9bd_0; ca-certificates pkgs/main::ca-certificates-2022.4.26-~ --> conda-forge::ca-certificates-2022.5.18.1-ha878542_0; h5py 2.10.0-nompi_py38h513d04c_102 --> 3.6.0-nompi_py38hfbb2109_100; hdf5 1.10.5-nompi_h5b725eb_1114 --> 1.12.1-nompi_h2750804_100. The following packages will be SUPERSEDED by a higher-priority channel:. certifi pkgs/main::certifi-2022.5.18.1-py38h0~ --> conda-forge::certifi-2022.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2265:41,load,load,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2265,1,['load'],['load']
Performance,"I was going crazy but I have since reproduced this finding using the minimal 3000 PBMC dataset clustering example. Essentially I run the same code either on a virtual machine with 8 CPUs or one with 16 CPUs and I get non-identical PCA results. It doesn't seem to matter if I use the arpack or the randomized solver even though using the randomized solver gives the warning:. `Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future.`. I'd like to just attach the jupyter notebook but it won't seem to let me do that so I'm copying the code below.; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; # First run on a machine with 8 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata). # Then run on a machine with 16 CPUs; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normaliz",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187:1140,cache,cache,1140,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187,1,['cache'],['cache']
Performance,"I was just trying to run the [1.3 million cell clustering example](https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells), but have come across some strange behavior. When loading in the `hdf5` file from 10x to an AnnData object, the whole process uses about 30gb. If I write that AnnData object to disk with `adata.write`, then try to load that file (with `sc.read`) I end up using all the memory on the machine (~60g) before segfault-ing. I'd think that any dataset I wrote from memory, I should be able to read back into memory. I've put the full scripts to reproduce on my system in [this gist](https://gist.github.com/ivirshup/42e70a745704b8c71d78e57dd43e3b0b), but essentially I've run:. ```python; # gen_h5ad.py; import scanpy.api as sc; adata = sc.read_10x_h5(""{DATAPTH}/1M_neurons_filtered_gene_bc_matrices_h5.h5"") # Uses about 30gb; adata.write(""./write/1M_neurons.h5ad""); ```; ```python; # load_anndata.py; import scanpy.api as sc; adata = sc.read(""./write/1M_neurons.h5ad"") # Ends with segfault; ```. I'm running `scanpy` installed with conda with the following versions:. ```; scanpy==1.0.4 anndata==0.6 numpy==1.14.2 scipy==1.0.1 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0; ```. Thanks for the great package! Let me know if you'd like any more details on my setup.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146:208,load,loading,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146,2,['load'],"['load', 'loading']"
Performance,"I was looking at the scanpy function to compute the mean and variance an noticed that it had some comments inside, pointing to performance issues. Thus, I looked for an alternative method, found the sklearn sparse function and then tested it in an artificially large matrix. Otherwise, I did not have any trouble with the current implementation. The floating point precision is higher in the sklearn method, thus I suppose this is not an issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026:127,perform,performance,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392049026,1,['perform'],['performance']
Performance,"I was wondering if someone who is familiar with sc.pp.regress_out could confirm the following:; I would like to regress out nonlinear effect, e.g. ~1 + a + a^2 + a^3, where a is non-categorical variable. I have looked at the code of regress_out: https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L677; It seems that the code performs the fitting for all specified variables at once, but I am not sure:; https://github.com/theislab/scanpy/blob/8fe1cf9cb6309fa0e91aa5cfd9ed7580e9d5b2ad/scanpy/preprocessing/_simple.py#L701; If the design passed to GLM is combined of all keys passed to the function then I could just create the necessary columns a, a^2, a^3 and pass this as keys. Can someone confirm if I understand this correctly and passing the polynomial columns will do the fitting of a polynom?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1839:388,perform,performs,388,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1839,1,['perform'],['performs']
Performance,"I would also like this, and will probably add it. The only issue is deciding how we name each element `pca` adds to an `anndata` object (i.e. the keys for observation loadings in `obsm`, variable loadings in `varm`, and metadata in `uns`. I'd thought of two options:. * `sc.pp.pca(adata, layer=layer, key_added=key)`; * Adds key `key` to `obsm`, `varm`, and `uns`.; * Makes it very easy to know which arrays match which.; * `sc.pp.pca(adata, layer=layer, key_prefix=prefix)`; * Adds `{prefix}_pca` to `obsm`, `{prefix}_PCs` to `varm`, and something like `prefix` to `uns`; * Makes it clearer how the arrays should be interpreted. Sorta fits current behaviour better.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068:167,load,loadings,167,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1301#issuecomment-654772068,2,['load'],['loadings']
Performance,I would love to see file I/O in Anndata. I imagine this would make things easier for episcanpy as well. That package can then focus more on setting up count tables where they are not nicely provided. Otherwise it becomes a bit difficult for the new user (me) to distinguish data loading and setting up new tables.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562:279,load,loading,279,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-879693562,1,['load'],['loading']
Performance,"I'd be happy to add this once I could figure it out. I've been able to load my tabular text files, store them as h5ad, then load them back again. I cannot see how to iterate over rows, then columns so that I can access all values of the dataframe with an awareness of which row/column each belongs to. My wishful code example:. ```py; adata = sc.read_h5ad(filename); selected = adata[:, adata.var_names.isin({'AAR2', 'ECT2'})]. ## this line spews information on the columns like:; # Empty DataFrameView; # Columns: []; # Index: [Cancer--Cell_1, Cancer--Cell_10, Cancer--Cell_100, Cancer--Cell_1000, Cancer--Cell_1001; #print(selected.obs). ## this line gives the row information:; # Empty DataFrameView; # Columns: []; #Index: [AAR2, ECT2]; #print(selected.var); ; # Nothing happens here at all; #for i, row in selected.obs.iteritems():; # print(i, row). for gene_name, row in selected.var.iterrows():; # this prints like: Series([], Name: AAR2, dtype: float64); print(row). # Nothing happens here; for cell_name, val in row.iteritems():; print(""{0}\t{1}\t{2}"".format(gene_name, cell_name, val)); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/70:71,load,load,71,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/70,2,['load'],['load']
Performance,"I'd love to help close this issue, but it's difficult for us to debug without a complete reproducible example. Could someone who's been experiencing this please provide a complete script which reproduces this issue?. This script should include loading data into Seurat, whatever minimal set of intermediate steps are necessary, then writing out the file which scanpy fails to read. Ideally, the data is computationally generated, something as simple as `x = matrix(1, nrow=10, ncol=10)` or `x = matrix(rpois(100, range(5)), ncol=10)`. If someone who is having this issue can please provide an example like this, we'll be able to help much faster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914:244,load,loading,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/598#issuecomment-497943914,1,['load'],['loading']
Performance,"I'll give a brief hand-wavy explanation now, before checking with someone who knows more about it whether my in depth understanding is correct. PCA is finding a set linearly independent variable which form a new basis for the data. ICA is finding N (user defined) discrete maximally independent signals from the data. They won't form a basis for the input data, and results can vary a lot based on the number of components you try and find. However, each of the signals is discrete and made of a sparser set of variables, which I think makes them more interpretable. I'd relate this to how the PCA components become a single blob while the ICA components keep separating clusters. For example, in the components that you point out, I would agree 1, 5, and 7 look to be the same (note: I may have underspecified N here). However, component 3 is picking up a signal which is largely colinear with those, except for one cluster. To me, that says the difference in variable loadings between component 3 and the others is worth investigating.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018:970,load,loadings,970,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/941#issuecomment-560059018,1,['load'],['loadings']
Performance,"I'm getting an error loading scanpy (#739 ), and it points to the line you moved about deferring loading of umap-learn. . When I revert back to commit abf95c645828e29edf5a7a27b05d9397f3c36f65 (the commit a couple before this), it works.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782:21,load,loading,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/704#issuecomment-511887782,2,['load'],['loading']
Performance,"I'm getting this too. This could be a problem with numpy's random: ; https://github.com/DLR-RM/stable-baselines3/issues/1579 ; https://github.com/SimonBlanke/Gradient-Free-Optimizers/issues/11. I'm seeing if I can specify explicitly the random state or seed. Found where the problem happens:. _leiden.py; Line 185 ; `part = g.community_leiden(**clustering_args)`. calls the following. community.py; Line 442; ```; membership, quality = GraphBase.community_leiden(; graph,; edge_weights=weights,; node_weights=node_weights,; resolution=resolution,; normalize_resolution=(objective_function == ""modularity""),; beta=beta,; initial_membership=initial_membership,; n_iterations=n_iterations,; ); ```. The debugger doesn't step into the `Graphbase.community_leiden` function any further, but this is where the loop with the error occurs. https://igraph.org/python/doc/api/igraph.Graph.html#community_leiden. **Update:**; Funnily enough, the Leiden clustering still executes correctly (took about 1 hour for me). How I did it was to create a simple .py file that loads the h5ad, just runs the leiden clustering, then writes a new h5ad, then ends. Ran that from a powershell window and just let it throw the warnings (which do not break the code execution). What I found is that I cannot run the leiden clustering in a notebook because the output gets overwhelmed and hangs VSCode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575:172,Optimiz,Optimizers,172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028#issuecomment-2078897575,2,"['Optimiz', 'load']","['Optimizers', 'loads']"
Performance,"I'm having the same error with `h5py==2.9.0`. Cellxgene doesn't seem to be working with the object that I created the object with scanpy `1.4.3+116.g0075c62`. I can however load it again with that version. But when I downgrade to 1.3.7 (recommendation from @mbuttner who had the same cellxgene issue) I can no longer load the object and get the above error. Back in the 1.4.3 dev version scanpy it no longer writes the object after loading, and gives me the following error:; ```; In [23]: adata.write(""cellxgene.h5ad"") ; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-23-33b15d710f71> in <module>; ----> 1 adata.write(""cellxgene.h5ad""). ~/new_anndata/anndata/anndata/core/anndata.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 2222 compression=compression,; 2223 compression_opts=compression_opts,; -> 2224 force_dense=force_dense,; 2225 ); 2226 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_h5ad(filepath, adata, force_dense, dataset_kwargs, **kwargs); 90 write_attribute(f, ""varp"", adata.varp, dataset_kwargs); 91 write_attribute(f, ""layers"", adata.layers, dataset_kwargs); ---> 92 write_attribute(f, ""uns"", adata.uns, dataset_kwargs); 93 write_attribute(f, ""raw"", adata.raw, dataset_kwargs); 94 if adata.isbacked:. ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key]; --> 105 _write_method(type(value))(f, key, value, dataset_kwargs); 106 ; 107 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_mapping(f, key, value, dataset_kwargs); 203 def write_mapping(f, key, value, dataset_kwargs=MappingProxyType({})):; 204 for sub_key, sub_value in value.items():; --> 205 write_attribute(f, f""{key}/{sub_key}"", sub_value, dataset_kwargs); 206 ; 207 . ~/new_anndata/anndata/anndata/readwrite/h5ad.py in write_attribute(f, key, value, dataset_kwargs); 103 if key in f:; 104 del f[key",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526:173,load,load,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832#issuecomment-544968526,3,['load'],"['load', 'loading']"
Performance,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487260802:435,optimiz,optimize,435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487260802,1,['optimiz'],['optimize']
Performance,"I'm not sure if this is a bug or not. It might just be something that I need clarification on, so apologies if adding it here is inappropriate. I've found that the `.obsp['distances']` matrix output by `sc.pp.neighbors()` is non-symmetric, which doesn't make sense to me. I don't see any parameters in the function for calculating directed vs undirected graph, which might have otherwise led to asymmetry. What am I missing?; Is there some special treatment of the matrix being performed to optimise downstream processing? Or is there something wrong causing this behaviour? . Many thanks. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sp; import pandas as pd. data = sp.datasets.pbmc3k(). sp.pp.normalize_total(data,target_sum=10000); sp.pp.log1p(data); sp.pp.highly_variable_genes(data, n_top_genes=2000) ; sp.pp.scale(data). sp.tl.pca(data, svd_solver='arpack', ); sp.pp.neighbors(data, n_neighbors=20). num_nonzeros = {}; for i in range(data.obsp['distances'].shape[0]):; num_nonzeros[i]= (data.obsp['distances'][i,:].count_nonzero(), data.obsp['distances'][:,i].count_nonzero()). df_nonzeros = pd.DataFrame.from_dict(num_nonzeros, orient = 'index', columns = ['row','column']). print((data.obsp['distances'].A == data.obsp['distances'].A.transpose()).all()) #demonstration that matrix is not symmetric. print(df_nonzeros) #number of non-zero entries in each row and column; #each row has 20 non-zero entries (when adding 1 for self-loops), which is the k parameter used in sp.pp.neighbors; #each column has a varying number of non-zero entries. print(df_nonzeros.column.mean()) #there is still an average of 20 connections in each column; ```. ```pytb; No error; ```. #### Versions. <details>. -----; anndata 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2381:478,perform,performed,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2381,1,['perform'],['performed']
Performance,"I'm still dubious of the value, especially when we provide different ways of ways of optimizing the score. What would you think of instead having `sc.metrics.modularity` where you match a clustering and a graph returning a modularity score?. It would basically wrap:. ```python; (; igraph.Graph.Weighted_Adjacency(adata.obsp[""connectivities""]); .modularity(adata.obs[""louvain""].cat.codes); ); ```. But you could also generate modularity scores for other labelings.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869:85,optimiz,optimizing,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1997873869,1,['optimiz'],['optimizing']
Performance,"I'm thinking. My favorite command line interfaces have the ability to query options and set options globally by writing to a config file (jupyter, npm, git, …). Maybe we should give scanpy that ability. People could use that if they use scanpy mainly through scripts. ```console; $ scanpy settings; Config file: ~/.config/scanpy/scanpy.toml; cachedir='~/.cache/scanpy' (default); ...; $ scanpy settings cachedir '/my/path'; Set cachedir to '/my/path' in ~/.config/scanpy/scanpy.toml; $ scanpy settings cachedir; /my/path; ```. And of course we also have a python API for this. People who use scanpy mainly interactively can use that one.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150:342,cache,cachedir,342,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477113150,5,['cache'],"['cache', 'cachedir']"
Performance,"I'm trying to import some data I downloaded from GEO using the read_10x_mtx() function. Since this data was generated with an older version of Cellranger, there is no features.tsv.gz file. I renamed the genes.tsv.gz file to features.tsv.gz but that still doesn't fix my problem. I am pasting the error message below: . ```pytb; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=Non",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:375,cache,cache,375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,2,['cache'],['cache']
Performance,"I'm trying to load the GSE164690 data using sc.read_10x_h5(), for which I'm including the path to the folders which contain the barcodes.tsv, features.tsv and matrix.mtx but I'm getting the IsADirectoryError every time I run the function. . `; adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'); reading GSE164690_RAW/GSM5017021_HN01_PBL/; ---------------------------------------------------------------------------; IsADirectoryError Traceback (most recent call last); Input In [3], in <cell line: 1>(); ----> 1 adatas = sc.read_10x_h5('GSE164690_RAW/GSM5017021_HN01_PBL/'). File ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py:180, in read_10x_h5(filename, genome, gex_only, backup_url); 178 if not is_present:; 179 logg.debug(f'... did not find original file {filename}'); --> 180 with h5py.File(str(filename), 'r') as f:; 181 v3 = '/matrix' in f; 182 if v3:. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:507, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds); 502 fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,; 503 locking, page_buf_size, min_meta_keep, min_raw_keep, **kwds); 504 fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,; 505 fs_persist=fs_persist, fs_threshold=fs_threshold,; 506 fs_page_size=fs_page_size); --> 507 fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr); 509 if isinstance(libver, tuple):; 510 self._libver = libver. File ~/opt/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py:220, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr); 218 if swmr and swmr_support:; 219 flags |= h5f.ACC_SWMR_READ; --> 220 fid = h5f.open(name, flags, fapl=fapl); 221 elif mode == 'r+':; 222 fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl). File h5py/_objects.pyx:54, in h5py._objects.with_ph",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2328:14,load,load,14,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2328,1,['load'],['load']
Performance,"I'm using Scanpy with the following software versions:. python==3.7; scanpy==1.4.4; numpy==1.17.2; anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py; sc.write(results_file, adata); ```; and to load it again with . ```py; adata = sc.read(results_file); ```. however if I save it after I run the command . ```py; sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'); ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb; ValueError Traceback (most recent call last); <ipython-input-141-159082f1696f> in <module>; 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)); 2 print(results_file); ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /opt/min",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/832:242,load,load,242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832,1,['load'],['load']
Performance,"I'm very sorry for having forgotten about this issue... Of course, `sc.pp.normalize_per_cell()` stores the total counts per cell *prior* to normalization as *n_counts*. See the examples here https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.normalize_per_cell.html. Performing the normalization removes the effect of having different total counts per cell by scaling each gene with the total counts. But one might want more: if there is still some correlation of a gene with *n_counts* *after* normalization, one concludes that the simple scaling done in normalization has *not* fully removed the effect of *n_counts* on that particular gene. Hence, using `sc.pp.regress_out`, one performs an additional gene-wise correction. I have to admit that I have not investigated how necessary this is. As you know, this is adapted from the Seurat tutorial - I guess the authors of Seurat found it useful in some cases to fully remove the effect of *n_counts* on each single gene.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902:274,Perform,Performing,274,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/48#issuecomment-347354902,2,"['Perform', 'perform']","['Performing', 'performs']"
Performance,"I've made a few changes:. ## Numba bug. First, the reason that you were getting different issues with floats is that there's a numba bug where the generated parallelized code is completely wrong. I ran into this with gearys_c too, so I've just done a similar thing. It seems to be triggered by having a `np.sum` in a `prange` loop, plus some minor other things. You can check the linked comment info for more details. I believe calculation of `z2ss` in the outer loop was triggering this bug, so I've just moved this into the inner function. Since we're not doing iterations anymore this shouldn't be a performance issue. ## Argument order. So, one bigger organizational change I made is to have consistent argument orders for the elements of a sparse matrix. Basically, always use the same order for positional arguments between functions, otherwise it's very easy to introduce bugs. ## Minor things. * I've modified one of the `morans_i` tests to check that if you pass a dense matrix and a sparse matrix of the same data, you should get the same results.; * I've removed the use of an intermediate array in `_morans_i_vec_W`, since you can just accumulated directly to `inum`.; * Fixed up typing, removed unused exports",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555:603,perform,performance,603,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1740#issuecomment-802562555,1,['perform'],['performance']
Performance,"IIRC, it's discussed in more detail in Malte's paper:. > ; In the same way that cellular count data can be normalized to make them comparable between cells, gene counts can be scaled to improve comparisons between genes. Gene normalization constitutes scaling gene counts to have zero mean and unit variance (z scores). This scaling has the effect that all genes are weighted equally for downstream analysis. There is currently no consensus on whether or not to perform normalization over genes. While the popular Seurat tutorials (Butler et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0020)) generally apply gene scaling, the authors of the Slingshot method opt against scaling over genes in their tutorial (Street et al, [2018](https://www.embopress.org/doi/full/10.15252/msb.20188746#core-msb188746-cit-0125)). The preference between the two choices revolves around whether all genes should be weighted equally for downstream analysis, or whether the magnitude of expression of a gene is an informative proxy for the importance of the gene. In order to retain as much biological information as possible from the data, we opt to refrain from scaling over genes in this tutorial. https://www.embopress.org/doi/full/10.15252/msb.20188746. Since there has been no new development on this topic, we cited Malte and also opted not to scale. This is also discussed by Malte himself in the issue that was cited above. I cannot comment on spatial data itself and make confident statements here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034415456:462,perform,perform,462,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034415456,1,['perform'],['perform']
Performance,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it.; yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096:338,load,load,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-757826096,1,['load'],['load']
Performance,"If we can cleanly switch to the igraph implementation for modularity with weights, it could make sense for that to be the default. Any chance you could point me to some benchmarks on performance? An initial test looks very impressive!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126:183,perform,performance,183,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586696126,1,['perform'],['performance']
Performance,"If you want to extract it in python, you can load the h5ad file using `adata = sc.read(filename)` and then use `adata.X`, which is the expression matrix. To extract the matrix into R, you can use the `rhdf5` library. That's a bit more complicated as there was a recent update to this library I believe. Note that you need to transpose the expression matrix from python into R due to different conventions (R expects a genes x cells matrix, python a cells x genes matrix). An alternative to the `rhdf5` library is to just save the expression matrix via `numpy.savetxt()` to save it, for example, as a space-delimited file. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246:45,load,load,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262#issuecomment-421082246,1,['load'],['load']
Performance,Import performance part 3,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756:7,perform,performance,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756,1,['perform'],['performance']
Performance,ImportError: DLL load failed while importing utilsextension,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:17,load,load,17,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['load'],['load']
Performance,ImportError: dlopen: cannot load any more object with static TLS,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1121:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1121,1,['load'],['load']
Performance,"In DropSeq experiments cell names are encoded by 12nt barcodes. It seems that no name check is performed when merging multiple datasets in ScanPy. . ```python; >>> from collections import Counter; >>> import scanpy.api as sc. >>> f = sc.read(""data1.txt"").transpose(); >>> g = sc.read(""data2.txt"").transpose(); >>> c = f.concatenate(g). >>> len(c.obs_names); 7932; >>> len(set(c.obs_names)); 7890. >>> cc = Counter(c.obs_names); >>> cc.most_common(10); [('AAAAAAAAAAAA', 2), ('TCCTGTCTCTTA', 2), ('CGCAAGGGAAAG', 2), ('ACCCGTCTATGT', 2), ('CTCCTGTCTCTT', 2), ('TTCCTGTCTCTT', 2), ('CCCTGTCTCTTA', 2), ('CCGCTGTCTCTT', 2), ('GACAAACCTACC', 2), ('ACACTGTCTCTT', 2)]. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/55:95,perform,performed,95,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/55,1,['perform'],['performed']
Performance,"In a recent paper, we found the brute force KNN computation to become very expensive as the data sizes increase. I’ve noticed the kNN graph computed during the “neighbors” computation can be cached and reused when Umap-learn is called downstream but when Cuml UMAP is used, the kNN graph is recomputed each time. . In cuml 0.13 we added an optional `knn_graph` argument to umap’s training and inference methods to allow it to accept pre-computed kNN graph. This will allow the kNN graph to be computed once and reused when `n_neighbors` has not been changed. I think this would further accelerate the exploratory data analysis and visualization process with scanpy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1279:191,cache,cached,191,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1279,1,['cache'],['cached']
Performance,"In pl.pca_loagings(), there should be an option to limit the number of points plotted (basically n_points from ranking). Why: I recently used the AnnData/scanpy suite to perform some analysis on a low number of genes (less than 30, amplified by qRT-PCR).; As the number of features is less than 30 (30 being the default value for n_points in ranking(adata,*args,**kwargs), the loadings appear twice on the sc.pl.loadings() graph.; (the slices [0:15] and 5:20] are overlapping, in case you have only 20 genes. definition should be:; ```; def pca_loadings(; adata: AnnData,; components: Union[str, Sequence[int], None] = None,; n_points=30,; include_lowest: bool = True,; show: Optional[bool] = None,; save: Union[str, bool, None] = None,; ):; ```. and later in implementation; ```; ranking(; adata,; 'varm',; 'PCs',; npoints=npoints,; indices=components,; include_lowest=include_lowest,; ); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2059:170,perform,perform,170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2059,3,"['load', 'perform']","['loadings', 'perform']"
Performance,"In scanpy, clustermap uses all the clusters and genes by default to plot the heatmap, however, it is more flexible if users can use a certain clusters and marker genes they are interested in. Can scanpy perform this function, or anyone who can add some extensions to scanpy to achieve this goal?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/178:203,perform,perform,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/178,1,['perform'],['perform']
Performance,"In supplementary figure 9 of our paper, I did a light comparison of tools using the demuxlet data as ground truth: https://www.cell.com/cms/10.1016/j.cels.2020.05.010/attachment/040c239d-1e70-42a4-8974-9fbd75c65551/mmc1.pdf; Which I think is a fine first stab at getting at this comparison, but it could be better. Hashsolo performance was comparable with other methods but is able to recover cell types with lower CMO counts. . I think that sounds great. That's an issue we had as well, but I noticed it occurring for NK cells in kidney; ![Screen Shot 2021-01-13 at 9 18 30 AM](https://user-images.githubusercontent.com/6864886/104486266-5d095680-5580-11eb-971e-c882063f2a45.png)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008:324,perform,performance,324,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/351#issuecomment-759597008,1,['perform'],['performance']
Performance,"Indeed, but then I believe UMAP should be derived from gene space and not from PCA. Even if the variance could be decomposed on the same components, the loadings could have opposite sign and UMAP would interpret them as totally different samples",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133906744:153,load,loadings,153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2259#issuecomment-1133906744,1,['load'],['loadings']
Performance,Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/859:45,perform,perform,45,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859,1,['perform'],['perform']
Performance,"Interesting... I know that there can be some difference between systems I use for how time is being recorded. But I still don't think I'd expect this. Either way, it looks like single threaded performance is good, and multithreaded is adding surprisingly little for a lot of spent computation. Once you've got the similarity measurements done, I think there's a little code organization to do, and this should be pretty much ready.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028:193,perform,performance,193,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1066#issuecomment-590137028,1,['perform'],['performance']
Performance,"Intuitively, I'd think having a more complete graph with weighted edges is more representative of the data than an arbitrary *k* neighbors. Even if you do use a hard cutoff on number of neighbors, I don't see how discounting all distance information would give a more accurate result. I would suspect using a weighted graph could perform better at identifying small subpopulations (where nearest neighbors from other cell types could be common), but that's just conjecture. From some preliminary attempts of my own, it seems clustering solutions become more stable with respect to parameter choice when I use the `method=gauss, knn=False` weighted network. I'm still in the process of verifying this, however. I would also note that the documentation for `sc.tl.louvain` references [this](; https://doi.org/10.1016/j.cell.2015.05.047) paper (the Phenograph method), which uses the louvain method on a a weighted graph. If the method is cited, why not allow using it?. Just from a package design/ usability perspective, I think it's nice to include. It would make the package more flexible and allows the user to take more advantage of the `louvain-igraph` library. If the user could also specify the kind of partition used, even better. @LuckyMD, it's definitely more memory intensive, but I'm not sure it's prohibitively computationally expensive. Also weights don't have to be based on the euclidean distance (`Phenograph` uses Jaccard distances between nodes' neighborhoods) and there's [some evidence](; https://doi.org/10.1093/bib/bby076) to suggest we should using correlation based distance metrics anyways.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113:330,perform,perform,330,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/240#issuecomment-415956113,1,['perform'],['perform']
Performance,"Is it because I have too many cells? But no memory error is reported. ```pycon; >>> adata; AnnData object with n_obs × n_vars = 1493240 × 4489; obs: 'orig.ident', 'nCount_RNA', 'nFeature_RNA', 'percent.mt', 'nCount_nFeature_ratio', 'brain_area', 'batch'; var: 'features', 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'mean', 'std'; uns: 'brain_area_colors', 'hvg', 'pca'; obsm: 'X_pca'; varm: 'PCs'; ```. ```python; topPC = 40; n_neigbor = 15; resolution = 0.3; sc.pp.neighbors(adata, n_neighbors=n_neigbor, n_pcs=topPC); ```. ```pytb; computing neighbors; using 'X_pca' with n_pcs = 40; Segmentation fault (core dumped); ### error file: core.212911; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.8.2; sinfo 0.3.1; -----; PIL 8.4.0; anndata 0.7.8; asciitree NA; attr 21.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; dask 2021.11.1; dateutil 2.8.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; encodings NA; entrypoints 0.3; fasteners NA; fsspec 2021.11.0; genericpath NA; h5py 3.4.0; idna 3.1; igraph 0.9.8; ipykernel 6.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 3.0.3; joblib 1.1.0; jsonschema 4.2.1; kiwisolver 1.3.2; leidenalg 0.8.8; llvmlite 0.36.0; louvain 0.7.0; markupsafe 2.0.1; matplotlib 3.4.3; mpl_toolkits NA; natsort 8.0.0; nbformat 5.1.3; nbinom_ufunc NA; ntpath NA; numba 0.53.1; numcodecs 0.9.1; numexpr 2.7.3; numpy 1.21.4; opcode NA; packaging 21.0; pandas 1.3.4; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; posixpath NA; prometheus_client NA; prompt_toolkit 3.0.22; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pyarrow 9.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pydoc_data NA; pyexpat NA; pygments 2.10.0; pyparsing 3.0.6; py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2361:969,concurren,concurrent,969,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2361,1,['concurren'],['concurrent']
Performance,"Is it that useful to see it by default? Why would you want to know unimportant genes?. IMHO it would be more interesting to know genes that have high loadings in another early PC but not in this one, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805#issuecomment-527405641:150,load,loadings,150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805#issuecomment-527405641,1,['load'],['loadings']
Performance,Is there any interest in writing a function to load any of the public datasets offered by 10x genomics?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1264:47,load,load,47,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1264,1,['load'],['load']
Performance,Is this backward compatible for cases where old AnnData object are loaded?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/957#issuecomment-567410803:67,load,loaded,67,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/957#issuecomment-567410803,1,['load'],['loaded']
Performance,Is this still fixed? I see that loading a 36GB h5ad file requires me to use a machine with 128GB RAM.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648:32,load,loading,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146#issuecomment-1944329648,1,['load'],['loading']
Performance,"Isaac,. this is great, thank you so much!. Regarding the default for the dataset directory. I like this solution!. Very small edits in addition to what I commented in the code:; * Can we call this `epi_sc_expression_atlas` instead of `expression_atlas`?; * For the time being, can we make this `settings.datasetsdir` instead of `settings.dataset_dir` and add it here: https://github.com/theislab/scanpy/blob/97c8b54ec884ac8e8396a80b6782a0d59a17a874/scanpy/api/__init__.py#L272; * Can we point it to the home directory by default, I'd say `~/scanpy-datasets/`?. Notes:; * By having a datasets dir, which is separate from the cache dir (which make sense), I guess, we can also use `user_cache_dir(…)` as the default for the cache dir (which would hopefully choose something in `~/.cache` on a Linux system, probably via `~/.cache/scanpy/`, I think this what you, also Phil (!) and Gökcen favored if I'm correctly summarizing the long thread?; * We already had a Scanpy config in the beginning (was an `.ini`) and we can reintroduce it in the future, and it should probably go into `~/.config/scanpy.ini` (or `.json` or `.yaml`). No reason not to have it. No need to have a CLI for this purpose.; * We can replace `.settings` with an instance of a class `._settings.Settings`. By that, attributes get auto-documented, we can do nice checks on setting attributes via properties, and we can also directly write to a `~/.config/scanpy.ini` file...; * `pyplot.rc_context` sounds awesome.; * Precedence for settings is correct as stated in https://github.com/theislab/scanpy/issues/558#issuecomment-478214932, this is also how I had it before removing the config file...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822:624,cache,cache,624,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478388822,4,['cache'],['cache']
Performance,It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdown 0.11; docutils 0.18.1; entrypoints 0.4; et-xmlfile 1.1.0; executing 0.8.3; fastjsonschema 2.16.2; filelock 3.9.0; flake8 6.0.0; Flask 2.2.2; fonttools 4.25.0; frozenlist 1.3.3; fsspec 2023.4.0; futur,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:2083,Bottleneck,Bottleneck,2083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['Bottleneck'],['Bottleneck']
Performance,It is reproduced. It is due to the `randint` producing a value outside the range of the default dtype `int32`. On windows 64 bit systems the default is `int32` despite the system being 64 bit. This is due to default for c long being `int32` on these systems. The part of the code that fails due to this is when using the context manager to perform the leiden clustering with igraph flavor.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969#issuecomment-2042435682:340,perform,perform,340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969#issuecomment-2042435682,1,['perform'],['perform']
Performance,"It is said that ""Be reminded that it is not advised to use the corrected data matrices for differential expression testing."" in scanpy document (http://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html) when execute MNN correction. However, Haghverdi Laleh (the one who presents MNN correction strategy, https://www.nature.com/articles/nbt.4091) says ""MNN correction improves differential expression analyses, After batch correction is performed, the corrected expression values can be used in routine downstream analyses such as clustering prior to differential gene expression identification"" in his Nature Biotech paper. So, I am a little confused. We have compared some corrections methods, such as regress_out, combat, MNN and MultiCCA (used by seurat), the results show that MNN and CCA have a better effect than regress_out and combat.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173:453,perform,performed,453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/168#issuecomment-395615173,1,['perform'],['performed']
Performance,"It looks like it's a bug in how we handle views of objects. If you copy the view you get from subsetting, that should give you the result you want, i.e.:. ```python; pbmc = sc.datasets.pbmc68k_reduced(); pbmc = pbmc[pbmc.obs['louvain'] == '0', :].copy(); sc.pp.scale(pbmc); ```. On our side, I think we should change the behavior of `ArrayViews` so they return `np.ndarrays` when operations are performed on them. Maybe we need to override `__array_ufunc__`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/699#issuecomment-504639751:395,perform,performed,395,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/699#issuecomment-504639751,1,['perform'],['performed']
Performance,"It seems to be scanpy-scripts itself. johnnydep analysis shows these (99% of lines removed):; ```. 2020-07-20 18:57:50 [info ] init johnnydist [johnnydep.lib] dist=scipy<1.3.0,>=1.2.0 parent=scanpy-scripts; 2020-07-20 18:58:10 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata<0.6.20; 2020-07-20 18:59:17 [info ] init johnnydist [johnnydep.lib] dist=scipy~=1.0 parent=anndata>=0.6.15; 2020-07-20 18:59:26 [info ] init johnnydist [johnnydep.lib] dist=scipy>=0.19.1 parent=scikit-learn>=0.19.1; 2020-07-20 18:59:58 [info ] init johnnydist [johnnydep.lib] dist=scipy>=1.3.1 parent=umap-learn>=0.3.0; ```. and later. ```; 2020-07-20 19:00:14 [info ] merged specs [johnnydep.lib] dist=scanpy-scripts extras=; set() name=scipy spec=<SpecifierSet('<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0', prereleases=True)>. ```. It cannot match both <1.3.0 and >= 1.3.1, and eventually bails out with:. ```; ERROR: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; pip._internal.exceptions.DistributionNotFound: No matching distribution found for scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0; subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'wheel', '-vvv', '--no-deps', '--no-cache-dir', '--disable-pip-version-check', '--pro; gress-bar=off', 'scipy<1.3.0,>=0.19.1,>=1.0,>=1.0.1,>=1.2.0,>=1.3.1,~=1.0']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497:1270,cache,cache-dir,1270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-661285497,1,['cache'],['cache-dir']
Performance,"It still does not work for me, even in a virtualenv. I always get:; ```. #Using legacy setup.py install for umap-learn, since package 'wheel' is not installed.; #ERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.2.3 which is incompatible. cd /usr/common/lib/python3.6/Envs; rm -rf ~/.cache/pip #make download clearer; python3 -m venv scanpy_scripts; source scanpy_scripts/bin/activate; python -m pip install -U pip; python -m pip install scanpy_scripts; #same error; python -m pip install -U setuptools #39.2 -> 47.3.1; python -m pip install scanpy_scripts; #same error; python -m pip install -U wheel; python -m pip install scanpy_scripts; #same error; echo $PYTHONPATH; #is blank. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039:312,cache,cache,312,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273#issuecomment-653279039,1,['cache'],['cache']
Performance,"It would probably good to see how important the `n_iterations` parameter is for our data. Hopefully after the first couple iterations it's only a few points of the million shuffling around per iteration. I'll try to take a closer when I can, but basically need to try something like:. ```python; def iterativley_cluster(; g: igraph.Graph,; *,; n_iterations: int = 10,; random_state: int = 0,; leiden_kwargs: dict = {}; ) -> list:; import random; random.seed(random_state). _leiden_kwargs = {""objective_function"": ""modularity"", ""weights"": ""weight""}; _leiden_kwargs.update(leiden_kwargs). partition = g.community_leiden(n_iterations=1, **_leiden_kwargs). steps = [partition]; for _ in range(n_iterations-1):; partition = g.community_leiden(n_iterations=1, initial_membership=partition.membership, **_leiden_kwargs); steps.append(partition). return steps; ```. My suspicion (and hope) would be that unstable clusters / points are the ones that drag on the optimization process. E.g. groups that aren't maintained when you change the random seed also aren't maintained through later iterations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081:953,optimiz,optimization,953,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1040854081,1,['optimiz'],['optimization']
Performance,"It's numpy.ndarray:; ```type(adata.X)```; ``` numpy.ndarray```; I guess it should be matrix? It's loaded once like this ; ```; path = '../count-genes/datafiles/all_counts.csv'; adata = sc.read(path, cache=True); ```; and then always manipulated with anndata interface. Maybe it should be transformed right after loading?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645:98,load,loaded,98,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/220#issuecomment-408263645,3,"['cache', 'load']","['cache', 'loaded', 'loading']"
Performance,"Just concatenate the datasets first and then use Combat. Something like:; ```; adata_merge = adata001.concatenate(adata002, adata003, batch_key='sample'); sc.pp.combat(adata_merge, batch='sample'); ```. Double check with the documentation... i'm not sure those are the exact parameters. Also note that Combat is a simple batch correction method that performs a linear correction of the batch effect. It assumes that you have the same cell identities in all datasets. MNN and BBKNN are more appropriate for scenarios with less similar batches. Scanorama is also implemented on top of the AnnData framework and is easily usable with scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807:350,perform,performs,350,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/702#issuecomment-527750807,1,['perform'],['performs']
Performance,"Just load any data, e.g. pbmc3k, then do `sc.pp.calculate_qc_metrics(adata, percent_top=[])` which gives the following: (this is on v1.3.7, haven't tested on earlier versions). ```; In [5]: sc.pp.calculate_qc_metrics(adata, percent_top=[]) ; ---------------------------------------------------------------------------; IndexError Traceback (most recent call last); <ipython-input-385-66af52bcd3f3> in <module>; ----> 1 sc.pp.calculate_qc_metrics(adata, percent_top=[]). ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in calculate_qc_metrics(adata, expr_type, var_type, qc_vars, percent_top, inplace); 70 obs_metrics[""log1p_total_{expr_type}""] = np.log1p(; 71 obs_metrics[""total_{expr_type}""]); ---> 72 proportions = top_segment_proportions(X, percent_top); 73 # Since there are local loop variables, formatting must occur in their scope; 74 # Probably worth looking into a python3.5 compatable way to make this better. ~/miniconda2/envs/py3/lib/python3.6/site-packages/scanpy/preprocessing/qc.py in top_segment_proportions(mtx, ns); 182 if not isspmatrix_csr(mtx):; 183 mtx = csr_matrix(mtx); --> 184 return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, ns); 185 else:; 186 return top_segment_proportions_dense(mtx, ns). IndexError: index -1 is out of bounds for axis 0 with size 0; ```. Not sure if there are other impacts, but I think perhaps basically one just need to check `percent_top` before calling `top_segment_proportions()` at line 72.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450:5,load,load,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/421#issuecomment-453896450,1,['load'],['load']
Performance,"Just saw that I forgot to comment on these two issues that @ivirshup mentioned above:. > ## Feature selection on an already transformed matrix; > ; > Would it be reasonable to include a way to compute the deviant genes from pearson normalized matrix? Ideally, we should not have to compute it twice to get all the results in one object. The easiest would probably be to add an `return_hvgs` option to `normalize_pearson_residuals()`, which would allow to skip our RAM-optimized HVG selection function for cases where speed / efficiency is needed and RAM usage is not a concern. ; This would give the same HVGs as our current function, but won't offer the batch correction currently implemented -- unless we implement the same batch correction option for `normalize_pearson_residuals()`, i.e. to compute residuals for each batch separately and then simply concatenate across cells... I would have to think a bit if this makes sense (maybe it does) and what properties these batch-corrected residuals will have. (@dkobak, do you want to comment?). If we can live without the batch correction for this ""fast lane case"", I can also just implement it without. Let me know!. > ## Docs consistency; > ; > A number of parameters are available in multiple functions. Would it make sense to use some of our tooling so there's only one place to edit these?. Sounds good - I think @giovp was suggesting something similar earlier, but recommended to wait for the next PR with this. > We really need another way to handle this (e.g. the way we do it in Squidpy with package constants) but this is for another PR. I have no experience with package constant yet but just let me know if I should do something here :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698:468,optimiz,optimized,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-903315698,1,['optimiz'],['optimized']
Performance,"Just to add to @ivirshup's points. There are several examples of lower PCs containing batch effects rather than higher PCs. I've seen this many times, but this has also been report for e.g., ATAC data in the [SCALE paper](https://www.nature.com/articles/s41467-019-12630-7). > Is it correct to say that the each embedded PC is given equal weight in the neighbourhood graph?. I'm not entirely sure, but I don't think you can say this... higher PCs that explain less variance will contribute less to the total variance if you use them as an input to e.g., UMAP, t-SNE, or a kNN graph building algorithm. This is because the variance of the loadings is proportional to the total variance explained (unless a rescaling is used in scanpy by default?). Thus, the contribution of higher PCs to the distance calculations will be less discriminative between points. Putting these two aspects together, you can see exactly why you need batch integration methods. These effects affect leading PCs, and therefore contribute a lot to any distance calculation based on an embedding. You can't just remove the effects by filtering for only leading PCs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611:638,load,loadings,638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-822621611,1,['load'],['loadings']
Performance,"Just to clarify, are you referring to 3 plots in the middle (PCA loading plots)? In new scanpy release, we render both positive and negative genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992:65,load,loading,65,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/991#issuecomment-573925992,1,['load'],['loading']
Performance,Loading from `.h5ad` taking much more memory than loading same dataset from 10x `.h5`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/146:0,Load,Loading,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/146,2,"['Load', 'load']","['Loading', 'loading']"
Performance,"Looking at #842 this is possible by subsetting the data on the groups of interest and then perform the analysis (one-vs-rest). If the reference group is an aggregate of other groups, then one has to define the new labels in the subset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/984#issuecomment-656111734:91,perform,perform,91,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/984#issuecomment-656111734,1,['perform'],['perform']
Performance,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494:230,scalab,scalable,230,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621#issuecomment-487019494,1,['scalab'],['scalable']
Performance,"Maybe this helps someone who encounters this problem as well. ; Here is what worked for me: Set the var **indices** of both X and raw and thereby the respective var_names get set automatically.; ```python; # load the adata object, converted using SeuratDisk; adata = sc.read_h5ad(object_path). # Set the new variable names by setting the respective indices otherwise gene names are not found; adata.var.index = adata.var.features; adata.raw.var.index = adata.var.features. # convert the cell type label data to type category (otherwise some methods do not work); adata.obs['cell_type'] = adata.obs['cell_type'].astype('category'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577:208,load,load,208,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1406#issuecomment-1962931577,1,['load'],['load']
Performance,"Multiple calls to `sc.pl.Dotplot.style()` cause resetting of parameters. This issue can be observed as a change in the default colormap shown by #1632 Other methods that set default parameters are also affected like `.add_totals()`. The following example should show the dots using the `Reds` colormap, but instead it uses the `winter` colormap because the second call sets the color map to `winter` if not given. This double call happens because when `sc.pl.dotplot()` is used (instead of `sc.pl.DotPlot`), internally a call to `.style()` is made and a subsequent explicit calls to `.style()` is required to tune the parameters as suggested in the documentation. ```python; adata = sc.datasets.pbmc68k_reduced(); markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']; sc.pl.DotPlot(adata, markers, groupby='bulk_labels').style(cmap='Reds').style(dot_edge_color='black').show(); ```; ![image](https://user-images.githubusercontent.com/4964309/107354555-9628de00-6ace-11eb-9eb8-c0baaa80b1f6.png). The problem is caused by the current implementation of `sc.pl.Dotplot.style()` that set the default parameters as:. ```; def style(; self,; cmap: str = DEFAULT_COLORMAP,; color_on: Optional[Literal['dot', 'square']] = DEFAULT_COLOR_ON,; dot_max: Optional[float] = DEFAULT_DOT_MAX,; dot_min: Optional[float] = DEFAULT_DOT_MIN,; .....; ```. Where DEFAULT_* are the default values defined at the beginning of the file (see https://github.com/theislab/scanpy/blob/master/scanpy/plotting/_dotplot.py#L84) . What is nice about this is that the documentation clearly shows the default values. The downside is that optional values are assigned a default value that rewrites previous calls to style. Ideally, all optional values should be `None`, then is easy to know if a new value is passed or a previous call has already set a value. But, doing so will remove the defaults from the documentation. @flying-sheep suggested to use a code he wrote to add default annotations to the documentation. https://gith",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1633:609,tune,tune,609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633,1,['tune'],['tune']
Performance,My impression has been that doing the densifying scale transform didn't seem to show performance improvements in a number of benchmarks. This is also the workflow used in [sc-best-practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html). @Zethson do you have a good citation for this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034405597:85,perform,performance,85,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2963#issuecomment-2034405597,1,['perform'],['performance']
Performance,"My issue is...; 1. I read the data into anndata and performed all preprocessed steps and then used data matrix to perform non-linear dimensionality reduction (DR).; 2. I performed DR and k-means clustering.; 3. I added back the data into one of the components (X_tsne); 4. Added KM labels also. Now, I am not able to select the clusters based on the cluster.; I want to select one of the clusters and perform clustering on that. Please find the code snippets below. mlle_3d_data=pd.read_csv(""C:/Users/saite/source/df.csv""); mlle_3dc_data=pd.read_csv(""C:/Users/saite/source/dfc.csv""). adata.obsm['X_tsne']=np.asanyarray(mlle_3d_data); adata.obs['km']=list(mlle_3dc_data['clusters']); adata.obs['km']=adata.obs['km'].astype('category'); sc.pl.tsne(adata,color=['km']). ------------------; I am trying to select based on the cluster number how we do with leiden or louvain clustering, but I am not seeing any data.; Please help me.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1452:52,perform,performed,52,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1452,4,['perform'],"['perform', 'performed']"
Performance,"My questions from #929 don’t apply since you don’t use numba here. Except for “What's the performance difference here”:. It’s not too bad, but we should use base 2 for everything that isn’t the natural logarithm: log2 can be calculated much faster on regular hardware due to binary storage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591:90,perform,performance,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/931#issuecomment-558143591,1,['perform'],['performance']
Performance,"My thinking on this right now is that:. * The code for masking logic (pre this PR) is kind of a mess; * This PR doesn't make the code nicer. But the performance benefit is quite good, and for sure the operation `X[mask_obs, :] = scale_rv` is something we don't want to do with sparse matrices. I also think we could get even faster, plus a bit cleaner if we instead modified scale array to use something like what I suggest [here](https://github.com/scipy/scipy/issues/20169#issuecomment-1973335172) to accept a `row_mask` argument:. ```python; from scipy import sparse; import numpy as np; from operator import mul, truediv. def broadcast_csr_by_vec(X, vec, op, axis):; if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Which *I think* would be something like:. ```python; def broadcast_csr_by_vec(X, vec, op, axis, row_mask: None | np.ndarray):; if row_mask is not None:; vec = np.where(row_mask, vec, 1); if axis == 0:; new_data = op(X.data, np.repeat(vec, np.diff(X.indptr))); elif axis == 1:; new_data = op(X.data, vec.take(X.indices, mode=""clip"")); return X._with_data(new_data); ```. Or, since we're doing numba already we could do just write out the operation with a check to see if we're on a masked row (which *should* be even faster since we're not allocating anything extra). I think either of these solutions would be simpler since we do the masking all in one place, and don't have to have a second update step.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345:149,perform,performance,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942#issuecomment-2024951345,1,['perform'],['performance']
Performance,Nadia single cell loading in scanpy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/366:18,load,loading,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/366,1,['load'],['loading']
Performance,"Need a file `highly_variable_genes.py` in `scanpy.preprocessing` with a function `highly_variable_genes`. This function is very similar to `filter_genes_dispersion`. However, by default, it assumes data has been logarithmized using `sc.pp.log1p`. Hence, in the “Seurat” method, an exponentiation with `expm1` is necessary (the current way in which the parameter `log` treats things is inconsistent as it doesn’t properly transform back using `expm`). Also, the new function doesn’t actually perform the filtering but simply annotates the data (`subset=False`). No need in an option `subset` in the new function. Of course, the old function remains for backwards compatibility.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/300:491,perform,perform,491,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/300,1,['perform'],['perform']
Performance,Nice! This was the original inspiration for having `scanpy.api` btw: `from scanpy.preprocessing import x` was instantaneous because all the other stuff wasn’t loaded.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/703#issuecomment-504923377:159,load,loaded,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/703#issuecomment-504923377,1,['load'],['loaded']
Performance,No idea about the error in the performance test. @flying-sheep ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626:31,perform,performance,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/945#issuecomment-561423626,1,['perform'],['performance']
Performance,"No problem!. * `features` sounds more natural to me, but `variables` is fine. Maybe we could do `vars` instead of `variables` for reduced verbosity?; * `expr_type` would work. Maybe `vars_type`?; * How about `n_genes_by_{exprs_type/vars_type}`? `n` works great for this, since it's integer valued. I might like `vars` over `genes` since the variables could be transcripts or surface markers, but I'm not sure on this. I like the `by_{vars_type}` convention for a couple reasons, which also apply to your last point:; * It allows recording at multiple steps in the process. You could imagine: `n_{vars/genes}_by_counts` and `n_{vars/genes}_by_imputed_counts` or `n_{vars/genes}_by_normed_expression`; * The convention allows for multi-omic measurements on a gene, `n_{vars/genes}_by_fluorescence` for example. This is a case where `genes` makes more sense than `vars`.; * `control_variables` does sound more natural. I'd possibly like to replace `control` as well, since these aren't necessarily controlled variables.; * Largely similar thoughts as the third point, e.g.; * Recording at multiple steps: `n_cells_by_counts` and `n_cells_by_imputed_counts`; * Multi-omic measurements: `n_cells_by_fluorescence`. I think `total` can be more widely used than `n`, allowing more consistency. To me, `total_cells` or `total_vars` make sense while `n_fluorescence` or `n_log_counts` don't. It's also totally fine to have a mix. Yeah, I figured I didn't want to make a whole copy of the object if I didn't want update or add all the metrics. About places in the codebase where naming would need to change, I'd argue the default shouldn't be to use a pre-computed value. I hadn't realized that `n_counts` fields were being stored or used until I started looking around. Since summing over a matrix is likely a pretty light computation compared to what follows, I don't think there's a strong performance argument for keeping it as the default.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904:1882,perform,performance,1882,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-436161904,1,['perform'],['performance']
Performance,"No problem, gave a chance to optimize the code a bit (peak memory was about 3x AnnData size, now down to about 2x).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410:29,optimiz,optimize,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316#issuecomment-437745410,1,['optimiz'],['optimize']
Performance,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```; $ pip install git+https://github.com/theislab/scanpy --upgrade --user; $ python planaria.py ; /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses; import imp; scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 ; ... storing 'clusters' as categorical; computing tSNE; using data matrix X directly; using the 'MulticoreTSNE' package by Ulyanov (2017); finished (0:01:09.28); Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba; rgba = _colors_full_map.cache[c, alpha]; KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter; colors = mcolors.to_rgba_array(c); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array; result[i] = to_rgba(cc, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba; rgba = _to_rgba_no_colorcycle(c, alpha); File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle; raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)); ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""planaria.py"", line 47, in <module>; sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'); File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 4",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145:916,cache,cache,916,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286#issuecomment-429198145,1,['cache'],['cache']
Performance,"Not only yes, but actually `spatialdata_io.visium_hd` only loads data that is **not** in the Zarr format, but the native format from SpaceRanger 3.x (which includes some Zarr files, but many other file extensions). The `SpatialData` object returned by `spatialdata_io.visium_hd` can then be saved to `.zarr` following the SpatialData format (described in this [design doc](https://github.com/scverse/spatialdata/blob/main/docs/design_doc.md) and [this page](https://github.com/scverse/spatialdata-notebooks/tree/main/notebooks/developers_resources/storage_format)), and read again using `spatialdata.read_zarr` (so, no need for the `spatialdata_io` package anymore). I hope this answers your question 😊",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2973#issuecomment-2388548992:59,load,loads,59,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2973#issuecomment-2388548992,1,['load'],['loads']
Performance,"Noticed that I did not normalized as intended, and made the input dictionary more flexible. Now:; 1. Normalization is not just performed so that rows/columns sum to 1, but instead over the number of marker genes in the reference/the number of marker genes used from the data.; 2. Reference marker dictionaries now accept `Union[Dict[str, set], Dict[str,list]]`. Dictionaries of lists are easier to use in other applications, like scoring based on gene sets. Still no idea why Travis is failing though :/.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/583:127,perform,performed,127,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/583,1,['perform'],['performed']
Performance,"Now seurat performs DE analysis using alternative tests including MAST and DESeq2 in a convinent way, such as FindMarkers(pbmc, ident.1 = ""CD14+ Mono"", ident.2 = ""FCGR3A+ Mono"", test.use = ""MAST""). So I hope that Scanpy could interated more methods too, such as diffxpy in this way:; sc.tl.rank_gene_groups(adata, method='diffxpy' or 'MAST'). Here is the hyperlink of DE analysis in Seurat:. https://satijalab.org/seurat/v3.0/de_vignette.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-529105173:11,perform,performs,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-529105173,1,['perform'],['performs']
Performance,OK got it! I still think `@jit` is too opaque – how should you know that some innocent-looking change results in a loop no longer being compiled? I think we should use `@njit` to be sure we have compiled performance-critical parts going forward.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-461002334:204,perform,performance-critical,204,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-461002334,1,['perform'],['performance-critical']
Performance,"OK! A global, per-install cache. Where is it stored?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862:26,cache,cache,26,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844#issuecomment-534485862,1,['cache'],['cache']
Performance,"OK, reproducible with smaller test data:. ```py; adata_file = cache.mkdir(""rank_gene_groups_violin"") / ""test_adata.h5ad""; if not Path(adata_file).exists():; ssl._create_default_https_context = ssl._create_unverified_context; urllib.request.urlretrieve(; ""https://apps-01.i-med.ac.at/resources/tmp/toy_adata.h5ad"", adata_file; ); adata_full = sc.read_h5ad(adata_file); adata = ad.concat([; adata[adata.obs.cell_type == 'Naive CD4+ T cells'][:4, :4],; adata[adata.obs.cell_type == 'Naive CD8+ T cells'][:4, :4],; ], merge='unique'); adata.write(data_path / 't-cells.h5ad'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2258#issuecomment-1658188074:62,cache,cache,62,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2258#issuecomment-1658188074,1,['cache'],['cache']
Performance,"OK, seems like I misunderstood the point about zero inflation here. You just meant “large number of zeroes” as in “pretty sparse” then?. A factor of 10 isn’t that bad for something that’s more complex, and I doubt PCA speed is the bottleneck for most datasets. So not a replacement, but an enhancement. As such, it would probably live in scanpy.external except if you want to develop it within scanpy instead of as a separate package (which is possible, but would tie you to our – currently slow but we’ll get better) release cycle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814:231,bottleneck,bottleneck,231,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868#issuecomment-540691814,1,['bottleneck'],['bottleneck']
Performance,"OK, thank you for the explanation! Let us think about it. There are a couple of sanity checks running in the background, which are easy to call at the beginning of the plotting functions, for instance, if they don't cost performance. E.g., there was a standard `adata._sanitize()` call in all the plotting functions. Is it still there, @fidelram? If not, no problem... We should have a solution that essentially doesn't require writing new code. Also, what are your thoughts, @flying-sheep?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/422#issuecomment-456034330:221,perform,performance,221,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/422#issuecomment-456034330,1,['perform'],['performance']
Performance,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619:235,load,load,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650#issuecomment-499511619,2,"['load', 'scalab']","['load', 'scalability']"
Performance,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938:288,perform,performance,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303#issuecomment-441476938,1,['perform'],['performance']
Performance,"OK, will talk to Philipp about this in Person... This only concerns speeding up the reading of slow (e.g., text-based) data file formats. This might also be relevant for this discussion: foreseeing the use of partially loaded data into memory, files for backing AnnData remain something the user has to actively interact with. With the creation of an AnnData object, she/he would then have the option to create a corresponding ""backing-file"", which is internally used by AnnData to load needed parts into memory and leave parts that are not needed on the disk. At any time when there is no active write or read to the file, the file stores the current state of AnnData. I felt that both cache files and ""backing files"" should happen in a project-specific './write' directory - that is, at a location where an inexperienced user directly ""sees"" what happens and how this affects disk space. One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... But I agree true cache files might be better placed in a tmp directory. As said, will discuss this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672:219,load,loaded,219,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346776672,5,"['cache', 'load']","['cache', 'load', 'loaded']"
Performance,OSError: [WinError 123] When importing 10x data with cache=TRUE,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:53,cache,cache,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['cache'],['cache']
Performance,"Oh, I also added tests for example dataset loading since checking they worked manually was a pain. These won't run by default (they take a while, and can fail for network access reasons), but will run with `pytest --internet-tests`. Note that `test_burczynski06` will fail until this get's rebased on master. Thoughts?. Also travis failed this for `scanpy/tests/test_marker_gene_overlap.py` failing an assertion on the first time around, but passed when I triggered a new build. Not sure what's up with that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/573#issuecomment-478414881:43,load,loading,43,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/573#issuecomment-478414881,1,['load'],['loading']
Performance,"Oh, that's wonderful and exactly what I had hoped pip on the travis server would do! :smile: You mentioned that you might look into it at some point. I just didn't notice the ; ```; cache: pip; ```; line in the commit... Great that you figured this out! Test times now are really nice, in particular, as I can easily speed them up further... So cool! :smile:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732:182,cache,cache,182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/360#issuecomment-439837732,1,['cache'],['cache']
Performance,"On master (37851434b2) from the base of the repo, I haven't seen the following code finish running:. ```python; import scanpy.api as sc; adata = sc.read(""./data/pbmc3k_raw.h5ad""); %time sc.pp.downsample_counts(adata, 1500); ```. This PR implements an optimized version of the same thing, which gives:. ```python; %time sc.pp.downsample_counts(adata, 1500) ; CPU times: user 2.25 s, sys: 44.7 ms, total: 2.29 s; Wall time: 2.32 s; ```. ## What's changed. * I've rewritten the function to use numba along with fewer allocations; * Added a test for the function; * Added argument `replace`, which indicates whether subsampling should happen with replacement. ## Notes. To me, it makes more sense to sample without replacement, since for small changes in total counts you'll have more similar profiles. However, I've set the default for replacement to `True` to preserve the current behavior. Neither this or the previous method scale well with sampling depth, and it's maybe worth using a call to sample a multinomial or multivariate hypergeometric distribution instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/340:251,optimiz,optimized,251,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/340,1,['optimiz'],['optimized']
Performance,"On the definition of modularity, I did go back over some literature and saw that modularity sometimes takes multiple meanings within a paper. It can be either the [specific quality function](https://leidenalg.readthedocs.io/en/latest/reference.html#modularityvertexpartition), or when used like ""modularity optimization"" can refer to the whole class of partition optimizing algorithms (which are generic wrt quality function) like `louvain` ([I like section IV F of this paper for an overview](https://arxiv.org/abs/1608.00163v2)). @LuckyMD. > The quality score is modularity, which is optimized. Thus a ""good"" partition is a high quality score by definition. Or what are you referring to as ""good""?. I think of the quality function/score as being determined by the `partition_type`. . To me, a good partition is one that seperates data points into discrete groups which reflect some true underlying structure. I put this in quotes since it’s ill-defined, however we can tell when it’s definitely not true. A high quality score for a partitioning is just a high quality score for a partitioning. @gokceneraslan . > we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. Personally, I would just report the quality metric calculated by the quality function used. To me, the point of returning this value would be to know if the optimization went well, which is probably best measured by looking at the optimized value. This would also simplify the code a bunch. I think there's another case for trying to tell if it's a ""good"" partitioning, but I think that should be handled seperatly. > Regarding the suggestion to record partition_type.__name__, I think it's a good idea. I'd record it in the uns[uns_key]['partition_type'] though, not in quality_function. That's reasonable. Just to be sure, we'd keep it in `uns[uns_key][""params""]['partition_type']` like it is now?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688:307,optimiz,optimization,307,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529791688,5,['optimiz'],"['optimization', 'optimized', 'optimizing']"
Performance,"On the point of the notebooks... some of the tutorials should probably be updated. The analysis steps that are performed in those are quite old and would not be considered as good practice anymore. Might be worth combining this effort... (see e.g., #1338 )",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138:111,perform,performed,111,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1357#issuecomment-669090138,1,['perform'],['performed']
Performance,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136:80,queue,queued,80,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563#issuecomment-787846136,1,['queue'],['queued']
Performance,PMBC3k tutorial - issue loading saved object from .h5ad file,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2497:24,load,loading,24,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2497,1,['load'],['loading']
Performance,Pca loadings n points patch,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2075:4,load,loadings,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2075,1,['load'],['loadings']
Performance,"Perf measurements for the use case of running the HVG tests on my machine (not very accurate, and not very reminiscent of how users use it). Tests get a bit slower, real world gets faster. - scanpy master:. ```console; $ git switch master; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 71.915,07 msec task-clock:u # 14,035 CPUs utilized ( +- 9,53% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.168.035 page-faults:u # 29,496 K/sec ( +- 9,58% ); 	 191.815.791.770 cycles:u # 4,844 GHz ( +- 9,53% ) (83,37%); 	 10.610.492.234 stalled-cycles-frontend:u # 10,05% frontend cycles idle ( +- 9,44% ) (83,34%); 	 59.853.476.395 stalled-cycles-backend:u # 56,69% backend cycles idle ( +- 9,56% ) (83,32%); 	 257.750.810.841 instructions:u # 2,44 insn per cycle; 	 # 0,13 stalled cycles per insn ( +- 9,57% ) (83,33%); 	 45.773.330.764 branches:u # 1,156 G/sec ( +- 9,58% ) (83,33%); 	 1.147.567.613 branch-misses:u # 4,56% of all branches ( +- 9,54% ) (83,37%); 	; 	 5,1241 +- 0,0242 seconds time elapsed ( +- 0,47% ); ```. - this PR:. ```console; $ git switch hvg_PR_numba; $ perf stat -r 10 -B hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py; 	 Performance counter stats for 'hatch run +py=3.11 test:run -n0 scanpy/tests/test_highly_variable_genes.py' (10 runs):; 	; 	 113.085,21 msec task-clock:u # 15,789 CPUs utilized ( +- 9,56% ); 	 0 context-switches:u # 0,000 /sec; 	 0 cpu-migrations:u # 0,000 /sec; 	 1.636.606 page-faults:u # 26,373 K/sec ( +- 9,55% ); 	 310.410.832.165 cycles:u # 5,002 GHz ( +- 9,55% ) (83,35%); 	 14.117.222.045 stalled-cycles-frontend:u # 8,30% frontend cycles idle ( +- 9,46% ) (83,38%); 	 75.813.970.243 stalled-cycles-backend:u # 44,56% backend cycles idle ( +- 9,57% ) (83,35%); 	 373.047.679.552 instructions:u # 2,19 insn per cycle; 	 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266:339,Perform,Performance,339,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2612#issuecomment-1688394266,1,['Perform'],['Performance']
Performance,Performance improvements for Regress Out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2781:0,Perform,Performance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2781,1,['Perform'],['Performance']
Performance,Performance: Investigate `pp.scale` with sparse matrices,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2986:0,Perform,Performance,0,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2986,1,['Perform'],['Performance']
Performance,"Probably quite trivial to most, but was hoping someone could help out with how to create custom annotations? I'm interested in creating annotations specific for cell types that I'd define through a few marker genes, (this being aside from the leiden clustering already performed).; I'd then be interested in running such an annotation against a list of genes and creating a correlation matrix, similar to what @fidelram showed with his correlation matrix in ""dendrograms, correlation and marker genes filtering #425"".; Any help is much appreciated, thanks before hand!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/516:269,perform,performed,269,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/516,1,['perform'],['performed']
Performance,"ProgramData\Miniconda3\lib\site-packages\sklearn\base.py"", line 17, in <module>; from .utils import _IS_32BIT; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\__init__.py"", line 28, in <module>; from .fixes import np_version, parse_version; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\fixes.py"", line 20, in <module>; import scipy.stats; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\__init__.py"", line 441, in <module>; from .stats import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\stats.py"", line 43, in <module>; from . import distributions; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\distributions.py"", line 8, in <module>; from ._distn_infrastructure import (rv_discrete, rv_continuous, rv_frozen); File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\_distn_infrastructure.py"", line 24, in <module>; from scipy import optimize; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\__init__.py"", line 400, in <module>; from .optimize import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\optimize.py"", line 36, in <module>; from ._numdiff import approx_derivative; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\_numdiff.py"", line 6, in <module>; from scipy.sparse.linalg import LinearOperator; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in <module>; from .eigen import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py"", line 9, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py"", line 20, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 42, in <module>; from . import _arpack; ImportError: DLL load failed while importing _arpack: The specified procedure could not be found.; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953:2040,optimiz,optimize,2040,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953,5,"['load', 'optimiz']","['load', 'optimize']"
Performance,"Pynndescent 0.3.0 was released yesterday with support for multi-threading. This change allows scanpy to take advantage of multi-threading for computing nearest neighbors. To use it, wrap the call to scanpy in a `joblib.parallel_backend` context manager:. ```python; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=16):; sc.pp.neighbors(adata); ```. Running on the 130K dataset on a 16 core machine before the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:01:31.54); ```; and with the change:. ```; computing neighbors; using 'X_pca' with n_pcs = 50; finished (0:00:32.02); ```. A threefold speedup. (Note that there is a small [bug](https://github.com/lmcinnes/pynndescent/pull/58) in pynndescent 0.3.0, which means that `n_jobs` needs to be set explicitly. When that's fixed you'll be able to leave it out to use all cores on a machine.)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659:58,multi-thread,multi-threading,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659,2,['multi-thread'],['multi-threading']
Performance,"Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess; 54 except ImportError:; ---> 55 raise ImportError(; 56 'Please install skmisc package via `pip install --user scikit-misc'; 57 ). ImportError: Please install skmisc package via `pip install --user scikit-misc; ```; Step4: run `from skmisc.loess import loess`; ```python; from skmisc.loess import loess; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/3052125001.py in <module>; ----> 1 from skmisc.loess import loess. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 49 pp. 829--836. 1979.; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,; 53 loess_confidence_intervals, loess_anova). ImportError: DLL load failed while importing _loess: The specified module could not be found.; ```; Step5: run `import skmisc; print(skmisc.__file__)`; ```python; import skmisc; print(skmisc.__file__); C:\Users\Park_Lab\AppData\Roaming\Python\Python38\site-packages\skmisc\__init__.py; ```; Step6: due to Step4, I follow the solution (https://github.com/has2k1/scikit-misc/issues/4) to install Numpy with mkl.; ```python; (base) C:\Users\Park_Lab>conda activate Python38; (Python38) C:\Users\Park_Lab>cd Downloads/; (Python38) C:\Users\Park_Lab\Downloads>pip install numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Processing c:\users\park_lab\downloads\numpy-1.21.5+mkl-cp38-cp38-win_amd64.whl; Installing collected packages: numpy; Attempting uninstall: numpy; Found existing installation: numpy 1.21.5; Uninstalling numpy-1.21.5:; Successfully uninstalled numpy-1.21.5; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:3858,load,load,3858,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['load'],['load']
Performance,"Recently I installed scanpy 0.4. However, with this new version I could not correctly load result files generated by an old version (v0.2.8). In particular, I could not load the old add_keys as uni_keys. Any suggestions?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/56:86,load,load,86,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/56,2,['load'],['load']
Performance,Run static analysis concurrently,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/846:20,concurren,concurrently,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846,1,['concurren'],['concurrently']
Performance,"Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/846:21,concurren,concurrently,21,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846,1,['concurren'],['concurrently']
Performance,"Scanpy 1.5.0; Out of curiousity I set min_fold_change to different values but it didn't work. Even when I set min_fold_change=100 it doesn't shorten the gene list. So is it because the filters are working in an OR logic? If that's the case, I don't think the default values for each filter should be the ones you set - they should be a very harsh condition to make sure customer-specified parameters are the bottleneck.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1447:408,bottleneck,bottleneck,408,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1447,1,['bottleneck'],['bottleneck']
Performance,"Scanpy stores the loadings for each PC in the `adata.varm['PCs']` slot. The order is the same is `obs_names`, but you can use pandas functions like `sort_values` to look at the top genes or do something like `np.argsort` or `scipy.stats.rankdata` on the columns (the PCs) to get their ranks. ```python; import scanpy as sc; import numpy as np. pbmc = sc.datasets.pbmc68k_reduced(); sc.tl.pca(pbmc, svd_solver='arpack', random_state=0); # Get loadings for each gene for each PC; df_loadings = pd.DataFrame(pbmc.varm['PCs'], index=pbmc.var_names); # get rank of each loading for each PC; df_rankings = pd.DataFrame((-1 * df_loadings.values).argsort(0).argsort(0), index=df_loadings.index, columns=df_loadings.columns); # c.f. with df_loadings.apply(scipy.stats.rankdata, axis=0); # evaluate ; print(""Top loadings for PC1...""); print(df_loadings[0].sort_values().tail()); print(""Rank of PTPRCAP for first 5 PCs...""); print(df_rankings.loc[""PTPRCAP""].head()); sc.pl.pca_loadings(pbmc). # alternatively, you can do SVD or PCA manually with scipy, numpy, sklearn, etc.; # from sklearn.decomposition import PCA; # pc = PCA(n_components=50, svd_solver='arpack', random_state=0).fit(pbmc.X); # pc.components_.T has the loadings; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582:18,load,loadings,18,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836#issuecomment-539649582,5,['load'],"['loading', 'loadings']"
Performance,"See [colab notebook](https://colab.research.google.com/drive/17m_3IiZApxpKUHluieWK6C7sGj2XGgTb?usp=sharing). With random initialization it's about 10x faster than UMAP on this system. The quadratic init (default) is as fast as UMAP, but there's an opportunity to optimize that code to use the GPU.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051102663:263,optimiz,optimize,263,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2154#issuecomment-1051102663,1,['optimiz'],['optimize']
Performance,"Should be possible to turn the y ticks legends on. But I just tested it and didn't work. I will try to fix it. The syntax is:; ```PYTHON; sc.pl.stacked_violin(adata,marker_genes,groupby='louvain', return_fig=True).style(yticklabels=True,row_palette='muted').show(); ```. `style` needs to be used to tune the graphical parameters to avoid overcrowding the parameters list. But I am open to have a discussion on what the users think is best. Documentation is here: https://scanpy.readthedocs.io/en/latest/api/scanpy.pl.DotPlot.style.html#scanpy.pl.DotPlot.style",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536:299,tune,tune,299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1321#issuecomment-666170536,1,['tune'],['tune']
Performance,Show PCA loadings for genes with lowest loadings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/805:9,load,loadings,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805,2,['load'],['loadings']
Performance,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/263#issuecomment-422099872:223,load,loading,223,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263#issuecomment-422099872,1,['load'],['loading']
Performance,"So I just tried to install the package from the master branch by running. ```; pip install git+https://github.com/theislab/scanpy.git; ```; (by the way, you can update the Installation section of the README.md because the above line is equivalent to cloning the repository and running pip install on that.) The installation failed with; ```; Collecting git+https://github.com/theislab/scanpy.git; Cloning https://github.com/theislab/scanpy.git to /tmp/pip-203inirx-build; Complete output from command python setup.py egg_info:; running egg_info; creating pip-egg-info/scanpy.egg-info; writing pip-egg-info/scanpy.egg-info/PKG-INFO; writing dependency_links to pip-egg-info/scanpy.egg-info/dependency_links.txt; writing entry points to pip-egg-info/scanpy.egg-info/entry_points.txt; writing requirements to pip-egg-info/scanpy.egg-info/requires.txt; writing top-level names to pip-egg-info/scanpy.egg-info/top_level.txt; writing manifest file 'pip-egg-info/scanpy.egg-info/SOURCES.txt'; warning: manifest_maker: standard file '-c' not found; ; error: package directory 'scanpy/exs' does not exist; ; ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-203inirx-build/; The command '/bin/sh -c pip install --upgrade --no-cache-dir git+https://github.com/theislab/scanpy.git' returned a non-zero code: 1; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715:1281,cache,cache-dir,1281,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/7#issuecomment-284343715,1,['cache'],['cache-dir']
Performance,"So assuming that we are only interested in downsampling, then I'd say `NearMiss` and related are straightforward and scalable (just need to compute a kmeans whcih is really fast)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030:117,scalab,scalable,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1043141030,1,['scalab'],['scalable']
Performance,"So the user experience will be:. 1. They’ll go through the example notebooks where they’ll learn how to download data. → The notebooks should mention where to configure the cache directory. 2. They’ll download data, probably not paying attention to the output immediately. → We should mention where the data are every time they get loaded (Either from the web or from the cache dir. Maybe even mention that the location can be configured in settings?). 3. Maybe they’ll eventually look at the settings module in the online documentation. → We should explain there that the default uses appdirs, and what directories that maps to on different OSs. 4. A user in some misconfigured HPC environment who manages to not see any of the warnings will end up filling heir home directory by downloading data to the default directory (Is that possible or will there be no error?). → We should mention that the directoy can be globally configured for all libraries and applications using XDG_CACHE_HOME, and for scanpy using `scanpy settings cachedir ....` or `scanpy.settings.cachedir = ....`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702:173,cache,cache,173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477119702,5,"['cache', 'load']","['cache', 'cachedir', 'loaded']"
Performance,So we still need to perform the following work around:; 1) change file name of file `tissue_positions.csv` to `tissue_positions_list.csv`; 2) delete the header in the file. Right?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607268186:20,perform,perform,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2499#issuecomment-1607268186,1,['perform'],['perform']
Performance,"So, we could also not early load `scanpy.testing._pytest` or load `pytest-cov` first?. I would like to keep the `xdist` support and use a similar interface.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175:28,load,load,28,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956920175,2,['load'],['load']
Performance,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution; - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. ; - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png); **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336:417,perform,performed,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498153336,1,['perform'],['performed']
Performance,"Sometimes, it can happen when downloading 10x files from e.g. GEO that they are not organized in; folders but instead, they have a sample-specific prefix. E.g. . ```console; sturm@zeus [SSH] processed % ll; total 156M; -rw-r--r-- 1 dbadmin dbadmin 29K May 21 2018 GSM3148575_BC09_TUMOR1_barcodes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 259K May 21 2018 GSM3148575_BC09_TUMOR1_genes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 34M May 21 2018 GSM3148575_BC09_TUMOR1_matrix.mtx.gz; -rw-r--r-- 1 dbadmin dbadmin 28K May 21 2018 GSM3148576_BC09_TUMOR2_barcodes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 259K May 21 2018 GSM3148576_BC09_TUMOR2_genes.tsv.gz; -rw-r--r-- 1 dbadmin dbadmin 33M May 21 2018 GSM3148576_BC09_TUMOR2_matrix.mtx.gz; ```. This PR adds a keyword argument `prefix` to `read_10x_mtx` which enables to load these files ; without manual renaming and moving, e.g. ; ```python; adata = sc.read_10x_mtx(""path/to/files"", prefix=""GSM3148575_BC09_TUMOR1_""); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1250:802,load,load,802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1250,1,['load'],['load']
Performance,"Sorry I missed the logging. I also didn't see the `sc.settings.logfile` option, which obviously makes absolute sense and is convenient to have persistent records when working interactively with anndata objects. I guess just more consistent logging across scanpy functions would be really great. Something like `sc.logging.get_operations(adata_id=id(adata))` would also be super cool, but would it be able to retrieve records of operations performed within rounds of object serialization?. e.g.:; ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""). adata = sc.read(""./cache/01_simple_process.h5ad""); sc.pp.scale(adata); adata.write(""./cache/01_simple_process.h5ad""); print(sc.logging.get_operations(adata_id=id(adata))); ```; would probably forget the first set of operations?; ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""scale"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```; I guess one solution would be to follow the path of ids up the log to retrieve all which seems doable, so this could be a good system. The one thing this wouldn't cover though is persistence within the h5ad object itself. This would be useful in the case of sharing the object with someone for example. As I mentioned before, I'm not sure this is a widespread use case yet, but could be useful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464691691:439,perform,performed,439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464691691,5,"['cache', 'perform']","['cache', 'performed']"
Performance,"Sorry about the wait, had to focus on getting the last release out. Now we can do new features!. > But the warning IMHO should not convey the message ""Do not do this!"". In my mind, it should convey the message ""What you are computing is not exactly t-SNE, but it is close enough to t-SNE that you can ignore this message. That sounds appropriate. > But we will have to control them anyway... Your suggested solution also controls them: namely, symmetrizes and normalizes. I think normalization is a ""lighter touch"" than binarization. To me, the alternative would be to error for non-normalized data since the optimization won't converge properly. Not knowing too much about the internals of tsne, is a symmetric graph necessary? If it's not, then I'd be fine with not doing that. Exactly how the option to do this is provided to users could take some consideration. I think it would be clean and composable to have graph weighting options separate from embedding layout options, but considering `tsne` has restrictions on graph weights there may have to be some exception here. Perhaps there needs to be a `weights` option on `tsne` which allows normalization, binarization, or just erroring if the passed graph doesn't have correct weighting. -------------------. From my perspective, what we have to gain here is:. * More efficient TSNE by default; * Consolidate implementation to a single well maintained library; * More flexibility in how tsne is computed. > Scanpy is in a unique position to offer people t-SNE with k=15 binary affinities as a convenient, faster, UMAP-independent, and nearly equivalent replacement for k=90, perplexity=30 affinities. I'm happy to have this be an option. I'm less comfortable with something like this being the ""recommended path"", since not using perplexity weights seems non-standard. -------------------. In general, are we agreed on these points?. * `tsne` should allow weights to be passed through (whether perplexity based, or not); * There should be a warn",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636:609,optimiz,optimization,609,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1561#issuecomment-773051636,1,['optimiz'],['optimization']
Performance,"Sorry for the late response, completley forgot to post my response here. @Fougere87, did that whitening issue occur with picard as well? I saw that with sklearn. I think we could get around that by whitening ourselves with ARPACK. Picard and sklearn look pretty similar to me in a quick comparison. Below are top 16/30 components (ranked by Geary's C, autocorrelation on the connectivity graph) cell loadings on the pbmc3k dataset. The umap and connectivity matrix here were computed on top of a PCA – which I should maybe do differently. However I think the results are similar enough that it's probably not of consequence. <details>; <summary> sklearn FastICA </summary>. ![sklearn_ica](https://user-images.githubusercontent.com/8238804/68647787-d53a4d80-0572-11ea-8b95-cde9122824f1.png). </details>. <details>; <summary> picard ICA </summary>. ![picard_ica2](https://user-images.githubusercontent.com/8238804/68647808-e5eac380-0572-11ea-8485-71a770849cc9.png). </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/767#issuecomment-552756716:400,load,loadings,400,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/767#issuecomment-552756716,1,['load'],['loadings']
Performance,"Sorry for the long delay – I was away on a retreat. By the way, congrats on the spatial letter!. I'm not sure I understand why you'd report the standard modularity if the partitioning was done with multi resolution modularity or some other quality function. To me, this makes the metric being pretty disconnected from the computation that was run. It seems likely that there could be non-proportional relationships between the whatever quality function is used and unscaled modularity. For example there could be a case where: partitioning A has higher unscaled modularity than partitioning B, but B has higher multi resolution modularity quality with resolution .8 than A. If the multi resolution modularity is what was run, I think the logging should reflect that run resulted in a ""better"" optimization. Separately, if it's meant to assess the quality of the clustering, why not calculate something like silhouette? From my perspective, it would be because that's separate enough from the process of clustering that it should be run separately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547:793,optimiz,optimization,793,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-531683547,1,['optimiz'],['optimization']
Performance,"Sounds good to me. How are you thinking of handling reproducibility w.r.t. random seeds?. To me, the best solution here is to make it easy to do small multiples for categorical plots like this, but that's a big change in the kind of plot being made. . As an aside, I've also tried coloring the pixel by which group showed up the most under it, but this can look weird (less so, if density is used to calculate the alpha level). ![image](https://user-images.githubusercontent.com/8238804/83601513-16985600-a5b4-11ea-8f0d-68a15a3fbf96.png). <details>; <summary> Example without accounting for density </summary>. ![image](https://user-images.githubusercontent.com/8238804/83601587-362f7e80-a5b4-11ea-8e1a-b1bc20948504.png). </details>. <details>; <summary> Snippet to reproduce </summary>. ```python; import datashader as ds; from datashader import transfer_functions as tf; import scanpy as sc; import numpy as np; import xarray as xr. # Where you load your AnnData, I was using a preprocessed set of 1.3 million mouse braincells. df = sc.get.obs_df(; adata,; [""Sox17"", ""louvain""],; obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]; ); louvain_colors = dict(; zip(; adata.obs[""louvain""].cat.categories, ; adata.uns[""louvain_colors""]; ); ). pts = (; ds.Canvas(500, 500); .points(df, ""X_umap-0"", ""X_umap-1"", agg=ds.count_cat(""louvain"")); ). newpts = xr.zeros_like(pts); newpts[:, :, pts.argmax(dim=""louvain"")] = pts.sum(dim=""louvain""); tf.shade(newpts, color_key=louvain_colors); ```. </details>. What datashader does by default is takes the average of the RGB values for the categories under a pixel, weighted by number of samples, and calculates an alpha level based on the number of samples present. This looks like:. ![image](https://user-images.githubusercontent.com/8238804/83599943-c9ff4b80-a5b0-11ea-8acf-3cfc640a9abb.png). <details>; <summary> Addendum to previous snippet for plotting this </summary>. ```python; tf.shade(pts, color_key=louvain_colors); ```; </details>. </details>. I've also been wond",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155:947,load,load,947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1263#issuecomment-637970155,1,['load'],['load']
Performance,"Starting from the end: I think if you could upload the clustering results from the Scanpy paper / PAGA preprint to the scanpy github repo, it would be great. I still have the dropbox link of course, but I guess in the long run it's better if that file was located here and linked from the https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells page. The issue with 1 cell missing was because I did not specify `header=None` when loading it with Pandas :) So my error, not yours. The file is correct as is. That said, I am worried about the influence the random seed in randomized PCA seems to give in this case. Let me show you how it looks:. ![mln-tsne-clustering-comparison](https://user-images.githubusercontent.com/8970231/47555195-71af9480-d90b-11e8-85fb-a3e8dcb7a66f.png). I would be fine with some cells getting into other clusters depending on the random seed, and it would even be okay if small clusters changed their identities, but what we see here is a very drastic change of the cluster structure. Are you sure that the only difference is the randomized PCA outcome? Can it be that some of the default parameters in `sc.pp.recipe_zheng17`, `sc.pp.neighbors`, or `sc.tl.louvain` changed since when you ran the clustering? The scanpy code I posted above is the full code I used, and I ran it yesterday after updating scanpy via pip. BTW, the visualization above is taken from https://www.biorxiv.org/content/early/2018/10/25/453449 which we posted yesterday. Any comments very welcome! I hope you don't mind being thanked in the acknowledgements!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926:464,load,loading,464,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/325#issuecomment-433334926,1,['load'],['loading']
Performance,Submodular optimization using apricot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2862:11,optimiz,optimization,11,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2862,1,['optimiz'],['optimization']
Performance,"Sure thing. I've dumped a couple of example entries into a numpy file and attached it below as a zip. You can replicate the behaviour with the following code. ```; import numpy; import scanpy as sc; import anndata as ad. features = np.load(""example_features.npy""). #perform scaling in sklearn; from sklearn.preprocessing import StandardScaler; scaler = StandardScaler(); scaled = scaler.fit_transform(features.copy()). print(np.isnan(scaled).sum()). #perform scaling in scanpy with the default settings; adata = ad.AnnData(features); sc.pp.scale(adata). print(np.isnan(adata.X).sum()); ```. For me this results in: . <img width=""477"" alt=""Screenshot 2024-06-26 at 16 08 41"" src=""https://github.com/scverse/scanpy/assets/15019107/806d4b70-faf2-4dce-84fc-575cd86c21f6"">. [example_features.npy.zip](https://github.com/user-attachments/files/15990483/example_features.npy.zip)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191814375:235,load,load,235,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2163#issuecomment-2191814375,3,"['load', 'perform']","['load', 'perform']"
Performance,"Sure, and I'm not against supporting special cases! Could you please explain the setup?. Do you have a user home? Is there a canonical cache directory outside of the user home? Is there a way to detect that we are on such a system or a environment variable pointing to the canonical cache directory?. Some systems are strange. We should be nice and support those systems while still doing the correct thing by default. We shouldn't do the wrong thing by default to accommodate strange cases.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606:135,cache,cache,135,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476680606,2,['cache'],['cache']
Performance,"Sure, don't see how that's mutually exclusive with having a package. We have a huge problem in the ecosystem right now that it's not straightforward to load data from non rna-seq experiments (no clear guidance where to go etc)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361:152,load,load,152,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1109903361,1,['load'],['load']
Performance,"TODOs:. 1. Figure out why some tests are passing when they shouldn't (hence why I pushed the branch, curious about CI). UPDATE: `tol` for `matplotlib.testing.compare.compare_images` is too high for a sparse-ish plot like `rank_genes_groups`. This is somewhat worrying so will need to be amended. Other than that, changed plotting outputs make sense so this should be resolved.; 2. Check with scanpy tutorials to see what needs to be changed there as well, if anything (if needed, the two PRs should be merged in tandem). The following use leiden in some capacity:; a. https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html; b. https://scanpy-tutorials.readthedocs.io/en/latest/plotting/core.html; c. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/basic-analysis.html; d. https://scanpy-tutorials.readthedocs.io/en/latest/spatial/integration-scanorama.html; 3. Do a large dataset test - check NMI for accuracy of the new default against the old one, check speed to confirm what we're doing makes sense (although this was covered, it seems, in #1053), and scalability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210:1072,scalab,scalability,1072,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2815#issuecomment-1894255210,1,['scalab'],['scalability']
Performance,"Thank you for developing the method, I'm looking forward to being able to use it. One thing that the deviances did was perform a chi-square test on the obtained values, with degrees of freedom based on the number of cells. I was fond of that as it translated into a data-driven cutoff for feature selection rather than requiring some number of top genes. Is there a chance of something similar showing up here? Apologies if this is not the place to ask this, but I'd be even more likely to switch over if I could have the option to avoid the parameterisation that tends to come with HVG identification.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063:119,perform,perform,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-872954063,1,['perform'],['perform']
Performance,"Thank you for the PR! It looks good to me. Also the function underlying, as far as I can tell. If there are performance problems, we can still address them in an update. 80 character lines would be nice also for the docstring. Then I could see whether they make sense. I'm seeing this on a 13-inch screen and the docstring looks like a mess through that. ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324:108,perform,performance,108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/382#issuecomment-443398324,1,['perform'],['performance']
Performance,"Thank you so much for fielding so many of the issues, @LuckyMD! :smile:. Can we elaborate a bit further on this one, though? For simple two-group comparisons, `rank_genes_groups` with `method='wilcoxon'` (Wilcoxon-Rank-Sum/Mann-Whitney U test) should be a legit choice, shouldn't it? It's used in many of this year's Nature, Cell and Science single-cell papers, it's the default test of Seurat (https://satijalab.org/seurat/de_vignette.html) and several people reported that it performs well in [Sonison & Robinson, Nat Meth (2018)](https://doi.org/10.1038/nmeth.4612). So, I don't think one needs to encourage people to immediately go to the great and powerful MAST, limma and DESeq2. Can you point me to a reference that shows that a Wilcoxon-Rank-Sum test is less _sensitive_? How is this even a useful statement if you don't talk about the false positives you buy in? We should look at an AUC that scans different p-values, right? A bit more than a year ago, @tcallies and I had a full paper draft discussing AUCs for marker gene detection formulated as a classification problem, but we never finished it. In the general setting, it's not at all straightforward to make the evaluation a well-defined problem and other people will for sure have done a better job. Unfortunately, I have never fully caught up with the literature, I fear...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981:478,perform,performs,478,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/397#issuecomment-447598981,1,['perform'],['performs']
Performance,"Thank you! I think we should only use `@njit` anyway. I don’t understand why `@jit` exists if it can silently fail. Could you please elaborate on the following?. > The ideal solution is it becoming possible to have numba functions which are both parallel and cached. So am I deducing correctly that numba can parallelize code and usually caches functions to reduce compilation times, but can’t do both for the same function yet?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211:259,cache,cached,259,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460938211,2,['cache'],"['cached', 'caches']"
Performance,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/670#issuecomment-498057572:249,optimiz,optimizing,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670#issuecomment-498057572,1,['optimiz'],['optimizing']
Performance,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545:674,bottleneck,bottleneck,674,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659#issuecomment-495256545,1,['bottleneck'],['bottleneck']
Performance,"Thanks fellas, it worked. . ```py; pp.filter_genes(adata, min_counts=1); ``` ; Weird, because the other datasets, when loaded fresh have the same pattern:; ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True ; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. Before and after removal of cell types. Yet they still regress out fine. Anyways, big help. . Cheers",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412237509:119,load,loaded,119,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412237509,1,['load'],['loaded']
Performance,"Thanks for diving in deeper. I would agree with you that settig the `max_mean` is not a great idea. I have never done this in any of my analyses. As mentioned, this tutorial was a copy of an early Seurat tutorial and does not represent a recommendation on what is the best way to perform a single-cell analysis. Instead it is designed to showcase the tools that exist in Scanpy. Indeed Seurat has updated its tutorials since then, but we have not. This should probably be considered, but at the moment it would be at the end of a long to-do list. . Instead, our recommendation for how a single-cell analysis workflow should be structured would be the notebook in the best-practices tutorial [here](https://github.com/theislab/single-cell-tutorial). This should probably be linked on the scanpy front page, although it doesn't only include Scanpy analysis tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151:280,perform,perform,280,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1338#issuecomment-665745151,1,['perform'],['perform']
Performance,Thanks for getting back @esrice! . I think I see now -- does that mean I should multiply the `X_pca_harmony` by the _original_ PC loadings to get the imputed counts?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240114021:130,load,loadings,130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314#issuecomment-1240114021,1,['load'],['loadings']
Performance,"Thanks for opening an issue!. Many of the function in scanpy do not support being applied on a backed anndata. `highly_variable_genes` hasn't had support for out of core computation implemented, so it errors. Better out of core support is something we're working for. Is it possible to load `X` into memory here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147#issuecomment-1049155583:286,load,load,286,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147#issuecomment-1049155583,1,['load'],['load']
Performance,"Thanks for taking a look at this @giovp!. `@cache` is new in 3.8, but the implementation is:. ```; def cache(user_function, /):; 'Simple lightweight unbounded cache. Sometimes called ""memoize"".'; return lru_cache(maxsize=None)(user_function); ```. Also I don't think it returns a copy, so you would need to handle that. I've got a branch which implements cached datasets for testing as:. ```python; from functools import wraps; import scanpy as sc. def cached_dataset(func):; store = []; @wraps(func); def wrapper():; if len(store) < 1:; store.append(func()); return store[0].copy(); return wrapper. pbmc3k = cached_dataset(sc.datasets.pbmc3k); pbmc68k_reduced = cached_dataset(sc.datasets.pbmc68k_reduced); pbmc3k_processed = cached_dataset(sc.datasets.pbmc3k_processed); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241:44,cache,cache,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-1050030241,4,['cache'],"['cache', 'cached']"
Performance,"Thanks for the docker image. I'll take a look at that when I can. I thought I had pinpointed the error via print statements in the tests and fixed it, but it's back now and when I put print statements the error is gone :/. Might try to experiment with Travis a bit by just pushing to #583. I don't know much about Travis, so not sure how cache comes into play here... or how Travis builds work. I guess it'll be a bit of reading later. Haven't tried forking to check what happens... good idea.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/580#issuecomment-478996906:338,cache,cache,338,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/580#issuecomment-478996906,1,['cache'],['cache']
Performance,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020:64,bottleneck,bottleneck,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277#issuecomment-427379020,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimization']"
Performance,"Thanks for the update! I'm not sure if we'll be able to migrate very easily though. We allow users to [choose the quality function](https://scanpy.readthedocs.io/en/stable/api/scanpy.tl.leiden.html#scanpy.tl.leiden), and use the `leidenalg.RBConfigurationVertexPartition` as the default. We've also been considering using the multiplex partitioning methods. * Do you think the performance improvements will also be implemented in leidenalg?; * Is modularity with a resolution parameter equivalent to `leidenalg.RBConfigurationVertexPartition`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992:377,perform,performance,377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586667992,1,['perform'],['performance']
Performance,"Thanks for your input! I updated my container using your versions, @ivirshup. The issue persists. I updated the example to highlight that pca sometimes is reproducible and sometimes not. ```python; %env PYTHONHASHSEED=0; import numpy as np; np.random.seed(42); import scanpy as sc. adata = sc.datasets.pbmc3k_processed(). equal = []; for i in range(10):; adata1 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'); adata2 = sc.pp.pca(adata, copy=True, random_state=42, svd_solver='arpack'). equal.append(np.array_equal(adata1.obsm['X_pca'], adata2.obsm['X_pca'])). np.sum(equal) / len(equal); ```; Output:; ```pytd; env: PYTHONHASHSEED=0. 0.6; ```; In this case 6 of the 10 runs produced identical results. #### My updated environment. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.2; numpy 1.20.1; packaging 20.9; pandas 1.2.2; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.1; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.24.1; sniffio 1.2.0; soc",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453:902,bottleneck,bottleneck,902,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749#issuecomment-806516453,1,['bottleneck'],['bottleneck']
Performance,"Thanks for your reply! (And no worries - mine is even later). . I see your point about ecosystem vs. external. My main qualm about ecosystem (at least in its current form) is that it's just links to external projects that happen to use scanpy, and the burden of downloading these projects, learning their unique syntax, and seeing how they apply to the scanpy project at hand is off-loaded to the user. The main reason I have pushed for inclusion in external is the convenience of being able to call the function with a single scanpy command, in a format the user is already very familiar with. On the other hand, I do see your point about code maintenance and syncing between my project and scanpy. Changes in my shannonca project might necessitate changes in the wrapper function. That said, since my wrapper is very agnostic to the underlying methods used, I would hope this wouldn't have to happen very often (basically, it just controls where the inputs are found and where the outputs are deposited. This wouldn't change unless scanpy's architecture did). However, as currently written, the documentation may have to change more frequently since it refers to specific function arguments used in my package. For now, I am willing to open a new pull request into ecosystem (if that is the correct workflow) and you can feel free to close this issue. For future releases, if you want to combine the convenience of external with the low maintenance burden of ecosystem, you might consider allowing external modules to ""outsource"" their documentation. So in scanpy's documentation, a function F under external would simply have the format sc.external.tl.F(adata, **kwargs), where **kwargs is passed directly to a method maintained by the tool developer, with a link to a docstring in the external repository. I would happily make this for shannonca as a proof of concept, if you think it's worth trying.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808:383,load,loaded,383,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1780#issuecomment-911791808,1,['load'],['loaded']
Performance,"Thanks to @yueqiw for the confidence. :). @falexwolf We have no issue with our package being included here, but we wouldn't be able to create a custom API for your package right now, if that's what you were suggesting?. Given my quick overview of your package, two things you should note:; 1. Our method expects the input count matrix to be from a single run. Performance takes a non-trivial hit on aggregate datasets.; 2. Currently, our runtime will not satisfy those impressive metrics cited in your 1.0 announcement. This may possibly change in the future, as we haven't focused on hard optimization yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838:360,Perform,Performance,360,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-398857838,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"The ""outs"" from `spaceranger count` v2 differ from v1. Specifically, `tissue_positions_list.csv` has been renamed `tissue_positions.csv` and now has headers. Fortunately the column names exactly match those used by `scanpy`. See https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/spatial (search for `tissue_positions.csv`). This PR adjusts `sc.read_visium()` so that it can load the outputs of `spaceranger count` v2. The API / method signature is unchanged, so no changes to the documentation are required. The version is inferred from the presence or otherwise of the old `tissue_positions_list.csv` (implying v1) and then the files in `spatial/` are handled accordingly, including differential handling of headers / column names. @linhuawang; @sopvdl; @TaopengWang",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2458:413,load,load,413,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2458,1,['load'],['load']
Performance,"The Leiden algorithm from `igraph` only implements two quality functions: CPM and modularity. Both indeed use resolution parameters, and are the equivalent of `CPMVertexPartition` and `RBConfigurationVertexPartition`. The reason the `igraph` implementation is more performant than `leidenalg` is exactly because it does not provide other quality functions or a multiplex approach. If you need the other quality functions (such as significance or surprise), you will need to stick to `leidenalg`. So if you need those other quality functions, or a multiplex approach, you might still want to stick to the `leidenalg` package after all.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271:265,perform,performant,265,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-586676271,1,['perform'],['performant']
Performance,The `to_dense` function only works for csr matrices. I think we need another kernel that handles `csc` or just `.T` the resulting array if csc. I also get performance warnings about the matmul in the numba_kernel. I'll investigate this further.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205859591:155,perform,performance,155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205859591,1,['perform'],['performance']
Performance,"The biggest ones are in order:. 1. [ ] `numba`: Hard to defer. We’d have to create our own `jit` decorator returning a callable object that numba-compiles and caches the real function on its first invocation; 2. ~~`pandas`~~: Used all over the place, not feasible to defer; 3. [x] `sklearn.metrics`: Easy to defer I think, let’s start with this.; 4. [ ] `matplotlib.pyplot`: Shouldn’t be used in a library at all. It exists to import the kitchen sink in order to be low-friction for interactive use. Hard to do since we rely on it a lot, but we should do it.; 5. [x] `networkx`: Used in DPT, paga and plotting. Pretty easy. We use pandas all over the place, and it’s hard to defer loading numba as it works with decorators. /edit: shaved off another 2/5 in a7729bc61ac569a718075edb4466852b0b4a696a via `sklearn.metrics`, `scipy.stats`, and `networkx`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433:159,cache,caches,159,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/756#issuecomment-516324433,2,"['cache', 'load']","['caches', 'loading']"
Performance,"The computation should be sparse. Otherwise I'd be fine with the user doing it themselves, but the main value add here would be lower memory overhead/ optimized implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2898#issuecomment-1981827424:151,optimiz,optimized,151,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2898#issuecomment-1981827424,1,['optimiz'],['optimized']
Performance,"The last test. In an environment with scanpy (1.9.3) and leidenalg installed, I can get reproducible runs for the code above. If I install the following packages:. ```; conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia; ```; Then I start to have problems with reproducibility. I have no idea how this is possible but perhaps one clue is that torch is being reported in the package versions even though I am not loading it or using scvi-tools. Probably this is related to the latest update in anndata. <details><summary>Details</summary>; <p>. -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 9.4.0; asttokens NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; cffi 1.15.1; colorama 0.4.6; comm 0.1.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; executing 1.2.0; gmpy2 2.1.2; h5py 3.8.0; hypergeom_ufunc NA; igraph 0.10.3; invgauss_ufunc NA; ipykernel 6.22.0; jedi 0.18.2; joblib 1.2.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; matplotlib 3.7.1; matplotlib_inline 0.1.6; mpl_toolkits NA; mpmath 1.3.0; natsort 8.3.1; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numpy 1.23.5; nvfuser NA; packaging 23.1; pandas 2.0.1; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.5.0; prompt_toolkit 3.0.38; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.10; pyparsing 3.0.9; pytz 2023.3; scipy 1.10.1; session_info 1.0.0; setuptools 67.7.2; six 1.16.0; skewnorm_ufunc NA; sklearn 1.2.2; stack_data 0.6.2; sympy 1.11.1; texttable 1.6.7; threadpoolctl 3.1.0; torch 2.0.0; tornado 6.3; tqdm 4.65.0; traitlets 5.9.0; typing_extensions NA; umap 0.5.3; wcwidth 0.2.6; zmq 25.0.2; zoneinfo NA; -----; IPython 8.13.1; jupyter_client 8.2.0; jupyter_core 5.3.0; -----; Python 3.10.10 | packaged by conda-forge | (main, Mar ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993:445,load,loading,445,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1533334993,1,['load'],['loading']
Performance,"The latest BED docker image is 1.62 GB, which is not that big for an ID mapping DB I reckon (with the docker environment of course). You can also build your own, more limited versions of the database. There are instructions for this in the [github](https://github.com/patzaw/bed). BED memory usage I haven't really checked. Server side it's using 1 GB idly at the moment, no idea how that changes at peak usage. It has a client-side cache system to speed up multiple look-ups, but that can be turned off to limit client-side memory usage. And yes, as @flying-sheep said it's really easy to use for mapping between biological entities and even organisms as well. For transcript locations, you may need to use a separate file that contains the locations which will probably be annotated with RefSeq IDs. You can get to the RefSeq IDs easily though. It was actually designed for company use... but I think Thompson-Reuters (now Clarivate analytics) or UCB aren't interested in hosting public servers ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/242#issuecomment-460942661:433,cache,cache,433,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/242#issuecomment-460942661,1,['cache'],['cache']
Performance,"The original set of default parameters used by SAM were geared more towards smaller datasets. As scRNAseq throughput is ever-increasing, I tweaked the default parameters to be better suited for large datasets.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1540:106,throughput,throughput,106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1540,1,['throughput'],['throughput']
Performance,The previous `sc.pl.umap` etc. had an option to export legend positions via 'on data export'. We need a solution in the docs... Presumably just by exposing the cached positions to the user.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/335:160,cache,cached,160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/335,1,['cache'],['cached']
Performance,"The reason for this directory is just project-specific configuration. Here, https://github.com/theislab/scanpy/commit/7a57fd4cf140dc4b2ffca7ef0651a355c74f0122, I removed the creation of this directory. Nonetheless, it's true that Scanpy, when you tell it to cache a file, it wants to create a directory (by default './write/') for it. Tell me if this is a problem for you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918:258,cache,cache,258,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346318918,1,['cache'],['cache']
Performance,"The scanpy install directory is super wrong, as it’s not writable for many people. There’s exactly one correct way of determining a global place for cache* files like this: [`appdirs.user_cache_dir(...)`](https://pypi.org/project/appdirs/). Alex and me talked in the past and decided for a visible directory in the working directory. I’d be up for changing it to `user_cache_dir(…)` for the data. *the data are cache files since reexccuting their function after deleting the files will redownload them without loss of information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564:149,cache,cache,149,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-476509564,2,['cache'],['cache']
Performance,"The weirdest thing is that if I write this adata object to an h5ad file with adata.write(""temp.h5ad""), load it from there and run the same command, it works. . I wonder if this indicates some issue with the .obs object or some version issue...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/728#issuecomment-508524376:103,load,load,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/728#issuecomment-508524376,1,['load'],['load']
Performance,Then it's probably a case of having an old python3. I loaded up an environment where I have python 3.6.9 and the newest version it saw was 0.7.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650:54,load,loaded,54,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1378452650,1,['load'],['loaded']
Performance,There are quite a few questions that need to be answered first:. 1. What's the performance difference here?; 2. Numpy only uses its internal code in the off chance that `#ifndef HAVE_LOG1P`: In which circumstances does accuracy suffer when using that naive implementation?; 3. Does numba.vectorize handle sparse matrices?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/929#issuecomment-558069573:79,perform,performance,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/929#issuecomment-558069573,1,['perform'],['performance']
Performance,"There is a function called `reticulate::install_miniconda()` which [isn't mentioned in the documentation ](https://rstudio.github.io/reticulate/articles/python_packages.html) that solves this problem. ```; install_miniconda(""scanpy""); repl_python(); ```. ```; import scanpy as sc # load successfully now; exit # close the Python interpreter ; ```. Here's my `sessionInfo()` in-case it helps anyone:. ```; R version 3.6.2 (2019-12-12); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Ubuntu 18.04.3 LTS. Matrix products: default; BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1; LAPACK: /home/tsundoku/.local/share/r-miniconda/envs/r-reticulate/lib/libmkl_rt.so. locale:; [1] LC_CTYPE=en_CA.UTF-8 LC_NUMERIC=C LC_TIME=en_CA.UTF-8 LC_COLLATE=en_CA.UTF-8 LC_MONETARY=en_CA.UTF-8 ; [6] LC_MESSAGES=en_CA.UTF-8 LC_PAPER=en_CA.UTF-8 LC_NAME=C LC_ADDRESS=C LC_TELEPHONE=C ; [11] LC_MEASUREMENT=en_CA.UTF-8 LC_IDENTIFICATION=C . attached base packages:; [1] stats graphics grDevices utils datasets methods base . other attached packages:; [1] reticulate_1.14. loaded via a namespace (and not attached):; [1] BiocManager_1.30.10 compiler_3.6.2 tools_3.6.2 rappdirs_0.3.1 Rcpp_1.0.3 jsonlite_1.6 packrat_0.5.0 ; [8] png_0.1-7 ; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722:282,load,load,282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/990#issuecomment-575292722,2,['load'],"['load', 'loaded']"
Performance,There is a really nice version of combat from the sva package that includes a reference batch. If this was added as a feature then you could perform your corrections separately for each sample. This might be a pretty easy addition.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551:141,perform,perform,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1977#issuecomment-953198551,1,['perform'],['perform']
Performance,"There’s a few uses:. 1. Humans. Once you understand the syntax ([very easy](https://docs.python.org/3/library/typing.html), i just get `Generator` wrong all the time) it improves your understanding what a function really accepts and returns; 2. IDEs. They’ll get better when inferring the types of variables and will show you more actual problems in the code and less false positives; 3. Testing. Some projects use mypy to check if all code in your repo typechecks properly, which can be integrated into a test suite; 4. Runtime type checking. Has a performance hit (as said) but given proper type hints, it makes your code safer and the error messages better (“Function blah excepted a parameter foo of type Bar, but you passed a foo of type Baz”). i’m not planning to do 3 and 4 (yet, and probably never)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142:550,perform,performance,550,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/373#issuecomment-441256142,1,['perform'],['performance']
Performance,"These t-SNE optimizations are mentioned in the following paper. Adding it here for reference.; https://arxiv.org/abs/2212.11506; Accelerating Barnes-Hut t-SNE Algorithm by Efficient Parallelization on Multi-Core CPUs; N Chaudhary, A Pivovar, P Yakovlev, A Gorshkov… - arXiv preprint arXiv:2212.11506, 2022",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798:12,optimiz,optimizations,12,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061#issuecomment-2116608798,1,['optimiz'],['optimizations']
Performance,"This hasn't been implemented yet, but a pull request would be welcome. There would also have to be documentation about changing results and how to get previous behavior. Some benchmarks of performance would also be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856:189,perform,performance,189,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1053#issuecomment-1038930856,1,['perform'],['performance']
Performance,"This is a bit of a grab bag, but is mostly `io` related. This started out as me trying to learn some vscode git integration, but turns out it's not great at figuring out what lines changed. Apologies for any weird stuff in the commits. Main changes:. * I've made the tests for `sc.datasets` more thorough. Now they actually check the data looks kinda okay, rather than whether they throw an error.; * I've removed cache-ing in a few places; * The `read_10x_*` tests, where that definitely shouldn't have been happening; * In a couple of the `sc.datasets`. I'd be willing to go back on this, but we shouldn't let them use the cache during testing.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/592:414,cache,cache-ing,414,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/592,2,['cache'],"['cache', 'cache-ing']"
Performance,"This is a small update to use numba more effectively and slightly decrease test times for calculating qc metrics. * I've enabled no python mode for `top_segment_proportions_sparse_csr`; * I've removed `numba` from functions currently only used for testing; * This mainly reduces test time. If anyone wants to use the `top_proportions` function to make `plotScater` type plots, maybe these should get re-enabled. Test times are still not great, but since it's due to numba compilation I'm not sure much can be done about it. The ideal solution is it becoming possible to have numba functions which are both parallel and cached.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462:619,cache,cached,619,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462,1,['cache'],['cached']
Performance,"This is fantastic, thank you!. A few things I'm unclear on:. * Why is this PR getting a build if there is no [`pr` trigger entry](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml#pr-triggers) in the `yaml`?; * Why isn't travis running on this PR? It might be that we've turned off branch CI since it was causing double runs with branches on this repo which were being used in PRs, but I thought it would still trigger once a pr was made. I think I'm just going to try and merge this, since it seems to be working. We can fine tune it via PRs as we go.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619:574,tune,tune,574,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1516#issuecomment-737013619,1,['tune'],['tune']
Performance,"This is something I'd very much be interested in. A few questions. * I'd really like to have scanpy and anndata work better with dask, but am wary of a high code overhead. Could you provide examples of where you were running into issues with arrays being materialized? I think this can be worked around in AnnData side in many cases.; * Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. And a few questions about sparse matrices on the GPU:. * How difficult do you think these methods would be to implement? It looks like there is functionality for taking the intersection of sparsity patterns in [`cusparseConstrainedGeMM`](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-cgemm) which could help.; * Have you looked into other backends for sparse matrices on the GPU? `suitesparse`/ `GraphBLAS` or `taco` may cover these use cases, though would need wrapping. > So I wrote a wrapper around scipy.sparse to implement NumPy's __array_function__ protocol. This allows sparse arrays to be chunks in a Dask array. 👍. ~~Any chance you've taken a look at implementing gufuncs?~~ Oops, missed the `__array_ufunc__` definition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161:417,perform,performance,417,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-554871161,1,['perform'],['performance']
Performance,"This issue is still persistent. I've created a colab notebook that shows the issue on a dataset we subsample to 6000 cells:. https://colab.research.google.com/drive/1QrnDFZ7nDNOLx9gr92eknhKShd2aTIdN. @gokceneraslan can you please throw a ""bug"" tag on this issue so it gets put in the queue?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/749#issuecomment-635348051:284,queue,queue,284,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/749#issuecomment-635348051,1,['queue'],['queue']
Performance,"This line:. https://github.com/scverse/scanpy/blob/383a61b2db0c45ba622f231f01d0e7546d99566b/pyproject.toml#L162. means that pytest imports that module together with the other plugins, and *then* runs tests. That means that it will `import scanpy.testing._pytest` (i.e. `scanpy` and everthing imported in there) before pytest-cov is loaded and can do anything.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122:332,load,loaded,332,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2874#issuecomment-1956887122,1,['load'],['loaded']
Performance,"This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844:96,cache,cacheing,96,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844,1,['cache'],['cacheing']
Performance,"This sounds interesting. If the performance is acceptable, it might make a good addition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/872#issuecomment-558999208:32,perform,performance,32,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872#issuecomment-558999208,1,['perform'],['performance']
Performance,"This will work for csc matrix; ```bash ; @numba.njit(cache=True, parallel=True); def to_dense_csc(; shape: tuple[int, int],; indptr: NDArray[np.integer],; indices: NDArray[np.integer],; data: NDArray[DT],; ) -> NDArray[DT]:; """"""\; Numba kernel for np.toarray() function; """"""; X = np.empty(shape, dtype=data.dtype). for c in numba.prange(shape[1]):; X[:,c] = 0; for i in range(indptr[c], indptr[c + 1]):; X[indices[i],c] = data[i]; return X; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205901859:53,cache,cache,53,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3110#issuecomment-2205901859,1,['cache'],['cache']
Performance,This would fix #2941 . I created some numba.njit() kernels that perform in-place substitutions based on the assumption that we only change existing values and don't add new ones (where all the scipy overhead comes from). . Benchmarks for 90k cells and 25k genes:; CSR:; old 23 s | new 1 s | 23x; CSC:; old 61 s | 1.6 s | 36x,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2942:64,perform,perform,64,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2942,1,['perform'],['perform']
Performance,Tracking operations performed on AnnData objects,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472:20,perform,performed,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472,1,['perform'],['performed']
Performance,"Trying out the tutorials these days and it seems this issue still persists. ---; Here is what I got from running the tutorial `pbmc3k.ipynb`:; Before writing the `AnnData` object to a `.h5ad` file (after the PCA step; before computing the neighborhood graph); - Inside `adata.uns`:; ```; OverloadedDict, wrapping:; 	OrderedDict([('log1p', {'base': None}), ('hvg', {'flavor': 'seurat'}), ('pca', {'params': {'zero_center': True, 'use_highly_variable': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)})]); With overloaded keys:; 	['neighbors'].; ```. ---; After loading the matrix from the `.h5ad` file:; - Inside `adata.uns`, the `log1p` key became an empty dictionary:; ```; OverloadedDict, wrapping:; 	{'hvg': {'flavor': 'seurat'}, 'log1p': {}, 'pca': {'params': {'use_highly_variable': True, 'zero_center': True}, 'variance': array([ (not showing the numbers for simplicity here) ],; dtype=float32), 'variance_ratio': array([ (not showing the numbers for simplicity here) ],; dtype=float32)}}; With overloaded keys:; 	['neighbors'].; ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016:696,load,loading,696,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2239#issuecomment-1319791016,1,['load'],['loading']
Performance,"Two major, and two minor, updates to qc metric calculation:. ## Tests run much faster now. `test_qc_metrics.py` used to take ~30 seconds, now takes ~2. These tests have been kinda slow for a while. This was mostly due to numba compilation. I was using `numba.njit(parallel=True)`, which cannot be cached so compilation occurred every time the tests ran. However, I expect most use cases only calculate QC metrics once in a session, and only for large datasets (at least 300,000 cells) is parallelization + compilation faster than performing the calculation in a single thread. Now a cached single threaded version is used unless the dataset is large. ## Can now calculate observation and variable metrics separately. Split the calculation of qc metrics into two functions for obs and var. These separate calls are now available as: `describe_obs` and `describe_var` after `pd.DataFrame.describe`. This is mostly to go along with my split-apply-combine experiments. In particular a use case like:. ```python; (adata; .groupby(obs=""leiden""); .apply(sc.pp.describe_var); .combine(...); ); ```. Where metrics like number of fraction of cells, mean expression, etc. are calculated within each group (useful for things like #562). ## Minor updates. * User can now choose to use expression from `layers` or `raw` instead of `adata.X`; * Doc updates 🤞 (am I polluting `sc.pp._docs.py` too much?)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/615:297,cache,cached,297,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615,3,"['cache', 'perform']","['cached', 'performing']"
Performance,"Update, the correct docs also show up on master for my local build. Not sure if this is a cacheing issue or a difference between my build and readthedocs'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262:90,cache,cacheing,90,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/737#issuecomment-510419262,1,['cache'],['cacheing']
Performance,Update:. Docs don't build with sphinx 4.1.0 due to a error triggered by `scanpydoc`. Sphinx will be pinned until this is solved (which is when this issue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentatio,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:421,load,loading,421,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,7,['load'],['loading']
Performance,Use cacheing and parallelism for qc metrics,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/844:4,cache,cacheing,4,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844,1,['cache'],['cacheing']
Performance,"Very nice! Thank you for the benchmark! 🙂 It's a bit astonishing as I investigated this some a bit more than a year ago but maybe there was some improvement... I am in principle happy to change this soon - I don't think that the change within floating point precision will change results [it does, for instance, in PCA... of course not a qualitative result, but nonetheless a tSNE could be rotated etc.]!. How did you come to investigating this? Was the computation a bottleneck for you? The matrix you provide is almost - for current standards - ""unrealistically large"".",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082:468,bottleneck,bottleneck,468,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/163#issuecomment-392030082,1,['bottleneck'],['bottleneck']
Performance,"We are very impressed with the scalability of scanpy. We are interested in performing gene co-expression clustering on large single-cell RNAseq datasets. This typically involves calculating pairwise correlations between genes, then using these correlations as distance metrics for hierarchical and k-means clustering. Does scanpy already support these kinds of analyses?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/72:31,scalab,scalability,31,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/72,2,"['perform', 'scalab']","['performing', 'scalability']"
Performance,"We didn't use the weights in Louvain (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_louvain.py#L31)?. Why did you decide to change the default in Leiden; (https://github.com/theislab/scanpy/blob/297d6246ccfbf398f771cee1bd4b81b57fc27c76/scanpy/tools/_leiden.py#L31)? I'm fine with it, but a brief discussion would have been appropriate. :wink:. @LuckyMD; > how different is that to clustering on the UMAP embedding directly?; It's very different. The choice of weights will likely not have a dramatic effect, you're always clustering a graph that proxies neighborhoods in high-dimensional space. If you embed this structure in 2 or 3 d, even if you use the fantastic UMAP for it, you'll make errors (https://twitter.com/falexwolf/status/1108284982001315840). Also, the most computationally intense part is the embedding optimization, not the graph construction.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/586#issuecomment-484424732:868,optimiz,optimization,868,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/586#issuecomment-484424732,1,['optimiz'],['optimization']
Performance,"We had a journal club about this recently and couldn't really come up with a good solution tbh. Mean-log is so much easier for a lot of applications. Log-transforming data also has a variance-stabilizing effect and it reduces skewness so that the data at least better approximates a normal distribution than before, which many downstream methods assume (although data is often still far from normal). So I don't see how we can forgo log transformation without modeling count data for everything directly. The effect outlined in the paper is the most pronounced for differential expression tests between groups for which size factor distributions differ... So you could check size factor distributions before performing the test to estimate whether the log-mean vs mean-log difference will affect the test. If yes, try without log transforming the data and see if the test can deal with the outliers. Especially for the t-test, the poorer approximation of normality may not have as strong an effect as the log-mean vs mean-log difference. However, in our experience size factors tend to have a range of ~100-fold difference, and not 1,000-10,000 fold as was shown in the paper. We weren't so taken with the suggestion in the paper of increasing the pseudocount as that essentially removes fold-change effects... and also removes zeros (making all matrices dense). As for embeddings... you could remove size factor outliers for the PCA calculation and do it without log-transformation. Although in practice we found it gives very similar results. Thus, our solution was to visualize size factor distributions on embeddings to see whether there is an effect. Usually you do see a count depth effect in the embedding though... and that's not that surprising for CPM normalization, as you assume that all cells are of similar molecule count, which is incorrect. With other normalization methods it shouldn't be as bad. But yeah... overall, it's a complicated problem without a good solution. I imagine it i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918:708,perform,performing,708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/517#issuecomment-471465918,1,['perform'],['performing']
Performance,"We knocked out a gene, then wanna to reveal the difference between KO embro cells and wild type embryo cells and illustrate the function of the gene in development process. The single cell data was generated using Smart-seq2 technology, which is a low throughput technology. And besides, there were only a few cells in eary embros because they are very small.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135:252,throughput,throughput,252,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1764#issuecomment-815387135,1,['throughput'],['throughput']
Performance,"We noticed some performance issues with `scanpy.api.pp.log1p` and I think these changes will make it faster and more memory efficient. The `out` argument of `np.log1p` can be used to modify data in-place, conserving memory (this does require special-casing for sparse matrices, unfortunately). Because it's a ufunc, it's quite fast and efficient. With those changes I removed the `chunked` and `chunk_size` arguments for this function, because I don't think they would actually help performance. In fact I found that using `chunked=True` was extremely slow, possibly because of having so many function calls. The only advantage I can imagine there is if you used parallelization on the chunks, but until that's implemented I think the option should be removed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191:16,perform,performance,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191,2,['perform'],['performance']
Performance,"We often identify a subset of cells as irrelevant noise cells and hope to discard them during the analysis. I am a bit confused about how to perform such operations in Scanpy. It's a common practice in other analysis tool like Seurat to do ScaleData across cells so that the relative expression level is adjusted without uninteresting cells' influences. . Such operation is supported by Seurat by providing multiple ""Assay"", such as `counts`, `data`, and `scale.data`, which stores the raw UMI counts, column normalized data (across genes, log1p), and row normalized data (across samples, zscores). . I noticed the scaled data are stored in `adata.X` in scanpy. With this design, how can we access the raw UMI count and do re-scaling if I hope to subset the data?. Looking forward your reply. Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1089:141,perform,perform,141,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1089,1,['perform'],['perform']
Performance,"We ran some data through spaceranger 3.0.1 locally, and in doing so found that 10X have reduced redundancy in the `spatial` folder of the binned outputs by moving the tissue images to a new, central location. This understandably breaks the existing loader. A hotfix is to copy the images back into the appropriate subdirectory, but that's not a feasible expectation on users. I added an optional argument `spaceranger_image_path` to point to the new folder if need be, which should hopefully be robust with regard to any sort of further restructuring 10X may choose to do in the future. The code is currently included in [bin2cell](https://github.com/Teichlab/bin2cell) in case anybody needs it or just wants to take it out for a spin, but I think it belongs in a more central location.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251:249,load,loader,249,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2992#issuecomment-2230448251,1,['load'],['loader']
Performance,"We're always up for improved performance! Would love to see improvements here. (Btw, I think I've already got your gist bookmarked on twitter). Do you have any benchmarks of performance here? Especially against our current implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546:29,perform,performance,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2060#issuecomment-981701546,2,['perform'],['performance']
Performance,"We're still hesitant about making AnnData more complex, for these reasons:; * It is not inefficient to load multiple versions of the full data into AnnData.; * It is not straightforward to determine the point of the preprocessing at which one would want to save a version of the raw data (probably after filtering out cells and taking the logarithm, but this might change in the future).; As the second point implies that some manual intervention would be necessary, anyway, we tend to leave it to the user to keep track of one, two or more versions of the data; each with annotations that can easily be exchanged. Specifically, would you be happy to proceed as in differential expression tests, see e.g., https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/seurat.ipynb? You keep track of two versions of the data, one for doing all the machine learning inference and another one for doing statistics and plotting. Using the linked example: for plotting, you would simply need to add the visualization basis to the AnnData that stores the raw data. Then you call `sc.pl.tsne`.; ```; adata_corrected = sc.read('pbmc3k_corrected'); adata_raw = sc.read('pbmc3k_filtered_raw_log'); adata_raw.smpm['X_tsne'] = adata_corrected.smpm['X_tsne']; adata_raw.smpm['X_pca'] = adata_corrected.smpm['X_pca']; sc.pl.tsne(adata_raw, color='NKG7'); ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609:103,load,load,103,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/41#issuecomment-347357609,1,['load'],['load']
Performance,"We've been dealing with long queue times for CI builds. This is at least partially because for each PR four jobs start, each of which takes at least 12 minutes. Since travis gives us at most five concurrent jobs, only one PR can be built at a time. This becomes worse if a PR is based on a branch on the main repo, since CI runs on those too. Azure offers 10 free concurrent jobs. Seems like an easy win. * 10 free concurrent jobs; * Easier to do multiple checks per build (i.e. linting and testing can happen in the same build, but be independent checks); * Output looks easy to navigate, has good integration with github; * We could test on windows (depending on how hard this is to set up); * (possible) Some projects seem to use multiple cores for testing. Cons:. * New system, will take some time to learn; * Maybe microsoft will start being evil again",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1358:29,queue,queue,29,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1358,4,"['concurren', 'queue']","['concurrent', 'queue']"
Performance,"Well, but the amount of memory should be a lot smaller than if you used; ```; adata = sc.read('test.h5ad'); ```; There should not be any difference between `'r`' and `'r+'`, so that's intended behavior. If you open it in backed mode, everything except the data matrix gets loaded into memory (using the `h5py` package). It should use as little memory as your custom solution. If it doesn't, I'd be happy to go through more details.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/434#issuecomment-456003799:273,load,loaded,273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/434#issuecomment-456003799,1,['load'],['loaded']
Performance,"What about comparing communities between for example CPM and RBERVertexPartition at the same resolution, using modularity score? This way we are not comparing scores obtained by optimization functions, just simple ""external"" measure.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127:178,optimiz,optimization,178,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2908#issuecomment-1999868127,1,['optimiz'],['optimization']
Performance,"What packages are conflicting with `h5py` 3.0? The 2 -> 3 update had some fairly hard to manage changes to how string dtypes are handled, and it'd be nice to drop 2.0 support once the ecosystem is caught up. ----------------. I'm actually not so sure this is h5py or anndata though, those are just common culprits. I've tried this in a conda environment with h5py 2.10.0 and it doesn't reproduce. I've even tried to make a conda environment from your `sinfo` and could not reproduce. <details>; <summary> Here's how I tried to create a replicate environment </summary>. ```python; $ mamba create -n issue-1850 'anndata==0.7.6' 'scanpy==1.7.2' 'sinfo==0.3.1' 'pillow==8.0.1' 'backcall==0.2.0' 'bottleneck==1.3.2' 'cffi==1.14.0' 'colorama==0.4.4' 'cycler==0.10.0' 'decorator==4.4.2' 'fcsparser==0.2.1' 'get_version==2.1' 'h5py==2.10.0' 'python-igraph>=0.7.1' 'ipykernel==5.3.4' 'ipython_genutils==0.2.0' 'ipywidgets==7.5.1' 'jedi==0.17.2' 'joblib==0.17.0' 'kiwisolver==1.2.0' 'leidenalg==0.8.2' 'llvmlite==0.34.0' 'lxml==4.6.1' 'matplotlib==3.3.2' 'natsort==7.0.1' 'networkx==2.5' 'numba==0.51.2' 'numexpr==2.7.1' 'numpy==1.19.2' 'packaging==20.4' 'pandas==1.2.4' 'parso==0.7.0' 'pexpect==4.8.0' 'pickleshare==0.7.5' 'prompt_toolkit==3.0.8' 'psutil==5.8.0' 'ptyprocess==0.6.0' 'pycparser==2.20' 'pygments==2.7.1' 'pyparsing==2.4.7' 'pytz==2020.1' 'scipy==1.5.2' 'scvelo==0.2.3' 'seaborn==0.11.1' 'sinfo==0.3.1' 'six==1.15.0' 'scikit-learn==0.23.2' 'statsmodels==0.12.0' 'pytables==3.6.1' 'traitlets==5.0.5' 'umap-learn==0.4.6' 'wcwidth==0.2.5' 'IPython==7.18.1' 'jupyter_client==6.1.7' 'jupyter_core==4.6.3' 'notebook==6.1.4'; ```. </details>. Could you create a fresh environment, and try again? I'm really confused about how you are ending up with a multi index anywhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613:693,bottleneck,bottleneck,693,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847526613,1,['bottleneck'],['bottleneck']
Performance,"When an anndata object is saved to h5ad the categorical variables in the .raw.var are casted to integers so that e.g. gene names are lost when loading the anndata object again. This functionality is especially unfortunate as anndata allows adata and adata.raw to have different sizes in the .var dimension. Thus, if anndata represents for example a highly variable gene set, where anndata.raw is the whole data set, then we can no longer visualize the expression of genes that were filtered out unless we call them by var_name and not by 'gene_name'. This bug is likely due to these lines:; https://github.com/theislab/anndata/blob/d9727cab88ba2100787e3e2ae0c6d72abd4d92b7/anndata/base.py#L1925-L1951. It would be good to add an uns_raw/ or raw_categories/ folder to the h5ad format which stores the categorical variables for raw.var.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/171:143,load,loading,143,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/171,1,['load'],['loading']
Performance,"When exploring various options of preprocessing data, I try to avoid having several copies of AnnData objects in memory if they're not sparse, so I save them to h5ad at key steps. Sometimes alas, after a few iterations I re-write stuff and forget what operations have been performed in my ""X"" (particularly in the preprocessing steps). So, because being lazy makes me creative, I started tracking these in the object itself (see example https://gist.github.com/afrendeiro/7ccaf324bfdbff042ae36f734f544860) by decorating the preprocessing functions post hoc (this could even easily be used to save the values of kwargs passed potentially). I wonder if an internal implementation of this would be of broad interest, particularly for functions which modify ""X"" inplace?; Of course this would be no replacement for proper documentation of one's steps, etc but I thought it could be an interesting addition to scanpy in any case.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472:273,perform,performed,273,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472,1,['perform'],['performed']
Performance,"When loading scanpy through `import scanpy.api as sc` the plotting style is changed. This prevents my from using it in my general workflow because I have particular plotting styles I use. This used to be a issue with seaborn, but was changed in version 0.8, but scanpy still seems to restyle the entire plotting environment?. If styles are applied locally in particular plotting functions, that is fine, but it shouldn't affect the users other plots.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/108:5,load,loading,5,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/108,1,['load'],['loading']
Performance,When reading in a .xlsx file with sc.read_excel() it says the sheet I asked to load in does not exist.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:79,load,load,79,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,1,['load'],['load']
Performance,"When the object is backed, but `copy=False`, the ValueError, which before occured for both `copy=False` and `copy=True`, is shown:; `ValueError: To copy an AnnData object in backed mode, pass a filename: '.copy(filename='myfilename.h5ad')'. To load the object into memory, use '.to_memory()'.`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691512482:244,load,load,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2624#issuecomment-1691512482,1,['load'],['load']
Performance,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191:194,perform,performing,194,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571#issuecomment-754704191,1,['perform'],['performing']
Performance,"With #3056 merged, this now says. > before | after | ratio | benchmark; > --- | --- | --- | ---; > 448±100ms | 381±100ms | ~0.85 | `preprocessing_counts.time_scrublet('pbmc68k_reduced')`. but ASV seems to think that’s not enough to report. Also unclear why it’s reported as taking 26 minutes by github:. > <img width=""252"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/291575/6117cece-a145-4b46-85a4-dd86a61819ef"">. I see in the server logs. > - May 14 10:15:57 scvbench benchmark[1462905]: 2024-05-14T10:15:57.547945Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting; > - […running benchmarks]; > - May 14 10:27:18 scvbench benchmark[1462905]: 2024-05-14T10:27:18.793352Z DEBUG handle_event:HTTP{http.method=PATCH http.url=https://api.github.com/repos/scverse/scanpy/check-runs/24942322156 otel.name=""HTTP"" otel.kind=""client""}: octocrab: requesting. which means that between setting the check run to “running” and to “done”, 11m21s passed. Maybe GitHub counts the queue time?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339:1105,queue,queue,1105,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3044#issuecomment-2109908339,1,['queue'],['queue']
Performance,"Would you say that there is an optimal range to set n_neighbors usually? And maybe a max value that rarely should be exceeded?. I'm trying to optimize louvain clustering for several datasets, and I'm aiming to automate at least a portion of the process, by going through a range of neighbor values (tl.neighbors) and resolution values (for tl.louvain), while keeping n_pcs constant, and most of my highest scoring clustering arrangements (measured by the silhouette index) uses neighbor parameters ~ 22 - 30. I know that these parameters will depend on the dataset, but I'm wondering if I should set a lower upper limit (For now it's 30), then go in and try to optimize the clustering of specific clusters using the restrict_to parameter for the louvain function. The clustering arrangements I have don't seem to be adequate based on certain markers that I'm plotting across the cells. . Hope this makes sense. Best",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/223:142,optimiz,optimize,142,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/223,2,['optimiz'],['optimize']
Performance,"Wow well that certainly makes things a lot easier, thank you for creating that code snippet! It seems as though Scanpy was smart enough to realize I hadn't performed PCA and thus did it for me using the default settings. However as you have suggested I should be using 'arpack' to make my results reproducible, will do! . Thank you for your reply!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/680#issuecomment-498856082:156,perform,performed,156,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/680#issuecomment-498856082,1,['perform'],['performed']
Performance,"Yeah, looking at the code, the color used will just be the first item in the color list that hasn't been assigned to a neighbor yet. Maybe you could replace color selection to pick from a queue prioritizing less used colors? This would be less likely to converge to a minimal number of colors though. Might need to consider a different algorithm for this property. <details>; <summary> Example </summary>. ```; A; /; C - B; ```. Lets say we have two colors [""red"", ""blue""] and we want to assign them. We iterate in order [A, B, C]. * If we don't use the priority queue, we'd end up with `{""A"": ""red"", ""B"": ""red"", ""C"": ""blue""}`; * If we do, we'd fail to converge after ending up at {""A"": ""red"", ""B"": ""blue""}`. </details>. Do you think you could share some part of the object you're plotting? I think it would make for a good use case to play around with plotting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360:188,queue,queue,188,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-770675360,2,['queue'],['queue']
Performance,"Yes , the sampling is done with weights and I used the coreset technique; for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>; wrote:. > I understand the benefits of sampling regarding computational speed up.; > What I'm not clear on is how you choose your weights for the calculations; > you perform here. You mentioned that you get wrong marker gene results when; > you sample and don't use weights. That makes sense if you get a; > non-representative set of cells in your sample. I wonder how you select the; > weights to fix this. I guess you don't just try a lot of different values; > until one works, right?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494:322,perform,perform,322,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644#issuecomment-494327494,1,['perform'],['perform']
Performance,"Yes, I also noticed this behavior. I'd say that your `min_dist=0.6` result is essentially the same as the tSNE result. I'd take this result. I'm not sure whether this is fundamentally solvable - optimizing an embedding is a very hard task and UMAP has the best approach to this so far - this is one of the reasons why we came up with PAGA, which is not affected by these problems. PAGA gives you the correct picture of what is connected and what isn't. Note the [updated preprint](https://rawgit.com/falexwolf/paga_paper/master/paga.pdf), which provides an indepth explanation of these issues. Will replace the current bioRxiv preprint or appear in a journal soon.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291:195,optimiz,optimizing,195,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/174#issuecomment-398681291,1,['optimiz'],['optimizing']
Performance,"Yes, I totally agree that creating a fast implementation is probably not straightforward. The major bottleneck IMHO is computing this many neighbors to maximize the rejection rate, especially with 900k cells. In the original paper, we tried to find a range of neighborhood sizes K that return a maximal rejection rate, which is roughly between K = 50 to 0.5 * N where N denotes the number of cells, but there might be also a dependence on the number of batches, which we did not fully explore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139:100,bottleneck,bottleneck,100,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/364#issuecomment-1372444139,1,['bottleneck'],['bottleneck']
Performance,"Yes, it works great for me. I can compare scores obtained on individual samples and on integrated data to show that genes that are spatially variable in samples remain such on integrated data. However, with default n_iters most pvalues were 0 (for my known marker set), but calculating more iters would take too long, so I might just use the I-score where possible. ; I would like to add Moran's I as an bio conservation integration metric to scIB - this is for me the only metric that does not require cell subtype annotation (which is cumbersome and unreliable procedure) and it performs similar to current scIB metrics. However, scIB has as dependency only scanpy, not squidpy. It seems a bit of an overkill to add package dependency to scIB for a single function. . Semitones (https://www.biorxiv.org/content/10.1101/2020.11.17.386664v1) is a package for finding genes linearly variable across embedding and I think Moran's I would also give me similar genes (must try it out) - Moran's I might be even better for the task and quicker + less complicated. This is another reason why it would be neat to have Moran's I directly in scanpy. You may not have spatial data, so not really needing squidpy. But finding gene patterns may be useful when you have continuous effects but no trajectories - this is what my main beta cell subtype analysis is currently based on.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982:581,perform,performs,581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1698#issuecomment-787504982,1,['perform'],['performs']
Performance,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:; ```; # if reference is not set, then the groups listed will be compared to the rest; # if reference is set, then the groups listed will be compared only to the other groups listed; from sklearn.linear_model import LogisticRegression; if len(groups) == 1:; raise Exception('Cannot perform logistic regression on a single cluster.'); adata_copy = adata[adata.obs[groupby].isin(groups_order)] ; adata_comp = adata_copy; if adata.raw is not None and use_raw:; adata_comp = adata_copy.raw; X = adata_comp.X. clf = LogisticRegression(**kwds); clf.fit(X, adata_copy.obs[groupby].cat.codes); ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467:750,perform,perform,750,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278#issuecomment-427093467,1,['perform'],['perform']
Performance,"Yes, we already have a good mask for sparse scaling. Boolean arrays are very effective for indicating where computations should be performed, as they eliminate the need for copying and reintegration. One clear example is the `tl.score_genes` function. masks there as booleans for the nanmean is a lot more efficent but less pythonic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711:131,perform,performed,131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2234#issuecomment-2311895711,1,['perform'],['performed']
Performance,"You are right I'm putting it on PyPI, and guess what, I'm already using numba on it😄. I finished some docs and uploaded it to my repo [mnnpy](https://github.com/chriscainx/mnnpy). There is still much to optimize, according to my tests, the 'adjust variance' step takes most time, and the best solution may still be rewriting it in C, as scran did, although somehow they made it single-threaded. In the current version I used jit and multiprocess. I'm afraid we'll have to remove the 'mnn_concatenate' from /rtools though 😂",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090:203,optimiz,optimize,203,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/125#issuecomment-384385090,1,['optimiz'],['optimize']
Performance,"Your way sounds sure better, many things into the scrublet algorithm are in; redundancy with components of scanpy. It will sure look great :); Just one thing: in the scrublet paper they suggest always to just run the; simulation of doublets and look at the expected vs estimated fraction of; doublets before removing doublets. If those two values do not match, they; say one should rerun scrublet and tune the expected fraction.; Does your script only run simulation of doublets and output the doublets; score, or does it also remove doublets at once? If you do the latter, then; one is not able to simulate doublets more than once to adjust the expected; doublet fraction.; Cheers. Den tor. 16. maj 2019 kl. 05.15 skrev Sam Wolock <notifications@github.com>:. > @cartal <https://github.com/cartal> @SamueleSoraggi; > <https://github.com/SamueleSoraggi>; > For some reason I decided to integrate Scrublet using Scanpy's functions; > where possible, rather than making a simple wrapper. The core functionality; > is up and running in this fork <https://github.com/swolock/scanpy>, and; > now I just need to add documentation, make some of the code more; > Scanpythonic(?), and add an example.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/173?email_source=notifications&email_token=ACC66UNQC744WOUTLRZ2CN3PVTGWTA5CNFSM4FE4LIF2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVQRA2I#issuecomment-492900457>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACC66UI4FF4LES7GRVKHZZDPVTGWTANCNFSM4FE4LIFQ>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700:401,tune,tune,401,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-492936700,1,['tune'],['tune']
Performance,"\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1925,cache,cache,1925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['cache'],['cache']
Performance,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3289,race condition,race condition,3289,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3569,race condition,race condition,3569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```; Step 2: Then I install tables; ```python; !pip install tables. Requirement already satisfied: tables in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (3.7.0); Requirement already satisfied: packaging in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (21.3); Requirement already satisfied: numpy>=1.19.0 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (1.21.5); Requirement already satisfied: numexpr>=2.6.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from tables) (2.8.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\users\hyjfo\.conda\envs\newpy38\lib\site-packages (from packaging->tables) (3.0.4). import tables. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_8256/574719567.py in <module>; ----> 1 import tables. ~\.conda\envs\NewPy38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsext",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:2349,load,load,2349,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"\site-packages\anndata\_core\index.py in _normalize_index(indexer, index); 73 return indexer; 74 elif isinstance(indexer, str):; ---> 75 return index.get_loc(indexer) # int; 76 elif isinstance(indexer, (Sequence, np.ndarray, pd.Index, spmatrix, np.matrix)):; 77 if hasattr(indexer, ""shape"") and (. D:\anaconda\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance); 3629 return self._engine.get_loc(casted_key); 3630 except KeyError as err:; -> 3631 raise KeyError(key) from err; 3632 except TypeError:; 3633 # If we have a listlike key, _check_indexing_error will raise. KeyError: 'CST3'. #### Versions; scanpy==1.9.2 anndata==0.8.0 umap==0.5.3 numpy==1.21.6 scipy==1.9.1 pandas==1.4.4 scikit-learn==1.0.2 statsmodels==0.13.2 python-igraph==0.10.4 pynndescent==0.5.8; <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.2; -----; PIL 9.2.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; ipykernel 6.15.2; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.1; jinja2 3.0.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.1.2; matplotlib 3.5.2; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; nt NA; ntsecuritycon NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.6; packaging 21.3; pandas 1.4.4; parso 0.8.3; patsy 0.5.2; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pynndescent 0.5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2430:4487,bottleneck,bottleneck,4487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2430,1,['bottleneck'],['bottleneck']
Performance,"][""quality_function""] = partition_type.__name__`?; > . Strictly speaking, you are correct that the quality function of `RBConfigurationVertexPartition` is not exactly the same as modularity, although it is called unscaled modularity in the [code](https://github.com/vtraag/louvain-igraph/blob/master/src/RBConfigurationVertexPartition.cpp#L123). . There are two main differences between RBConfigurationVertexPartition and ModularityVertexPartition which uses typical modularity optimization. 1) Scaling by the number of edges and 2) the resolution parameter (as it's written [here in the note](https://louvain-igraph.readthedocs.io/en/latest/reference.html#rbconfigurationvertexpartition)). I account for 1) in the code but using a resolution parameter other than 1.0 would lead to values different than modularity due to 2). Right now, for example, you can get a perfect quality (=1.0) by just setting the resolution to 0.0 :D I don't think that'd mislead users though. After all, that's what the algorithm uses for optimization. I can think of two solutions. We can report typical modularity regardless of the `partition_type`, namely:. ```; modularity_part = leidenalg.ModularityVertexPartition(g, initial_membership=part.membership); q = modularity_part.quality(); ```. or we can report the original quality value as ""raw quality"" (whatever it is) and the modularity together. It's in the ""hint"" verbosity level anyway. Regarding the suggestion to record `partition_type.__name__`, I think it's a good idea. I'd record it in the `uns[uns_key]['partition_type']` though, not in `quality_function`. > > To me, scaled modularity is like any statistical measure which gives a rough idea about a concept, like correlation or silhouette coef. It's far from conclusive just by itself, but it gives a ""feeling"" of how ""well-clustered"" the data is (and how good we are at finding them). Without complementing it with other measures, it's not more than just a ""feeling"" :); > ; > A couple follow up points ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088:1363,optimiz,optimization,1363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819#issuecomment-529494088,1,['optimiz'],['optimization']
Performance,"]['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works; adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2565,cache,cache,2565,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['cache'],['cache']
Performance,"_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\Python38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\Python38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```. #### Versions. <details>. Package Version; ------------------- ---------; anndata 0.7.8; anyio 2.2.0; argon2-cffi 20.1.0; async-generator 1.10; attrs 21.2.0; Babel 2.9.1; backcall 0.2.0; bleach 4.1.0; Bottleneck 1.3.2; brotlipy 0.7.0; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.4; colorama 0.4.4; cryptography 36.0.0; cycler 0.11.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fonttools 4.25.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; importlib-metadata 4.8.2; ipykernel 6.4.1; ipython 7.29.0; ipython-genutils 0.2.0; jedi 0.18.0; Jinja2 3.0.2; joblib 1.1.0; json5 0.9.6; jsonschema 3.2.0; jupyter-client 7.1.0; jupyter-core 4.9.1; jupyter-server 1.4.1; jupyterlab 3.2.1; jupyterlab-pygments 0.1.2; jupyterlab-server 2.10.2; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.37.0; MarkupSafe 2.0.1; matplotlib 3.5.0; matplotlib-inline 0.1.2; mistune 0.8.4; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; mock 4.0.3; munkres 1.1.4; natsort 8.0.2; nbclassic 0.2.6; nbclient 0.5.3; nbconvert 6.1.0; nbformat 5.1.3; nest-asyncio 1.5.1; networkx 2.6.3; notebook 6.4.6; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; olefile 0.46; packaging 21.3; pandas 1.3.5; pandocfilters 1.4.3; pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:3375,Bottleneck,Bottleneck,3375,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['Bottleneck'],['Bottleneck']
Performance,"_float; from skimage.util import invert; from scipy.spatial import ConvexHull, convex_hull_plot_2d; from multiprocessing import Pool; import time; import math; from collections import Counter; import scanpy as sc; import networkx as nx; import pandas as pd; import numpy as np; import itertools; import random; from scipy.stats import mannwhitneyu; import os; import warnings; import pickle; import matplotlib.pyplot as plt; import pandas as pd; import gseapy as gp; from matplotlib import pyplot as plt; from matplotlib_venn import venn2; import mygene; import seaborn as sns; from gseapy import barplot, dotplot; import random; import matplotlib.pyplot as plt; import numpy as np; import decoupler as dc; from numpy.random import default_rng; from copy import deepcopy; import scanpy as sc; import squidpy as sq; import time; from neighborhood_enrichment import neighborhood_enrichment; import schist as scs; with open('TNBC_41patients_KerenEtAl.pkl', 'rb') as f:; pickle_= pickle.load(f); pickle_=dict(list(pickle_.items())[7:8]); for i in pickle_:; cnt+=1; adata=pickle_[i]; sc.pp.neighbors(adata, n_neighbors=10, n_pcs=3); sc.tl.leiden(adata); scs.inference.planted_model(adata); sc.pp.scale(adata); sc.tl.rank_genes_groups(adata, groupby='ppbm', pts=True, method='wilcoxon'); adata_rankGenes_names, adata_rankGenes_scores, adata_rankGenes_logfoldchanges, adata_rankGenes_pvals, adata_rankGenes_pvals_adj, adata_rankGenes_pts, adata_rankGenes_pts_rest=pd.DataFrame(adata.uns['rank_genes_groups']['names']), pd.DataFrame(adata.uns['rank_genes_groups']['scores']), pd.DataFrame(adata.uns['rank_genes_groups']['logfoldchanges']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals']), pd.DataFrame(adata.uns['rank_genes_groups']['pvals_adj']), pd.DataFrame(adata.uns['rank_genes_groups']['pts']), pd.DataFrame(adata.uns['rank_genes_groups']['pts_rest']); ```. ### Error output. ```pytb; It's not really an error, it's just unexpected output. 1. https://github.com/scverse/scanpy/issues/701 says the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2586:3170,load,load,3170,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2586,1,['load'],['load']
Performance,"_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename):; --> 937 while (filename in get_used_files()):; 938 time.sleep(1); 939 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in get_used_files(); 919 def get_used_files():; 920 """"""Get files used by processes with name scanpy.""""""; --> 921 loop_over_scanpy_processes = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:1379,cache,cache,1379,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,1,['cache'],['cache']
Performance,"_get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex. ```. </details>. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.2; sinfo 0.3.1; -----; MulticoreTSNE NA; PIL 8.0.1; anndata 0.7.5; annoy NA; backcall 0.2.0; bbknn NA; bottleneck 1.3.2; cairo 1.19.1; cffi 1.14.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; fcsparser 0.2.1; future_fstrings NA; get_version 2.1; google NA; h5py 2.10.0; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.2.0; legacy_api_wrap 0.0.0; leidenalg 0.8.2; llvmlite 0.34.0; lxml 4.6.1; matplotlib 3.3.2; mkl 2.3.0; mpl_toolkits NA; natsort 7.0.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.19.2; packaging 20.4; palantir 1.0.0; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; phenograph 1.5.7; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.8.0; ptyprocess 0.6.0; pycparser 2.20; pygments 2.7.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.7.2; scipy 1.5.2; scvelo 0.2.3; seaborn 0.11.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; sphinxcontrib NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tornado 6.0.4; traitlets 5.0.5; typing_extensions NA; umap ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:5743,bottleneck,bottleneck,5743,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['bottleneck'],['bottleneck']
Performance,"_groups(adata, min_fold_change=1); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 1.5828611850738525 seconds ---; ```. with version 1.6.0:; ```python; import time; start_time = time.time(); sc.tl.rank_genes_groups(adata, groupby = 'leiden', method = 'wilcoxon'); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 49.53031611442566 seconds ---. start_time = time.time(); sc.tl.filter_rank_genes_groups(adata, min_fold_change=1); print(""--- %s seconds ---"" % (time.time() - start_time)); # --- 600.4000315666199 seconds ---; ```; I also noticed that it was using up 98% of my CPU while running `filter_rank_genes_groups`. #### Versions. <details>. `scanpy==1.5.1 anndata==0.7.4 umap==0.4.6 numpy==1.19.1 scipy==1.5.2 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-igraph==0.8.2 leidenalg==0.8.1`. and. ```; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; Bio 1.77; PIL 7.2.0; adjustText NA; anndata 0.7.4; annoy NA; backcall 0.2.0; bbknn NA; brotli NA; cachecontrol 0.12.6; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.1; changeo 1.0.0; chardet 3.0.4; cycler 0.10.0; cython_runtime NA; dandelion 0.0.15; dateutil 2.8.0; decorator 4.4.2; descartes NA; distance NA; get_version 2.1; h5py 2.10.0; hdmedians NA; idna 2.10; igraph 0.8.2; importlib_metadata 1.7.0; ipykernel 5.3.3; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.2; joblib 0.16.0; kiwisolver 1.2.0; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.33.0; markupsafe 1.1.1; matplotlib 3.3.0; mizani 0.7.1; mpl_toolkits NA; msgpack 1.0.0; natsort 7.0.1; networkx 2.4; numba 0.50.1; numexpr 2.7.1; numpy 1.19.1; packaging 20.4; palettable 3.3.0; pandas 1.0.5; parso 0.7.1; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotnine 0.6.0; polyleven NA; presto 0.6.1; prompt_toolkit 3.0.6; ptyprocess 0.6.0; pycparser 2.20; pygments 2.6.1; pyparsing 2.4.7; pytz 2019.2; requests 2.24.0; rpy2 3.3.5; scanpy 1.6.0; scipy 1.5.2; scrublet NA; seaborn 0.10.1; setuptools_scm NA; sinf",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1449:1802,cache,cachecontrol,1802,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1449,1,['cache'],['cachecontrol']
Performance,"_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py"", line 45, in <module>; @numba.njit(cache=True); File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function 'sparse_mean_var_minor_axis': no locator available for file '/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py'. ```. I would highly appreciate if you could please point out how to fix this issue. . Thank you in advance!. Best wishes,; Abdelrahman . ```. #### Versions. <details>. numba==0.53.1; scanpy==1.8.1. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:3352,cache,cache,3352,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,2,['cache'],['cache']
Performance,"`@jit` can be fine for supporting a greater range of `numba` versions or just compiling parts of the function through lifted loops (which this was using before). I don't think caching is on by default, but you can cache compiled functions to reduce compilation times ([docs](https://numba.pydata.org/numba-doc/dev/user/jit.html#cache)). However, I don't think you can use `@jit(parallel=True, cached=True)`. Here's an issue for it: https://github.com/numba/numba/issues/2712",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854:214,cache,cache,214,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/462#issuecomment-460941854,3,['cache'],"['cache', 'cached']"
Performance,"`_sparse_nanmean` makes two copies of the data matrix and performs a set index operation on a sparse array. It could be much faster by not doing this things. Noticed while reviewing #1890. <details>; <summary> possible solution </summary>. ```python; from numba import njit, prange; import numpy as np. @njit(parallel=True); def nanmean_lowlevel(data, indices, indptr, shape):; N, M = shape; sums = np.zeros(N, dtype=np.float64); nans = np.zeros(N, dtype=np.int64); for i in prange(N):; start = indptr[i]; stop = indptr[i+1]; window = data[start:stop]; n_nan = np.int64(0); i_sum = np.float64(0.); for j_val in window:; if np.isnan(j_val):; n_nan += 1; else:; i_sum += j_val; sums[i] = i_sum; nans[i] = n_nan; sums /= (M - nans); return sums; ```. Has more error from dense reference compared to current solution, not sure why. Something about the sums being different. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1894:58,perform,performs,58,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1894,1,['perform'],['performs']
Performance,"```; adata=sc.read('./CONFIDENTIAL_04022019.h5ad'); ```; ---------------------------------------------------------------------------; ```; OSErrorTraceback (most recent call last); <ipython-input-11-759ccdc7c8be> in <module>(); ----> 1 adata=sc.read('/gpfs/ysm/pi/zhao/wd262/sc/CONFIDENTIAL_04022019.h5ad'); 2 #> AnnData object with n_obs × n_vars = 312928 × 45947. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:543,cache,cache,543,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,3,['cache'],['cache']
Performance,"```pytb; >>> adata.write(""./result.h5ad""); >>> bdata = sc.read(""./result.h5ad). ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-15-d90a365327a0> in <module>(); 1 adata.write(“./results.h5ad”); ----> 2 bdata = sc.read(“./results.h5ad""). /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed); 347 # load everything into memory; 348 d = _read_h5ad(filename=filename); --> 349 return AnnData(d); 350 ; 351 . /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in __init__(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode, asview, oidx, vidx); 632 obsm=obsm, varm=varm, raw=raw,; 633 dtype=dtype, single_col=single_col,; --> 634 filename=filename, filemode=filemode); 635 ; 636 def _init_as_view(self, adata_ref, oidx, vidx):. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _init_as_actual(self, X, obs, var, uns, obsm, varm, raw, dtype, single_col, filename, filemode); 741 raise ValueError(; 742 'If `X` is a dict no further arguments must be provided.'); --> 743 X, obs, var, uns, obsm, varm, raw = self._from_dict(X); 744 ; 745 # init from AnnData. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _from_dict(ddata); 1591 d_true_keys['obs'][k_stripped] = pd.Categorical.from_codes(; 1592 codes=d_true_keys['obs'][k_stripped].values,; -> 1593 categories=v); 1594 if k_stripped in d_true_keys['var']:; 1595 d_true_keys['var'][k_stripped] = pd.Categorical.from_codes(. /fastdata/chris/bin/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in from_codes(cls, codes, categories, ordered); 616 ""codes need to be convertible to an arrays of integers""); 617 ; --> 618 categories = CategoricalDtype._validate_categories(categories); 619 ; 620 if len(codes) and (codes.max() >= len(categories) or codes.min() < -1):. /fastdata/chris/bin/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102:447,load,load,447,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102,1,['load'],['load']
Performance,"```pytb; package=scanpy; pversion=1.5.1 #found in docs/release-latest.rst after git download; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; cd /usr/common/src; git clone https://github.com/theislab/scanpy.git; cd scanpy; module load python3-libraries #for PYTHONPATH; python3 ./setup.py install \; --install-scripts=$TOPDIR/bin --prefix /usr/common \; 2>&1 | tee ../install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:268,load,load,268,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,2,['load'],['load']
Performance,"```python; import scanpy as sc; ```. ```pytb; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_22924/912249142.py in <module>; ----> 1 import scanpy as sc. ~\Anaconda3\lib\site-packages\scanpy\__init__.py in <module>; 13 Verbosity,; 14 ) # start with settings as several tools are using it; ---> 15 from . import tools as tl; 16 from . import preprocessing as pp; 17 from . import plotting as pl. ~\Anaconda3\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\Anaconda3\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\Anaconda3\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\Anaconda3\lib\site-packages\tables\__init__.py in <module>; 40 # Import the user classes from the proper modules; 41 from .exceptions import *; ---> 42 from .file import File, open_file, copy_file; 43 from .node import Node; 44 from .group import Group. ~\Anaconda3\lib\site-packages\tables\file.py in <module>; 21 import numpy as np; 22 ; ---> 23 from . import hdf5extension; 24 from . import utilsextension; 25 from . import parameters. ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173:1656,load,load,1656,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173,1,['load'],['load']
Performance,``biomart_annotations`` still creates cache when ``use_cache=False``,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2861:38,cache,cache,38,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2861,1,['cache'],['cache']
Performance,"`sc.external.pp.harmony_integrate(adata, 'Batch')` only adds an obsm key `'X_pca_harmony'`. In order to analyze values for a specific gene after harmony, you would need to multiply `X_pca_harmony` by a loadings matrix, but `adata.varm` doesn't seem to get a `harmony_PCs` to accompany vanilla `adata.varm['PCs']`. . Is there a method I'm missing to analyze a harmony-corrected counts matrix in scanpy? If so, I'd love to see it documented. If not, I'd love to see it added.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2314:202,load,loadings,202,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2314,1,['load'],['loadings']
Performance,"`sc.get` is a good suggestion, too! I'd be fine with it. > diffxpy. @davidsebfischer: do you feel you have a mature solution for storing simple difftest results that could be reused for `rank_genes_groups`? If yes, can you point us to it? It might be that you don't as you have these relatively powerful objects that do a lot more than what we want in the context of a simple Wilcoxon Rank group-vs-reference comparison. > My impression is xarray were designed to be similar to netCDF files, which are a subset of hdf5. pandas, on the other hand, has a pretty opaque hdf5 representation. If xarray does everything we want (sparse and categorical data), that would be great, of course. I was investigating pandas hdf5 early on and decided against it as it was very opaque (e.g., I couldn't see how to easily implement on-disk concatenation on it) and it didn't seem to offer performance gains.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836:874,perform,performance,874,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487930836,1,['perform'],['performance']
Performance,"`score_genes` does not work on a dataset loaded with `backed=""r+""`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/883:41,load,loaded,41,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883,1,['load'],['loaded']
Performance,"a-forge; threadpoolctl 3.2.0 pyha21a80b_0 conda-forge; tk 8.6.13 noxft_h4845f30_101 conda-forge; tqdm 4.66.1 pyhd8ed1ab_0 conda-forge; tzdata 2023d h0c530f3_0 conda-forge; umap-learn 0.5.5 py311h38be061_0 conda-forge; wheel 0.42.0 pyhd8ed1ab_0 conda-forge; xorg-libxau 1.0.11 hd590300_0 conda-forge; xorg-libxdmcp 1.1.3 h7f98852_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; zstd 1.5.5 hfc55251_0 conda-forge; ```. 2) I imported the scanpy, seaborn, pandas, numpy and matplotlib libraries. Then I called the `read_10x_mtx()` function. The code is given below. ```pycon; >>> import scanpy as sc; >>> import pandas as pd; >>> import numpy as np; >>> import matplotlib; >>> import seaborn as sns; >>> !ls -lh ./H004/; -rwxrwxrwx. 1 nikolay nikolay 49K Mar 25 2021 barcodes.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 424K Mar 25 2021 features.tsv.gz; -rwxrwxrwx. 1 nikolay nikolay 101M Mar 25 2021 matrix.mtx.gz; >>> adata = sc.read_10x_mtx(; ... './H004/', ; ... var_names='gene_symbols', ; ... cache=True); >>> adata.var_names_make_unique(); >>> print(adata.var); gene_ids feature_types; Gm26206 ENSMUSG00000064842 Gene Expression; Gm26206-1 ENSMUSG00000064842 Gene Expression; Gm26206-2 ENSMUSG00000064842 Gene Expression; Gm26206-3 ENSMUSG00000064842 Gene Expression; Gm26206-4 ENSMUSG00000064842 Gene Expression; ... ... ...; Gm26206-55445 ENSMUSG00000064842 Gene Expression; Gm26206-55446 ENSMUSG00000064842 Gene Expression; Gm26206-55447 ENSMUSG00000064842 Gene Expression; Gm26206-55448 ENSMUSG00000064842 Gene Expression; Gm26206-55449 ENSMUSG00000064842 Gene Expression. [55450 rows x 2 columns]; ```. **The problem is the error in importing gene names both when using id and when using symbolic labeling. All genes have the same name. if you use `anndata=0.10.3` instead of `anndata=0.10.4`, then everything works correctly.**. ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; import matplotlib; import seaborn as sns. path='<path_to_files>'. adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:6107,cache,cache,6107,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cache']
Performance,"a/Roaming/Python/Python312/site-packages/pandas/io/common.py:777) ). File c:\Program Files\Python312\Lib\gzip.py:192, in GzipFile.__init__(self, filename, mode, compresslevel, fileobj, mtime); [190](file:///C:/Program%20Files/Python312/Lib/gzip.py:190) mode += 'b'; [191](file:///C:/Program%20Files/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:19950,cache,cache,19950,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the memory usage is due to something else than the data not being backed? It is only 8000 cells and 15000 genes. Cheers,; Samuele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:2410,Perform,Perform,2410,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['Perform'],['Perform']
Performance,"adata.obs[""single_cat""] = 1; adata.obs['single_cat'] = pd.Categorical(adata.obs['single_cat']); adata.write('/tmp/adata.h5ad'); sc.read('/tmp/adata.h5ad'); ```. Returns this error message:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-5-adde38d13544> in <module>; ----> 1 sc.read('/tmp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1199,cache,cache,1199,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['cache'],['cache']
Performance,added example for PCA loadings plot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1815:22,load,loadings,22,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1815,1,['load'],['loadings']
Performance,"ading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 501 in get_async; File ""<venv>/lib/p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3585,concurren,concurrent,3585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"age](https://user-images.githubusercontent.com/8238804/105129304-dc76c700-5b38-11eb-9be6-3f5613b24037.png). *Additional plot using color generating code from above:*. <details>; <summary> code </summary>. ```python; from matplotlib.colors import to_hex. colors = list(itertools.islice(rgbs(), len(set(color_map.values())))); tarashansky_palette = {k: to_hex(colors[v]) for k, v in color_map.items()}. tf.shade(pts, color_key=tarashansky_palette); ```. </details>. ![image](https://user-images.githubusercontent.com/8238804/105131791-c3244980-5b3d-11eb-83cc-2691b392b1c1.png). I think it's good, but there's room for improvement. I was initially worried about this example since the cluster connectivity graph is highly connected, but this seems to have worked out alright. Overplotting (points sitting on top of eachother) is still a problem, but I think that's a separate problem from choosing colors. * Even if they do not touch, can we make it so clusters close to each-other are less likely to get similar colors? This would start becoming more of an optimization problem, and more complicated.; * Picking a color palette where all colors are very visible is important. (maybe adding a light border when there's nothing in the background? maybe something similar to ""player model contrast boost"" shaders?); * I think there are some ""spurious"" connections in the graph. Many clusters have >20 neighbors. I think this has to do with outlier points and dispersed points. Not completely sure how to deal with this. Maybe less of an issue with smaller datasets/ leiden clustering?; * When categories are disconnected, how do we indicate they're the same category if unique color is no longer an option? Do we require disconnected categories be uniquely assigned a color? Is this a case for interactivity?. I've been thinking that it might be worth starting a package for dealing with common issues in plotting single cell data. Largely involving color assignment and overplotting. I think this should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345:4482,optimiz,optimization,4482,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1366#issuecomment-763341345,1,['optimiz'],['optimization']
Performance,"aging import version. ~\.conda\envs\NewPy38\lib\site-packages\anndata\__init__.py in <module>; 5 if not within_flit():; 6 del within_flit; ----> 7 from ._core.anndata import AnnData, ImplicitModificationWarning; 8 from ._core.merge import concat; 9 from ._core.raw import Raw. ~\.conda\envs\NewPy38\lib\site-packages\anndata\_core\anndata.py in <module>; 15 from typing import Tuple, List # Generic; 16 ; ---> 17 import h5py; 18 from natsort import natsorted; 19 import numpy as np. ~\.conda\envs\NewPy38\lib\site-packages\h5py\__init__.py in <module>; 31 raise; 32 ; ---> 33 from . import version; 34 ; 35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:. ~\.conda\envs\NewPy38\lib\site-packages\h5py\version.py in <module>; 13 ; 14 from collections import namedtuple; ---> 15 from . import h5 as _h5; 16 import sys; 17 import numpy. h5py\h5.pyx in init h5py.h5(). ImportError: DLL load failed while importing defs; ````; Step4: I do `!pip uninstall h5py` and `conda install -c conda-forge pytables h5py`, then; ```python; import numpy as np; import pandas as pd; import scanpy as sc; import scanpy.external as sce; import scipy; sc.settings.verbosity = 3; sc.logging.print_header(); sc.set_figure_params(dpi=100, dpi_save=600). ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_14912/1710492625.py in <module>; 1 import numpy as np; 2 import pandas as pd; ----> 3 import scanpy as sc; 4 import scanpy.external as sce; 5 import scipy. ~\.conda\envs\NewPy38\lib\site-packages\scanpy\__init__.py in <module>; 4 ; 5 if not within_flit(): # see function docstring on why this is there; ----> 6 from ._utils import check_versions; 7 ; 8 check_versions(). ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\__init__.py in <module>; 27 from .. import logging as logg; 28 ; ---> 29 from .compute.is_constant import is_constant; 30 ; 31 . ~\.conda\envs\NewPy38\lib\site-packages\scanpy\_utils\compute\is_constant.py in <module>; 3 ; 4 import numpy as np; ---->",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841:5470,load,load,5470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108#issuecomment-1012790841,1,['load'],['load']
Performance,"ah, yes, i confused `.scanpy` and `.write`. i think caching in a real cache directory instead of `./.write` would be better in any case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346320991:70,cache,cache,70,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346320991,1,['cache'],['cache']
Performance,"also, the fact that reshuflling is performed is not in docs and should be documented. @bio-la do you plan to work on this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637:35,perform,performed,35,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/987#issuecomment-1054226637,1,['perform'],['performed']
Performance,"analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; In function read_10x_mtx there could be an option to search for non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, ca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1130,cache,cache,1130,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"anceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. when I run; ```; from joblib import parallel_backend; with parallel_backend('threading', n_jobs=15):; sc.pp.neighbors(adata, n_neighbors=100, n_pcs=12); ```. as suggested in #659, it takes 1 min 28 seconds, empirically mostly uses 1 cpu, and gives the following warning. ```; /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/rp_tree.py"", line 135:; @numba.njit(fastmath=True, nogil=True, parallel=True); def euclidean_random_projection_split(data, indices, rng_state):; ^. self.func_ir.loc)); /opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/numba/compiler.py:602: NumbaPerformanceWarning: ; The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible. To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help. File ""../../../../../opt/miniconda3/envs/py37_2/lib/python3.7/site-packages/umap/nndescent.py"", line 47:; @numba.njit(parallel=True); def nn_descent(; ^. self.func_ir.loc)); ```. Are there any tips on how I can benefit from parallelization in these nearest neighbor computations? This is the bottleneck in my work flow.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/913:3669,bottleneck,bottleneck,3669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/913,1,['bottleneck'],['bottleneck']
Performance,"anpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L105; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/neighbors/__init__.py#L258. There is a chance that this can also be solved with an import from UMAP.; https://github.com/theislab/scanpy/blob/07606455c524b38c4efec475a0d7ba87251754bb/scanpy/tools/_umap.py#L107. As just discussed, @Koncopd, can you look into this and make a PR that gets rid of the umap legacy code?. Thank you so much!; Alex. PS: Just wrote an explanation for the reasons why I intorduced the duplicated code in the first place.; > The duplicated code in Scanpy came about as I wanted to very quickly move forward with a version 1.0 of Scanpy about a year ago. UMAP was just becoming available on GitHub and there wasn’t even a preprint, I think. It changed very quickly and there were dramatic bugs every now and then. Nonetheless it was clear that it’s a major improvement over existing solutions, both in terms of computational performance, quality of the result and ease of installation and use. I wanted to achieve two things: (i) I had to rewrite some parts of UMAP so that I could decompose it a neighbors computing and a dedicated embedding step; you know that in Scanpy, the neighborhood graph is used for many other things other than for the embedding (clustering and trajectory inference). I also added the Gaussian kernel solution that I had before switching to a “UMAP backend” for `pp.neighbors`; which was needed so that results for DPT could be reproduced. All of this would have been quite a discussion with Leland. Until we would have had settled on the “Scanpy needs” that certainly weren’t aligned with the development of an independent young package, PRs would have been integrated to much time would have been lost. Finally, I wanted absolute reproducibility for Scanpy users, which could only be achieved by “freezing the code”. So, I asked Leland whether he is OK if I add a frozen ver",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/522:1315,perform,performance,1315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/522,1,['perform'],['performance']
Performance,"apper(*args, **kwargs); 201 try:; --> 202 return func(*args, **kwargs); 203 except Exception as e:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\registry.py:235, in Reader.read_elem(self, elem, modifiers); 234 if self.callback is not None:; --> 235 return self.callback(read_func, elem.name, elem, iospec=get_spec(elem)); 236 else:. File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\h5ad.py:241, in read_h5ad..callback(func, elem_name, elem, iospec); 240 return read_dataframe(elem); --> 241 return func(elem). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_io\specs\methods.py:500, in read_sparse(elem, _reader); 495 @_REGISTRY.register_read(H5Group, IOSpec(""csc_matrix"", ""0.1.0"")); 496 @_REGISTRY.register_read(H5Group, IOSpec(""csr_matrix"", ""0.1.0"")); 497 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csc_matrix"", ""0.1.0"")); 498 @_REGISTRY.register_read(ZarrGroup, IOSpec(""csr_matrix"", ""0.1.0"")); 499 def read_sparse(elem, _reader):; --> 500 return SparseDataset(elem).to_memory(). File c:\Users\echen\Anaconda3\envs\lineageOT\lib\site-packages\anndata\_core\sparse_dataset.py:379, in SparseDataset.to_memory(self); ...; 189 f""Above error raised while reading key {elem.name!r} of ""; 190 f""type {type(elem)} from {parent}.""; 191 ) from e. AnnDataReadError: Above error raised while reading key '/X' of type from /.; ```. ### Versions. <details>. ```; -----; anndata 0.9.2; scanpy 1.9.3; -----; PIL 10.0.0; PyQt5 NA; anyio NA; arrow 1.2.3; asttokens NA; attr 23.1.0; attrs 23.1.0; babel 2.12.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.7; brotli 1.0.9; certifi 2023.07.22; cffi 1.15.1; charset_normalizer 3.2.0; colorama 0.4.6; comm 0.1.4; cvxopt 1.3.1; cycler 0.10.0; cython_runtime NA; ...; Python 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:34:57) [MSC v.1936 64 bit (AMD64)]; Windows-10-10.0.19045-SP0; -----; Session information updated at 2023-08-04 10:17; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2592:2340,bottleneck,bottleneck,2340,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2592,1,['bottleneck'],['bottleneck']
Performance,"arding the two other things you changed:; - `chunked` and `chunk_size` are in particular important when running an `AnnData` object in `backed` mode, when it's so large that it doesn't fit into memory. To date, this only works for the two functions that were the bottleneck for very large data (`pp.log1p` and `pp.pca`), where it already gives remarkable memory use reduction in `memory` mode. Of course, this is considerably slower than feeding in the full data matrix. We'll use AnnData's chunked functionality in other tools, soon. We're also using it when working with tensorflow. At some point, when you open an AnnData in `backed` mode, the whole pipeline will run through by processing chunks and the user won't have to do a single change to his or her code. By that, code that has been written for data that fits into memory will automatically scale to many millions of observations. Also, there will be global settings that allow to manually determine whether the whole pipeline should run on chunks but still load the basic data matrix into memory, something we've found useful in several occasions.; - not returning `None` when modifying a reference inplace: the very first draft of Scanpy was written this way. then @flying-sheep remarked, that it shouldn't and I agreed with him right away: if you return the changed object, you'll allow two different variable names for the same reference. This is a dangerous source for bugs - this was one of the few instances where I produced more bugs than in C++, where one would always write inplace functions (taking pointers or references) that return `void`. In addition, returning `None` directly tells the user that the typical code for writing pipelines does not have to be redundant: `function(adata)` instead of `adata = function(adata)`. Finally: all of Scanpy is consistently written using these principles and it would cause a lot of trouble both changing it in a simple function and changing it everywhere. Why do you think that _it al",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196:1613,load,load,1613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/191#issuecomment-403240196,1,['load'],['load']
Performance,"as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3456 if self.columns.nlevels > 1:; 3457 return self._getitem_multilevel(key); -> 3458 indexer = self.columns.get_loc(key); 3459 if is_integer(indexer):; 3460 indexer = [indexer]. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:; -> 3363 raise KeyError(key) from err; 3364 ; 3365 if is_scalar(key) and isna(key) and not self.hasnans:. KeyError: 1; ```. Any ideas?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:1825,cache,cache,1825,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['cache'],['cache']
Performance,"as np; import pandas as pd; import scanpy as sc; import pdb. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_versions(). results_file = './write/pbmc3k.h5ad' # the file that will store the analysis result. sc.settings.set_figure_params(dpi=80). adata = sc.read_10x_mtx( 'filtered_gene_bc_matrices/hg19/', var_names='gene_symbols', cache=True) . adata.var_names_make_unique() # this is unnecessary if using 'gene_ids'; print(adata). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). pdb.set_trace(); ```. It sadly spits out the following output (see below), it seems like a mismatch of data structures somewhere inside the code. Or I hope I am trying to run an out of date example file. Thanks for all your help in advance.; Cheers. ```pytb; > scanpy==1.4.3 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 ; ... reading from cache file cache/filtered_gene_bc_matrices-hg19-matrix.h5ad; AnnData object with n_obs × n_vars = 2700 × 32738 ; var: 'gene_ids'. Traceback (most recent call last):; File ""test.py"", line 23, in <module>; sc.pp.filter_cells(adata, min_genes=200); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scanpy/preprocessing/_simple.py"", line 126, in filter_cells; adata._inplace_subset_obs(cell_subset); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1372, in _inplace_subset_obs; adata_subset = self[index].copy(); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1230, in __getitem__; return self._getitem_view(index); File ""/Users/Person/Library/Python/3.6/lib/python/site-packages/anndata-0.6.22.post1-py3.6.egg/anndata/core/anndata.py"", line 1234, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Use",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/734:1201,cache,cache,1201,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/734,2,['cache'],['cache']
Performance,"ase.py"",; > line 635, in *init*; > self._init_as_view(X, oidx, vidx); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"",; > line 661, in _init_as_view; > var_sub = adata_ref.var.iloc[vidx_normalized]; > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1478, in *getitem*; > return self._getitem_axis(maybe_callable, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 2087, in _getitem_axis; > return self._getbool_axis(key, axis=axis); > File; > ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"",; > line 1494, in _getbool_axis; > inds, = key.nonzero(); > ValueError: too many values to unpack (expected 1); >; > I've tried several variations of this yet I don't see why your command; > wouldn't work, it seems like it should do what you intend ..; >; > Note however, I ran :; >; > print(np.any(adata.X.sum(axis=0) == 0)) # True; > print(np.any(adata.X.sum(axis=1) == 0)) # False; >; > right after loading the dataset and it still shows True and False, yet if; > I were to regress out WITHOUT removing cell types via:; >; > Temp = [i for i in adata.obs.index if i not in adata_blood.obs.index]; > adata = adata[Temp,:]; >; > or; >; > adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells; >; > ... the regression will work. However, once I remove, it won't. I could; > try to remove the 0 columns with R, unless you have another suggestion?; >; > Thank you for any feedback.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/theislab/scanpy/issues/230#issuecomment-412098297>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEu_1bD5gUmq-bUVfAJT2AGlXKXEkOmxks5uPZfTgaJpZM4V0Faw>; > .; >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475:2046,load,loading,2046,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412105475,1,['load'],['loading']
Performance,"aster branch of scanpy. ### What happened?. When I install scanpy==1.9.6 with pip (anndata==0.10.4), something wrong and adata.X.nnz is 0.; I changed the version of anndata to 0.9.2, it works normal. ### Minimal code sample. ```python; import numpy as np; import pandas as pd; import scanpy as sc; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(my_sample, # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=False) # write a cache file for faster subsequent reading; # sc.pl.highest_expr_genes(adata, n_top=20, ); adata.X.nnz; ```. ### Error output. _No response_. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.5; -----; PIL 9.5.0; asttokens NA; backcall 0.2.0; bottleneck 1.3.5; cffi 1.16.0; comm 0.1.2; cycler 0.12.0; cython_runtime NA; dateutil 2.8.2; debugpy 1.6.7; decorator 4.4.2; defusedxml 0.7.1; entrypoints 0.4; executing 1.2.0; google NA; h5py 3.7.0; hurry NA; ipykernel 6.25.0; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.1; joblib 1.2.0; kiwisolver 1.4.5; llvmlite 0.41.1; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; numba 0.58.1; numexpr 2.8.7; numpy 1.26.0; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; prompt_toolkit 3.0.36; psutil 5.9.0; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 13.0.0; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.15.1; pynndescent 0.5.11; pyparsing 3.0.9; pytz 2023.3.post1; ruamel NA; scipy 1.11.3; session_info 1.0.0; setuptools 68.0.0; setuptools_scm NA; six 1.16.0; sklearn 1.3.0; sphinxcontrib NA; stack_data 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2822:1242,bottleneck,bottleneck,1242,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2822,1,['bottleneck'],['bottleneck']
Performance,"aster branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). This is exactly the code on https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html using the dataset downloaded per the instructions. The umap step does not produce the correct result. tsne however produces sensible results.; ```python; import numpy as np; import pandas as pd; import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'). results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results. adata = sc.read_10x_mtx(; 'data/filtered_gene_bc_matrices/hg19/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) . adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`. sc.pl.highest_expr_genes(adata, n_top=20, ). sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3). adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True). sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'],; jitter=0.4, multi_panel=True). sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'). adata = adata[adata.obs.n_genes_by_counts < 2500, :]; adata = adata[adata.obs.pct_counts_mt < 5, :]. sc.pp.normalize_total(adata, target_sum=1e4). sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5). sc.pl.highly_variable_genes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2178:1247,cache,cache,1247,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2178,1,['cache'],['cache']
Performance,"aster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1500,cache,cache,1500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,3,['cache'],['cache']
Performance,"at32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser 2.20; pygments 2.9.0; pyparsing 2.4.7; pytz 2021.1; scipy 1.6.2; scvelo 0.2.3; six 1.16.0; sklearn 0.24.2; storemagic NA; tables 3.6.1; texttable 1.6.4; tornado 6.1; traitlets 5.0.5; wcwidth 0.2.5; zipp NA; zmq 22.1.0; -----; IPython 7.26.0; jupyter_client 6.1.12; jupyter_core 4.7.1; notebook 6.4.0; -----; Python 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05) [GCC 9.3.0]; Linux-4.18.0-240.22.1.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:4384,bottleneck,bottleneck,4384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['bottleneck'],['bottleneck']
Performance,"ata for xarray) for sc-seq analyses. ### Minimal code sample (that we can copy&paste without having any data). ```python; import zarr; import anndata as ad; import dask.array as da; import scanpy as sc. # write data to zarr file; rel_zarr_path = 'data/pbmc3k_processed.zarr'; adata = sc.datasets.pbmc3k_processed(); adata.write_zarr(f'./{rel_zarr_path}', chunks=[adata.shape[0], 5]); zarr.consolidate_metadata(f'./{rel_zarr_path}'). # read data from zarr file with X as a dask array; def read_dask(store):; f = zarr.open(store, mode=""r""). def callback(func, elem_name: str, elem, iospec):; if iospec.encoding_type in (; ""dataframe"",; ""csr_matrix"",; ""csc_matrix"",; ""awkward-array"",; ):; # Preventing recursing inside of these types; return ad.experimental.read_elem(elem); elif iospec.encoding_type == ""array"":; return da.from_zarr(elem); else:; return func(elem). adata = ad.experimental.read_dispatched(f, callback=callback). return adata; adata_dask = read_dask(f'./{rel_zarr_path}'). # perform preprocessing; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); sc.pp.scale(adata_dask, max_value=10); ```. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); Cell In[1], line 39; 37 sc.pp.log1p(adata); 38 # sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); ---> 39 sc.pp.scale(adata_dask, max_value=10). File ~/miniconda3/envs/omics/lib/python3.10/functools.py:889, in singledispatch..wrapper(*args, **kw); 885 if not args:; 886 raise TypeError(f'{funcname} requires at least '; 887 '1 positional argument'); --> 889 return dispatch(args[0].__class__)(*args, **kw). File ~/miniconda3/envs/omics/lib/python3.10/site-packages/scanpy/preprocessing/_simple.py:844, in scale_anndata(adata, zero_center, max_value, copy, layer, obsm); 842 view_to_actual(adata); 843 X = _get_obs_rep(adata, layer=layer,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2491:2836,perform,perform,2836,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2491,1,['perform'],['perform']
Performance,"ateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4029,cache,cached,4029,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-versio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4368,cache,cached,4368,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"b ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer'); 598 def do_3d_projection(self, renderer=None):; --> 599 xs, ys, zs = self._offsets3d; 600 vxs, vys, vzs, vis = proj3d.proj_transform_clip(xs, ys, zs,; 601 self.axes.M); 602 # Sort the points based on z coordinates; 603 # Performance optimization: Create a sorted index array and reorder; 604 # points and point properties according to the index array. AttributeError: 'Path3DCollection' object has no attribute '_offsets3d'. ```. #### Versions. <details>. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.1.1; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; asttokens NA; astunparse 1.6.3; backcall 0.2.0; batchglm v0.7.4; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.4; certifi 2022.05.18.1; cffi 1.15.0; charset_normalizer 2.0.12; cloudpickle 2.1.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; dask 2022.6.1; dateutil 2.8.2; debugpy 1.6.0; decorator 5.1.1; defusedxml 0.7.1; deprecated 1.2.13; diffxpy v0.7.4; entrypoints 0.4; executing 0.8.3; flatbuffers NA; fsspec 2022.5.0; future 0.18.2; gast NA; google NA; graphtools 1.5.2; h5py 3.7.0; hypergeom_ufunc NA; idna 3.3; ipykernel 6.15.0; ipython_genutils 0.2.0; ipywidgets 7.7.0; jedi 0.18.1; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:10172,Perform,Performance,10172,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,2,"['Perform', 'optimiz']","['Performance', 'optimization']"
Performance,"brief recap: https://github.com/theislab/scanpy/pull/130 was the initial work on integrating RNA velocity into scanpy, which was a slimmed version of velocyto; yet not working well due to its simplification and several missing required processing steps. Consequently, and with the additional objective of extending velocyto, we outsourced that to scvelo. For directed paga this is already adjusted. I think we missed https://github.com/theislab/scanpy/blob/740c4a510ec598ab03ff3de1d9b1c091f0aac292/scanpy/plotting/_utils.py#L334; the convention became `'velocity_' + basis ` (instead of `'Delta_' + basis `). This is used only for scatter plots, if I get it correctly. The velocity plotting modules within scvelo have been extensively optimized, thus questionable whether still needed within scanpy. Anything else I am missing?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420:735,optimiz,optimized,735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/792#issuecomment-523824420,1,['optimiz'],['optimized']
Performance,"bs(); adata.raw = adata; sc.tl.score_genes(adata, ['0']); ```. ```pytb; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-9-c835b3ad221d> in <module>; 2 adata = sc.datasets.blobs(); 3 adata.raw = adata; ----> 4 sc.tl.score_genes(adata, ['0']). ~/miniconda3/envs/spols200618/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw); 174 elif len(gene_list) == 1:; 175 if _adata[:, gene_list].X.ndim == 2:; --> 176 vector = _adata[:, gene_list].X.toarray()[:, 0] # new anndata; 177 else:; 178 vector = _adata[:, gene_list].X # old anndata. AttributeError: 'numpy.ndarray' object has no attribute 'toarray'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 7.1.2; anndata 0.7.4; backcall 0.2.0; bottleneck 1.3.2; cairo 1.19.1; cffi 1.14.0; cloudpickle 1.3.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; dask 2.18.1; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; google NA; h5py 2.10.0; igraph 0.8.2; importlib_metadata 1.6.1; ipykernel 5.3.0; ipyparallel 6.3.0; ipython_genutils 0.2.0; jedi 0.17.0; joblib 0.15.1; kiwisolver 1.2.0; legacy_api_wrap 0.0.0; leidenalg 0.8.0; llvmlite 0.32.1; louvain 0.7.0; matplotlib 3.2.1; mpl_toolkits NA; natsort 7.0.1; numba 0.49.1; numexpr 2.7.1; numpy 1.17.5; packaging 20.4; pandas 1.0.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.7; pytz 2020.1; scanpy 1.6.0; scipy 1.4.1; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.1; storemagic NA; tables 3.6.1; texttable 1.6.2; tlz 0.10.0; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.4; yaml 5.3.1; zipp NA; zmq 19.0.1; -----; IPython 7.15.0; jupyter_client",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1395:1480,bottleneck,bottleneck,1480,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1395,1,['bottleneck'],['bottleneck']
Performance,"busercontent.com/26215587/118845227-7ec67180-b8c3-11eb-9bce-a877ef26b47a.png). If i don't use harmony and just ran with `n_pcs = 50`, the result is similar.; ```python; adatax = adata.copy(); sc.pp.neighbors(adatax, n_pcs = 50); sc.tl.umap(adatax, min_dist = .3); sc.pl.umap(adatax, color = 'sample'); ```; ![image](https://user-images.githubusercontent.com/26215587/118845811-01e7c780-b8c4-11eb-8ca9-3ea639d97684.png). What i don't get is, why does the following work then?; ```python; # my current workaround; adata2 = adata.copy(); sc.external.pp.harmony_integrate(adata2, key = 'sample', adjusted_basis='X_pca'); sc.pp.neighbors(adata2, n_pcs = 20); sc.tl.umap(adata2); sc.pl.umap(adata2, color = 'sample'); ```; ![image](https://user-images.githubusercontent.com/26215587/118843486-04492200-b8c2-11eb-8e2b-6aaf59564721.png). #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; Bio 1.78; PIL 8.2.0; adjustText NA; anndata 0.7.5; annoy NA; backcall 0.2.0; brotli NA; cachecontrol 0.12.6; cairo 1.19.1; certifi 2020.06.20; cffi 1.14.5; changeo 1.0.2; chardet 4.0.0; cloudpickle 1.6.0; cycler 0.10.0; cython_runtime NA; dandelion 0.1.2; dask 2021.03.0; dateutil 2.8.1; decorator 5.0.6; descartes NA; distance NA; get_version 2.1; google NA; h5py 2.10.0; harmonypy NA; hdmedians NA; idna 2.10; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.34.0; matplotlib 3.3.4; mizani 0.7.2; mpl_toolkits NA; msgpack 1.0.2; natsort 7.1.1; networkx 2.5; numba 0.51.2; numexpr 2.7.1; numpy 1.20.2; packaging 20.9; palettable 3.3.0; pandas 1.2.4; parso 0.8.2; patsy 0.5.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotnine 0.7.1; polyleven NA; presto 0.6.2; prompt_toolkit 3.0.17; ptyprocess 0.7.0; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pytoml NA; pytz 2021.1; pywt 1.1.1; requests 2.25.1; scanpy 1.7.1; scipy 1.6.3; scrublet ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1846:2248,cache,cachecontrol,2248,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1846,1,['cache'],['cachecontrol']
Performance,"c.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 53, in _highly_variable_genes_seurat_v3; from skmisc.loess import loess; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); File ""C:\Users\Administrator\AppData\Roaming\Python\Python39\site-packages\skmisc\loess\__init__.py"", line 51, in <module>; from ._loess import (loess, loess_model, loess_inputs, loess_control,; File ""D:\pycharm\PyCharm Community Edition 2021.3.3\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import; module = self._system_import(name, *args, **kwargs); ImportError: DLL load failed while importing _loess: 找不到指定的模块。; During handling of the above exception, another exception occurred:; Traceback (most recent call last):; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\IPython\core\interactiveshell.py"", line 3378, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-22-92878e4bd6f9>"", line 1, in <module>; sc.pp.highly_variable_genes(adata, n_top_genes = 2000, subset = True, flavor = 'seurat_v3'); File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 422, in highly_variable_genes; return _highly_variable_genes_seurat_v3(; File ""D:\Anaconda3\ana\envs\scvi\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py"", line 55, in _highly_variable_genes_seurat_v3; raise ImportError(; ImportError: Please install skmisc package via `pip install --user scikit-misc; ```. #### Versions. <details>; scanpy 1.9.1; [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. </details>; I don't know how to deal with this problem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2352:1669,load,load,1669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2352,1,['load'],['load']
Performance,c3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 2023.4.12; tinycss2 1.2.1; tldextract 3.6.0; tokenizers 0.14.0; toml 0.10.2; tomli 2.0.1; tomlkit 0.12.1; toolz 0.12.0; toposort 1.10; tornado 6.3.3; tqdm 4.66.1; traitlets 5.11.2; transformers 4.34.0; truststore 0.8.0; Twisted 22.10.0; types-python-dateutil 2.8.19.14; typing_extensions 4.8.0; typing-utils 0.1.0; tzdata 2023.3; uc-micro-py 1.0.1; ujson 5.8.0; umap-learn 0.5.4; uri-template 1.3.0; urllib3 1.26.15; virtualenv 20.24.5; w3lib 2.1.2; watchdog 3.0.0; wcwidth 0.2.8; webcolors 1.13; webencodings 0.5.1; websocket-client 1.6.4; Werkzeug 3.0.0; whatthepatch 1.0.5; wheel 0.38.4; widgetsnbextension 4.0.9; wrapt 1.15.0; wurlitzer 3.0.3; xarray 2023.9.0; xxhash 3.4.1; xyzservices 2023.10.0; yapf 0.24.0; yarl 1.9.2; yte 1.5.1; zict 3.0.0; zipp 3.17.0; zope.interface 6.1; zstandard 0.21.0. ```. </details>,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:9936,throttle,throttler,9936,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['throttle'],['throttler']
Performance,"c88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f7411679000-7f741168d000 r--p 00075000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168d000-7f741168e000 r--p 00088000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168e000-7f741168f000 rw-p 00089000 fd:03 177974808 /home/dgc88/miniconda3/envs/scvi/lib/libwebp.so.7.1.5; 7f741168f000-7f7411691000 rw-p 00000000 00:00 0; 7f7411691000-7f741169a000 r--p 00000000 fd:03 339598995 /home/dgc88/miniconda3/envs/scvi/lib/libtiff.so.6.0.0; 7f741169a000-7f74116e8000 r-xp 00009000 fd:03 339598995 /home/dgc88/miniconda3/envs/scvi/lib/libtiff.so.6.0.0Aborted; ```. #### Versions. <details>. python: /home/dgc88/miniconda3/envs/scvi/bin/python; libpython: /home/dgc88/miniconda3/envs/scvi/lib/libpython3.10.so; pythonhome: /home/dgc88/miniconda3/envs/scvi:/home/dgc88/miniconda3/envs/scvi; version: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]; numpy: /home/dgc88/miniconda3/envs/scvi/lib/python3.10/site-packages/numpy; numpy_version: 1.23.5. python versions found:; /home/dgc88/miniconda3/bin/python3; /home/dgc88/miniconda3/bin/python; /home/dgc88/miniconda3/envs/scvi/bin/python. R version 4.2.3 (2023-03-15); Platform: x86_64-pc-linux-gnu (64-bit); Running under: Red Hat Enterprise Linux. Matrix products: default; BLAS: /opt/R/4.2.3/lib64/R/lib/libRblas.so; LAPACK: /opt/R/4.2.3/lib64/R/lib/libRlapack.so. locale:; [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C; [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8; [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8; [7] LC_PAPER=en_US.UTF-8 LC_NAME=C; [9] LC_ADDRESS=C LC_TELEPHONE=C; [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C. attached base packages:; [1] stats graphics grDevices utils datasets methods base. other attached packages:; [1] reticulate_1.28. loaded via a namespace (and not attached):; [1] compiler_4.2.3 Matrix_1.5-4 Rcpp_1.0.10 grid_4.2.3 jsonlite_1.8.4; [6] png_0.1-8 lattice_0.21-8. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2479:9538,load,loaded,9538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2479,1,['load'],['loaded']
Performance,"cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2019,cache,cache,2019,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,3,['cache'],['cache']
Performance,"cal\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 218 return; 219 try:; --> 220 mkdir(name, mode); 221 except OSError:; 222 # Cannot rely on checking for EEXIST, since the operating system. OSError: [WinError 123] The filename, directory name, or volume label syntax is incorrect: './cache/C:'; ```. Looks like the directory name for writing the file is messed up ./cache/C: it is trying to use linux formatting on a Windows machine. It works with cache = FALSE but I wanna try figuring out this problem first before moving on the other parts of the analysis. Any idea?. Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3849,race condition,race condition,3849,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,5,"['cache', 'race condition']","['cache', 'race condition']"
Performance,cannot load scanpy after updating to 1.4.6 via miniconda,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1166:7,load,load,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1166,1,['load'],['load']
Performance,"canpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/anndata/readwrite/read.py in read_excel(filename, sheet, dtype); 59 # rely on pandas for reading an excel file; 60 from pandas import read_excel; ---> 61 df = read_excel(fspath(filename), sheet, dtype=dtype); 62 X = df.values[:, 1:]; 63 row = {'row_names': df.iloc[:, 0].values.astype(str)}. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; --> 188 return func(*args, **kwargs); 189 return wrapper; 190 return _deprecate_kwarg. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 186 else:; 187 kwargs[new_arg_name] = new_arg_value; -",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:2030,cache,cache,2030,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,1,['cache'],['cache']
Performance,"canpy/preprocessing/utils.py in _get_mean_var(X); 16 mean_sq = np.multiply(X, X).mean(axis=0); 17 # enforece R convention (unbiased estimator) for variance; ---> 18 var = (mean_sq - mean**2) * (X.shape[0]/(X.shape[0]-1)); 19 else:; 20 from sklearn.preprocessing import StandardScaler. ~/miniconda3/lib/python3.6/site-packages/numpy/matrixlib/defmatrix.py in pow(self, other); 226; 227 def pow(self, other):; --> 228 return matrix_power(self, other); 229; 230 def ipow(self, other):. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in matrix_power(a, n); 600 a = asanyarray(a); 601 _assertRankAtLeast2(a); --> 602 _assertNdSquareness(a); 603; 604 try:. ~/miniconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py in _assertNdSquareness(*arrays); 213 m, n = a.shape[-2:]; 214 if m != n:; --> 215 raise LinAlgError('Last 2 dimensions of the array must be square'); 216; 217 def _assertFinite(*arrays):; ```. </details>. Versions of my modules:; scanpy==1.3.7 anndata==0.6.17 numpy==1.15.4 scipy==1.2.0 pandas==0.24.0 scikit-learn==0.20.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. I have downgraded pandas to 0.23.4, however, it not works. But I figured out where the problem lies in. ```py; adata.X /= adata.obs['size_factors'].values[:,None]; ```. This step transform the adata.X to a structure of matrix.; Before the adata.X is. ```; <6242x15065 sparse matrix of type '<class 'numpy.float32'>'; with 19234986 stored elements in Compressed Sparse Row format>; ```. But after performing this step, the adata.X is; This is my adata.X looks like right now:. ```py; matrix([[0. , 0. , 0. , ..., 0. , 0. , 0. ],; [0. , 0. , 1.203, ..., 0. , 0. , 0. ],; [0. , 1.096, 0. , ..., 0. , 0. , 0. ],; ...,; [0. , 0. , 2.042, ..., 0. , 0. , 0. ],; [0. , 0. , 0. , ..., 0.926, 0. , 0. ],; [0. , 0. , 2.951, ..., 0. , 0. , 0. ]]),; ```. And this format of adata.X caused error of sc.pp.highly_variable_genes. But I don't know how to fix it. Looking forward your response!; Thank you !",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/456:2595,perform,performing,2595,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/456,1,['perform'],['performing']
Performance,"canpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.6.1; kiwisolver 1.3.1; leidenalg 0.8.4; llvmlite 0.36.0; louvain 0.7.0; lxml 4.6.3; markupsafe 2.0.1; matplotlib 3.4.2; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.18.5; packaging 21.0; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.9.0; pylab NA; pynndescent",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3944,bottleneck,bottleneck,3944,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['bottleneck'],['bottleneck']
Performance,"canpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4535,cache,cached,4535,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"cb_kw = {k: v for k, v in kw.items() if k not in NON_COLORBAR_KEYS}; -> 2238 cb = cbar.colorbar_factory(cax, mappable, **cb_kw); 2239 ; 2240 self.sca(current_ax). [/usr/local/lib/python3.7/dist-packages/matplotlib/colorbar.py](https://localhost:8080/#) in colorbar_factory(cax, mappable, **kwargs); 1683 cb = ColorbarPatch(cax, mappable, **kwargs); 1684 else:; -> 1685 cb = Colorbar(cax, mappable, **kwargs); 1686 ; 1687 cid = mappable.callbacksSM.connect('changed', cb.on_mappable_changed). [/usr/local/lib/python3.7/dist-packages/matplotlib/colorbar.py](https://localhost:8080/#) in __init__(self, ax, mappable, **kw); 1228 kw['alpha'] = mappable.get_alpha(); 1229 ; -> 1230 ColorbarBase.__init__(self, ax, **kw); 1231 ; 1232 def on_mappable_changed(self, mappable):. TypeError: __init__() got an unexpected keyword argument 'location'; ```. #### Versions. <details>. anndata 0.8.0; scanpy 1.9.0. PIL 7.1.2; aa8f2297d25b4dc6fd3d98411eb3ba53823c4f42 NA; absl NA; astor 0.8.1; astunparse 1.6.3; bottleneck 1.3.4; cached_property 1.5.2; certifi 2021.10.08; cffi 1.15.0; chardet 3.0.4; cloudpickle 1.3.0; cvxopt 1.2.7; cycler 0.10.0; cython_runtime NA; dask 2.12.0; dateutil 2.8.2; debugpy 1.0.0; decorator 4.4.2; dill 0.3.4; flatbuffers 2.0; gast 0.5.3; google NA; google_auth_httplib2 NA; googleapiclient NA; h5py 3.1.0; httplib2 0.17.4; idna 2.10; igraph 0.9.9; ipykernel 4.10.1; ipython_genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.4; jaxlib 0.3.2; joblib 1.1.0; keras 2.8.0; keras_preprocessing 1.1.2; kiwisolver 1.4.0; leidenalg 0.8.9; llvmlite 0.34.0; matplotlib 3.2.2; mpl_toolkits NA; natsort 5.5.0; numba 0.51.2; numexpr 2.8.1; numpy 1.21.5; oauth2client 4.1.3; opt_einsum v3.3.0; packaging 21.3; pandas 1.3.5; patsy 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; portpicker NA; prompt_toolkit 1.0.18; psutil 5.4.8; ptyprocess 0.7.0; pyarrow 6.0.1; pyasn1 0.4.8; pyasn1_modules 0.2.8; pycparser 2.21; pydev_ipython NA; pydevconsole NA; pydevd 2.0.0; pydevd_concurrency_analyser NA; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2208:3729,bottleneck,bottleneck,3729,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2208,1,['bottleneck'],['bottleneck']
Performance,"changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so already, but for the time being just call `scanpy-mycommand` with a dash in there). > I was writing up how I'd like configuration to work when I realized the implementation could be getting complicated enough it might be worth just using a library. […] Generally, I think there should be a lon",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1423,cache,cache,1423,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cache']
Performance,"che, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz.; ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the future. Any solutions to these different formats???. Also, if you could provide the ling to a manual or a documentation written for scanpy that would be great.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:2363,cache,cache,2363,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,2,['cache'],['cache']
Performance,"ck (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\Python312\site-packages\pandas\io\parsers\readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:21074,cache,cache,21074,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"class 'numpy.int32'>`. I've tried numpy versions 1.26.3 and 1.24.4 as well. I've also tried uninstalling and reinstalling scanpy, jupyter notebook, and (consequently) other packages used in the tutorials (igraph, leidenalg, etc.). `numpy.random.test(label='full', verbose=2)` returned True, but noted 1322 passed, 6 skipped in 30.78s. ### Minimal code sample. ```python; sc.tl.leiden(; adata, ; resolution=0.9,; random_state=0,; flavor=""igraph"",; n_iterations=2,; directed=False,; ); ```. ### Error output. ```pytb; Exception ignored in: <class 'ValueError'>; Traceback (most recent call last):; File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint; File ""numpy\\random\\_bounded_integers.pyx"", line 1423, in numpy.random._bounded_integers._rand_int32; ValueError: high is out of bounds for int32; ```. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.1; -----; PIL 10.3.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; bottleneck 1.3.7; brotli 1.0.9; certifi 2024.02.02; cffi 1.16.0; charset_normalizer 2.0.4; colorama 0.4.6; comm 0.2.2; cycler 0.12.1; cython_runtime NA; dateutil 2.9.0; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.11.0; idna 3.4; ipykernel 6.29.3; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.4.0; json5 0.9.25; jsonpointer 2.1; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.10.0; jupyter_server 2.14.0; jupyterlab_server 2.26.0; kiwisolver 1.4.5; legacy_api_wrap NA; llvmlite 0.42.0; markupsafe 2.1.5; matplotlib 3.8.4; mpl_toolkits NA; natsort 8.4.0; nbformat 5.10.4; numba 0.59.1; numexpr 2.8.7; numpy 1.26.4; overrides NA; packaging 23.1; pandas 2.2.1; parso 0.8.4; pickleshare 0.7.5; platformdirs 3.10.0; prometheus_client NA; prompt_toolkit 3.0.42; psutil 5.9.8; pure_eval 0.2.2; pydev_ipython NA; pydevconsole NA; pydevd 2.9.5; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.17.2; pyparsing",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3028:2131,bottleneck,bottleneck,2131,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3028,1,['bottleneck'],['bottleneck']
Performance,cleaning caches did the trick. let’s see if the absolute path is also needed (rebuilding master): https://travis-ci.org/github/theislab/scanpy/builds/663442852. /edit: it did,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786:9,cache,caches,9,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113#issuecomment-600052786,1,['cache'],['caches']
Performance,"ct, you'd call `sc.ex.function`. I think DataFrames (a case like `tl.marker_gene_overlap`) should definitely be handled within AnnData and no `extract` function is necessary. But the differential expression result is a prime example for such a case. I think a function `rank_genes_groups` that returns a `RankGenesGroups` object, which then has `.to_df()` function (e.g. the function `rank_genes_groups` from (https://github.com/theislab/scanpy/pull/619) could immediately go into that namespace. Maybe we can even borrow a `diffxpy` object for that. The good thing is, we can keep the current rec arrays as they are very efficient and basic data types, which will work with hdf5 and zarr and xarray and everything else that might come in the future. And: Fidel wrote a ton of plotting functions around them already, which we don't want to simply rewrite... We don't have to as users won't see the recarrays anymore... Other possible names for the API would be `sc.cast` or `sc.object` (`sc.ob`), less conflicting with `sc.external`. I think `sc.ob` makes sense as it really makes clear that Scanpy's main API is for writing convenient scripts for compute-heavy stuff in a functional way. If one wants to transition to more light-weight ""post-analysis"", one can transition to objects that are designed for specific tasks. PS: I'd love to move away from the name `rank_genes_groups` at some point, and simply have something like `difftest` or `DiffTest`... I always thought that we might have differential expression tests for longitudinal data at some point (like Monocle), otherwise the function would be `rank_genes` but I don't think this is gonna happen soon, and if, it will be in the `external` API... A minimal difftest API should though continue be in the core of Scanpy, with at its heart, a scalable Wilcoxon rank (much more scalable than scipy's or diffxpy's), the t test and the scikit learn logreg approach. `diffxpy` with it's tensorflow dependency can then handle very complex cases...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:4818,scalab,scalable,4818,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,2,['scalab'],['scalable']
Performance,"cted components in umap (indices == -1) *replaced with our own implementation, see below*; - add tests; - [x] pyknndescent (we already depend on it through umap). Maybe in another PR?. - [ ] maybe store index in unified way?; - [ ] for umap: use `UMAP(precomputed_knn=...)` instead of `compute_connectivities`?; - [ ] unifiy transformer args, e.g. verbose. ## Implementation. ### The way it used to be. Our default use case was basically a thin wrapper around umap’s `nearest_neighbors` function, which in turn is a thin wrapper around PyNNDescent. We did some special casing around euclidean distance and small data sizes. That special casing reduced our test coverage: . - we don’t actually test umap’s pynndescent codepath at all (just the fast `precomputed` path for small data); - umap’s `precomputed` code does some weird things to its knn `indices` array, which we don’t test for: ; ![grafik](https://github.com/scverse/scanpy/assets/291575/7f36cafe-98fb-48cc-9e35-3972fad65a3e). The logic was:. - if small data (<8000) and euclidean metric or knn==False, calculate `pairwise_distances`; - if knn=True, then sparsify that matrix by pulling out KNN using `_get_indices_distances_from_dense_matrix` and converting that into a sparse one; - if not, run `umap.nearest_neighbors`.; - if even smaller data (<4000), calculate `pairwise_distances` like above and run `umap.nearest_neighbors` with `metric='precomputed'`. its internal logic then does the same as `_get_indices_distances_from_dense_matrix`; - else run `umap.nearest_neighbors` directly, which simply runs PyNNDescent with `n_trees` and `n_iters` set with a heuristic. ### now the logic is. - if small data (for either definition of small), calculate `pairwise_distances`, sparsify on demand. (this covers both our previous optimization and the umap `'precomputed'` code path); - else run a KNN transformer. if the old `'umap'` method was chosen, pick the same parameters mentioned aboveto the `PyNNDescent` transformer. ---. Fixes #2519",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2536:3801,optimiz,optimization,3801,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2536,1,['optimiz'],['optimization']
Performance,"ction to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1333,cache,cache,1333,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"d_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'; ```. ---; scanpy==1.0.4+11.gc241098 anndata==0.5.10 numpy==",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:2669,cache,cache,2669,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['cache'],['cache']
Performance,da-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; debugpy 1.6.7 py310heca2aa9_0 conda-forge; decorator 5.1.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9878,cache,cached-property,9878,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cached-property']
Performance,"dered=None.; --> 273 dtype = CategoricalDtype(categories, ordered); 274 ; 275 return dtype. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered); 158 ; 159 def __init__(self, categories=None, ordered: Ordered = False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:4133,Cache,CachedProperty,4133,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['Cache'],['CachedProperty']
Performance,"dex); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename))",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1487,cache,cache,1487,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,1,['cache'],['cache']
Performance,"ding to a regressed gene column). ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\scanpy\preprocessing\simple.py in _regress_out_chunk(data); 798 ; 799 responses_chunk_list = []; --> 800 import statsmodels.api as sm; 801 from statsmodels.tools.sm_exceptions import PerfectSeparationError; 802 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\api.py in <module>(); 3 from . import tools; 4 from .tools.tools import add_constant, categorical; ----> 5 from . import regression; 6 from .regression.linear_model import OLS, GLS, WLS, GLSAR; 7 from .regression.recursive_ls import RecursiveLS. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\__init__.py in <module>(); ----> 1 from .linear_model import yule_walker; 2 ; 3 from statsmodels import PytestTester; 4 test = PytestTester(); 5 . ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\regression\linear_model.py in <module>(); 46 cache_readonly,; 47 cache_writable); ---> 48 import statsmodels.base.model as base; 49 import statsmodels.base.wrapper as wrap; 50 from statsmodels.emplike.elregress import _ELRegOpts. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\base\model.py in <module>(); 13 from statsmodels.tools.sm_exceptions import ValueWarning, \; 14 HessianInversionWarning; ---> 15 from statsmodels.formula import handle_formula_data; 16 from statsmodels.compat.numpy import np_matrix_rank; 17 from statsmodels.base.optimizer import Optimizer. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\__init__.py in <module>(); 3 ; 4 ; ----> 5 from .formulatools import handle_formula_data. ~\AppData\Local\conda\conda\envs\scanpy\lib\site-packages\statsmodels\formula\formulatools.py in <module>(); 1 from statsmodels.compat.python import iterkeys; 2 import statsmodels.tools.data as data_util; ----> 3 from patsy import dmatrices, NAAction; 4 import numpy as np; 5 . ModuleNotFoundError: No module named 'patsy'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/212:3615,optimiz,optimizer,3615,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/212,2,"['Optimiz', 'optimiz']","['Optimizer', 'optimizer']"
Performance,"disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.8; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.8; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.3; pyparsing 2.4.7; pyrsistent NA; pytz 2020.4; rpy2 3.3.6; scanpy 1.6.0; scipy 1.5.4; seaborn 0.11.0; send2trash NA; setuptools_scm NA; sinfo 0.3.1; six 1.15.0; sklearn 0.23.2; skmisc 0.1.3; sphinxcontrib NA; statsmodels 0.12.1; storemagic NA; tables 3.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2830,bottleneck,bottleneck,2830,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,1,['bottleneck'],['bottleneck']
Performance,"dy been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. I am not 100% this is a bug, so please correct me if I'm doing something wrong... I am using scanpy with a very large scRNAseq dataset (SEA-AD, the sparse h5ad is ~35GB); When I use the read_h5ad file, even when running in backed mode, I get an out of memory exception. I expect it to be due to the fact that parts of the dataset are still read to memory, even in backed mode.; As you can see in the stack trace below, eventually anndata's `read_sparse()` function is called (in `_io/specs/methods.py`; But this method has the following implementation in the latest version:; ```python; def read_sparse(elem):; return SparseDataset(elem).to_memory(); ```; Thus, loading (part of) the dataset to memory. Like I said, I am not sure whether this is a bug, or supposed to happen. But to me it seems odd that backed mode still loads large portions of the dataset to memory. ### Workaround. For my own project, I got around this issue, by removing the call to `.to_memory()` within the source of anndata. I am not sure whether this breaks any other functionality, but I can use the dataset the way I need it right now. ### Minimal code sample (that we can copy&paste without having any data); (this cannot be run without any data, because the problem is that it fails with a big dataset. I included an aws download command to the big dataset that is causing the crash on my 24GB memory machine). ```python; import scanpy. # Download command; # aws s3 cp --no-sign-request s3://sea-ad-single-cell-profiling/MTG/RNAseq/SEAAD_MTG_RNAseq_final-nuclei.2022-08-18.h5ad ./final.h5ad. PATH = './final.h5py'; adata = scanpy.read_h5ad(PATH, backed=True); ```. (the stack trace below was redacted a bit to hide my private information. `[python-path]` replaces the path to pythons directory); ```pytb; Traceback (most recent call last):; Fi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2365:1051,load,loads,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2365,1,['load'],['loads']
Performance,"e at a bit of a philosophical divide, so perhaps it's best for me to just register which use cases I have that AnnData / scanpy are personally causing me friction:. Instead of pasting all errors, I'm just going to paste code blocks I wish worked. Note, these are actual use cases I have regularly encountered. **1. Cannot pass AnnData to numpy or sklearn operators**. ```python; import scanpy as sc; import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; from sklearn import decomposition, cluster. data = np.random.normal(size=(100,10)); adata = sc.AnnData(data). # All of the following raise errors; np.sqrt(adata); adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]. adata.obsm['X_PCA'] = decomposition.PCA(2).fit_transform(adata); ```; To answer the question above, I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease? If I do `adata = np.sqrt(adata)` then isn't this the same footprint as modifying inplace? If I do `adata_sq = np.sqrt(adata)` then my intention is to duplicate the adata object. In this case, it is my intention to create a duplicate object, and I would like AnnData to respect this intention. ; **2. Requirement to use .var_vector or .obs_vector for single columns**; ```python; # This works as expected; adata[:, adata.var_names[0:3]]. # I wish this did as well.; adata[:, adata.var_names[0]]; ```; **3. .var_vector doesn't return a Series**. ```python; pdata = pd.DataFrame(data); # Returns series; pdata[0]. # Returns ndarray; adata.var_vector[0]; ```. **4. Clusters as categories creates confusing scatterplots**; ```python; sc.pp.neighbors(adata); sc.tl.leiden(adata). plt.scatter(adata.obs['leiden'], adata.X[:,0]); ```; Produces the following plot. I would like it to have order 0-5 by default. <img width=""393"" alt=""image"" src=""https://user-images.githubusercontent.com",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458:1124,perform,performance,1124,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-607952458,1,['perform'],['performance']
Performance,"e repository using git and then install it works! (I am sure there is an explanation). ```; test@mac ~/PythonPackages/forceatlas2$ git pull; Already up to date.; test@mac ~/PythonPackages/forceatlas2$ pip3 install . --user; Processing /Users/test/PythonPackages/forceatlas2; Preparing metadata (setup.py) ... done; Requirement already satisfied: numpy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.21.5); Requirement already satisfied: scipy in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (1.8.0); Requirement already satisfied: tqdm in /Users/test/.local/lib/python3.10/site-packages (from fa2==0.3.5) (4.63.0); Building wheels for collected packages: fa2; Building wheel for fa2 (setup.py) ... done; Created wheel for fa2: filename=fa2-0.3.5-cp310-cp310-macosx_12_0_x86_64.whl size=155419 sha256=23d907bfec5df0e9d0d522865d1c288b1f8894134bd61b6c5a02467128dfd102; Stored in directory: /private/var/folders/0s/67yn6b6n3lx4882xx_86ps2m0000gp/T/pip-ephem-wheel-cache-i69s_t3j/wheels/51/1c/a5/5a9ef4f0bc9387d300190bc15adbb98dbda9d90c6da9c2da04; Successfully built fa2; Installing collected packages: fa2; Successfully installed fa2-0.3.5 ; test@mac ~/PythonPackages/forceatlas2$; ```. However, if you try to install the release version you get an error:. ```; test@mac ~/PythonPackages$ wget https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; --2022-03-24 02:54:21-- https://github.com/bhargavchippada/forceatlas2/archive/refs/tags/v0.3.5.tar.gz; Resolving github.com (github.com)... 140.82.114.3; Connecting to github.com (github.com)|140.82.114.3|:443... connected.; HTTP request sent, awaiting response... 302 Found; Location: https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5 [following]; --2022-03-24 02:54:21-- https://codeload.github.com/bhargavchippada/forceatlas2/tar.gz/refs/tags/v0.3.5; Resolving codeload.github.com (codeload.github.com)... 140.82.114.9; Connecting to codeload.github.c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096:1384,cache,cache-,1384,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2067#issuecomment-1077457096,1,['cache'],['cache-']
Performance,"e sample (that we can copy&paste without having any data); ```bash; conda install -c bioconda scanpy; ````. ```python; import scanpy as sc; annot = sc.queries.biomart_annotations(""mmusculus"", [""ensembl_gene_id"", ""external_gene_name""]); ```; ```pytb; line 108, in biomart_annotations; return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/scanpy/queries/_queries.py"", line 64, in simple_query; ""This method requires the `pybiomart` module to be installed.""; ImportError: This method requires the `pybiomart` module to be installed.; ```. ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); adata.write_loom('dummy.loom'); ```; ```pytb; write_loom(filename, self, write_obsm_varm=write_obsm_varm); File ""/Users/jfnavarro/opt/anaconda3/envs/nf-core/lib/python3.7/site-packages/anndata/_io/write.py"", line 112, in write_loom; from loompy import create; ModuleNotFoundError: No module named 'loompy'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.1; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; get_version 3.5; h5py 2.10.0; joblib 1.0.1; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; llvmlite 0.36.0; matplotlib 3.4.2; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.3.2; pkg_resources NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; tables 3.6.1; tqdm 4.62.1; typing_extensions NA; yaml 5.4.1; zipp NA; -----; Python 3.7.11 (default, Jul 27 2021, 07:03:16) [Clang 10.0.0 ]; Darwin-20.4.0-x86_64-i386-64bit; 12 logical CPU cores, i386; -----; Session information updated at 2021-09-15 10:41. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2000:1708,bottleneck,bottleneck,1708,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2000,1,['bottleneck'],['bottleneck']
Performance,"e the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). Another option would be to see if you can swap out Anndata for Xarray. This is a big change obviously, and probably pretty disruptive to the existing codebase, but it would align you with many other software projects and scientific communities that are currently thinking about these exact same problems. My guess is that in the long run it would save you time, assuming that Xarray DataArrays meet your needs semantically. > Many operations work, however cupyx.scipy.sparse has a number of missing features that mean it can’t be used for Zheng17 yet. It would require significant work in CuPy to get it working:. I could imagine that these might be in scope for NVidia folks to work on in a few months (no promises though). If you wanted to raise these as issues there to track things that would be helpful. cc @jakirkham @pentschev. > However, when I tried NumPy 1.17 the Dask implementation slowed down significantly. I haven't been able to pinpoint the issue. I would be curious to know what's going on here if you find out. >> Any chance you did any profiling of these runs? I'd be interested in seeing the performance impact across the pipeline. > The closest I got to this was using the Dask web UI to watch tasks being run (see this part of the benchmark script: https://github.com/tomwhite/scanpy/blob/sparse-dask/benchmark.py#L54-L55). This is useful to see what operations are bottlenecks. The only timings I did were to run the complete recipe. +1 on profiling. I suggest that you first start with `compute(scheduler=""single-threaded"")` and the cProfile module. This will avoid any parallelism, and hopefully let you use profiling techniques that are more familiar to you. I personally like snakeviz. . If you want to get on a screenshare some time I'm happy to look at dashboard plots with you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880:2420,perform,performance,2420,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921#issuecomment-557191880,2,"['bottleneck', 'perform']","['bottlenecks', 'performance']"
Performance,"e to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next time; 483 adata.write(filename_cache). ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist_ok); 208 if head and tail and not path.exists(head):; 209 try:; --> 210 makedirs(head, mode, exist_ok); 211 except FileExistsError:; 212 # Defeats race condition when another thread created the path. ~\AppData\Local\conda\conda\envs\Scanpy\lib\os.py in makedirs(name, mode, exist",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:3009,race condition,race condition,3009,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,1,['race condition'],['race condition']
Performance,"e ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/anndata/_core/anndata.py:1808, in AnnData.concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas); 1799 pat = rf""-({'|'.join(batch_categories)})$""; 1800 out.var = merge_dataframes(; 1801 [a.var for a in all_adatas],; 1802 out.var_names,; 1803 partial(merge_outer, batch_keys=batch_categories, merge=merge_same),; 1804 ); 1805 out.var = out.var.iloc[; 1806 :,; 1807 (; -> 1808 out.var.columns.str.extract(pat, expand=False); 1809 .fillna(""""); 1810 .argsort(kind=""stable""); 1811 ),; 1812 ]; 1814 return out. File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/accessor.py:224, in CachedAccessor.__get__(self, obj, cls); 221 if obj is None:; 222 # we're accessing the attribute of the class, i.e., Dataset.geo; 223 return self._accessor; --> 224 accessor_obj = self._accessor(obj); 225 # Replace the property with the accessor object. Inspired by:; 226 # https://www.pydanny.com/cached-property.html; 227 # We need to use object.__setattr__ because we overwrite __setattr__ on; 228 # NDFrame; 229 object.__setattr__(obj, self._name, accessor_obj). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:181, in StringMethods.__init__(self, data); 178 def __init__(self, data) -> None:; 179 from pandas.core.arrays.string_ import StringDtype; --> 181 self._inferred_dtype = self._validate(data); 182 self._is_categorical = is_categorical_dtype(data.dtype); 183 self._is_string = isinstance(data.dtype, StringDtype). File ~/miniconda3/envs/scanpy/lib/python3.10/site-packages/pandas/core/strings/accessor.py:235, in StringMethods._validate(data); 232 inferred_dtype = lib.infer_dtype(values, skipna=True); 234 if inferred_dtype not in allowed_types:; --> 235 raise AttributeError(""Can only use .str accessor with string values!""); 236 return inferred_dtype. AttributeError: Can only use .str accessor with string values!; ```; I did try to eleminate e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2474:2174,cache,cached-property,2174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2474,1,['cache'],['cached-property']
Performance,"e-commit, myst-parser, anndata, umap-learn, sphinx-book-theme, jupyter-cache, scanpy, nbsphinx, myst-nb; Attempting uninstall: tbb; Found existing installation: TBB 0.2; ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.5.0; aiofiles 22.1.0; aiohttp 3.8.5; aioitertools 0.7.1; aiosignal 1.2.0; aiosqlite 0.18.0; alabaster 0.7.12; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.1; anaconda-cloud-auth 0.1.3; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anyio 3.5.0; appdirs 1.4.4; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; arrow 1.2.3; astroid 2.14.2; astropy 5.1; asttokens 2.0.5; async-timeout 4.0.2; atomicwrites 1.4.0; attrs 22.1.0; Automat 20.2.0; autopep8 1.6.0; Babel 2.11.0; backcall 0.2.0; backports.functools-lru-cache 1.6.4; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 3.2.0; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 0.0; bleach 4.1.0; bokeh 3.2.1; boltons 23.0.0; botocore 1.29.76; Bottleneck 1.3.5; brotlipy 0.7.0; certifi 2023.7.22; cffi 1.15.1; chardet 4.0.0; charset-normalizer 2.0.4; click 8.0.4; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.2; conda 23.7.4; conda-build 3.26.1; conda-content-trust 0.2.0; conda_index 0.3.0; conda-libmamba-solver 23.7.0; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; constantly 15.1.0; contourpy 1.0.5; cookiecutter 1.7.3; cryptography 41.0.3; cssselect 1.1.0; cycler 0.11.0; Cython 3.0.3; cytoolz 0.12.0; daal4py 2023.1.1; dask 2023.6.0; datasets 2.12.0; datashader 0.15.2; datashape 0.5.4; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; diff-match-patch 20200713; dill 0.3.6; distributed 2023.6.0; docstring-to-markdo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:1889,cache,cache,1889,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['cache'],['cache']
Performance,"e_total; ' The following highly-expressed genes are not considered during '; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = Index(['0', '1', '2'], dtype='object'); key = dask.array<invert, shape=(3,), dtype=bool, chunksize=(3,), chunktype=numpy.ndarray>. def __getitem__(self, key):; """"""; Override numpy.ndarray's __getitem__ method to work as desired.; ; This function adds lists and Series as valid boolean indexers; (ndarrays only supports ndarray with dtype=bool).; ; If resulting ndim != 1, plain ndarray is returned instead of; corresponding `Index` subclass.; ; """"""; getitem = self._data.__getitem__; ; if is_integer(key) or is_float(key):; # GH#44051 exclude bool, which would return a 2d ndarray; key = com.cast_scalar_indexer(key, warn_float=True); return getitem(key); ; if isinstance(key, slice):; # This case is separated from the conditional above to avoid; # pessimization com.is_bool_indexer and ndim checks.; result = getitem(key); # Going through simple_new for performance.; return type(self)._simple_new(result, name=self._name); ; if com.is_bool_indexer(key):; # if we have list[bools, length=1e5] then doing this check+convert; # takes 166 µs + 2.1 ms and cuts the ndarray.__getitem__; # time below from 3.8 ms to 496 µs; # if we already have ndarray[bool], the overhead is 1.4 µs or .25%; key = np.asarray(key, dtype=bool); ; > result = getitem(key); E IndexError: too many indices for array: array is 1-dimensional, but 3 were indexed. ../../venvs/single-cell/lib/python3.8/site-packages/pandas/core/indexes/base.py:5055: IndexError; ```. #### Versions. <details>. -----; anndata 0.9.0rc2.dev18+g7771f6ee; scanpy 1.10.0.dev50+g3e3427d0; -----; PIL 9.1.1; asciitree NA; beta_ufunc NA; binom_ufunc NA; cffi 1.15.0; cloudpickle 2.2.1; cycler 0.10.0; cython_runtime NA; dask 2023.3.2; dateutil 2.8.2; defusedxml 0.7.1; entrypoints 0.4; fasteners 0.17.3; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.4; jinja2 3.1.2; joblib 1.1.0; kiwisolve",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2465:3331,perform,performance,3331,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2465,1,['perform'],['performance']
Performance,"eading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x00000001f9bdf240 (most recent call first):; File ""<venv>/lib/python3.12/threading.py"", line 355 in wait; File ""<venv>/lib/python3.12/queue.py"", line 171 in get; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 138 in queue_get; File ""<venv>/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3510,concurren,concurrent,3510,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"earn` and I want this to ""just work"". I agree with you that `iloc` vs `loc` indexing is not straightforward to lay users, but I think it's a mistake to change the convention for how one indexes positionally vs using labels. _Especially when the underlying data structures is often a dataframe._ Instead of breaking these conventions, I would love to see the tool ""just work"" and make sure the tutorials and documentation make the conventions exceedingly clear for new users. I'm not sure what's the best way to resolve this, because I think this line of thinking results in a couple larger design questions for scanpy as well. For example, should AnnData objects be valid input for numpy ufuncs? I.e. should the following code work?. ```python; import numpy as np; import pandas as pd; import scanpy as sc. data = pd.DataFrame(np.random.normal(size=(100,2))); adata = sc.AnnData(data); np.sqrt(adata); ```; Currently this raises a `TypeError`. Why shouldn't this ""Just work""? What about the convention of returning a copy by default, instead of modifying objects in place? I can't think of many other Python toolkits that don't return a copy when you perform some operation on a data object. I would really love to use scanpy / anndata more in my day to day work. Right now, this lack of compatibility is the hurdle that prevents that. I disagree that people who are familiar with pandas and numpy will have an easy time coming to grips with leveraging a tool that doesn't interact well with the greater ecosystem of data analysis tools in Python. I think these users are more likely to use scanpy / anndata only to get access to the methods that are only implemented in scanpy, and then return to the ecosystem of tools that all work together. I can only speak to my experience, but this is how I use scanpy. If scanpy were to adopt greater inter-compatibility, I would be happy both to use it more and also to help contribute to its development and documentation. I'm excited to hear your thoughts!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715:2638,perform,perform,2638,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-583875715,1,['perform'],['perform']
Performance,"eat tutorials. Recently, I've been encountering an error after I import ScanPy that causes the kernel I'm using to die and restart. After this happens, I can no longer call sc.- commands and have to import the library again. And then the kernel quickly dies again. . ### Minimal code sample. ```python; import numpy as np. import pandas as pd. import scanpy as sc. sc.settings.verbosity = 3 . sc.logging.print_header(). sc.settings.set_figure_params(dpi=80, facecolor='white'); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; NameError Traceback (most recent call last); /var/folders/jt/m9v9k7dd4f31m5pl8v_g8m3m0000gn/T/ipykernel_3816/1026803476.py in <module>; ----> 1 sc.settings.set_figure_params(dpi=80, facecolor='white'). NameError: name 'sc' is not defined; ```. ### Versions. <details>. ```. -----; anndata 0.9.2; scanpy 1.9.4; -----; PIL 9.2.0; PyObjCTools NA; appnope 0.1.2; astunparse 1.6.3; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.7; entrypoints 0.4; fsspec 2022.7.1; gmpy2 2.1.2; google NA; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.1; ipykernel 6.15.2; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.1; llvmlite 0.38.0; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; mpl_toolkits NA; mpmath 1.2.1; natsort 7.1.1; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.1; numexpr 2.8.4; numpy 1.21.6; opt_einsum v3.3.0; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pyarrow 13.0.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2675:1367,bottleneck,bottleneck,1367,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2675,1,['bottleneck'],['bottleneck']
Performance,"eatures`. **Note**; All of this does not affect the fairly good match, up to potentially numerics, between `sc.pp.highly_variable_genes(adata, flavor=""seurat_v3"", batch_key=None)` and Seurat's `FindVariableFeatures` with `selection.method = 'vst'` introduced in Stuart et al.; If it helps to avoid confusion between the two: `FindVariableFeatures` is called within `SelectIntegrationFeatures`, on each batch separately. **Technical additions here**; This PR suggests to solve this by introducing a new flavor. Either. -`seurat_v3_paper` This fixes to exactly what @jlause noticed and @adamgayoso pinpointed in #1733.; OR; -`seurat_v3_implementation` This matches more closely the suspected Seurat implementation I mentioned above. They select the same genes. Leaning towards favoring the style of `seurat_v3_paper`. Better naming suggestions more than welcome. **Examples**; - Good when no `batch_key` used:; ```py; import numpy as np; import pandas as pd; import scanpy as sc. # load exactly the data from seurat tutorial. pbmc = sc.datasets.pbmc3k(); # use the exact filterin from Seurat tutorial; sc.pp.filter_cells(pbmc, min_genes=200) # this doesnt do anything btw; sc.pp.filter_genes(pbmc, min_cells=3). print(pbmc). # default settings in scanpy are the same as for Seurat; sc.pp.highly_variable_genes(pbmc, flavor=""seurat_v3""). # this has been prepared in the R script ""scanpy/scanpy/tests/_scripts/seurat_extract_hvg_v3.R"" (adapted from https://satijalab.org/seurat/articles/pbmc3k_tutorial); pbmc3k_tutorial_FindVariableGenes_seurat = pd.read_csv(""scanpy/scanpy/scanpy/tests/_scripts/seurat_hvg_v3.csv.gz"", index_col=0). # This is used to order and rank the hvg when no batch information used; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""variance.standardized""],; pbmc.var[""variances_norm""],; ). # Another quantity reported by both; assert np.allclose(; pbmc3k_tutorial_FindVariableGenes_seurat[""mean""],; pbmc.var[""means""],; ). # Another quantity reported by both; assert ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2792:3109,load,load,3109,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2792,1,['load'],['load']
Performance,ecting h5py>=2.10.0; Using cached h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.wh,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1213,cache,cached,1213,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1662,cache,cache,1662,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"ed: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-api-wrap-1.2 leidenalg-0.8.8 llvmlite-0.36.0 matplotlib-3.3.4 natsort-8.0.0 networkx-2.5.1 numba-0.53.1 numexpr-2.7.3 numpy-1.19.5 pandas-1.1.5 patsy-0.5.2 pillow-8.4.0 pynndescent-0.5.5 python-igraph-0.9.8 scanpy-1.7.2 scikit-learn-0.24.2 scipy-1.5.4 seaborn-0.11.2 sinfo-0.3.4 statsmodels-0.12.2 stdlib-list-0.8.0 tables-3.6.1 texttable-1.6.4 threadpoolctl-3.0.0 tqdm-4.62.3 umap-learn-0.5.2 xlrd-1.2.0",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4865,cache,cached-property,4865,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,2,['cache'],"['cached-property', 'cached-property-']"
Performance,"equent error bellow. . ```py; adata = sc.read_10x_mtx(; 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; ```; ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-17-e7dd3543f8df> in <module>(); 2 'C:\\Users\\correap\\Documents\\03152019_scRNAseq\\filtered_feature_bc_matrix_1', # the directory with the `.mtx` file; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1190,cache,cache,1190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,3,['cache'],['cache']
Performance,"equirement already satisfied: pillow>=6.2.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (9.0.1); Requirement already satisfied: kiwisolver>=1.0.1 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (1.3.2); Requirement already satisfied: fonttools>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; F",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4377,cache,cached,4377,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"er/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: xlrd in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: scikit-learn>=0.19.1 in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: statsmodels in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: networkx in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: natsort in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: joblib in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: profilehooks in /cluster/software/lib/python3.6/site-packages (from scanpy); Requirement already up-to-date: cycler>=0.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Collecting python-dateutil (from matplotlib==2.0.0->scanpy); Using cached python_dateutil-2.6.1-py2.py3-none-any.whl; Collecting pytz (from matplotlib==2.0.0->scanpy); Using cached pytz-2018.3-py2.py3-none-any.whl; Requirement already up-to-date: six>=1.10 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: numpy>=1.7.1 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scanpy); Requirement already up-to-date: pyparsing!=2.0.0,!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /cluster/software/lib/python3.6/site-packages (from matplotlib==2.0.0->scan; Requirement already up-to-date: patsy in /cluster/software/lib/python3.6/site-packages (from statsmodels->scanpy); Requirement already up-to-date: decorator>=4.1.0 in /cluster/software/lib/python3.6/site-packages (from networkx->scanpy); Installing collected packages: scanpy, python-dateutil, pytz; Running setup.py install for scanpy: started; Running setup.py install for scanpy: finished with status 'error'; Complete output from command ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/90:1845,cache,cached,1845,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/90,1,['cache'],['cached']
Performance,"ers, fill_value, copy, allow_dups); 4887 fill_value=fill_value,; 4888 allow_dups=allow_dups,; -> 4889 copy=copy,; 4890 ); 4891 # If we've made a copy once, no need to make another one. ~/.local/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice); 668 # some axes don't allow reindexing with dups; 669 if not allow_dups:; --> 670 self.axes[axis]._validate_can_reindex(indexer); 671 ; 672 if axis >= self.ndim:. ~/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py in _validate_can_reindex(self, indexer); 3783 # trying to reindex on an axis with duplicates; 3784 if not self._index_as_unique and len(indexer):; -> 3785 raise ValueError(""cannot reindex from a duplicate axis""); 3786 ; 3787 def reindex(. ValueError: cannot reindex from a duplicate axis. ```. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.9.1; -----; PIL 7.2.0; backcall 0.1.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.0; cloudpickle 1.3.0; colorama 0.4.3; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.1; dask 2.13.0; dateutil 2.8.1; decorator 4.4.2; google NA; h5py 2.10.0; igraph 0.9.7; ipykernel 5.1.4; ipython_genutils 0.2.0; jedi 0.15.2; joblib 0.17.0; kiwisolver 1.1.0; leidenalg 0.8.8; llvmlite 0.39.1; louvain 0.7.0; matplotlib 3.5.3; mpl_toolkits NA; natsort 7.0.1; nbinom_ufunc NA; numba 0.56.2; numexpr 2.7.3; numpy 1.21.6; packaging 20.3; pandas 1.3.4; parso 0.5.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.4; psutil 5.7.0; ptyprocess 0.6.0; pygments 2.6.1; pyparsing 2.4.6; pyteomics NA; pytz 2019.3; scipy 1.7.1; session_info 1.0.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tblib 1.6.0; texttable 1.6.3; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth NA; yaml 5.3.1; zipp NA; zmq 17.1.2; -----; IPython 7.13.0; jupyter_client 6.1.2; ju",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:5731,bottleneck,bottleneck,5731,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,1,['bottleneck'],['bottleneck']
Performance,"ession,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcore 0.2.0; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; frozendict 2.3.8; h5py 3.9.0; ikarus NA; importlib_resources NA; ipykernel 6.24.0; ipython_genutils 0.2.0; jedi 0.18.2; jinja2 3.1.2; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; packaging 23.1; pandas 2.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2652,cache,cache,2652,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,"et things a bit into order here, as at the moment some wrong impressions are around I think:. **1. Incorrect comparisons done here**; To my current knowledge,; - `sc.pp.highly_variable_genes(…, flavor=“seurat”)` mimics `FindVariableFeatures(…, method=“mean.var.plot”)`, operating on count-normalised, log1p-ed data.; - `sc.pp.highly_variable_genes(…, flavor=“seurat_v3”)` mimics `FindVariableFeatures(…, method=“vst”)` operating on raw gene counts (from the [Stuart et al. 2019 Seurat Version 3 paper](https://www.cell.com/cell/pdf/S0092-8674(19)30559-8.pdf)). @flying-sheep, lets put something like this into the doctstring in #2792? Will add a suggestion for you to check there. Think this is very useful information super hard to find atm. These are 2 different methods, which scanpy implements. > Even when using the Seurat flavor in scanpy, the differences seem pretty drastic. Any guidance on this would be appreciated. Guidance:; In your example, you are comparing two different methods, that produce different results (like really just perform different computations). Notice `flavor=“seurat”` is default in `sc.pp.highly_variable_genes`, but `method=""vst""` is default in `FindVariableFeatures`. (I see this can be confusing, we'll try to make this as clear as possible in the doc). **2. Incorrect assumption about Seurat**; > This means that the implementation in scanpy is according to the method in the paper? And the implementation in Seurat uses some other method. Thanks!. This is not correct. There are 2 options of Seurat mixed up in this conversation here, causing quite some confusion. Seurat is giving the selected features based on what they write to the best of my knowledge. **3. Open question on small detail**; > Yes: While working on #2792, @eroell has discovered that seurat’s gene ordering doesn’t match their definition in the paper. The one in the paper makes most sense, as it’s stable (hvg(..., n_top_genes=n) == hvg(..., n_top_genes=n+i)[:n]). Need to emphasise this is",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935:1106,perform,perform,1106,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892761935,1,['perform'],['perform']
Performance,"f = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique; 530 ) -> pd.DataFrame:; --> 531 dfs = [df.reindex(index=new_index) for df in dfs]; 532 # New dataframe with all shared data; 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 310 @wraps(func); 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 312 return func(*args, **kwargs); 313 ; 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4174 kwargs.pop(""axis"", None); 4175 kwargs.pop(""labels"", None); -> 4176 return super().reindex(**kwargs); 4177 ; 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4810 # perform the reindex on the axes; 4811 return self._reindex_axes(; -> 4812 axes, level, limit, tolerance, method, fill_value, copy; 4813 ).__finalize__(self, method=""reindex""); 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4021 if index is not None:; 4022 frame = frame._reindex_index(; -> 4023 index, method, copy, level, fill_value, limit, tolerance; 4024 ); 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4043 copy=copy,; 4044 fill_value=fill_value,; -> 4045 allow_dups=False,; 4046 ); 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4881 fill_value=fill_value,; 4882 allow_dups=allow_dups,; -> 4883 copy=copy,; 4884 ); 4885 # If we've made a copy once, no need to make another one. ~/anaconda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683:2734,perform,perform,2734,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267#issuecomment-1018908683,1,['perform'],['perform']
Performance,"fied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successfully installed anndata-0.7.6 cached-property-1.5.2 cycler-0.11.0 decorator-4.4.2 get-version-2.1 h5py-3.1.0 igraph-0.9.8 joblib-1.1.0 kiwisolver-1.3.1 legacy-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4451,cache,cached,4451,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"from the doc of sc.tl.tsne, I knew it was implemented by scikit, but I can't get the same result between sc.tl.tsne and scikit.TSNE，my test code is below; ```; #!/usr/bin/env python3; # -*- coding: utf-8 -*-; """""". from sklearn import datasets; import scanpy as sc; import numpy as np; import random; import matplotlib.pyplot as plt; from sklearn.manifold import TSNE; np.random.seed(1); random.seed(1). iris = datasets.load_iris(); X = iris.data; label=iris.target. adata=sc.AnnData(X); adata.obs[""celltype""]=label.astype(int).astype(str); sc.tl.tsne(adata,random_state=0,use_fast_tsne=False); axis=sc.pl.tsne(adata,color=[""celltype""],size=100,show=False); sc_tsne=adata.obsm[""X_tsne""]; #print(ax). print(np.min(sc_tsne[:,0])); print(np.max(sc_tsne[:,0])); print(np.min(sc_tsne[:,1])); print(np.max(sc_tsne[:,1])); print(""====================""). # import pickle; # #with open(); # file = open('/Users/xiaokang/Desktop/data/tsne.pkl', 'rb'); # tsne2=pickle.load(file). target=label; tsne = TSNE(learning_rate=1000,init='random', random_state=0); X_transformed = tsne.fit_transform(X); fig=plt.figure(); for label in np.unique(target):; plt.scatter(X_transformed[label==target,0], X_transformed[label==target,1],label=label); plt.legend(loc=""upper left""); plt.show(); #print(X_transformed); print(np.min(X_transformed[:,0])); print(np.max(X_transformed[:,0])); print(np.min(X_transformed[:,1])); print(np.max(X_transformed[:,1])). print(""==================""); params_sklearn = dict(; perplexity=30,; random_state=0,; verbose=False,; early_exaggeration=12,; learning_rate=1000,; ); from sklearn.manifold import TSNE; # unfortunately, sklearn does not allow to set a minimum number; # of iterations for barnes-hut tSNE; tsne3 = TSNE(**params_sklearn); X_transformed=tsne3.fit_transform(X); print(np.min(X_transformed[:,0])); print(np.max(X_transformed[:,0])); print(np.min(X_transformed[:,1])); print(np.max(X_transformed[:,1])); ```; I get the result; ![image](https://user-images.githubusercontent.com/5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1759:956,load,load,956,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1759,1,['load'],['load']
Performance,"g intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1496,load,loading,1496,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"g intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, release-notes/1.7.2.rst, release-notes/1.8.0.rst, release-notes/1.8.1.rst, release-notes/1.8.2.rst, release-notes/1.9.0.rst, release-notes/index.rst, release-notes/release-latest.r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1745,load,loading,1745,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"g that's being worked on for numpy, but the assumptions a ufunc has about it's input data does not match with what an AnnData object is. I've worked on a side project of just wrapping the sklearn transformers so you can pass anndata objects, and could try and get that cleaned up for use if it'd be valuable. --------------------------------. I'm not really sure what you expect this line to do though:. ```python; adata[:, adata.var_names[0:3]] - adata[:, adata.var_names[3:6]]; ```. I would probably throw an error for that, since the var names wouldn't be the same. It's also not obvious to me which arrays would be subtracted (all of them? some of them?). If this is meant to do:. ```python; adata[:, adata.var_names[0:3]].X - adata[:, adata.var_names[3:6]].X; ```. I don't think that's so much more work. > I think it should return the whole AnnData object, like how DataFrames return themselves. I don't know if we think it should ""update"" the original AnnData. I'm also confused by how this results in a performance decrease?. If it should return the whole object, but not update the original, then all of the values from the original need to be copied to prevent unintentional modification. This is really expensive for large objects, which single cell datasets often are. For your example of `adata = np.sqrt(adata)` vs `adata_sq = np.sqrt(adata)`, there's no way for us to tell which of those statements was made while evaluating `np.sqrt`. That would require the ability to overload assignment, and for python to have different evaluation rules. ### 2. Requirement to use .var_vector or .obs_vector for single columns. Is what you're saying that you want: `adata[:, adata.var_names[0]].X` to be one dimensional?. This used to be the behaviour, but it got confusing quickly. Suddenly, `adata.X` could be a different shape from `adata`. I would recommend reading the issues that were opened about this on `anndata` for more context. Here's one of the main ones: https://github.com/theislab/a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245:1190,perform,performance,1190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-608231245,1,['perform'],['performance']
Performance,"genes with the highest mean. C:\ProgramData\Anaconda3\lib\site-packages\scanpy\preprocessing\_normalization.py in normalize_total(adata, target_sum, exclude_highly_expressed, max_fraction, key_added, layer, layers, layer_norm, inplace, copy); 174 counts_per_cell = X[:, gene_subset].sum(1); 175 else:; --> 176 counts_per_cell = X.sum(1); 177 start = logg.info(msg); 178 counts_per_cell = np.ravel(counts_per_cell). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```; And when I run the command:; `type(adata_orig.X)`; I get the output as:; `anndata._core.sparse_dataset.SparseDataset`. After reading your comment, I feel that earlier the scanpy module was expecting the sparse dataset as the input, but you have changed it to expect the dense format , and maybe that's the reason for this error? I am just two days into the world of scanpy and any help would be highly appreciated, in order to make this error go away. Also, to help you in debugging, I'd like to mention that the raw data in this dataset is present in the layer, which has the name 'raw' and the adata_orig.raw is set to null as of now. and when i try to run : `adata_orig.layers['raw'].sum(1)` it runs with no error. While running: `adata_orig.X.sum(1)` gives me the error as : ; ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-23-951a31c71c45> in <module>; ----> 1 adata_orig.X.sum(1). AttributeError: 'SparseDataset' object has no attribute 'sum'; ```. PS: I downloaded this` .h5ad` file from a published research paper to perform some analysis over it, would be happy to provide you the link to same if required. . Also this is the environment that I am working in:; `scanpy==1.8.2 anndata==0.7.8 umap==0.5.2 numpy==1.20.1 scipy==1.6.2 pandas==1.2.4 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.6`. Hello @LuckyMD, tagging you for just in case you might be knowing the resolution.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2147:3348,perform,perform,3348,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2147,1,['perform'],['perform']
Performance,"gorical; ... storing 'feature_types-180905-3' as categorical; ... storing 'feature_types-180905-4' as categorical; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-72-19c7ca58c3a2> in <module>; ----> 1 df_dev.write_h5ad('2019-03-04_OTUD6B_dev_sig.h5'). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in write_h5ad(self, filename, compression, compression_opts, force_dense); 1951 ; 1952 _write_h5ad(filename, self, compression=compression,; -> 1953 compression_opts=compression_opts, force_dense=force_dense); 1954 ; 1955 if self.isbacked:. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/readwrite/write.py in _write_h5ad(filename, adata, force_dense, **kwargs); 217 if not dirname.is_dir():; 218 dirname.mkdir(parents=True, exist_ok=True); --> 219 d = adata._to_dict_fixed_width_arrays(); 220 # we're writing to a different location than the backing file; 221 # - load the matrix into the memory... /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in _to_dict_fixed_width_arrays(self); 2183 """"""; 2184 self.strings_to_categoricals(); -> 2185 obs_rec, uns_obs = df_to_records_fixed_width(self._obs); 2186 var_rec, uns_var = df_to_records_fixed_width(self._var); 2187 layers = self.layers.as_dict(). /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/anndata/base.py in df_to_records_fixed_width(df); 212 names.append(k); 213 if is_string_dtype(df[k]):; --> 214 max_len_index = df[k].map(len).max(); 215 arrays.append(df[k].values.astype('S{}'.format(max_len_index))); 216 elif is_categorical(df[k]):. /projects/ps-gleesonlab5/user/zhen/bin/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py in stat_func(self, axis, skipna, level, numeric_only, **kwargs); 10954 skipna=skipna); 10955 return self._reduce(f, name, axis=axis, skipna=skipna,; > 10956 n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/515:2089,load,load,2089,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/515,1,['load'],['load']
Performance,"h-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3190,cache,cached,3190,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"has been detected. - The workqueue threading layer is not threadsafe and may not be accessed concurrently by multiple threads. Concurrent access typically occurs through a nested parallel region launch or by calling Numba parallel=True functions from multiple Python threads.; - Try using the TBB threading layer as an alternative, as it is, itself, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Curr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1640,optimiz,optimization,1640,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,"he functionalities and setup and it does look very nice!. - BCR makes sense to add, there seems to be generally less happening in this space in single-cell though right now, compared to TCR. Would be good to have somebody on board who actually works on this data.; - [tcellmatch](https://github.com/theislab/tcellmatch)'s primary purpose is specificity prediction, this could be easily added ontop of this, I will look into your data structure and will think about the necessary changes. I am in the process of making this code public anyway, hopefully next week or so.; - You mentioned distance metrics, this is definitely an interesting and relevant area, in [tcellmatch](https://github.com/theislab/tcellmatch), we implicitly use 1. manhatten distances, 2. euclidian distances in BLOSUM embedding and 3. learned embedding distances, 2. and maybe 3. could be potentially integrated, would be worth discussing in any case.; - Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs? These anticipated use cases would determine how and whether this makes sense i think.; - Potentially additionally relevant: An integration with dextramer counts to ""stain"" TCR specificity? There is the purely numeric, standard multi-modal single-cell, nature to this data that can be covered by standard scanpy work flows. This data is especially useful in the context of clonotypes etc which then would require additional functionalities, which could be built on what you have here. I have been looking into this type of analysis a lot in context of tcellmatch. Would be to contribute but also happy to see what other people do here, too!. Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. Best,; David",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254:1015,load,loaders,1015,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613297254,1,['load'],['loaders']
Performance,"he ordinal regression test case to run on my MacBook. The single cpu case works fine, but if I ask for multiple processes, they launch, but activity monitor has them all at 0% cpu, with the main thread locking while waiting. Sometimes (routinely if I switch out `map_async` with `map`) I will get a crash log telling me:. ```; Crashed Thread: 0 Dispatch queue: com.apple.main-thread. Exception Type: EXC_BAD_ACCESS (SIGSEGV); Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110; Exception Note: EXC_CORPSE_NOTIFY. Termination Signal: Segmentation fault: 11; Termination Reason: Namespace SIGNAL, Code 0xb; Terminating Process: exc handler [0]. VM Regions Near 0x110:; --> ; __TEXT 0000000102a16000-0000000102a18000 [ 8K] r-x/rwx SM=COW j [/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/Resources/Python.app/Contents/MacOS/Python]. Application Specific Information:; *** multi-threaded process forked ***; crashed on child side of fork pre-exec. Thread 0 Crashed:: Dispatch queue: com.apple.main-thread; 0 libdispatch.dylib 	0x00007fff572df8e1 _dispatch_root_queue_push + 108; 1 libBLAS.dylib 	0x00007fff2bfd0c9a rowMajorTranspose + 546; 2 libBLAS.dylib 	0x00007fff2bfd0a65 cblas_dgemv + 757; 3 multiarray.cpython-36m-darwin.so	0x00000001035c4f86 gemv + 182; 4 multiarray.cpython-36m-darwin.so	0x00000001035c4527 cblas_matrixproduct + 2807; 5 multiarray.cpython-36m-darwin.so	0x000000010358ab27 PyArray_MatrixProduct2 + 215; 6 multiarray.cpython-36m-darwin.so	0x000000010358626c array_dot + 188; 7 org.python.python 	0x0000000102a5d12e _PyCFunction_FastCallDict + 463; 8 org.python.python 	0x0000000102ac30e6 call_function + 491; 9 org.python.python 	0x0000000102abb621 _PyEval_EvalFrameDefault + 1659; 10 org.python.python 	0x0000000102ac3866 _PyEval_EvalCodeWithName + 1747; ```. Here's what I was running to cause that:. ```python; import numpy as np; import scanpy.api as sc; from anndata import AnnData; from scipy.sparse import random. adata = AnnData(random(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/182:1027,queue,queue,1027,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/182,1,['queue'],['queue']
Performance,"he_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2; ```. Versions of all packages:. `scanpy==1.4.5.pos",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:2781,cache,cache,2781,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"hey all, thanks for feedback. @LuckyMD I totally see the point but disagree; > i guess one of the difficult things to actually using this is tuning the inter layer weight. . exactly and this will be different (I think?) across different multi modal tech integration (e.g. cite-seq, or spatial etc.) and e.g. for spatial it will potentially different across tissues (some tissues have more structure spatial/image features graphs than others). . Nervetheless, I think it would be very empowering to users to be able to play around with this. It is ""just"" another knob to tune that would nonetheless enrich the analysis experience imho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212:570,tune,tune,570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818#issuecomment-830652212,1,['tune'],['tune']
Performance,"hi @jlause ,. thanks again for moving the code over `experimental` and sorry for the delay. I give up with the docs, they keep failing on a very weird issue that I can't address (now it's request error from scipy, but before was some stupid indentation that I could not fix). . I realized that you forgot to copy over the `recipes`. Now it's there and working, I have a minor comment on copying over `X_pca` to `X_pearson_residuals_pca`. I think it should remain `X_pca` since the normalization is performed on `X`. Or am I missing something for such return to be chosen?. Meanwhile I'd also like to ping @ivirshup for taking a look at the experimental API and whether he agrees on the current structure as well as docs. remaining TOD:. - [ ] fix docs; - [ ] add tutorial to docs (should be done when tutorial has been reviewed)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314:498,perform,performed,498,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1715#issuecomment-890768314,1,['perform'],['performed']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/744:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/744,2,['perform'],['perform']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/744:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/744,1,['perform'],['perform']
Performance,"how to perform analysis to get bulk_labels, S_score, G2M_score, phase?. After following the tutorial ""[Clustering 3K PBMCs](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/pbmc3k.html)"", I got the object ""adata"" with following description:; _>>> adata; AnnData object with n_obs × n_vars = 691 × 4549; **obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'**; var: 'gene_ids', 'feature_types', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'; uns: 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_ . however, the tutorial ""[Visualizing marker genes](https://icb-scanpy-tutorials.readthedocs-hosted.com/en/latest/visualizing-marker-genes.html?highlight=%27bulk_labels%27)"" use the object ""pbmc"" with following description:; _>>> pbmc; AnnData object with n_obs × n_vars = 700 × 765; **obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'**; var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'; uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'; obsm: 'X_pca', 'X_umap'; varm: 'PCs'_. i want to process my data according those two tutorials and got the figures. but without those information (obs: 'bulk_labels', 'S_score', 'G2M_score', 'phase'), i cannot continue the tutorial from ""Clustering 3K PBMCs"" to ""Visualizing marker genes""",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/745:7,perform,perform,7,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/745,1,['perform'],['perform']
Performance,"https://stackoverflow.com/questions/51593527/oserror-unable-to-open-file-unable-to-open-file. /edit: that was not it. I’ll set up a Python 3.7 venv and look if I can reproduce it locally. /edit: nope, cleaning caches …",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1113:210,cache,caches,210,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1113,1,['cache'],['caches']
Performance,"hvg_mvp.csv, I used; ```R; library(dplyr); library(Seurat); library(patchwork). ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc <- NormalizeData(pbmc, normalization.method=""LogNormalize"", scale.factor=10000). pbmc <- FindVariableFeatures(pbmc, selection.method = ""mean.var.plot""). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_mvp.csv""); ```. And to generate seurat_hvg_v3.csv, I used; ```R; ################################################################################; ### FindVariableFeatures (no batch covariate). # Load the PBMC dataset - load the data from the link above!; # pbmc.data <- Read10X(data.dir = ""<INSERT_PATH_TO_DATA_HERE>/filtered_gene_bc_matrices/hg19/""); pbmc.data <- Read10X(data.dir = ""/Users/eljas.roellin/Documents/R_stuff/filtered_gene_bc_matrices/hg19/""). # Initialize the Seurat object with the raw (non-normalized data).; pbmc <- CreateSeuratObject(counts = pbmc.data, project = ""pbmc3k"", min.cells = 3, min.features = 200); pbmc. pbmc <- FindVariableFeatures(pbmc, mean.function=ExpMean, selection.method = 'vst', nfeatures = 2000). hvf_info <- HVFInfo(pbmc). write.csv(hvf_info, ""seurat_hvg_v3.csv""); ```. *unless when using `batches`, a bug we are currently solving. Nevertheless, implementation details, choices for numeric stability and numerics of the underlying libraries are indeed a challenge and do cause some discrepancies - surely interested how your comparison looks @carversh if you give these suggestions here a try!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132:4406,Load,Load,4406,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2780#issuecomment-1892766132,2,"['Load', 'load']","['Load', 'load']"
Performance,"i think the reason we do this is for project-specific caching, not temporary files. using the dedicated cache dir is of course preferable to using the working dir, since the OS knows about them (and can clean them once prompted or necessary), and preferable to a tempdir, as they survive restarts. we should use <code>cache_dir = Path([appdirs](https://pypi.python.org/pypi/appdirs/1.4.3).user_cache_dir('scanpy', 'F. Alex Wolf'))</code>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346303221:104,cache,cache,104,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346303221,1,['cache'],['cache']
Performance,"ib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:737, in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 734 return read_h5ad(path_cache); 736 if not is_present:; --> 737 raise FileNotFoundError(f'Did not find file {filename}.'); 738 logg.debug(f'reading {filename}'); 739 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file GSE123366_Combined/matrix.mtx.gz.; ```. ### Versions. <details>. ```; -----; anndata 0.9.1; scanpy 1.9.3; -----; PIL 10.0.0; appnope 0.1.3; asttokens NA; attr 23.1.0; backcall 0.2.0; boltons NA; cffi 1.15.1; cloudpickle 2.2.1; comm 0.1.3; ctxcore 0.2.0; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.1; dask 2023.7.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; frozendict 2.3.8; h5py 3.9.0; ikarus NA; importlib_resources NA; ipykernel 6.24.0; ipython_genutils 0.2.0; jedi 0.18.2; jinja2 3.1.2; joblib 1.3.1; kiwisolver 1.4.4; llvmlite 0.40.1; lz4 4.3.2; markupsafe 2.1.3; matplotlib 3.7.2; mpl_toolkits NA; natsort 8.4.0; numba 0.57.1; numexpr 2.8.4; numpy 1.24.4; packaging 23.1; pandas 2.0.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.8.1; prompt_toolkit 3.0.39; psutil 5.9.5; ptyprocess 0.7.0; pure_eval 0.2.2; pyarrow 12.0.1; pydev_ipython",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:2888,cache,cache,2888,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,"ib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; A",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4117,cache,cached,4117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,iling env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 con,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:29787,load,loader,29787,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,"ils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2378,concurren,concurrent,2378,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"in(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453580_P5_exp.txt.gz').T; lung_P8 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453583_P8_exp.txt.gz').T; lung_P16 = sc.read_text('D:\课题\博士课题\单细胞",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2908,cache,cache,2908,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"in(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453580_P5_exp.txt.gz').T; lung_P8 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453583_P8_exp.txt.gz').T; lung_P16 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453591_P16_exp.txt.gz').T; lung_P24 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RAW\GSM4453599_P24_exp.txt.gz').T; lung_P28 = sc.read_t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:3064,cache,cache,3064,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"index=new_index). ~/.local/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0); 524 ; 525 def merge_dataframes(dfs, new_index, merge_strategy=merge_unique):; --> 526 dfs = [df.reindex(index=new_index) for df in dfs]; 527 # New dataframe with all shared data; 528 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/.local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs); 322 @wraps(func); 323 def wrapper(*args, **kwargs) -> Callable[..., Any]:; --> 324 return func(*args, **kwargs); 325 ; 326 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs); 4770 kwargs.pop(""axis"", None); 4771 kwargs.pop(""labels"", None); -> 4772 return super().reindex(**kwargs); 4773 ; 4774 @deprecate_nonkeyword_arguments(version=None, allowed_args=[""self"", ""labels""]). ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4817 # perform the reindex on the axes; 4818 return self._reindex_axes(; -> 4819 axes, level, limit, tolerance, method, fill_value, copy; 4820 ).__finalize__(self, method=""reindex""); 4821 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4596 if index is not None:; 4597 frame = frame._reindex_index(; -> 4598 index, method, copy, level, fill_value, limit, tolerance; 4599 ); 4600 . ~/.local/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance); 4618 copy=copy,; 4619 fill_value=fill_value,; -> 4620 allow_dups=False,; 4621 ); 4622 . ~/.local/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups); 4887 fill_value=fill_value,; 4888 allow_dups=allow_dups,; -> 4889 copy=copy,; 4890 ); 4891 # If we've made a copy once, no need to make another one. ~/.local/lib/python",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2364:3921,perform,perform,3921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2364,1,['perform'],['perform']
Performance,"ine 82, in <module>; from .base import clone; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\base.py"", line 17, in <module>; from .utils import _IS_32BIT; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\__init__.py"", line 28, in <module>; from .fixes import np_version, parse_version; File ""C:\ProgramData\Miniconda3\lib\site-packages\sklearn\utils\fixes.py"", line 20, in <module>; import scipy.stats; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\__init__.py"", line 441, in <module>; from .stats import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\stats.py"", line 43, in <module>; from . import distributions; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\distributions.py"", line 8, in <module>; from ._distn_infrastructure import (rv_discrete, rv_continuous, rv_frozen); File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\stats\_distn_infrastructure.py"", line 24, in <module>; from scipy import optimize; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\__init__.py"", line 400, in <module>; from .optimize import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\optimize.py"", line 36, in <module>; from ._numdiff import approx_derivative; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\optimize\_numdiff.py"", line 6, in <module>; from scipy.sparse.linalg import LinearOperator; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\__init__.py"", line 114, in <module>; from .eigen import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\__init__.py"", line 9, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\__init__.py"", line 20, in <module>; from .arpack import *; File ""C:\ProgramData\Miniconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 42, in <module>; from . import _arpack; ImportError: DLL load failed while importing _ar",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953:1922,optimiz,optimize,1922,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1073170953,2,['optimiz'],['optimize']
Performance,ing tables; Using cached tables-3.6.1-2-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting numpy>=1.17.0; Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecti,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1300,cache,cached,1300,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size); 487 # backwards compat: save X with the correct name; 488 if 'X' not in d:. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size); 511 return; 512 . ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size); 508 d[key_write] = OrderedDict() if key == 'uns' else {}; 509 for k in f[key].keys():; --> 510 _read_key_value_from_h5(f, d[key_write], key +",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2914,load,load,2914,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,1,['load'],['load']
Performance,"ional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2575,optimiz,optimization,2575,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,"irmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I am following the tutorial but everytime I try to run a violin plot the kernel crashes, this doesnt happen with other seaborn graphs. I have tried updating packeges, changing environment, etc, etc & nothing works any help would be great !!. ![image](https://github.com/scverse/scanpy/assets/127498480/b5cc12b1-00af-4919-abd0-7ea99b72cade). ### Minimal code sample. ```python; import scanpy as sc; import pandas as pd; import numpy as np; sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); sc.logging.print_header(); sc.settings.set_figure_params(dpi=80, facecolor='white'); results_file = 'write/pbmc3k.h5ad' # the file that will store the analysis results; adata = sc.read_10x_mtx(; 'data/', # the directory with the `.mtx` file; var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); cache=True) # write a cache file for faster subsequent reading; adata.var_names_make_unique() # this is unnecessary if using `var_names='gene_ids'` in `sc.read_10x_mtx`; adata; sc.pl.highest_expr_genes(adata, n_top=20, ); sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); adata.var['mt'] = adata.var_names.str.startswith('MT-') # annotate the group of mitochondrial genes as 'mt'; sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True); sc.pl.scatter(adata, x='total_counts', y='pct_counts_mt'); sc.pl.scatter(adata, x='total_counts', y='n_genes_by_counts'); sc.pl.violin(adata, ['n_genes_by_counts', 'total_counts', 'pct_counts_mt'], jitter=0-4, multi_panel=True); ```. ### Error output. ```pytb; Kernel Restarting; The kernel for Tests/scanpytutorial/Untitled.ipynb appears to have died. It will restart automatically.; ```. ### Versions. <details>. ```; -----; anndata 0.10.5.post1; scanpy 1.9.8; -----; PIL 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2840:1150,cache,cache,1150,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2840,2,['cache'],['cache']
Performance,"it shouldn’t affect performance at all, just readability",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3316#issuecomment-2437410637:20,perform,performance,20,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316#issuecomment-2437410637,1,['perform'],['performance']
Performance,"ixbuf2.0-common (= 2.42.6+dfsg-2),; libgfortran5 (= 11.2.0-10),; libgirepository-1.0-1 (= 1.70.0-2),; libglib2.0-0 (= 2.70.1-1),; libglpk40 (= 5.0-1),; libgmp10 (= 2:6.2.1+dfsg-2),; libgnutls30 (= 3.7.2-2),; libgomp1 (= 11.2.0-10),; libgpg-error0 (= 1.42-3),; libgraphite2-3 (= 1.3.14-1),; libgssapi-krb5-2 (= 1.18.3-7),; libgtk-3-0 (= 3.24.30-3),; libgtk-3-common (= 3.24.30-3),; libharfbuzz0b (= 2.7.4-1),; libhdf5-103-1 (= 1.10.7+repack-4),; libhdf5-hl-100 (= 1.10.7+repack-4),; libheif1 (= 1.12.0-2+b3),; libhogweed6 (= 3.7.3-1),; libicu67 (= 67.1-7),; libidn2-0 (= 2.3.2-2),; libigraph1 (= 0.8.5+ds1-1),; libimagequant0 (= 2.12.2-1.1),; libip4tc2 (= 1.8.7-1),; libisl23 (= 0.24-2),; libitm1 (= 11.2.0-10),; libjbig0 (= 2.1-3.1+b2),; libjpeg62-turbo (= 1:2.1.1-1),; libjs-jquery (= 3.5.1+dfsg+~3.5.5-8),; libjs-jquery-hotkeys (= 0~20130707+git2d51e3a9+dfsg-2.1),; libjs-jquery-isonscreen (= 1.2.0-1.1),; libjs-jquery-metadata (= 12-3),; libjs-jquery-tablesorter (= 1:2.31.3+dfsg1-2),; libjs-jquery-throttle-debounce (= 1.1+dfsg.1-1.1),; libjs-jquery-ui (= 1.13.0+dfsg-1),; libjs-sphinxdoc (= 4.2.0-5),; libjs-underscore (= 1.9.1~dfsg-4),; libjson-c5 (= 0.15-2),; libk5crypto3 (= 1.18.3-7),; libkeyutils1 (= 1.6.1-2),; libkmod2 (= 29-1),; libkrb5-3 (= 1.18.3-7),; libkrb5support0 (= 1.18.3-7),; liblapack3 (= 3.10.0-1),; liblbfgsb0 (= 3.0+dfsg.3-9),; liblcms2-2 (= 2.12~rc1-2),; libldap-2.4-2 (= 2.4.59+dfsg-1),; libllvm11 (= 1:11.1.0-4),; liblqr-1-0 (= 0.4.2-2.1),; liblsan0 (= 11.2.0-10),; libltdl7 (= 2.4.6-15),; liblz4-1 (= 1.9.3-2),; liblzf1 (= 3.6-3),; liblzma5 (= 5.2.5-2),; liblzo2-2 (= 2.10-2),; libmagic-mgc (= 1:5.39-3),; libmagic1 (= 1:5.39-3),; libmagickcore-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmagickwand-6.q16-6 (= 8:6.9.11.60+dfsg-1.3),; libmd0 (= 1.0.4-1),; libmount1 (= 2.37.2-4),; libmpc3 (= 1.2.1-1),; libmpdec3 (= 2.5.1-2),; libmpfr6 (= 4.1.0-3),; libncurses6 (= 6.2+20210905-1),; libncursesw6 (= 6.2+20210905-1),; libnettle8 (= 3.7.3-1),; libnghttp2-14 (= 1.43.0-1),; libnsl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616:6253,throttle,throttle-debounce,6253,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2048#issuecomment-969885616,1,['throttle'],['throttle-debounce']
Performance,"kages/anndata/base.py"", line 1205, in __getitem__; return self._getitem_view(index); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 1209, in _getitem_view; return AnnData(self, oidx=oidx, vidx=vidx, asview=True); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 635, in __init__; self._init_as_view(X, oidx, vidx); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/anndata/base.py"", line 661, in _init_as_view; var_sub = adata_ref.var.iloc[vidx_normalized]; File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1478, in __getitem__; return self._getitem_axis(maybe_callable, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 2087, in _getitem_axis; return self._getbool_axis(key, axis=axis); File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1494, in _getbool_axis; inds, = key.nonzero(); ValueError: too many values to unpack (expected 1); ```. I've tried several variations of this yet I don't see why your command wouldn't work, it seems like it should do what you intend .. . Note however, I ran:. ```py; print(np.any(adata.X.sum(axis=0) == 0)) # True; print(np.any(adata.X.sum(axis=1) == 0)) # False; ```. right after loading the dataset and it still shows True and False, yet if I were to regress out WITHOUT removing cell types via:. ```py; keep_cells = [i for i in adata.obs.index if i not in adata_blood.obs.index]; adata = adata[keep_cells, :]; ```. or . ```py; adata = adata[adata.obs['blood'] < 0.25, :] # classification score threshold for blood cells ; ```. ... the regression will work. However, once I remove, it won't. I could try to remove the 0 columns with R, unless you have another suggestion?. Thank you for any feedback.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297:1744,load,loading,1744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/230#issuecomment-412098297,1,['load'],['loading']
Performance,kernel 6.29.3 pypi_0 pypi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0 ; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0 ; jupyter_core 5.5.0 py311haa95532_0 ; jupyter_events 0.8.0 py311haa95532_0 ; jupyter_server 2.10.0 py311haa95532_0 ; jupyter_server_terminals 0.4.4 py311haa95532_1 ; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0 ; jupyterlab_server 2.25.1 py311haa95532_0 ; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0 ; libsodium 1.0.18 h62dcd97_0 ; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2 ; m2w64-bzip2 1.0.6 6 ; m2w64-expat 2.1.1 2 ; m2w64-fftw 3.3.4 6 ; m2w64-flac 1.3.1 3 ; m2w64-gcc-libgfortran 5.3.0 6 ; m2w64-gcc-libs 5.3.0 7 ; m2w64-gcc-libs-core 5.3.0 7 ; m2w64-gettext 0.19.7 2 ; m2w64-gmp 6.1.0 2 ; m2w64-gsl 2.1 2 ; m2w64-libiconv 1.14 6 ; m2w64-libjpeg-turbo 1.4.2 3 ; m2w64-libogg 1.3.2 3 ; m2w64-libpng 1.6.21 2 ; m2w64-libsndfile 1.0.26 2 ; m2w64-libsodium 1.0.10 2 ; m2w64-libtiff 4.0.6 2 ; m2w64-libvorbis 1.3.5 2 ; m2w64-libwinpthread-git 5.0.0.4634.697f757 2 ; m2w64-libxml2 2.9.3 4 ; m2w64-mpfr 3.1.4 4 ; m2w64-openblas 0.2.19 1 ; m2w64-pcre 8.38 2 ; m2w64-speex 1.2rc2 3 ; m2w64-speexdsp 1.2rc3 3 ; m2w64-tcl 8.6.5 3 ; m2w64-tk 8.6.5 3 ; m2w64-tktable 2.10 5 ; m2w64-wineditline 2.101 5 ; m2w64-xz 5.2.2 2 ; m2w64-zeromq 4.1.4 2 ; m2w64-zlib 1.2.8 10 ; markupsafe 2.1.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:6920,load,loader,6920,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['load'],['loader']
Performance,"l, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds); 632 ; 633 if isinstance(asheetname, str):; --> 634 sheet = self.get_sheet_by_name(asheetname); 635 else: # assume an integer if not a string; 636 sheet = self.get_sheet_by_index(asheetname). ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py in get_sheet_by_name(self, name); 543 ; 544 def get_sheet_by_name(self, name: str):; --> 545 self.raise_if_bad_sheet_by_name(name); 546 return self.book[name]; 547 . ~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py in raise_if_bad_sheet_by_name(self, name); 568 def raise_if_bad_sheet_by_name(self, name: str) -> None:; 569 if name not in self.sheet_names:; --> 570 raise ValueError(f""Worksheet named '{name}' not found""); 571 ; 572 def parse(. ValueError: Worksheet named 'expression' not found]; ```. #### '1.9.1'. <details>. [-----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.2.0; PyObjCTools NA; appnope 0.1.2; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.5; cffi 1.15.1; cloudpickle 2.0.0; colorama 0.4.5; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2022.7.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; entrypoints 0.4; fsspec 2022.7.1; h5py 3.7.0; hypergeom_ufunc NA; igraph 0.10.2; ipykernel 6.15.2; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; jupyter_server 1.18.1; kiwisolver 1.4.2; leidenalg 0.9.0; llvmlite 0.38.0; louvain 0.8.0; lxml 4.9.1; lz4 3.1.3; markupsafe 2.0.1; matplotlib 3.5.2; mkl 2.4.0; mpl_toolkits NA; natsort 8.2.0; nbinom_ufunc NA; ncf_ufunc NA; numba 0.55.1; numexpr 2.8.3; numpy 1.21.5; openpyxl 3.0.10; packaging 21.3; pandas 1.4.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; plotly 5.9.0; prompt_toolkit 3.0.20; psutil 5.9.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.9; pytz 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2371:3886,bottleneck,bottleneck,3886,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2371,1,['bottleneck'],['bottleneck']
Performance,"le writing key {key!r} of {type(elem)}"". TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'Batch' of <class 'h5py._hl.group.Group'> from /. Above error raised while writing key 'obs' of <class 'h5py._hl.files.File'> from /. ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.4; -----; PIL 8.2.0; anyio NA; appnope 0.1.2; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.04.0; dateutil 2.8.1; decorator 5.0.6; fsspec 2021.05.0; get_version 2.2; google NA; h5py 3.2.1; idna 2.10; igraph 0.7.1; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.4.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.7.0; llvmlite 0.36.0; loompy 3.0.6; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.3; numba 0.53.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 20.9; pandas 1.2.4; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pyparsing 2.4.7; pyrsistent NA; pytz 2021.1; requests 2.25.1; scipy 1.6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1866:6607,bottleneck,bottleneck,6607,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1866,1,['bottleneck'],['bottleneck']
Performance,"lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1518, in _format_strings; return list(self.get_result_as_array()); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1482, in get_result_as_array; formatted_values = format_values_with(float_format); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1456, in format_values_with; values = format_with_na_rep(values, formatter, na_rep); File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1427, in format_with_na_rep; [; File ""/root/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/io/formats/format.py"", line 1428, in <listcomp>; formatter(val) if not m else na_rep; ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(); ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 8.3.2; anndata 0.7.6; beta_ufunc NA; binom_ufunc NA; cffi 1.14.6; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; dunamai 1.6.0; encodings NA; genericpath NA; get_version 3.5; h5py 3.4.0; joblib 1.0.1; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; llvmlite 0.37.0; matplotlib 3.4.3; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.0; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.3; pkg_resources NA; posixpath NA; pycparser 2.20; pyexpat NA; pyparsing 2.4.7; pytz 2021.1; scanpy 1.7.2; scipy 1.7.1; setuptools_scm NA; sinfo 0.3.1; six 1.16.0; sklearn 1.0; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tables 3.6.1; -----; Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) [GCC 9.4.0]; Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.31; 24 logical CPU cores, x86_64; -----; Session information updated at 2021-10-01 14:56. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2008:4425,concurren,concurrent,4425,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2008,1,['concurren'],['concurrent']
Performance,limit number of loadings in pl.pca_loadings,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2059:16,load,loadings,16,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2059,1,['load'],['loadings']
Performance,"ls>=4.22.0 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (4.25.0); Requirement already satisfied: python-dateutil>=2.7 in c:\users\charles\anaconda3\lib\site-packages (from matplotlib>=3.1.2->scanpy) (2.8.2); Requirement already satisfied: llvmlite>=0.29.0 in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (0.29.0); Requirement already satisfied: pytz>=2017.3 in c:\users\charles\anaconda3\lib\site-packages (from pandas>=0.21->scanpy) (2021.3); Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\charles\anaconda3\lib\site-packages (from scikit-learn>=0.21.2->scanpy) (2.2.0); Collecting numba>=0.41.0; Using cached numba-0.55.1-cp37-cp37m-win_amd64.whl (2.4 MB); Requirement already satisfied: pynndescent>=0.5 in c:\users\charles\anaconda3\lib\site-packages (from umap-learn>=0.3.10->scanpy) (0.5.2); Requirement already satisfied: setuptools in c:\users\charles\anaconda3\lib\site-packages (from numba>=0.41.0->scanpy) (58.0.4); Collecting llvmlite>=0.29.0; Using cached llvmlite-0.38.0-cp37-cp37m-win_amd64.whl (23.2 MB); Requirement already satisfied: get-version>=2.0.4 in c:\users\charles\anaconda3\lib\site-packages (from legacy-api-wrap->scanpy) (2.2); Requirement already satisfied: stdlib-list in c:\users\charles\anaconda3\lib\site-packages (from sinfo->scanpy) (0.8.0); Requirement already satisfied: numexpr>=2.6.2 in c:\users\charles\anaconda3\lib\site-packages (from tables->scanpy) (2.8.1); Requirement already satisfied: colorama in c:\users\charles\anaconda3\lib\site-packages (from tqdm->scanpy) (0.4.4); Installing collected packages: llvmlite, numba, xlrd; Attempting uninstall: llvmlite; Found existing installation: llvmlite 0.29.0; Note: you may need to restart the kernel to use updated packages.; ERROR: Cannot uninstall 'llvmlite'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.; ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626:4735,cache,cached,4735,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2173#issuecomment-1063704626,1,['cache'],['cached']
Performance,"ly my stance as well. > How about printing the absolute path of the data's destination on download?. I thought that too. Only we should do it not just on download, but on every use, e.g. “reading cached data from ~/.cache/scanpy/paul15.h5ad”. And put help on how to change the cache dir in the settings docs. > I thought the older ones would just be deleted, right?. Since those systems aren't configured well, probably not. On those systems, it would just be another directory. But on a laptop with a common Linux distribution, there would be a pop-up once your disk space gets low, which allows you to clear that directory with a click. > If you had space for a couple datasets, wouldn't it be likely that installing a couple things with pip would clear these datasets on a system like we're describing? I'm not sure I find this behavior intuitive for this use case. You'd not notice it much, because datasets are just being re-downloaded on demand. That's a feature!. > [We don't have XDG_CACHE_HOME set]. Yes, because you only need it if you want your cache files to not be in `~/.cache`. > When I think about example datasets that are available through scientific computing packages I think of […]. I'm on mobile, so I don't want to check all of those, but. - miniconda is somewhere else for me by default, and it contains everything, not just data; - nltk pops up a window asking you to where to put stuff, and [recommends /use/local/share/nltk_data](https://www.nltk.org/data.html) for global installs, with no recommendation for per-user installs. I have a lot more stuff in my cache dir, not just applications. And as said: for good reason, because the OS often knows about this, which helps the user to delete the stuff with one click if needed. ---. My personal hell certainly includes dozens of libraries and applications putting all kinds of crap in unhidden directories in my home. All of them have a different way to configure that location or none at all. Chills me right to the core.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890:1291,cache,cache,1291,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-477102890,3,['cache'],['cache']
Performance,"m/h5py/h5py/issues/1732. Also, I've encoutered this bug when coming up with example (seems unrelated):; ```python; import scanpy as sc. adata = sc.datasets.paul15(). sc.read('data/paul15/paul15.h5'); ```; The last line raises:. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-24-3d2f3a02bf09> in <module>; ----> 1 sc.read('data/paul15/paul15.h5'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank 1.0.0; cffi 1.14.3; cycler 0.10.0; cython_runtime NA; dateutil 2.8.1; decorator 4.4.2; docrep 0.3.1; future_fstrings NA; get_version 2.1; h5py 2.10.0; igraph 0.8.3; ipykernel 5.3.4; ipython_genutils 0.2.0; jax 0.2.5; jaxlib 0.1.56; jedi 0.17.2; joblib 0.17.0; kiwisolver 1.3.1; lapack NA; legacy_api_wrap 1.2; leidenalg 0.8.1; llvmlite 0.34.0; l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:1959,cache,cache,1959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['cache'],['cache']
Performance,"md64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2246,cache,cached,2246,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"most recent call last); /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3077 try:; -> 3078 return self._engine.get_loc(key); 3079 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-13-884b80f3079d> in <module>; ----> 1 adata = sc.read_10x_mtx('/Users/kulkarnia2/Box/scRNASeq/HNSCC/Combined_TC_CK_scRNAseq/all_samples/HD_1_PBL'). ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 302 make_unique=make_unique,; 303 cache=cache,; --> 304 cache_compression=cache_compression,; 305 ); 306 if genefile_exists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applica",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:1475,cache,cache,1475,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,3,['cache'],['cache']
Performance,"mp/adata.h5ad'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 500 if not backed:; 501 f.close(); --> 502 return AnnData._args_from_dict(d); 503 ; 504 . /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/anndata/core/anndata.py in _args_from_dict(ddata); 2182 d_true_keys[ann][k_stripped] = pd.Categorical.from_codes(; 2183 codes=d_true_keys[ann][k_stripped].values,; -> 2184 categories=v,; 2185 ); 2186 k_to_delete.append(k). /usr/local/anaconda3/envs/diffxpy/lib/python3.6/site-packages/pandas/core/arrays/categorical.py in from_codes(cls, codes, categories, ordered, dtype); 638 dtype = CategoricalDtype._from_values_or_dtype(categories=categories,; 639 ordered=ordered,; --> 640 dtype=dtype); 641 if dtype.categories is None:; 642 msg = (""The categories must be provided in 'categorie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409:1569,load,load,1569,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/102#issuecomment-566126409,1,['load'],['load']
Performance,"mpile(self, args, return_type); 77 ; 78 def compile(self, args, return_type):; ---> 79 status, retval = self._compile_cached(args, return_type); 80 if status:; 81 return retval. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_cached(self, args, return_type); 91 ; 92 try:; ---> 93 retval = self._compile_core(args, return_type); 94 except errors.TypingError as e:; 95 self._failed_cache[key] = e. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/dispatcher.py in _compile_core(self, args, return_type); 104 ; 105 impl = self._get_implementation(args, {}); --> 106 cres = compiler.compile_extra(self.targetdescr.typing_context,; 107 self.targetdescr.target_context,; 108 impl,. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in compile_extra(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class); 602 compiler pipeline; 603 """"""; --> 604 pipeline = pipeline_class(typingctx, targetctx, library,; 605 args, return_type, flags, locals); 606 return pipeline.compile_extra(func). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/compiler.py in __init__(self, typingctx, targetctx, library, args, return_type, flags, locals); 308 config.reload_config(); 309 typingctx.refresh(); --> 310 targetctx.refresh(); 311 ; 312 self.state = StateDict(). ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/base.py in refresh(self); 282 pass; 283 self.install_registry(builtin_registry); --> 284 self.load_additional_registries(); 285 # Also refresh typing context, since @overload declarations can; 286 # affect it. ~/miniconda3/envs/scanpy_dev/lib/python3.8/site-packages/numba/core/cpu.py in load_additional_registries(self); 76 ; 77 # load 3rd party extensions; ---> 78 numba.core.entrypoints.init_all(); 79 ; 80 @property. AttributeError: module 'numba' has no attribute 'core'; ```. </details>. so the solution would be to pin `umap-learn==0.5.1`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466:4726,load,load,4726,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1756#issuecomment-846931466,1,['load'],['load']
Performance,"n <module>(); ----> 1 import scanpy as sc. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\__init__.py in <module>(); 1 # some technical stuff; 2 import sys; ----> 3 from .utils import check_versions, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\utils.py in <module>(); 16 ; 17 from . import settings; ---> 18 from . import logging as logg; 19 ; 20 EPS = 1e-15. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\logging.py in <module>(); 4 import time as time_module; 5 import datetime; ----> 6 from anndata import logging; 7 from . import settings; 8 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .base import AnnData; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\base.py in <module>(); 40 return 'mock zappy.base.ZappyArray'; 41 ; ---> 42 from . import h5py; 43 from .layers import AnnDataLayers; 44 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 5 ; 6 import six; ----> 7 import h5py; 8 import numpy as np; 9 import scipy.sparse as ss. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587:2160,load,load,2160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587,1,['load'],['load']
Performance,"n `filter_genes_dispersion` exposed. Here's an example of the error using `scanpy` pulled from github, but the same issue occurs on the release on pypi:. ```python; In [1]: import numpy as np; ...: import pandas as pd; ...: import scanpy.api as sc; ...: ; ...: sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3); ...: sc.settings.set_figure_params(dpi=80) # low dpi (dots per inch) yields small inline figures; ...: sc.logging.print_versions(); /Users/isaac/miniconda3/envs/scanpy/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.; from ._conv import register_converters as _register_converters; adatascanpy==1.0.4+91.ge9ae4ff anndata==0.6 numpy==1.14.3 scipy==1.1.0 pandas==0.22.0 scikit-learn==0.19.1 statsmodels==0.8.0 . In [2]: adata = sc.read(""./data/pbmc3k_filtered_gene_bc_matrices/hg19/matrix.mtx"").T; --> This might be very slow. Consider passing `cache=True`, which enables much faster reading from a cache file.; In [3]: sc.pp.recipe_zheng17(adata, plot=True); running recipe zheng17; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-c19f237f1c6e> in <module>(); ----> 1 sc.pp.recipe_zheng17(adata, plot=True). ~/github/scanpy/scanpy/preprocessing/recipes.py in recipe_zheng17(adata, n_top_genes, log, plot, copy); 106 if plot:; 107 from .. import plotting as pl # should not import at the top of the file; --> 108 pl.filter_genes_dispersion(filter_result, log=True); 109 # actually filter the genes, the following is the inplace version of; 110 # adata = adata[:, filter_result.gene_subset]. AttributeError: module 'scanpy.plotting' has no attribute 'filter_genes_dispersion'; ```. It looks like there's a pretty easy fix here, so I'd be up for making a pull request if you'd like.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/153:1428,cache,cache,1428,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/153,2,['cache'],['cache']
Performance,"n(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047641_P4-1N_matrix.tsv.gz')).T. # 240520; lung_P5 = sc.read_text('D:\课题\博士课题\单细胞分析\GSE148071_RA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2752,cache,cache,2752,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"n3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 58 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016cd0b000 (most recent call first):; File ""<venv>/lib/python3.12/socket.py"", line 295 in accept; File ""<venv>/lib/python3.12/site-packages/pytest_rerunfailures.py"", line 433 in run_server; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:3182,optimiz,optimization,3182,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['optimiz'],['optimization']
Performance,n_amd64.whl (13.2 MB); Collecting joblib; Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB); Collecting pandas>=0.21; Using cached pandas-1.1.5-cp36-cp36m-win_amd64.whl (8.7 MB); Collecting tqdm; Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB); Collecting matplotlib>=3.1.2; Using cached matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB); Collecting networkx>=2.3; Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB); Collecting sinfo; Using cached sinfo-0.3.4-py3-none-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1455,cache,cached,1455,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"n_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Coll",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:2174,cache,cached,2174,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"nd:; 419 self.add_legend_data(ax, func, common_kws, semantic_kws=semantic_kws); --> 420 handles, _ = ax.get_legend_handles_labels(); 421 if handles:; 422 ax.legend(title=self.legend_title). AttributeError: 'NoneType' object has no attribute 'get_legend_handles_labels'; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.6.0; aiohttp 3.8.6; aioitertools 0.11.0; aiosignal 1.3.1; alabaster 0.7.13; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.0; anaconda-cloud-auth 0.1.4; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anndata 0.10.1; anndata 0.10.0rc1; annoy 1.17.2; anyio 4.0.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:3981,cache,cache,3981,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['cache'],['cache']
Performance,"nd; 272 # ordered=None.; --> 273 dtype = CategoricalDtype(categories, ordered); 274 ; 275 return dtype. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in __init__(self, categories, ordered); 158 ; 159 def __init__(self, categories=None, ordered: Ordered = False):; --> 160 self._finalize(categories, ordered, fastpath=False); 161 ; 162 @classmethod. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in _finalize(self, categories, ordered, fastpath); 312 ; 313 if categories is not None:; --> 314 categories = self.validate_categories(categories, fastpath=fastpath); 315 ; 316 self._categories = categories. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/dtypes.py in validate_categories(categories, fastpath); 505 if not fastpath:; 506 ; --> 507 if categories.hasnans:; 508 raise ValueError(""Categorical categories cannot be null""); 509 . pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in hasnans(self); 2193 """"""; 2194 if self._can_hold_na:; -> 2195 return bool(self._isnan.any()); 2196 else:; 2197 return False. pandas/_libs/properties.pyx in pandas._libs.properties.CachedProperty.__get__(). ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/anaconda3/envs/scanpy1_7/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for Mu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850:4436,Cache,CachedProperty,4436,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850,1,['Cache'],['CachedProperty']
Performance,"nda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3360 try:; -> 3361 return self._engine.get_loc(casted_key); 3362 except KeyError as err:. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 1. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); /tmp/ipykernel_29519/245170133.py in <module>; ----> 1 Carraro=sc.read_10x_mtx('/mnt/Carraro',var_names='gene_ids'). ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only); 452 genefile_exists = (path / 'genes.tsv').is_file(); 453 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 454 adata = read(; 455 str(path),; 456 var_names=var_names,. ~/miniconda3/envs/flng/lib/python3.8/site-packages/scanpy/readwrite.py in _read_legacy_10x_mtx(path, var_names, make_unique, cache, cache_compression); 491 elif var_names == 'gene_ids':; 492 adata.var_names = genes[0].values; --> 493 adata.var['gene_symbols'] = genes[1].values; 494 else:; 495 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""). ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3456 if self.columns.nlevels > 1:; 3457 return self._getitem_multilevel(key); -> 3458 indexer = self.columns.get_loc(key); 3459 if is_integer(indexer):; 3460 indexer = [indexer]. ~/miniconda3/envs/flng/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3361 return self",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2053:1470,cache,cache,1470,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2053,1,['cache'],['cache']
Performance,"ne 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_requires'; warnings.warn(msg); running install; running build; running build_py; creating build; creating build/lib; creating build/lib/scanpy; copying scanpy/settings.py -> build/lib/scanpy; copying scanpy/readwrite.py -> build/lib/scanpy; copying scanpy/_version.py -> build/lib/scanpy; copying scanpy/__init__.py -> bui",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:2947,cache,cache,2947,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['cache'],['cache']
Performance,"ne-any.whl; Requirement already satisfied: importlib-metadata>=0.7 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (4.8.1); Collecting scikit-learn>=0.21.2; Using cached scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB); Collecting scipy>=1.4; Using cached scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB); Collecting numba>=0.41.0; Using cached numba-0.53.1-cp36-cp36m-win_amd64.whl (2.3 MB); Collecting patsy; Using cached patsy-0.5.2-py2.py3-none-any.whl (233 kB); Collecting natsort; Using cached natsort-8.0.0-py3-none-any.whl (37 kB); Collecting seaborn; Using cached seaborn-0.11.2-py3-none-any.whl (292 kB); Requirement already satisfied: packaging in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from scanpy[leiden]) (21.0); Collecting statsmodels>=0.10.0rc2; Using cached statsmodels-0.12.2-cp36-none-win_amd64.whl (9.3 MB); Collecting umap-learn>=0.3.10; Using cached umap_learn-0.5.2-py3-none-any.whl; Collecting anndata>=0.7.4; Using cached anndata-0.7.6-py3-none-any.whl (127 kB); Collecting legacy-api-wrap; Using cached legacy_api_wrap-1.2-py3-none-any.whl (37 kB); Collecting leidenalg; Using cached leidenalg-0.8.8-cp36-cp36m-win_amd64.whl (107 kB); Collecting python-igraph; Using cached python_igraph-0.9.8-py3-none-any.whl; Collecting xlrd<2.0; Using cached xlrd-1.2.0-py2.py3-none-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:1921,cache,cached,1921,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ned?. In #2235, more gradually enabled some so far disabled tests. Before, all tests in `TestPreprocessingDistributed` were disabled with the available optional dependencies we run our tests with:. https://github.com/scverse/scanpy/blob/06802b459648a219a10f74243efe4d6c2f912016/scanpy/tests/test_preprocessing_distributed.py#L15-L22. now, the dask tests are enabled and only the zappy tests are disabled:. https://github.com/scverse/scanpy/blob/6b9e734f4979a8ba450c0eaa052451f98b000753/scanpy/tests/test_preprocessing_distributed.py#L18-L31. ### Minimal code sample. ```bash; pytest -v --disable-warnings -k test_normalize_per_cell[dask] --runxfail; ```. ### Error output. ```pytb; ===================================================================================================== test session starts ======================================================================================================; platform linux -- Python 3.8.17, pytest-7.3.1, pluggy-1.0.0 -- /home/phil/Dev/Python/venvs/single-cell/bin/python; cachedir: .pytest_cache; rootdir: /home/phil/Dev/Python/Single Cell/scanpy; configfile: pyproject.toml; testpaths: scanpy; plugins: cov-4.1.0, nunit-1.0.3, memray-1.4.0, xdist-3.3.1; collected 986 items / 985 deselected / 1 selected . scanpy/tests/test_preprocessing_distributed.py::TestPreprocessingDistributed::test_normalize_per_cell[dask] FAILED [100%]. =========================================================================================================== FAILURES ===========================================================================================================; __________________________________________________________________________________ TestPreprocessingDistributed.test_normalize_per_cell[dask] __________________________________________________________________________________. self = <scanpy.tests.test_preprocessing_distributed.TestPreprocessingDistributed object at 0x7fdd21f2e9d0>, adata = AnnData object with n_obs × n_vars = 9999 × 1000; o",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2526:1308,cache,cachedir,1308,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2526,1,['cache'],['cachedir']
Performance,"nes, method=method). File ""/home/smith/anaconda3/envs/scanpy18/lib/python3.9/site-packages/scanpy/tools/_rank_genes_groups.py"", line 574, in rank_genes_groups; raise ValueError(. ValueError: reference = (0, 0) needs to be one of groupby = [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 14), (0, 15), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15)].; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.1; -----; 2f7ece400a652629565c523b34ee61b04afa385c NA; ACWS_filterCells NA; PIL 8.3.1; PyQt5 NA; absl NA; anndata 0.7.6; appdirs 1.4.4; astunparse 1.6.3; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.0; cloudpickle 1.6.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.0; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; encodings NA; et_xmlfile 1.0.1; flatbuffers NA; fsspec 2021.07.0; gast NA; genericpath NA; google NA; gprofiler 1.0.0; h5py 3.3.0; idna 3.1; igraph 0.9.6; imagecodecs 2021.6.8; imageio 2.9.0; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.17.2; joblib 1.0.1; keras_preprocessing 1.1.2; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.36.0; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; networkx 2.5; ntpath NA; numba 0.53.1; numexpr 2.7.3; numpy 1.21.1; opcode NA; openpyxl 3.0.7; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.0; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; pooch v1.4.0; posixpath NA; prompt_toolkit 3.0.19; psutil 5.8.0; ptyprocess 0.7.0; pycparser 2.20; pydoc_data NA; pyexpat NA; pygments 2.9.0; pynndescent 0.5.4; pyparsing 2.4.7; pytz 2021.1; requests 2.26.0; scanpy 1.8.1; scipy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1971:4160,concurren,concurrent,4160,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1971,1,['concurren'],['concurrent']
Performance,"nes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 65 ); 66 ; ---> 67 df['means'], df['variances'] = _get_mean_var(X); 68 ; 69 if batch_key is None:. ~/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py in _get_mean_var(X, axis); 6 def _get_mean_var(X, *, axis=0):; 7 if sparse.issparse(X):; ----> 8 mean, var = sparse_mean_variance_axis(X, axis=axis); 9 else:; 10 mean = np.mean(X, axis=axis, dtype=np.float64). ~/anaconda3/lib/python3.7/site-packages/scanpy/preprocessing/_utils.py in sparse_mean_variance_axis(mtx, axis); 40 ); 41 else:; ---> 42 return sparse_mean_var_minor_axis(mtx.data, mtx.indices, *shape, np.float64); 43 ; 44 . SystemError: CPUDispatcher(<function sparse_mean_var_minor_axis at 0x7fcea12550e0>) returned a result with an error set; ```. #### Versions. <details>. -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 6.2.0; absl NA; attr 19.2.0; backcall 0.1.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.2.1; cffi 1.12.3; cloudpickle 1.2.2; colorama 0.4.1; cycler 0.10.0; cython_runtime NA; cytoolz 0.10.0; dask 2.5.2; dateutil 2.8.0; decorator 4.4.0; deprecate 0.3.0; fsspec 2021.08.1; google NA; h5py 2.10.0; ipykernel 5.1.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.15.1; joblib 0.13.2; kiwisolver 1.1.0; llvmlite 0.29.0; matplotlib 3.4.3; more_itertools NA; mpl_toolkits NA; natsort 7.0.1; nbinom_ufunc NA; numba 0.45.1; numexpr 2.7.0; numpy 1.21.2; opt_einsum v3.3.0; packaging 21.0; pandas 1.3.2; parso 0.5.1; pexpect 4.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 2.0.10; psutil 5.6.3; ptyprocess 0.6.0; pycparser 2.19; pygments 2.10.0; pyparsing 2.4.2; pyro 1.7.0; pytorch_lightning 1.3.8; pytz 2019.3; rich NA; scipy 1.7.1; scvi 0.13.0; seaborn 0.9.0; setuptools 41.4.0; setuptools_scm NA; simplejson 3.17.2; six 1.12.0; sklearn 0.24.2; skmisc 0.1.4; sphinxcontrib NA; statsmodels 0.10.1; storemagic NA; tables 3.5.2; tblib 1.4.0; tensorboard 2.6.0; th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1995:2540,bottleneck,bottleneck,2540,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1995,1,['bottleneck'],['bottleneck']
Performance,"ng-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:2043,concurren,concurrent,2043,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,"ngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); <ipython-input-20-26443e0aed95> in <module>; ----> 1 rnaseq1 = sc.read_10x_mtx(""GSE145328_RAW""). ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 479 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 480 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 481 adata = read(; 482 str(path),; 483 var_names=var_names,. ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 560 else:; 561 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 562 adata.var['feature_types'] = genes[2].values; 563 adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[; 564 0. ~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3022 if self.columns.nlevels > 1:; 3023 return self._getitem_multilevel(key); -> 3024 indexer = self.columns.get_loc(key); 3025 if is_integer(indexer):; 3026 indexer = [indexer]. ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3080 return self._engine.get_loc(casted_key); 3081 except KeyError as err:; -> 3082 raise KeyError(key) from err; 3083 ; 3084 if tolerance is not None:. KeyError: 2; ```. #### Versions; scanpy==1.8.0 anndata==0.7.6 umap==0.5.1 numpy==1.19.2 scipy==1.6.3 pandas==1.2.4 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.1 pynndescent==0.5.2. <details>. [Paste the output of scanpy.logging.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916:1675,cache,cache,1675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916,1,['cache'],['cache']
Performance,"none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Usin",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3361,cache,cached,3361,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"ns, annotate_doc_types; 4 from ._version import get_versions # version generated by versioneer; 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(); 17 from pandas.api.types import CategoricalDtype; 18 ; ---> 19 from ._settings import settings; 20 from . import logging as logg; 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(); 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional; 8 ; ----> 9 from . import logging; 10 from .logging import _set_log_level, _set_log_file, RootLogger; 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(); 7 from typing import Optional; 8 ; ----> 9 import anndata.logging; 10 ; 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(); ----> 1 from .core.anndata import AnnData, Raw; 2 from .readwrite import (; 3 read_h5ad, read_loom, read_hdf,; 4 read_excel, read_umi_tools,; 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(); 46 LayersBase, Layers; 47 ); ---> 48 from .. import h5py; 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView; 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(); 22 SparseDataset; 23 """"""; ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse; 25 from h5py import Dataset, special_dtype; 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(); 4 from typing import Optional, Union, KeysView, NamedTuple; 5 ; ----> 6 import h5py; 7 import numpy as np; 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(); 34 _errors.silence_errors(); 35 ; ---> 36 from ._conv import register_converters as _register_converters; 37 _register_converters(); 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/900:2256,load,load,2256,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900,1,['load'],['load']
Performance,"nse for integer valued numbers. At the time I couldn't see a reason to convert the output to a different type. I figure that `log1p` should be able to take an integer valued expression matrix. However, I tried to implement that and ended up adding a lot of flow control to an already flow control heavy function, which got ugly:. <details>; <summary> 🍝 </summary>. ```python; def log1p(data, copy=False, chunked=False, chunk_size=None):; """"""Logarithmize the data matrix. Computes `X = log(X + 1)`, where `log` denotes the natural logarithm. Parameters; ----------; data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`; The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond; to cells and columns to genes.; copy : `bool`, optional (default: `False`); If an :class:`~anndata.AnnData` is passed, determines whether a copy; is returned. Returns; -------; Returns or updates `data`, depending on `copy`.; """"""; if copy:; if not isinstance(data, AnnData):; data = data.astype(np.floating); data = data.copy(); elif not isinstance(data, AnnData) and np.issubdtype(data.dtype, np.integer):; raise TypeError(""Cannot perform inplace log1p on integer array""). def _log1p(X):; if issparse(X):; np.log1p(X.data, out=X.data); else:; np.log1p(X, out=X). return X. if isinstance(data, AnnData):; if not np.issubdtype(data.X.dtype, np.floating):; data.X = data.X.astype(np.floating, copy=False); if chunked:; for chunk, start, end in data.chunked_X(chunk_size):; data.X[start:end] = _log1p(chunk); else:; _log1p(data.X); else:; _log1p(data). return data if copy else None; ```. </details>. I'll give that another shot, and open a PR. On the return type of `downsample_counts`, I've noticed many functions in scanpy return `float32` matrices regardless of what was given to them. Is this a design that's meant to be propagated? Even if not, what should the return type of `downsample_counts` be? At the time I figured it didn't matter, since anything downstream should be able to deal with it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239:1453,perform,perform,1453,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/435#issuecomment-475842239,1,['perform'],['perform']
Performance,"number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters in the original space are in the; embedded space and how much space will be between them. For larger; values, the space between natural clusters will be larger in the; embedded space. Again, the choice of this parameter is not very; critical. If the cost function increases during initial optimization,; the early exaggeration factor or the learning rate might be too high. **learning_rate** : `float`, optional (default: 1000). Note that the R-package ""Rtsne"" uses a default of 200.; The learning rate can be a critical parameter. It should be; between 100 and 1000. If the cost function increases during initial; optimization, the early exaggeration factor or the learning rate; might be too high. If the cost function gets stuck in a bad local; minimum increasing the learning rate helps sometimes. **random_state** : `int` or `None`, optional (default: 0). Change this to use different intial states for the optimization. If `None`,; the initial state is not reproducible. **use_fast_tsne** : `bool`, optional (default: `True`). Use the MulticoreTSNE package by D. Ulyanov if it is installed. **n_jobs** : `int` or `None` (default: `sc.settings.n_jobs`). Number of jobs. **copy** : `bool` (default: `False`). Return a copy instead of writing to adata. :Returns:. Depending on `copy`, returns or updates `adata` with the following fields. . **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`); ```. Now let's look at `pp.neighbors` where you're reading the type annotations from the signature.; - Obviously, the signature itself now is a mess for humans to read. But ok, that's fine if the docstring is easy to read.; - There i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:2901,optimiz,optimization,2901,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['optimiz'],['optimization']
Performance,"nv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, community.rst, contributors.rst, dev/ci.rst, dev/code.rst, dev/documentation.rst, dev/external-tools.rst, dev/getting-set-up.rst, ..., release-notes/1.7.1.rst, rele",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1573,load,loading,1573,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"nvs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 433 if ext in {'h5', 'h5ad'}:; 434 if sheet is None:; --> 435 return read_h5ad(filename, backed=backed); 436 else:; 437 logg.msg('reading sheet', sheet, 'from file', filename, v=4). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 442 else:; 443 # load everything into memory; --> 444 return AnnData(*_read_args_from_h5ad(filename=filename, chunk_size=chunk_size)); 445 ; 446 . /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 471 f = adata.file._file; 472 else:; --> 473 f = h5py.File(filename, 'r'); 474 for key in f.keys():; 475 if backed and key in AnnData._BACKED_ATTRS:. /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/anndata/h5py/h5sparse.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, force_dense, **kwds); 139 userblock_size=userblock_size,; 140 swmr=swmr,; --> 141 **kwds,; 142 ); 143 super().__init__(self.h5f, force_dense). /ysm-gpfs/pi/zhao/Softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds); 267 with phil:; 268 fapl = make_fapl(driver, libver, **kwds); --> 269 fid = make_fid(name, mode, userblock_s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/626:1373,load,load,1373,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626,1,['load'],['load']
Performance,"o my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipywidgets 8.1.2; isoduration NA; jedi 0.19.1; jinja2 3.1.3; joblib 1.3.2; json5 NA; jsonpointer 2.4; jsonschema 4.21.1; jsonschema_specifications NA; jupyter_events 0.9.0; jupyter_server 2.12.5; jupyterlab_server 2.25.3; kiwisolver 1.4.5; legacy_api_wrap NA; leidenalg 0.10.2; llvmlite 0.40.0; louvain 0.8.0; lz4 4.3.3; markupsafe 2.1.5; matplotlib 3.8.0; matplotlib_inline 0.1.6; mpl_toolkits NA; natsort 8.4.0; nbformat 5.9.2; nt NA; numba 0.57.1; numexpr 2.8.7; numpy 1.23.0; overrides NA; packaging 23.2; pandas 1.5.3; parso 0.8.3; patsy 0.5.6; pickleshare 0.7.5; pkg_resou",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:1570,bottleneck,bottleneck,1570,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['bottleneck'],['bottleneck']
Performance,"o the update of the anndata package to 0.10.4 (January 14, 2024); (?Error reading the file *features.tsv.gz*). The launch was carried out on the following data: ; https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM5733023. You can also download files using my google drive:; https://drive.google.com/drive/folders/1p6ilbsJX_cYZb4HG0OSbLHAwQObqmncW?usp=sharing. # **My actions**:. 1) I have installed the latest version of `scanpy=1.9.6` using conda:; ```console; $ conda --version; conda 23.10.0; $ conda install scanpy; # Channels:; # - conda-forge; # - bioconda; # - defaults; # Platform: linux-64. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.4 pyhd8ed1ab_0 conda-forge; array-api-compat 1.4 pyhd8ed1ab_0 conda-forge; brotli 1.1.0 hd590300_1 conda-forge; brotli-bin 1.1.0 hd590300_1 conda-forge; bzip2 1.0.8 hd590300_5 conda-forge; c-ares 1.25.0 hd590300_0 conda-forge; ca-certificates 2023.11.17 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.11.17 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; contourpy 1.2.0 py311h9547e67_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.0 pyhd8ed1ab_2 conda-forge; fonttools 4.47.2 py311h459d7ec_0 conda-forge; freetype 2.12.1 h267a509_2 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forge; h5py 3.10.0 nompi_py311hebc2b07_101 conda-forge; hdf5 1.14.3 nompi_h4f84152_100 conda-forge; icu 73.2 h59595ed_0 conda-forge; joblib 1.3.2 pyhd8ed1ab_0 conda-forge; keyutils 1.6.1 h166bdaf_0 conda-forge; kiwisolver 1.4.5 py311h9547e67_1 conda-forge; krb5 1.21.2 h659d440_0 conda-forge; lcms2 2.16 hb7c19ff_0 conda-forge; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; lerc 4.0.0 h27087fc_0 conda-forge; libaec 1.1.2 h59595ed_1 conda-forge; libblas 3.9.0 20_linux64_openblas conda-forge; libbrotlicommon 1.1.0 hd590300_1 conda-forge; libbrotlidec 1.1.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2806:1345,cache,cached-property,1345,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2806,1,['cache'],['cached-property']
Performance,"oftware/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session information updated at 2022-05-17 14:56",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:1758,concurren,concurrent,1758,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['concurren'],['concurrent']
Performance,"ogg.msg(. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in __getitem__(self, key); 909 key = check_bool_indexer(self.index, key); 910 ; --> 911 return self._get_with(key); 912 ; 913 def _get_with(self, key):. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in _get_with(self, key); 951 return self.loc[key]; 952 ; --> 953 return self.reindex(key); 954 except Exception:; 955 # [slice(0, 5, None)] will break if you convert to ndarray,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/series.py in reindex(self, index, **kwargs); 3732 @Appender(generic.NDFrame.reindex.__doc__); 3733 def reindex(self, index=None, **kwargs):; -> 3734 return super(Series, self).reindex(index=index, **kwargs); 3735 ; 3736 def drop(self, labels=None, axis=0, index=None, columns=None,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs); 4354 # perform the reindex on the axes; 4355 return self._reindex_axes(axes, level, limit, tolerance, method,; -> 4356 fill_value, copy).__finalize__(self); 4357 ; 4358 def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,. ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/generic.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy); 4367 ax = self._get_axis(a); 4368 new_index, indexer = ax.reindex(labels, level=level, limit=limit,; -> 4369 tolerance=tolerance, method=method); 4370 ; 4371 axis = self._get_axis_number(a). ~/jupyterminiconda3/envs/scanpy137/lib/python3.6/site-packages/pandas/core/indexes/category.py in reindex(self, target, method, level, limit, tolerance); 501 else:; 502 if not target.is_unique:; --> 503 raise ValueError(""cannot reindex with a non-unique indexer""); 504 ; 505 indexer, missing = self.get_indexer_non_unique(np.array(target)). ValueError: cannot reindex with a non-unique indexer; ```. These t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264:1763,perform,perform,1763,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/450#issuecomment-460303264,1,['perform'],['perform']
Performance,"oin(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047633_P8_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047636_P8_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047632_P8_T2_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm2_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047635_P8_T2_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # https://cloud.tencent.com/developer/article/2385592这儿得转置一下，不然不对; lung_ti_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047637_P4-2T1_matrix.tsv.gz')).T; # lung_ni_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047638_P4-2T2_matrix.tsv.gz')).T; lung_ts1_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047639_P4-2N_matrix.tsv.gz')).T; lung_ts2_p4 = sc.read_text(os.path.join(root, 'GSE200972_RAW', 'GSM6047640_P4-1T_matrix.tsv.gz')).T; # lung_ns_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:2596,cache,cache,2596,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,onda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 py_0 conda-forge; dbus 1.13.6 h5008d03_3 conda-forge; d,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9816,cache,cachecontrol-with-filecache,9816,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol-with-filecache']
Performance,"one-any.whl (103 kB); Collecting cached-property; Using cached cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB); Requirement already satisfied: typing-extensions>=3.6.4 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.10.0.2); Requirement already satisfied: zipp>=0.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Us",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3270,cache,cached,3270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"op:1px;; 	padding-right:1px;; 	padding-left:1px;; 	mso-ignore:padding;; 	color:black;; 	font-size:11.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-number-format:General;; 	text-align:general;; 	vertical-align:middle;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Package | Version; -- | --; Anaconda | 2.1.0; Python | 3.6.13; anndata | 0.7.6; anyio | 2.2.0; argon2-cffi | 20.1.0; async-generator | 1.1; attrs | 21.2.0; Babel | 2.9.1; backcall | 0.2.0; bleach | 4.0.0; brotlipy | 0.7.0; cached-property | 1.5.2; certifi | 2021.5.30; cffi | 1.14.6; charset-normalizer | 2.0.4; colorama | 0.4.4; contextvars | 2.4; **cryptography | 35.0.0**; cycler | 0.11.0; dataclasses | 0.8; decorator | 4.4.2; defusedxml | 0.7.1; entrypoints | 0.3; get-version | 2.1; h5py | 3.1.0; idna | 3.2; igraph | 0.9.8; immutables | 0.16; importlib-metadata | 4.8.1; ipykernel | 5.3.4; ipython | 7.16.1; ipython-genutils | 0.2.0; jedi | 0.17.0; **Jinja2 | 3.0.2**; joblib | 1.1.0; json5 | 0.9.6; jsonschema | 3.2.0; jupyter-client | 7.0.1; jupyter-core | 4.8.1; jupyter-server | 1.4.1; **jupyterlab | 3.2.1**; jupyterlab-pygments | 0.1.2; jupyterlab-server | 2.8.2; kiwisolver | 1.3.1; legacy-api-wrap | 1.2; leidenalg | 0.8.8; llvmlite | 0.36.0; MarkupSafe | 2.0.1; matplotlib | 3.3.4; mistune | 0.8.4; **natsort | 8.0.0**; nbclassic | 0.2.6; nbclient | 0.5.3; nbconvert | 6.0.7; nbformat | 5.1.3; nest-asyncio | 1.5.1; networkx | 2.5.1; notebook | 6.4.3; numba | 0.53.1; numexpr | 2.7.3; numpy | 1.19.5; packaging | 21;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699:2032,cache,cached-property,2032,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2046#issuecomment-963453699,1,['cache'],['cached-property']
Performance,"or faster subsequent reading; 5 ; 6 . ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\Continuum\anaconda3\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 447 else:; 448 if not is_present:; --> 449 raise FileNotFoundError('Did not find file {}.'.format(filename)); 450 logg.msg('reading', filename, v=4); 451 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file C:\Users\correap\Documents\03152019_scRNAseq\filtered_feature_bc_matrix_1\matrix.mtx.gz.; ```. ​My filtered_feature_bc_matrix_1 contains the folders barcodes.tsv, gene_symbols.tsv (manually changed to this from default 10X output of genes) and the matrix.mtx file. I could manually change the matrix.mtx to matrix.mtx.gz. but this might corrupt the file and not be very good practice in the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733:1942,cache,cache,1942,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/587#issuecomment-479994733,3,['cache'],['cache']
Performance,"ork"".; > > 1. Return cluster labels as ints. I'm not sure using strings breaks any compatability. Doesn't scikit-learn work fine with strings representing categories?. <details>; <summary> Example of sklearn working with string categories </summary>. ```python; from sklearn import metrics; import numpy as np; from string import ascii_letters. x = np.random.randint(0, 10, 50); y = np.array(list(ascii_letters))[np.random.randint(0, 10, 50)]. metrics.adjusted_rand_score(x, y); ```. </details>. > but I think it's a mistake to change the convention for how one indexes positionally vs using labels; > 2. Support non-string indexes (and adopt loc vs iloc). I don't think the conventions are so set in stone. Numpy behaves differently than pandas, which behaves differently than xarray. I personally like the conventions of [DimensionalData.jl](https://github.com/rafaqz/DimensionalData.jl), but think xarray is a likely the direction we'll head. > 3. Support ufuncs with AnnData. What does `np.log1p(adata)` return? Is it the whole object? Do we want to copy the whole object just to update values in X?. I think probably not. I also think AnnData <-> pd.DataFrame is the wrong analogy. In my view, an AnnData object is a collection of arrays, more akin to an xarray.Dataset, Bioconductor SummarizedExperiment, or an OLAP cube. I think a syntax that could work better would be something like:. ```python; adata.apply_ufunc(np.log1p, in=""X"", out=""X""); adata.apply_ufunc(np.log1p, in=(""layers"", ""counts""), out=(""layers"", ""log_counts"")); ```. As an aside, I think we could do something similar with sklearn style transformers, i.e. ```python; clf = SVC.fit(labelled, X=(""obsm"", ""X_pca""), y=""leiden""); clf.predict(unlabelled, X=(""obsm"", ""X_pca""), key_added=""transferred_labels""); ```. > 4. (maybe) Return copies of input for most scanpy functions. I think a core advantage of scanpy over the bioconductor ecosystem is the performance. If we always returned copies by default, a lot of that would go away.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629:2549,perform,performance,2549,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1030#issuecomment-584460629,1,['perform'],['performance']
Performance,"ort anndata as ad; from scipy.sparse import csr_matrix; print(ad.__version__). mtx = np.array([[1.2,2.1,3.9],[2.01,3.99,4.23],[4.21,5.12,6.87],[0,20.12,100.96]]). adata = sc.AnnData(mtx); adata.raw = adata; print(adata); print(adata.X). sc.pp.normalize_total(adata,target_sum=1e4); sc.pp.log1p(adata); print(adata.X) . print(adata.raw.X[0:10,0:10]); ```; I get following result; <img width=""525"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/59059267/5eec641b-3542-471b-be22-51ef8e8f31a8"">. It sems strange for me? Shouldn't I save raw data for float data? Could you give some suggestions? My environment is; <img width=""647"" alt=""image"" src=""https://github.com/scverse/scanpy/assets/59059267/2267345f-1a2b-4708-90f9-d1892adfb42f"">. ### Error output. _No response_. ### Versions. <details>. ```; -----; anndata 0.10.1; scanpy 1.9.5; -----; CoreFoundation NA; Foundation NA; PIL 9.4.0; PyObjCTools NA; anyio NA; appnope 0.1.2; asttokens NA; attr 22.1.0; babel 2.11.0; backcall 0.2.0; bottleneck 1.3.5; brotli NA; certifi 2023.07.22; cffi 1.15.1; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.6.0; dateutil 2.8.2; debugpy 1.6.7; decorator 5.1.1; defusedxml 0.7.1; dill 0.3.6; entrypoints 0.4; executing 0.8.3; fastjsonschema NA; gmpy2 2.1.2; h5py 3.9.0; idna 3.4; igraph 0.10.8; ipykernel 6.25.0; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonpointer 2.1; jsonschema 4.17.3; jupyter_server 1.23.4; jupyterlab_server 2.22.0; kiwisolver 1.4.4; leidenalg 0.10.1; llvmlite 0.40.0; louvain 0.8.1; lz4 4.3.2; markupsafe 2.1.1; matplotlib 3.7.2; mpl_toolkits NA; mpmath 1.3.0; natsort 8.4.0; nbformat 5.9.2; numba 0.57.1; numexpr 2.8.4; numpy 1.24.3; numpydoc 1.5.0; objc 10.0; packaging 23.1; pandas 2.0.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 3.10.0; plotly 5.9.0; prometheus_client NA; prompt_toolkit 3.0.36; psutil 5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2748:2062,bottleneck,bottleneck,2062,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2748,1,['bottleneck'],['bottleneck']
Performance,"ow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already satisfied: colorama in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from tqdm->scanpy[leiden]) (0.4.4); Installing collected packages: numpy, threadpoolctl, scipy, llvmlite, joblib, texttable, scikit-learn, pillow, numba, kiwisolver, cycler, cached-property, xlrd, tqdm, stdlib-list, pynndescent, patsy, pandas, numexpr, natsort, matplotlib, igraph, h5py, get-version, decorator, umap-learn, tables, statsmodels, sinfo, seaborn, python-igraph, networkx, legacy-api-wrap, anndata, scanpy, leidenalg; Attempting uninstall: decorator; Found existing installation: decorator 5.1.0; Uninstalling decorator-5.1.0:; Successfully uninstalled decorator-5.1.0; Successf",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:4278,cache,cached,4278,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"own installation of scanpy but it seems like a version issue with networkx2.6.2 (which is the latest of this date).; Minimal working example:; ```python; import scanpy as sc; paul15 = sc.datasets.paul15(); sc.pp.recipe_zheng17(paul15); sc.pp.neighbors(paul15, n_neighbors=4, n_pcs=20); sc.tl.paga(paul15, groups='paul15_clusters'); sc.pl.paga(paul15, labels=None, color='paul15_clusters'). gexf=sc.pl.paga(paul15, labels=None, color='paul15_clusters', export_to_gexf=True). ```. ```pytb. File ""anaconda3\lib\site-packages\scanpy\plotting\_tools\paga.py"", line 860, in _paga_graph; nx_g_solid.node[count]['label'] = str(node_labels[count]). AttributeError: 'Graph' object has no attribute 'node'```. ```; #### Versions; scanpy 1.8.1; networkx 2.6.2; matplotlib 3.4.3. <details>. sc.logging.print_versions(); WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.1; -----; PIL 8.0.1; PyQt5 NA; anndata 0.7.6; autoreload NA; backcall 0.2.0; bottleneck 1.3.2; bs4 4.9.3; cairo 1.20.1; cffi 1.14.3; chardet 3.0.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2.30.0; dateutil 2.8.1; decorator 4.4.2; h5py 2.10.0; html5lib 1.1; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; jedi 0.17.1; joblib 1.0.1; kiwisolver 1.3.0; leidenalg 0.8.7; llvmlite 0.34.0; lxml 4.6.1; matplotlib 3.4.3; mpl_toolkits NA; natsort 7.1.1; networkx 2.6.2; nt NA; ntsecuritycon NA; numba 0.51.2; numexpr 2.7.1; numpy 1.20.3; packaging 20.4; pandas 1.3.2; parso 0.7.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.8; psutil 5.7.2; pyarrow 0.16.0; pycparser 2.20; pygments 2.7.2; pynndescent 0.5.2; pyparsing 2.4.7; pythoncom NA; pytz 2020.1; pywintypes NA; scanpy 1.8.1; scipy 1.5.2; sinfo 0.3.1; sip NA; six 1.15.0; sklearn 0.23.2; soupsieve 2.0.1; sphinxcontrib NA; spyder 4.1.5; spyder_kernels 1.9.4; spydercustomize NA; statsmodels 0.12.0; storemagic NA; tables 3.6.1; tblib 1.7.0; texttable 1.6.4; tlz 0.11",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1997:1082,bottleneck,bottleneck,1082,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1997,1,['bottleneck'],['bottleneck']
Performance,"p us gain ground. > Who manages the sub-packages?. Scverse (also it's one package not many). We are talking about 5-15 readers that have been touched a handful of times in 4-5 years. I don't think this is a complicated package to maintain. Agree that one person needs to take the lead on releases (probably very infrequent). > I feel like complicated dependency management was what we were trying to avoid here. Where is the complicated dependency management? We have a core set of readers (h5, pandas, scipy) and more complex readers (lazy import). We can have a conda env file too for everything if we want. Even anndata lazy imports loom for example. It's a small price to pay for ecosystem synchronization and enhanced user experience. > Packages which read in package specific formats with a minimal set of dependencies. It's also unclear to me what package specific stuff muon has in particular. The way I see it there's one `read_10x_h5(return_anndata=True, return_mudata=False, gex_only=None)` I don't think muon is loading any extra information or putting it in any package specific places?. > How does this impact users vs. developers?. Developers: (1) export `scio` readers into their packages, can contribute improvements to readers, (2), access to many more practical readers for their packages (scvi-tools has no 10x h5 reader because we don't feel the need to depend on scanpy for one function). Users: (1) no impact if they continue using the packages they like (e.g., scanpy reader will be completely unchanged). (2) Can go ahead and just use `scio` and then be on their way (a reality that many people do not feel the need to use scanpy/muon). If there are R converters, this would be a major use case. > What we read in, and how we represent it, is very tightly coupled to the methods we have. Up for discussion, but read the maximal amount of information by default. If necessary (don't see any particular cases at the moment), package devs use the underlying `scio` function and r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352:1469,load,loading,1469,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1387#issuecomment-1059551352,1,['load'],['loading']
Performance,p>. # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_kmp_llvm conda-forge; absl-py 1.4.0 pyhd8ed1ab_0 conda-forge; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; annotated-types 0.5.0 pyhd8ed1ab_0 conda-forge; anyio 3.7.1 pyhd8ed1ab_0 conda-forge; arpack 3.7.0 hdefa2d7_2 conda-forge; arrow 1.2.3 pyhd8ed1ab_0 conda-forge; asttokens 2.2.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; backcall 0.2.0 pyh9f0ad1d_0 conda-forge; backports 1.0 pyhd8ed1ab_3 conda-forge; backports.cached-property 1.0.2 pyhd8ed1ab_0 conda-forge; backports.functools_lru_cache 1.6.5 pyhd8ed1ab_0 conda-forge; beautifulsoup4 4.12.2 pyha770c72_0 conda-forge; blas 1.0 mkl conda-forge; blessed 1.19.1 pyhe4f9e05_2 conda-forge; brotli 1.0.9 h166bdaf_9 conda-forge; brotli-bin 1.0.9 h166bdaf_9 conda-forge; brotlipy 0.7.0 py310h5764c6d_1005 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; cachecontrol 0.12.14 pyhd8ed1ab_0 conda-forge; cachecontrol-with-filecache 0.12.14 pyhd8ed1ab_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; chex 0.1.82 pyhd8ed1ab_0 conda-forge; cleo 2.0.1 pyhd8ed1ab_0 conda-forge; click 8.1.6 unix_pyh707e725_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.1.3 pyhd8ed1ab_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; contourpy 1.1.0 py310hd41b1e2_0 conda-forge; crashtest 0.4.1 pyhd8ed1ab_0 conda-forge; croniter 1.3.15 pyhd8ed1ab_0 conda-forge; cryptography 41.0.2 py310h75e40e8_0 conda-forge; cuda-cudart 11.8.89 0 nvidia; cuda-cupti 11.8.87 0 nvidia; cuda-libraries 11.8.0 0 nvidia; cuda-nvrtc 11.8.89 0 nvidia; cuda-nvtx 11.8.86 0 nvidia; cuda-runtime 11.8.0 0 nvidia; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; dateutils 0.6.12 p,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205:9769,cache,cachecontrol,9769,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2480#issuecomment-1646783205,1,['cache'],['cachecontrol']
Performance,"packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Park_Lab\\anaconda3\\envs\\Python38\\Lib\\site-packages\\~umpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll'; Consider using the `--user` option or check the permissions.; ```; Step3: same errors.; ```python; sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); sc.pl.highly_variable_genes(adata); ImportError Traceback (most recent call last); ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 52 try:; ---> 53 from skmisc.loess import loess; 54 except ImportError:. ~\AppData\Roaming\Python\Python38\site-packages\skmisc\loess\__init__.py in <module>; 50 """"""; ---> 51 from ._loess import (loess, loess_model, loess_inputs, loess_control,; 52 loess_outputs, loess_prediction,. ImportError: DLL load failed while importing _loess: The specified module could not be found. During handling of the above exception, another exception occurred:. ImportError Traceback (most recent call last); ~\AppData\Local\Temp/ipykernel_11028/1877627730.py in <module>; ----> 1 sc.pp.highly_variable_genes(adata, n_top_genes=5000, flavor='seurat_v3'); 2 sc.pl.highly_variable_genes(adata); 3 print(sum(adata.var.highly_variable)); 4 adata. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. ~\anaconda3\envs\Python38\lib\site-packages\scanpy\preprocessing\_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 53 from skmisc.loess import loess;",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342:2078,load,load,2078,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2073#issuecomment-1000601342,1,['load'],['load']
Performance,"pct_counts_mt < 10, :]; ---> 18 sc.pp.highly_variable_genes(zf_48, flavor='seurat_v3', span=1). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key); 413 ; 414 if flavor == 'seurat_v3':; --> 415 return _highly_variable_genes_seurat_v3(; 416 adata,; 417 layer=layer,. ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, span, subset, inplace); 59 X = adata.layers[layer] if layer is not None else adata.X; 60 if check_nonnegative_integers(X) is False:; ---> 61 raise ValueError(; 62 ""`pp.highly_variable_genes` with `flavor='seurat_v3'` expects ""; 63 ""raw count data."". ValueError: `pp.highly_variable_genes` with `flavor='seurat_v3'` expects raw count data.; ```. Am I loading the data in wrong? This processing has worked for data loaded in using 'sc.read_10x_mtx()'. #### Versions. <details>. -----; anndata 0.7.5; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; PyObjCTools NA; anndata 0.7.5; anndata2ri 1.0.5; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.4; cloudpickle 1.6.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2020.12.0; dateutil 2.8.1; decorator 4.4.2; get_version 2.1; h5py 3.1.0; idna 2.10; igraph 0.8.3; ipykernel 5.4.2; ipython_genutils 0.2.0; ipywidgets 7.5.1; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.0; jsonschema 3.2.0; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1.0; nbformat 5.0.8; numba 0.52.0; numexpr 2.7.1; numpy 1.19.4; packaging 20.8; pandas 1.1.5; parso 0.7.0; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.8; psutil 5.7.3; ptyprocess 0.6.0; pvectorc NA; pygments 2.7.3; ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1782:2544,load,loading,2544,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1782,2,['load'],"['loaded', 'loading']"
Performance,"pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.5; psutil 5.7.0; ptyprocess 0.6.0; pyarrow 8.0.0; pygments 2.6.1; pyparsing 2.4.7; pytoml NA; pytz 2020.1; scipy 1.5.0; setuptools_scm NA; six 1.14.0; sklearn 1.0.2; sphinxcontrib NA; storemagic NA; tables 3.6.1; tblib 1.6.0; texttable 1.6.4; threadpoolctl 2.1.0; tlz 0.10.1; toolz 0.10.0; tornado 6.0.4; traitlets 4.3.3; typing_extensions NA; wcwidth 0.2.5; yaml 5.3.1; zmq 19.0.1; zope NA; -----; IPython 7.16.1; jupyter_client 6.1.6; jupyter_core 4.6.3; jupyterlab 2.1.5; notebook 6.0.3; -----; Python 3.8.3 (default, Jul 2 2020, 16:21:59) [GCC 7.3.0]; Linux-3.10.0-1160.76.1.el7.x86_64-x86_64-with-glibc2.10; 52 logical CPU cores, x86_64; -----; </details>. The following is my package version list. <details>. -----; anndata 0.8.0; scanpy 1.7.0; sinfo 0.3.4; -----; OpenSSL 21.0.0; PIL 8.4.0; absl NA; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bioservices 1.9.0; bottleneck 1.3.2; brotli NA; bs4 4.10.0; bson NA; cairo NA; cattr NA; certifi 2021.10.08; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; colorlog NA; cryptography 3.4.8; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; deprecate 0.3.1; dill 0.3.5.1; docutils 0.17.1; dunamai 1.11.1; easydev 0.12.0; entrypoints 0.3; fsspec 2021.08.1; get_version 3.5.4; google NA; gridfs NA; gseapy 0.10.8; h5py 3.6.0; html5lib 1.1; idna 3.2; igraph 0.9.11; importlib_metadata NA; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; itsdangerous 2.0.1; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.9; llvmlite 0.36.0; lxml 4.6.3; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2310:4581,bottleneck,bottleneck,4581,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2310,1,['bottleneck'],['bottleneck']
Performance,pip install anndata --upgrade works.; The issue occurs when you saved anndata from a new version and when you try to load with old version.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220:117,load,load,117,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1399843220,1,['load'],['load']
Performance,"pmc3 results. BUT right at the beginning (sc.read()) the following error! I will appreciate your help.; thanks. `--------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-3-ef7315cdb8ff> in <module>(); 2 filename_genes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/genes.tsv'; 3 filename_barcodes = '/ifs/projects/proj077/backup/public_data/scanpy_tutorials_data/PBMC3K/filtered_gene_bc_matrices/hg19/barcodes.tsv'; ----> 4 adata = sc.read(filename_data, cache=True).transpose(); 5 adata.var_names = np.genfromtxt(filename_genes, dtype=str)[:, 1]; 6 adata.smp_names = np.genfromtxt(filename_barcodes, dtype=str). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename_or_filekey, sheet, ext, delimiter, first_column_names, backup_url, return_dict, cache); 73 if is_filename(filename_or_filekey):; 74 data = read_file(filename_or_filekey, sheet, ext, delimiter,; ---> 75 first_column_names, backup_url, cache); 76 if isinstance(data, dict):; 77 return data if return_dict else AnnData(data). /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in read_file(filename, sheet, ext, delimiter, first_column_names, backup_url, cache); 364 os.makedirs(os.path.dirname(filename_cache)); 365 # write for faster reading when calling the next time; --> 366 write_dict_to_file(filename_cache, ddata, sett.file_format_data); 367 return ddata; 368 . /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in write_dict_to_file(filename, d, ext); 771 d_write[key] = value; 772 # now open the file; --> 773 wait_until_file_unused(filename) # thread-safe writing; 774 if ext == 'h5':; 775 with h5py.File(filename, 'w') as f:. /ifs/devel/hashem/sw-v1/conda/lib/python3.6/site-packages/scanpy/readwrite.py in wait_until_file_unused(filename); 935 ; 936 def wait_until_file_unused(filename)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/35:980,cache,cache,980,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/35,2,['cache'],['cache']
Performance,"port scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test8_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Then run on a machine on with 16 CPUs; %env PYTHONHASHSEED=0; import numpy as np; import pandas as pd; import scanpy as sc; adata = sc.read_10x_mtx(; './data/filtered_gene_bc_matrices/hg19/', ; var_names='gene_symbols',; cache=True) . sc.pp.filter_cells(adata, min_genes=200); sc.pp.filter_genes(adata, min_cells=3); sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); adata = adata.copy(); sc.pp.scale(adata, max_value=10); sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5); adata = adata[:, adata.var.highly_variable]; sc.tl.pca(adata, svd_solver='arpack', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16.h5ad', adata); sc.tl.pca(adata, svd_solver='randomized', random_state=14); sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40, random_state=14); sc.write('test16_randomized.h5ad', adata); ! echo $PYTHONHASHSEED. # Running on a machine with 16 CPUs, evaluate the differences between the results first from the arpack solver; adata8 = sc.read('test8.h5ad'); adata16 = sc.read('test16.h5ad'); print((adata8.X != adata16.X).sum()); print((adata8.obsm['X_pca'] != adata16.obsm['X_pca']).sum()); pri",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409:1405,cache,cache,1405,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1187#issuecomment-620841409,1,['cache'],['cache']
Performance,"ps://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); [558](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:558) prefix = """" if prefix is None else prefix; [559](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:559) is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> [560](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:560) adata = _read_10x_mtx(; [561](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:561) path,; [562](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:2925,cache,cache,2925,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,put:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1315,load,loading,1315,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,pydoc`. Sphinx will be pinned until this is solved (which is when this issue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1051,load,loading,1051,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,pyhd3eb1b0_0 ; anaconda custom py38_1 ; anaconda-client 1.7.2 py38_0 ; anaconda-project 0.10.0 pyhd3eb1b0_0 ; anndata 0.7.6 pypi_0 pypi; anyio 2.2.0 py38h06a4308_1 ; appdirs 1.4.4 py_0 ; argh 0.26.2 py38_0 ; argon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5616,bottleneck,bottleneck,5616,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['bottleneck'],['bottleneck']
Performance,"pytb; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3079 try:; -> 3080 return self._engine.get_loc(casted_key); 3081 except KeyError as err:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). KeyError: 2. The above exception was the direct cause of the following exception:. KeyError Traceback (most recent call last); <ipython-input-20-26443e0aed95> in <module>; ----> 1 rnaseq1 = sc.read_10x_mtx(""GSE145328_RAW""). ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 479 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 480 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 481 adata = read(; 482 str(path),; 483 var_names=var_names,. ~/anaconda3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 560 else:; 561 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 562 adata.var['feature_types'] = genes[2].values; 563 adata.obs_names = pd.read_csv(path / f'{prefix}barcodes.tsv.gz', header=None)[; 564 0. ~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py in __getitem__(self, key); 3022 if self.columns.nlevels > 1:; 3023 return self._getitem_multilevel(key); -> 3024 indexer = self.columns.get_loc(key); 3025 if is_integer(indexer):; 3026 indexer = [indexer]. ~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3080 return self._",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1916:1318,cache,cache,1318,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1916,1,['cache'],['cache']
Performance,"qual(adata1.obsm['X_pca'], adata2.obsm['X_pca'])); print(adata1.obsm['X_pca'] - adata2.obsm['X_pca']); ```. ```pytb; env: PYTHONHASHSEED=0; False; [[-4.7683716e-07 -3.5762787e-07 -1.1920929e-07 ... -2.7803779e-03; -5.5277348e-04 8.6665154e-05]; [ 9.5367432e-07 4.7683716e-07 6.5565109e-07 ... 3.1501055e-03; -3.6475658e-03 -1.0871887e-04]; [ 1.6689301e-06 0.0000000e+00 -1.1920929e-07 ... 1.7441511e-03; 6.9665909e-04 4.2915344e-04]; ...; [-5.9604645e-07 4.7683716e-07 -3.5762787e-07 ... 1.5980005e-04; -1.0134950e-03 -1.2260675e-04]; [-2.0861626e-07 1.4305115e-06 -8.3446503e-07 ... -7.1705580e-03; -2.4490356e-03 2.4688244e-04]; [ 5.3644180e-07 -5.9604645e-07 -6.8545341e-07 ... -1.5603602e-03; 7.9727173e-04 8.6218119e-04]]; ```. #### Versions. I am running scanpy in this singularity container: https://singularity-hub.org/collections/5095. <details>. ```. -----; anndata 0.7.5; scanpy 1.7.1; sinfo 0.3.1; -----; PIL 8.1.2; anndata 0.7.5; anyio NA; attr 20.3.0; babel 2.9.0; backcall 0.2.0; bottleneck 1.3.2; brotli NA; cairo 1.20.0; certifi 2020.12.05; cffi 1.14.5; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; constants NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.03.0; dateutil 2.8.1; decorator 4.4.2; future_fstrings NA; get_version 2.1; google NA; h5py 3.1.0; highs_wrapper NA; idna 2.10; igraph 0.8.3; ipykernel 5.5.0; ipython_genutils 0.2.0; jedi 0.18.0; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.3.0; kiwisolver 1.3.1; legacy_api_wrap 0.0.0; leidenalg 0.8.3; llvmlite 0.35.0; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.4; mpl_toolkits NA; natsort 7.1.1; nbclassic NA; nbformat 5.1.2; numba 0.52.0; numexpr 2.7.3; numpy 1.20.1; packaging 20.9; pandas 1.2.3; parso 0.8.1; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.17; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA; pycparser 2.20; pygments 2.8.1; pynndescent 0.5.2; pyparsing 2.4.7; pyrsistent NA; pytz 2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1749:1843,bottleneck,bottleneck,1843,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1749,1,['bottleneck'],['bottleneck']
Performance,"r non-gzipped files when reading v3 10x. Currently, I have files barcodes.tsv features.tsv matrix.mtx, but the function will not read them as they are not gzipped.; ...; ```; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); <ipython-input-8-72e92bd46023> in <module>; ----> 1 adata=sc.read_10x_mtx(path,; 2 var_names='gene_symbols',; 3 make_unique=True,; 4 cache=False,; 5 cache_compression=None,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 468 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 469 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 470 adata = read(; 471 str(path),; 472 var_names=var_names,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 530 """"""; 531 path = Path(path); --> 532 adata = read(; 533 path / f'{prefix}matrix.mtx.gz',; 534 cache=cache,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/miniconda3/envs/rpy2_3/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 713 ; 714 if not is_present:; --> 715 raise FileNotFoundError(f'Did not find file {filename}.'); 716 logg.debug(f'reading {filename}'); 717 if not cache and not suppress_cache_warning:. FileNotFoundError: Did not find file /storage/groups/ml01/projects/2020_pancreas_karin.hrovatin/data/pancreas/scRNA/islets_aged_fltp_iCre/rev6/c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1731:1500,cache,cache,1500,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1731,1,['cache'],['cache']
Performance,"r/miniconda3/envs/Scanpy/lib/python3.7/site-packages/scanpy/readwrite.py"", line 10, in <module>; import tables. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/__init__.py"", line 62, in <module>; from .file import File, open_file, copy_file. File ""/home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/file.py"", line 33, in <module>; from . import hdf5extension. ImportError: /home/augustoer/miniconda3/envs/Scanpy/lib/python3.7/site-packages/tables/hdf5extension.cpython-37m-x86_64-linux-gnu.so: undefined symbol: H5Pset_fapl_direct; ```; Removing anndata2ri with `conda remove anndata2ri` fixes the issue. Installing with `pip install anndata2ri` prevents the issue. #### Versions; After fixing the issue.; <details>; WARNING: If you miss a compact list, please try `print_header`!; -----; anndata 0.7.8; scanpy 1.7.2; sinfo 0.3.1; -----; PIL 9.0.1; PyQt5 NA; anndata 0.7.8; atomicwrites 1.4.0; autoreload NA; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.15.0; chardet 4.0.0; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.5.1; decorator 5.1.1; defusedxml 0.7.1; dunamai 1.10.0; fsspec 2022.01.0; get_version 3.5.4; h5py 2.10.0; igraph 0.9.9; ipykernel 6.4.1; ipython_genutils 0.2.0; jedi 0.18.1; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.2; legacy_api_wrap 0.0.0; leidenalg 0.8.9; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.5.1; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; numba 0.54.1; numexpr 2.8.1; numpy 1.20.3; packaging 21.3; pandas 1.3.4; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pydev_ipython NA; pydevconsole NA; pydevd 2.6.0; pydevd_concurrency_analyser NA; pydevd_file_utils NA; pydevd_plugins NA; pydevd_tracing NA; pygments 2.11.2; pyparsing 3.0.4; pytz 2021.3; scanpy 1.7.2; scipy 1.7.3; setuptools_sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2172:2094,bottleneck,bottleneck,2094,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2172,1,['bottleneck'],['bottleneck']
Performance,"raph_k'] = k. path_to_h5ad_file = '~/test.h5ad'; adata.write_h5ad(path_to_h5ad_file) # works. # but if I run; sc.tl.rank_genes_groups(adata, n_genes=21515,groupby='PhenoGraph_clusters', method='wilcoxon'); rcParams['figure.figsize'] = 4,4; rcParams['axes.grid'] = True; sc.pl.rank_genes_groups(adata); pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(5). path_to_h5ad_file = '~/test.h5ad' # works; adata.write_h5ad(path_to_h5ad_file) # gives ERROR bellow. ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); <ipython-input-23-cb0bc3c267ae> in <module>; ----> 1 adata = sc.read(path_to_h5ad_file). ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 95 filename, backed=backed, sheet=sheet, ext=ext,; 96 delimiter=delimiter, first_column_names=first_column_names,; ---> 97 backup_url=backup_url, cache=cache, **kwargs,; 98 ); 99 # generate filename and read to dict. ~/miniconda3/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 497 if ext in {'h5', 'h5ad'}:; 498 if sheet is None:; --> 499 return read_h5ad(filename, backed=backed); 500 else:; 501 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size); 445 else:; 446 # load everything into memory; --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size); 448 X = constructor_args[0]; 449 dtype = None. ~/miniconda3/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size); 484 d[key] = None; 485 else:; --> 486 _read_key_value_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/937:2187,cache,cache,2187,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/937,3,['cache'],['cache']
Performance,rgon2-cffi 20.1.0 py38h27cfd23_1 ; asn1crypto 1.4.0 py_0 ; astroid 2.5 py38h06a4308_1 ; astropy 4.2.1 py38h27cfd23_1 ; async-timeout 3.0.1 pypi_0 pypi; async_generator 1.10 pyhd3eb1b0_0 ; atomicwrites 1.4.0 py_0 ; attrs 21.2.0 pyhd3eb1b0_0 ; autopep8 1.5.6 pyhd3eb1b0_0 ; babel 2.9.1 pyhd3eb1b0_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; backports 1.0 pyhd3eb1b0_2 ; backports.shutil_get_terminal_size 1.0.0 pyhd3eb1b0_3 ; bbknn 1.4.0 py38h0213d0e_0 bioconda; beautifulsoup4 4.9.3 pyha847dfd_0 ; binutils_impl_linux-64 2.33.1 he6710b0_7 ; binutils_linux-64 2.33.1 h9595d00_15 ; bitarray 2.1.0 py38h27cfd23_1 ; bkcharts 0.2 py38_0 ; black 19.10b0 py_0 ; blas 1.0 mkl ; bleach 3.3.0 pyhd3eb1b0_0 ; blessings 1.7 pypi_0 pypi; blosc 1.21.0 h8c45485_0 ; bokeh 2.3.2 py38h06a4308_0 ; boto 2.49.0 py38_0 ; bottleneck 1.3.2 py38heb32a55_1 ; brotlipy 0.7.0 py38h27cfd23_1003 ; bwidget 1.9.11 1 ; bzip2 1.0.8 h7b6447c_0 ; c-ares 1.17.1 h27cfd23_0 ; ca-certificates 2021.4.13 h06a4308_1 ; cached-property 1.5.2 py_0 ; cachetools 4.2.2 pypi_0 pypi; cairo 1.14.12 h8948797_3 ; capital 1.0.0 pypi_0 pypi; cellrank 1.2.0 pypi_0 pypi; certifi 2020.12.5 py38h06a4308_0 ; cffi 1.14.0 py38h2e261b9_0 ; chardet 4.0.0 py38h06a4308_1003 ; click 8.0.0 pypi_0 pypi; cloudpickle 1.6.0 py_0 ; clyent 1.2.2 py38_1 ; cmake 3.18.4.post1 pypi_0 pypi; colorama 0.4.4 pyhd3eb1b0_0 ; conda-pack 0.6.0 pyhd3eb1b0_0 ; contextlib2 0.6.0.post1 py_0 ; cryptography 3.4.7 py38hd23ed53_0 ; curl 7.69.1 hbc83047_0 ; cycler 0.10.0 py38_0 ; cython 0.29.22 pypi_0 pypi; cytoolz 0.11.0 py38h7b6447c_0 ; dask 2021.4.0 pyhd3eb1b0_0 ; dask-core 2021.4.0 pyhd3eb1b0_0 ; dbus 1.13.18 hb2f20db_0 ; decorator 5.0.9 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; deprecated 1.2.11 pypi_0 pypi; diff-match-patch 20200713 py_0 ; distributed 2021.5.0 py38h06a4308_0 ; docrep 0.3.2 pyh44b312d_0 conda-forge; docutils 0.17.1 py38h06a4308_1 ; dorothea-py 1.0.3 pypi_0 pypi; entrypoints 0.3 py38_0 ; et_xmlfile 1.1.0 py38h06a4308_0 ; expat 2.4.1 h2531618_2 ; fa2 0.3.5 ,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310:5824,cache,cachetools,5824,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1850#issuecomment-847928310,1,['cache'],['cachetools']
Performance,"rgs, **kw: P.kwargs) -> R:; [79](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:79) if len(args_all) <= n_positional:; ---> [80](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:80) return fn(*args_all, **kw); [82](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:82) args_pos: P.args; [83](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/legacy_api_wrap/__init__.py:83) args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); [122](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:122) filename = Path(filename) # allow passing strings; [123](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:123) if is_valid_filename(filename):; --> [124](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:124) return _read(; [125](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:125) filename,; [126](https://vscode-remote+ssh-002dremote-002baws-005fcpu.vscode-resource.vscode-cdn.net/data/LSY/venv/lib/python3.10/site-packages/scanpy/readwrite.py:126) backed=backed,; [127](https://vscode-remote+ssh-002dremote-002b",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845:2311,cache,cache,2311,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/323#issuecomment-2041512845,1,['cache'],['cache']
Performance,rking env </summary>. ```; # packages in environment at /mnt/workspace/mambaforge/envs/scanpy-dev2:; #; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; anndata 0.10.7 pypi_0 pypi; array-api-compat 1.6 pypi_0 pypi; asciitree 0.3.3 pypi_0 pypi; attrs 23.2.0 pypi_0 pypi; bzip2 1.0.8 hd590300_5 conda-forge; ca-certificates 2024.2.2 hbcca054_0 conda-forge; cfgv 3.4.0 pypi_0 pypi; click 8.1.7 pypi_0 pypi; cloudpickle 3.0.0 pypi_0 pypi; contourpy 1.2.1 pypi_0 pypi; coverage 7.4.4 pypi_0 pypi; cycler 0.12.1 pypi_0 pypi; dask 2024.4.1 pypi_0 pypi; dask-expr 1.0.10 pypi_0 pypi; distlib 0.3.8 pypi_0 pypi; execnet 2.1.1 pypi_0 pypi; fasteners 0.19 pypi_0 pypi; filelock 3.13.3 pypi_0 pypi; fonttools 4.51.0 pypi_0 pypi; fsspec 2024.3.1 pypi_0 pypi; h5py 3.10.0 pypi_0 pypi; identify 2.5.35 pypi_0 pypi; igraph 0.11.4 pypi_0 pypi; imageio 2.34.0 pypi_0 pypi; iniconfig 2.0.0 pypi_0 pypi; joblib 1.4.0 pypi_0 pypi; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.4 pypi_0 pypi; ld_impl_linux-64 2.40 h41732ed_0 conda-forge; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libexpat 2.6.2 h59595ed_0 conda-forge; libffi 3.4.2 h7f98852_5 conda-forge; libgcc-ng 13.2.0 h807b86a_5 conda-forge; libgomp 13.2.0 h807b86a_5 conda-forge; libnsl 2.0.1 hd590300_0 conda-forge; libsqlite 3.45.2 h2797004_0 conda-forge; libuuid 2.38.1 h0b41bf4_0 conda-forge; libxcrypt 4.4.36 hd590300_1 conda-forge; libzlib 1.2.13 hd590300_5 conda-forge; llvmlite 0.42.0 pypi_0 pypi; locket 1.0.0 pypi_0 pypi; matplotlib 3.8.4 pypi_0 pypi; natsort 8.4.0 pypi_0 pypi; ncurses 6.4.20240210 h59595ed_0 conda-forge; networkx 3.3 pypi_0 pypi; nodeenv 1.8.0 pypi_0 pypi; numba 0.59.1 pypi_0 pypi; numcodecs 0.12.1 pypi_0 pypi; numpy 1.26.4 pypi_0 pypi; openssl 3.2.1 hd590300_1 conda-forge; packaging 24.0 pypi_0 pypi; pandas 2.2.1 pypi_0 pypi; partd 1.4.1 pypi_0 pypi; patsy 0.5.6 pypi_0 pypi; pbr 6.0.0 pypi_0 pypi; pillow 10.3.0 pypi_0 pypi; pip 24.0 pyhd8ed1ab_0 con,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2993:26422,load,loader,26422,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2993,1,['load'],['loader']
Performance,rmdirs 3.10.0; plotly 5.9.0; pluggy 1.0.0; ply 3.11; poyo 0.5.0; prometheus-client 0.14.1; prompt-toolkit 3.0.36; Protego 0.1.16; psutil 5.9.0; ptyprocess 0.7.0; pure-eval 0.2.2; py-cpuinfo 8.0.0; pyarrow 11.0.0; pyasn1 0.4.8; pyasn1-modules 0.2.8; pycodestyle 2.10.0; pycosat 0.6.4; pycparser 2.21; pyct 0.5.0; pycurl 7.45.2; pydantic 1.10.8; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0; pyflakes 3.0.1; Pygments 2.15.1; PyJWT 2.4.0; pylint 2.16.2; pylint-venv 2.3.0; pyls-spyder 0.4.0; pyodbc 4.0.34; pyOpenSSL 23.2.0; pyparsing 3.0.9; PyQt5-sip 12.11.0; pyrsistent 0.18.0; PySocks 1.7.1; pytest 7.4.0; python-dateutil 2.8.2; python-dotenv 0.21.0; python-json-logger 2.0.7; python-lsp-black 1.2.1; python-lsp-jsonrpc 1.0.0; python-lsp-server 1.7.2; python-slugify 5.0.2; python-snappy 0.6.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz-comms 2.3.0; PyWavelets 1.4.1; pyxdg 0.27; PyYAML 6.0; pyzmq 23.2.0; QDarkStyle 3.0.2; qstylizer 0.2.2; QtAwesome 1.2.2; qtconsole 5.4.2; QtPy 2.2.0; queuelib 1.5.0; regex 2022.7.9; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; responses 0.13.3; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rope 1.7.0; Rtree 1.0.1; ruamel.yaml 0.17.21; ruamel-yaml-conda 0.17.21; s3fs 2023.4.0; safetensors 0.3.2; scikit-image 0.20.0; scikit-learn 1.3.0; scikit-learn-intelex 20230426.111612; scipy 1.11.1; Scrapy 2.8.0; seaborn 0.12.2; SecretStorage 3.3.1; Send2Trash 1.8.0; service-identity 18.1.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 5.2.1; sniffio 1.2.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.4; Sphinx 5.0.2; sphinxcontrib-applehelp 1.0.2; sphinxcontrib-devhelp 1.0.2; sphinxcontrib-htmlhelp 2.0.0; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.3; sphinxcontrib-serializinghtml 1.1.5; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 1.4.39; stack-data 0.2.0; statsmodels 0.14.0; sympy 1.11.1; tables 3.8.0; tabulate 0.8.10; TBB 0.2; tblib 1.7.0; TELR 1.0; tenacity 8.2.2; terminado 0.17.1; text-unid,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2706:6200,queue,queuelib,6200,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2706,1,['queue'],['queuelib']
Performance,"rogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_ti1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047631_P8_T1_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm1_P3 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047634_P8_T1_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1489,cache,cache,1489,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"s pl. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\__init__.py in <module>; 15 from ._leiden import leiden; 16 from ._louvain import louvain; ---> 17 from ._sim import sim; 18 from ._score_genes import score_genes, score_genes_cell_cycle; 19 from ._dendrogram import dendrogram. ~\.conda\envs\Python38\lib\site-packages\scanpy\tools\_sim.py in <module>; 21 from anndata import AnnData; 22 ; ---> 23 from .. import _utils, readwrite, logging as logg; 24 from .._settings import settings; 25 from .._compat import Literal. ~\.conda\envs\Python38\lib\site-packages\scanpy\readwrite.py in <module>; 8 import pandas as pd; 9 from matplotlib.image import imread; ---> 10 import tables; 11 import anndata; 12 from anndata import (. ~\.conda\envs\Python38\lib\site-packages\tables\__init__.py in <module>; 43 ; 44 # Necessary imports to get versions stored on the cython extension; ---> 45 from .utilsextension import get_hdf5_version as _get_hdf5_version; 46 ; 47 . ImportError: DLL load failed while importing utilsextension; ```. #### Versions. <details>. Package Version; ------------------- ---------; anndata 0.7.8; anyio 2.2.0; argon2-cffi 20.1.0; async-generator 1.10; attrs 21.2.0; Babel 2.9.1; backcall 0.2.0; bleach 4.1.0; Bottleneck 1.3.2; brotlipy 0.7.0; certifi 2021.10.8; cffi 1.15.0; charset-normalizer 2.0.4; colorama 0.4.4; cryptography 36.0.0; cycler 0.11.0; debugpy 1.5.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fonttools 4.25.0; h5py 3.6.0; idna 3.3; igraph 0.9.9; importlib-metadata 4.8.2; ipykernel 6.4.1; ipython 7.29.0; ipython-genutils 0.2.0; jedi 0.18.0; Jinja2 3.0.2; joblib 1.1.0; json5 0.9.6; jsonschema 3.2.0; jupyter-client 7.1.0; jupyter-core 4.9.1; jupyter-server 1.4.1; jupyterlab 3.2.1; jupyterlab-pygments 0.1.2; jupyterlab-server 2.10.2; kiwisolver 1.3.1; leidenalg 0.8.8; llvmlite 0.37.0; MarkupSafe 2.0.1; matplotlib 3.5.0; matplotlib-inline 0.1.2; mistune 0.8.4; mkl-fft 1.3.1; mkl-random 1.2.2; mkl-service 2.4.0; mock 4.0.3; munkres 1.1.4; nat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2108:3125,load,load,3125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2108,1,['load'],['load']
Performance,s plt; 13 from anndata import AnnData; ---> 14 from pandas.api.types import is_categorical; 16 from ..preprocessing._utils import _get_mean_var; 17 from .._utils import NeighborsView. ImportError: cannot import name 'is_categorical' from 'pandas.api.types' (/Users/michael/mambaforge/envs/basic_comp_bio/lib/python3.9/site-packages/pandas/api/types/__init__.py); ```. ### Versions. <details>. ```; conda list; # packages in environment at /Users/michael/mambaforge/envs/basic_comp_bio:; #; # Name Version Build Channel; anndata 0.9.1 pyhd8ed1ab_0 conda-forge; appnope 0.1.2 py39hca03da5_1001 anaconda; asttokens 2.0.5 pyhd3eb1b0_0 anaconda; backcall 0.2.0 pyhd3eb1b0_0 anaconda; blosc 1.21.4 hc338f07_0 conda-forge; brotli 1.0.9 h1a8c8d9_9 conda-forge; brotli-bin 1.0.9 h1a8c8d9_9 conda-forge; brotli-python 1.0.9 py39h23fbdae_9 conda-forge; bzip2 1.0.8 h3422bc3_4 conda-forge; c-ares 1.19.1 hb547adb_0 conda-forge; c-blosc2 2.10.0 h068da5f_0 conda-forge; ca-certificates 2022.4.26 hca03da5_0 anaconda; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2022.6.15 py39hca03da5_0 anaconda; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; curl 8.1.2 hc52a3a8_1 conda-forge; cycler 0.11.0 pyhd8ed1ab_0 conda-forge; debugpy 1.5.1 py39hc377ac9_0 anaconda; decorator 5.1.1 pyhd3eb1b0_0 anaconda; dunamai 1.18.0 pyhd8ed1ab_0 conda-forge; entrypoints 0.4 py39hca03da5_0 anaconda; executing 0.8.3 pyhd3eb1b0_0 anaconda; fonttools 4.41.0 py39h0f82c59_0 conda-forge; freetype 2.12.1 hd633e50_1 conda-forge; get_version 3.5.4 pyhd8ed1ab_0 conda-forge; gettext 0.21.1 h0186832_0 conda-forge; git 2.41.0 pl5321h46e2b6d_0 conda-forge; h5py 3.9.0 nompi_py39he9c2634_101 conda-forge; hdf5 1.14.1 nompi_h3aba7b3_100 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; ipykernel 6.9.1 py39hca03da5_0 anaconda; ipython 8.3.0 py39,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2564:2704,cache,cached-property,2704,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2564,1,['cache'],['cached-property']
Performance,"s/Python312/Lib/gzip.py:191) if fileobj is None:; --> [192](file:///C:/Program%20Files/Python312/Lib/gzip.py:192) fileobj = self.myfileobj = builtins.open(filename, mode or 'rb'); [193](file:///C:/Program%20Files/Python312/Lib/gzip.py:193) if filename is None:; [194](file:///C:/Program%20Files/Python312/Lib/gzip.py:194) filename = getattr(fileobj, 'name', ''). FileNotFoundError: [Errno 2] No such file or directory: 'GSE212966\\GSM6567159_PDAC2_features.tsv.gz'; ```. I have tried with other datasets which are originally named ad matrix, features and barcodes, and those are working properly. Any idea?. ### Minimal code sample. ```python; data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20245,cache,cache,20245,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"s=keys, layer=layer, use_raw=use_raw); 782 if groupby is None:; 783 obs_tidy = pd.melt(obs_df, value_vars=keys). ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/get/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 270 alias_index = None; 271 ; --> 272 obs_cols, var_idx_keys, var_symbols = _check_indices(; 273 adata.obs,; 274 var.index,. ~/opt/anaconda3/lib/python3.9/site-packages/scanpy/get/get.py in _check_indices(dim_df, alt_index, dim, keys, alias_index, use_raw); 165 not_found.append(key); 166 if len(not_found) > 0:; --> 167 raise KeyError(; 168 f""Could not find keys '{not_found}' in columns of `adata.{dim}` or in""; 169 f"" {alt_repr}.{alt_search_repr}."". KeyError: ""Could not find keys '['n_genes_by_counts', 'pct_counts_mt', 'total_counts']' in columns of `adata.obs` or in adata.var_names.""; ]; ```. #### Versions. <details>. anndata 0.8.0; scanpy 1.9.1; -----; PIL 8.4.0; anyio NA; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; brotli NA; certifi 2022.06.15; cffi 1.14.6; chardet 4.0.0; charset_normalizer 2.0.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; debugpy 1.4.1; decorator 5.1.0; defusedxml 0.7.1; entrypoints 0.3; fastjsonschema NA; fsspec 2021.08.1; google NA; h5py 3.2.1; idna 3.2; igraph 0.9.11; ipykernel 6.4.1; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.18.0; jinja2 2.11.3; joblib 1.1.0; json5 NA; jsonschema 3.2.0; jupyter_server 1.4.1; jupyterlab_server 2.8.2; kiwisolver 1.3.1; leidenalg 0.8.10; llvmlite 0.37.0; louvain 0.7.1; markupsafe 1.1.1; matplotlib 3.4.3; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 8.1.0; nbclassic NA; nbformat 5.1.3; nbinom_ufunc NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; packaging 21.0; pandas 1.4.2; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prometheus_client NA; prompt_toolkit 3.0.20; psutil 5.8.0; ptyprocess 0.7.0; pvectorc NA",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2305:2935,bottleneck,bottleneck,2935,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2305,1,['bottleneck'],['bottleneck']
Performance,"s__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg 0.8.4; llvmlite 0.3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2458,cache,cachetools,2458,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cachetools']
Performance,"s_with_different_data_types():; tree1 = MockTree(hyperplanes=[[1.0, 2.0]], offsets=[3], children=[[4, 5]], indices=[[6, 7]]); tree2 = MockTree(hyperplanes=[[8, 9]], offsets=[10], children=[[11, 12]], indices=[[13, 14]]); forest = [tree1, tree2]; result = _make_forest_dict(forest); assert result[""hyperplanes""][""data""].dtype == np.float64. # Test with trees that have properties with NaN or inf values; @pytest.mark.parametrize(""value"", [np.nan, np.inf, -np.inf]); def test_trees_with_special_values(value):; tree = MockTree(hyperplanes=[[value, value]], offsets=[value], children=[[value, value]], indices=[[value, value]]); forest = [tree]; result = _make_forest_dict(forest); assert np.isnan(result[""hyperplanes""][""data""]).all() or np.isinf(result[""hyperplanes""][""data""]).all(). # Test with a large number of trees; def test_large_number_of_trees():; trees = [MockTree(hyperplanes=[[i, i+1]], offsets=[i+2], children=[[i+3, i+4]], indices=[[i+5, i+6]]) for i in range(1000)]; forest = trees; result = _make_forest_dict(forest); assert len(result[""hyperplanes""][""start""]) == 1000; assert result[""hyperplanes""][""data""].shape == (2000, 2). # Test with trees missing one of the expected properties; def test_trees_missing_properties():; class IncompleteTree:; def __init__(self, hyperplanes, children, indices):; self.hyperplanes = np.array(hyperplanes); self.children = np.array(children); self.indices = np.array(indices); tree = IncompleteTree(hyperplanes=[[1, 2]], children=[[3, 4]], indices=[[5, 6]]); forest = [tree]; with pytest.raises(AttributeError):; _make_forest_dict(forest); ```; </details>. This optimization was discovered by [Codeflash AI](https://codeflash.ai) ⚡️. <!--; Thanks for opening a PR to scanpy!; Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.; -->. <!-- Please check (“- [x]”) and fill in the following boxes -->; - [X] Tests included",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/2971:4459,optimiz,optimization,4459,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/2971,1,['optimiz'],['optimization']
Performance,"self, threadsafe. Docs: https://numba.readthedocs.io/en/stable/user/threading-layer.html. Fatal Python error: Aborted. Thread 0x000000016fd2f000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 157 in get; File ""<venv>/lib/python3.12/site-packages/dask/optimization.py"", line 1001 in __call__; File ""<venv>/lib/python3.12/site-packages/dask/core.py"", line 127 in _execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 225 in execute_task; File ""<venv>/lib/python3.12/site-packages/dask/local.py"", line 239 in batch_execute_tasks; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 64 in run; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 92 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Thread 0x000000016ed23000 (most recent call first):; File ""<venv>/lib/python3.12/concurrent/futures/thread.py"", line 89 in _worker; File ""<venv>/lib/python3.12/threading.py"", line 1010 in run; File ""<venv>/lib/python3.12/threading.py"", line 1073 in _bootstrap_inner; File ""<venv>/lib/python3.12/threading.py"", line 1030 in _bootstrap. Current thread 0x000000016dd17000 (most recent call first):; File ""~/Dev/scanpy/src/scanpy/_compat.py"", line 133 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 109 in _; File ""<venv>/lib/python3.12/functools.py"", line 909 in wrapper; File ""~/Dev/scanpy/src/scanpy/_utils/compute/is_constant.py"", line 30 in func; Fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478:1968,concurren,concurrent,1968,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335#issuecomment-2457625478,1,['concurren'],['concurrent']
Performance,some of the datasets like pbmc68k-reduced also seem to have an issue loading in conda.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158:69,load,loading,69,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851#issuecomment-533797158,1,['load'],['loading']
Performance,sonpy-ykxq6c1e/meson-python-native-file.ini; Preparing metadata (pyproject.toml) did not run successfully.; ```. ### Error output. _No response_. ### Versions. <details>. ```; # Name Version Build Channel; absl-py 2.1.0 pyhd8ed1ab_0 conda-forge; anndata 0.10.8 pypi_0 pypi; anyio 4.2.0 py39hca03da5_0 ; appnope 0.1.2 py39hca03da5_1001 ; argon2-cffi 21.3.0 pyhd3eb1b0_0 ; argon2-cffi-bindings 21.2.0 py39h1a28f6b_0 ; arpack 3.9.1 nompi_h593882a_101 conda-forge; array-api-compat 1.7.1 pyhd8ed1ab_0 conda-forge; asttokens 2.0.5 pyhd3eb1b0_0 ; async-lru 2.0.4 py39hca03da5_0 ; attrs 23.1.0 py39hca03da5_0 ; babel 2.11.0 py39hca03da5_0 ; backcall 0.2.0 pyhd3eb1b0_0 ; beautifulsoup4 4.12.3 py39hca03da5_0 ; blas 1.0 openblas ; bleach 4.1.0 pyhd3eb1b0_0 ; blosc2 2.0.0 pypi_0 pypi; brotli 1.1.0 hb547adb_1 conda-forge; brotli-bin 1.1.0 hb547adb_1 conda-forge; brotli-python 1.0.9 py39h313beb8_8 ; bzip2 1.0.8 h80987f9_6 ; c-ares 1.28.1 h93a5062_0 conda-forge; ca-certificates 2024.7.4 hf0a4a13_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; certifi 2024.6.2 py39hca03da5_0 ; cffi 1.16.0 py39he153c15_0 conda-forge; charset-normalizer 2.0.4 pyhd3eb1b0_0 ; chex 0.1.81 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; comm 0.2.1 py39hca03da5_0 ; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; cycler 0.12.1 pyhd8ed1ab_0 conda-forge; cython 3.0.10 pypi_0 pypi; debugpy 1.6.7 py39h313beb8_0 ; decorator 5.1.1 pyhd3eb1b0_0 ; defusedxml 0.7.1 pyhd3eb1b0_0 ; dm-tree 0.1.7 py39h2666b31_0 conda-forge; docrep 0.3.2 pyh44b312d_0 conda-forge; et_xmlfile 1.1.0 pyhd8ed1ab_0 conda-forge; etils 1.6.0 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.2.1 pypi_0 pypi; executing 0.8.3 pyhd3eb1b0_0 ; flax 0.6.1 pyhd8ed1ab_1 conda-forge; fonttools 4.53.0 py39hfea33bf_0 conda-forge; freetype 2.12.1 hadb7bae_2 conda-forge; fsspec 2024.6.1 pyhff2d567_0 conda-forge; future 1.0.0 pyhd8ed1ab_0 conda-forge; get-annotations 0.1.2 pyhd8ed1ab_0 conda-forg,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3144:6788,cache,cached-property,6788,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3144,1,['cache'],['cached-property']
Performance,"spatch(args[0].__class__)(*args, **kw); 841 ; 842 funcname = getattr(func, '__name__', 'singledispatch function'). /opt/conda/lib/python3.7/site-packages/anndata/_io/utils.py in func_wrapper(elem, *args, **kwargs); 161 parent = _get_parent(elem); 162 raise AnnDataReadError(; --> 163 f""Above error raised while reading key {elem.name!r} of ""; 164 f""type {type(elem)} from {parent}.""; 165 ). AnnDataReadError: Above error raised while reading key '/layers' of type <class 'h5py._hl.group.Group'> from /.; adata_ast=sc.read_h5ad('../../data_processed/Leng_2020/adata_ast.h5ad'); ```. <details>; <summary>Versions</summary>. Package Version; ----------------------- ------------; absl-py 1.1.0; aiohttp 3.8.1; aiosignal 1.2.0; anndata 0.7.5; anndata2ri 1.0.6; annoy 1.17.0; argon2-cffi 21.3.0; argon2-cffi-bindings 21.2.0; asn1crypto 1.4.0; async-timeout 4.0.2; asynctest 0.13.0; attrs 20.3.0; backcall 0.2.0; beautifulsoup4 4.11.1; bleach 5.0.0; boto3 1.17.66; botocore 1.20.66; brotlipy 0.7.0; cached-property 1.5.2; cachetools 5.2.0; certifi 2020.12.5; cffi 1.14.5; chardet 4.0.0; charset-normalizer 2.0.12; chex 0.1.3; click 8.1.3; colormath 3.0.0; commonmark 0.9.1; conda 4.6.14; conda-package-handling 1.7.3; cryptography 3.4.7; cycler 0.10.0; Cython 0.29.30; decorator 5.0.7; defusedxml 0.7.1; dill 0.3.3; dm-tree 0.1.7; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; fa2 0.3.5; fastjsonschema 2.15.3; flatbuffers 2.0; flax 0.5.0; frozenlist 1.3.0; fsspec 2022.5.0; future 0.18.2; get-version 2.2; google-auth 2.6.6; google-auth-oauthlib 0.4.6; google-pasta 0.2.0; grpcio 1.46.3; h5py 3.2.1; idna 2.10; imageio 2.19.3; importlib-metadata 4.11.4; importlib-resources 5.7.1; ipykernel 5.5.4; ipython 7.23.1; ipython-genutils 0.2.0; ipywidgets 7.7.0; jax 0.3.13; jaxlib 0.3.10; jedi 0.18.0; Jinja2 3.1.2; jmespath 0.10.0; joblib 1.0.1; jsonschema 4.6.0; jupyter-client 6.1.12; jupyter-core 4.7.1; jupyterlab-pygments 0.2.2; jupyterlab-widgets 1.1.0; kiwisolver 1.3.1; legacy-api-wrap 1.2; leidenalg",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336:2435,cache,cached-property,2435,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1351#issuecomment-1146346336,1,['cache'],['cached-property']
Performance,"sphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/doc/numpy/objects.inv -> https://numpy.org/doc/stable/objects.inv; intersphinx inventory has moved: http://docs.h5py.org/en/stable/objects.inv -> https://docs.h5py.org/en/stable/objects.inv; [autosummary] generating autosummary for: _key_contributors.rst, api.rst, basic_usage.rst, co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1411,load,loading,1411,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"ss CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/osx-64::__osx==10.16=0; - feature:|@/osx-64::__osx==10.16=0; - scanpy -> matplotlib-base[version='>=3.4'] -> __osx[version='>=10.12']. Your installed version is: 10.16; ```. Repeating this with python=3.10 does not give an error.; Running this with ```pip -vv install scanpy``` as you suggested indeed gives an error with numba, . ```; Collecting numba>=0.41.0; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-unpack-9g89heod; Looking up ""https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz"" in the cache; Current age based on date: 1302943; Ignoring unknown cache-control directive: immutable; Freshness lifetime from max-age: 365000000; The response is ""fresh"", returning cached response; 365000000 > 1302943; Using cached numba-0.56.4.tar.gz (2.4 MB); Added numba>=0.41.0 from https://files.pythonhosted.org/packages/e2/1e/de917b683bb5f0b6078fb1397293eab84c4eaa825fbf94d73d6488eb354f/numba-0.56.4.tar.gz (from scanpy) to build tracker '/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-build-tracker-740xp5sy'; Running setup.py (path:/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py) egg_info for package numba; Created temporary directory: /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; Running command python setup.py egg_info; Traceback (most recent call last):; File ""<string>"", line 2, in <module>; File ""<pip-setuptools-caller>"", line 34, in <module>; File ""/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:1212,cache,cache,1212,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,4,['cache'],"['cache', 'cache-control', 'cached']"
Performance,"ssion"". Completely agreed. > I'm interested in the `sc.ex` module you're suggesting. Would you mind elaborating a bit more on that, particularly on some functions that would be there?. **Re sc.extract**. One of the core ideas of Scanpy (as opposed to, say, scikit learn) was to have this model of taking the burden of bookkeeping from the user as much as possible. This design messed up, in particular, the return values of `rank_genes_groups`. I would have loved to return a collection of dataframes, but I didn't want to mess this up. Also, the return values of `pp.neighbors` or `pl.paga` aren't great. There is a trade-off between having nice APIs and return values (such as dataframes) and a transparent and efficient on-disk representation in terms of HDF5, zarr or another format. These days, I'd even consider simply pickling things, which would have saved us a lot of work; but I thought that we'd need established compression facilities, concatenation possibilities, some way to manually ""look into"" an on-disk object (both from R and from the command line) so that it's maximally transparent and then the widely established, cross-language, but old-school and not entirely scalable HDF5 seemed the best. The Human Cell Atlas decided in favor of zarr meanwhile. But that's not a drama, because Scanpy only writes ""storage-friendly"" values to AnnData, that is, arrays and dicts. HDF5 knows how to handle them and zarr also. If one uses xarray or dataframes, one has to think about how this gets written to disk. That being said: it's likely that we'll continue to choose representations for on-disk (and in-memory) storage that aren't convenient (rec arrays, for instance), a three-dimensional xarray and dicts. A general solution for this problem would be the mentioned `sc.extract` API, similar to `sc.plotting` (which also completely hides the complexity of the object from the user), but not for returning visualizations, but nice objects. The first function in that namespace should be `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358:1651,scalab,scalable,1651,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/562#issuecomment-487409358,1,['scalab'],['scalable']
Performance,"st version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi, I am running the Scrublet function to remove doublets.; the annadata was generated by concat several samples from two articles:. [1] Wu F, Fan J, He Y, et al. Single-cell profiling of tumor heterogeneity and the microenvironment in advanced non-small cell lung cancer[J]. Nature Communications, 2021, 12(1): 2540.; [2] Wang Y, Chen D, Liu Y, et al. Multidirectional characterization of cellular composition and spatial architecture in human multiple primary lung cancers[J]. Cell Death & Disease, 2023, 14(7): 462. However, there was an error I cann't handle. ### Minimal code sample. ```python; # 240520鳞癌，不用; # lung_ti_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047623_P1_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_tm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047624_P1_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p1 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047626_P1_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # 240520 去掉癌旁，只用癌; lung_ti_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047627_P2_T_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); lung_tm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047629_P2_T_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_ni_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047628_P2_N_R_I'), var_names='gene_symbols', cache=True, cache_compression='gzip'); # lung_nm_p2 = sc.read_10x_mtx(os.path.join(root, 'GSE200972_RAW', 'GSM6047630_P2_N_R_M'), var_names='gene_symbols', cache=True, cache_compression='gzip'); ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3070:1177,cache,cache,1177,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3070,1,['cache'],['cache']
Performance,"stall \; --install-scripts=$TOPDIR/bin --prefix /usr/common \; 2>&1 | tee ../install_2020_06_10.log; #setup a module ""scanpy"" which puts $TOPDIR/bin on path and; #defines PYTHONPATH, then do; module load scanpy; scanpy; /home/common/lib/python3.6/site-packages/anndata/base.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version. The public classes are available in the top-level namespace.; from pandas.core.index import RangeIndex; Traceback (most recent call last):; File ""/usr/common/modules/el8/x86_64/software/scanpy/1.5.1-CentOS-vanilla/bin/scanpy"", line 11, in <module>; load_entry_point('scanpy==1.5.2.dev7+ge33a2f33', 'console_scripts', 'scanpy')(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 490, in load_entry_point; return get_distribution(dist).load_entry_point(group, name); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2862, in load_entry_point; return ep.load(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2462, in load; return self.resolve(); File ""/usr/common/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2468, in resolve; module = __import__(self.module_name, fromlist=['__name__'], level=0); File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/__init__.py"", line 3, in <module>; from ._utils import pkg_version, check_versions, annotate_doc_types; File ""/home/common/lib/python3.6/site-packages/scanpy-1.5.2.dev7+ge33a2f33-py3.6.egg/scanpy/_utils.py"", line 17, in <module>; from anndata import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/__init__.py"", line 1, in <module>; from .base import AnnData; File ""/home/common/lib/python3.6/site-packages/anndata/base.py"", line 21, in <module>; from scipy.sparse.sputils import IndexMixin; ImportError: cannot import name 'IndexMixin'; ```. ```bash; #pip3 install works any better?; pip3 install scanpy --target $PYT",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1273:1310,load,load,1310,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1273,1,['load'],['load']
Performance,"sts on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. I've been having an issue trying to load published single cell data (GEO GSE123366) onto my jupyter notebook. Despite providing the directory where all 3 files are located, I get the error that it cannot find my matrix.mtx.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1173,cache,cache,1173,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,2,['cache'],['cache']
Performance,"sudo rm /media/ubuntu/d0b69706-4d42-40a3-b531-382041477d35/home/cns/biosoft/cellranger/cellranger-3.0.2/deng2_count_myself -fr. At 2019-07-27 18:48:50, ""Cristian"" <notifications@github.com> wrote:. Good day!. I have been trying to run the single cell tutorial but have had some issues concatenating several datasets. I am able to read successfully the first data set. However, once I want to load the other datasets, there is a problem concatenating the files. This happens in the first loop to load all the datasets. If I run only one dataset the same error (unsupported operand type(s) for +: 'int' and 'str') showed up when I plot some data quality summary plots:. For instance:; p1 = sc.pl.scatter(adata, 'n_counts', 'n_genes', color='mt_frac') p2 = sc.pl.scatter(adata[adata.obs['n_counts']<10000], 'n_counts', 'n_genes', color='mt_frac'); adata = adata[adata.obs['mt_frac'] < 0.2] print('Number of cells after MT filter: {:d}'.format(adata.n_obs)); sc.pp.filter_cells(adata, min_genes = 700) print('Number of cells after gene filter: {:d}'.format(adata.n_obs)). I am using data generated by 10x V3 and CellRanger v3.0.1. I really do not know where the problem is. I really appreciate any advice/help to solve this issue. Thanks in advance. —; You are receiving this because you are subscribed to this thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613:392,load,load,392,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/751#issuecomment-515740613,2,['load'],['load']
Performance,sue should be closed). It's not obvious to me at the moment whether sphinx or scanpydoc is at fault. ---------------. Trying to build the docs with Sphinx 4.1.0 fails with the following output:. <details>; <summary> </summary>. ```sh; $ make html; Running Sphinx v4.1.0; loading intersphinx inventory from https://anndata.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://bbknn.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...; loading intersphinx inventory from http://docs.h5py.org/en/stable/objects.inv...; loading intersphinx inventory from https://ipython.readthedocs.io/en/stable/objects.inv...; loading intersphinx inventory from https://leidenalg.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://louvain-igraph.readthedocs.io/en/latest/objects.inv...; loading intersphinx inventory from https://matplotlib.org/objects.inv...; loading intersphinx inventory from https://networkx.github.io/documentation/networkx-1.10/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/numpy/objects.inv...; loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...; loading intersphinx inventory from https://docs.pytest.org/en/latest/objects.inv...; loading intersphinx inventory from https://docs.python.org/3/objects.inv...; loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...; loading intersphinx inventory from https://seaborn.pydata.org/objects.inv...; loading intersphinx inventory from https://scikit-learn.org/stable/objects.inv...; loading intersphinx inventory from https://scanpy-tutorials.readthedocs.io/en/latest/objects.inv...; intersphinx inventory has moved: https://networkx.github.io/documentation/networkx-1.10/objects.inv -> https://networkx.org/documentation/networkx-1.10/objects.inv; intersphinx inventory has moved: https://docs.scipy.org/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1946:1125,load,loading,1125,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1946,1,['load'],['loading']
Performance,"sure, we’ll talk in 10 days or so, after my holidays 😄. except if you want to earlier, then we can skype or so. > One could think about renaming the ""data"" subdirectory to something like ""data_cache"" or so to make evident that this only stores cache files, which can simply be deleted, and everything else stores ""AnnData backing files"" = ""result files"" or exported files... definitely. > But I agree true cache files might be better placed in a tmp directory. no, as i said: cache directory, not temp directory. both have (overridable) standard locations on all OSs. e.g. on linux:. - `$TMPDIR` or `/tmp/`: temporary means that the files are only to be read during the same function/script execution, and deleted after. temp files forgotten by the application that created them are deleted after `$TMPTIME` and on reboot. (on linux now usually because `/tmp/` is a ramdisk and RAM contents don’t survive a reboot). ```py; # python gives you a context manager that deletes the file after its block; with tempfile.TemporaryFile() as fp:; use(fp); # fp and the file are gone now; ```. - `$XDG_CACHE_HOME` or `~/.cache/`: cache files are permanent until the user or OS cleans up or the application decides it no longer needs them (i think e.g. browsers clear out the parts of their cache periodically). since scanpy has a notion of a project directory, putting the cache there is OK as well. the advantage is visibility, but that only works if the user knows what the directory/ies are for. using `cache` in the name of the cache directory would certainly help to signify that the stuff can be safely deleted.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457:244,cache,cache,244,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/50#issuecomment-346781457,9,['cache'],['cache']
Performance,"t I meant is that a wheel cached by pip (such as scipy) ends up in ~/.cache. And since some of those wheels are big, you need to clean that directory from time to time anyway if you have little space. > I think I'd prefer printing on write, info logging on read. I'd put a higher precedence on changing stuff on disk rather than reading. My idea was that showing it every time would help people discover this. But the default scanpy log level is INFO anyway, right? So it would get shown by default if we info-log it?. > I like this model of having all the data in one place, makes it much easier to have multiple environments and uninstall things. Me too: All cache data in ~/.cache, all configs in ~/.config, …. If you need to uninstall a thing that behaves correctly, you can just do `package-manager uninstall thing && rm -rf ~/.{cache,config,local/share}/thing/`. > If those datasets were being implicitly cleared from disk, I'd find that confusing. I guess I don't think of downloaded datasets being cached in the way you've defined before. If I've downloaded a dataset though sklearn or tensorflow , I expect it to stay on disk. As said: it doesn’t happen automatically on desktops, they show you a popup asking you to do it. I think the HPC servers don’t help you with your tiny $HOME, so everything you download manually or cache just stays there. On a well-configured system with little space in $HOME, you’d have $XDG_CACHE_DIR point to a separate disk that has more space and isn’t backupped. In an ideal world everyone would respect that and your $HOME would never be filled up with ephemeral files. > I'm not sure I'd want to support a command line interface just for configs, if there was more it could do, maybe. Also, there's gotta be a generic tool for this, right?. The idea we agreed on was to allow something similar as `jupyter`: Just delegate `scanpy foocmd` to `scanpy-foocmd` except for 1-3 built-in commands. Other people are interested in creating those scripts (and did so",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940:1096,cache,cached,1096,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/558#issuecomment-478230940,1,['cache'],['cached']
Performance,"t aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python; adata = sc.AnnData(; X=np.ones((2, 3)),; obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),; var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),; ); sc.get.obs_df(adata, [""gene-1""]); ``````. ### This PR (errors). ```pytb; ---------------------------------------------------------------------------; InvalidIndexError Traceback (most recent call last); <ipython-input-62-405d671e2970> in <module>; ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 213 var_idx = adata.raw.var_names.get_indexer(var_names); 214 else:; --> 215 var_idx = adata.var_names.get_indexer(var_names); 216 ; 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance); 3169 ; 3170 if not self.is_unique:; -> 3171 raise InvalidIndexError(; 3172 ""Reindexing only valid with uniquely valued Index objects""; 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects; ```. ### 1.6 (suceeds). ```python; gene-1; cell-0 1.0; cell-1 1.0; ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this?. My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421:6041,perform,performance,6041,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583#issuecomment-770167421,1,['perform'],['performance']
Performance,"t import NNDescent; [48](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/umap_.py?line=47) from pynndescent.distances import named_distances as pynn_named_distances. File D:\Users\xiangrong1\Miniconda3\envs\py48\lib\site-packages\umap\layouts.py:39, in <module>; [25](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=24) else:; [26](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=25) return val; [29](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=28) @numba.njit(; [30](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=29) ""f4(f4[::1],f4[::1])"",; [31](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=30) fastmath=True,; [32](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=31) cache=True,; [33](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=32) locals={; [34](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=33) ""result"": numba.types.float32,; [35](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=34) ""diff"": numba.types.float32,; [36](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=35) ""dim"": numba.types.int32,; [37](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=36) },; [38](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=37) ); ---> [39](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=38) def rdist(x, y):; [40](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap/layouts.py?line=39) """"""Reduced Euclidean distance.; [41](file:///d%3A/Users/xiangrong1/Miniconda3/envs/py48/lib/site-packages/umap",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659:11618,cache,cache,11618,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2160#issuecomment-1107838659,1,['cache'],['cache']
Performance,"t time. ---------------------------------------------------------------------------; OSError Traceback (most recent call last); <ipython-input-10-894335192e05> in <module>; 2 'C:\\Users\\david\\Desktop\\10x_hiv_mcherry\\aggregate\\outs\\filtered_feature_bc_matrix',; 3 var_names='gene_symbols', # use gene symbols for the variable names (variables-axis index); ----> 4 cache=True) # write a cache file for faster subsequent reading. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read_10x_mtx(path, var_names, make_unique, cache, gex_only); 244 else:; 245 adata = _read_v3_10x_mtx(path, var_names=var_names,; --> 246 make_unique=make_unique, cache=cache); 247 if not gex_only:; 248 return adata. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache); 277 Read mex from output from Cell Ranger v3 or later versions; 278 """"""; --> 279 adata = read(os.path.join(path, 'matrix.mtx.gz'), cache=cache).T # transpose the data; 280 genes = pd.read_csv(os.path.join(path, 'features.tsv.gz'), header=None, sep='\t'); 281 if var_names == 'gene_symbols':. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~\AppData\Local\conda\conda\envs\Scanpy\lib\site-packages\scanpy\readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 479 'cache file to speedup reading next time'); 480 if not os.path.exists(os.path.dirname(filename_cache)):; --> 481 os.makedirs(os.path.dirname(filename_cache)); 482 # write for faster reading when calling the next t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/563:1180,cache,cache,1180,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/563,2,['cache'],['cache']
Performance,t-SNE optimization using scikit-learn-intelex,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3061:6,optimiz,optimization,6,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3061,2,['optimiz'],['optimization']
Performance,"ta1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); ```. ### Error output. ```pytb; ---------------------------------------------------------------------------; FileNotFoundError Traceback (most recent call last); Cell In[62], line 1; ----> 1 data1 = sc.read_10x_mtx(""GSE212966"", prefix=""GSM6567159_PDAC2_"", var_names='gene_symbols', cache=True); 2 data1.var_names_make_unique(). File ~\AppData\Roaming\Python\Python312\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw); 77 @wraps(fn); 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:; 79 if len(args_all) <= n_positional:; ---> 80 return fn(*args_all, **kw); 82 args_pos: P.args; 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:560, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 558 prefix = """" if prefix is None else prefix; 559 is_legacy = (path / f""{prefix}genes.tsv"").is_file(); --> 560 adata = _read_10x_mtx(; 561 path,; 562 var_names=var_names,; 563 make_unique=make_unique,; 564 cache=cache,; 565 cache_compression=cache_compression,; 566 prefix=prefix,; 567 is_legacy=is_legacy,; 568 ); 569 if is_legacy or not gex_only:; 570 return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); 588 suffix = """" if is_legacy else "".gz""; 589 adata = read(; 590 path / f""{prefix}matrix.mtx{suffix}"",; 591 cache=cache,; 592 cache_compression=cache_compression,; 593 ).T # transpose the data; --> 594 genes = pd.read_csv(; 595 path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; 596 header=None,; 597 sep=""\t"",; 598 ); 599 if var_names == ""gene_symbols"":; 600 var_names_idx = pd.Index(genes[1].values). File ~\AppData\Roaming\Python\P",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:20822,cache,cache,20822,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,te 'get_legend_handles_labels'; ```. ### Versions. <details>. ```; Package Version; ----------------------------- ---------------; aiobotocore 2.6.0; aiohttp 3.8.6; aioitertools 0.11.0; aiosignal 1.3.1; alabaster 0.7.13; anaconda-anon-usage 0.4.2; anaconda-catalogs 0.2.0; anaconda-client 1.12.0; anaconda-cloud-auth 0.1.4; anaconda-navigator 2.5.0; anaconda-project 0.11.1; anndata 0.10.1; anndata 0.10.0rc1; annoy 1.17.2; anyio 4.0.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array-api-compat 1.4; array-api-compat 1.4; arrow 1.3.0; astroid 2.15.7; astropy 5.3.4; asttokens 2.4.0; async-lru 2.0.4; async-timeout 4.0.3; atomicwrites 1.4.1; attrs 23.1.0; Automat 22.10.0; autopep8 2.0.4; Babel 2.12.1; backcall 0.2.0; backports.functools-lru-cache 1.6.5; backports.tempfile 1.0; backports.weakref 1.0.post1; bcrypt 4.0.1; beautifulsoup4 4.12.2; binaryornot 0.4.4; black 23.9.1; bleach 6.1.0; blinker 1.6.3; bokeh 3.2.2; boltons 23.0.0; botocore 1.31.17; brotlipy 0.7.0; cached-property 1.5.2; celltypist 1.6.1; certifi 2023.7.22; cffi 1.16.0; chardet 5.2.0; charset-normalizer 3.3.0; click 8.1.7; cloudpickle 2.2.1; clyent 1.2.2; colorama 0.4.6; colorcet 3.0.1; comm 0.1.4; conda 23.9.0; conda-build 3.27.0; conda-content-trust 0+unknown; conda_index 0.2.3; conda-libmamba-solver 23.9.1; conda-pack 0.6.0; conda-package-handling 2.2.0; conda_package_streaming 0.9.0; conda-repo-cli 1.0.75; conda-token 0.4.0; conda-verify 3.4.2; ConfigArgParse 1.7; connection-pool 0.0.3; constantly 15.1.0; contourpy 1.1.1; cookiecutter 2.4.0; cryptography 40.0.1; cssselect 1.2.0; cycler 0.12.1; cytoolz 0.12.2; daal4py 2023.2.1; dask 2023.9.3; dataclasses 0.8; datasets 2.14.5; datashader 0.15.2; datashape 0.5.4; datrie 0.8.2; debugpy 1.8.0; decorator 5.1.1; decoupler 1.5.0; defusedxml 0.7.1; diff-match-patch 20230430; dill 0.3.7; distlib 0.3.7; distributed 2023.9.3; docopt 0.6.2; docstring-to-markdown 0.12; docutils 0.20.1; dpath 2.1.6; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:4209,cache,cached-property,4209,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['cache'],['cached-property']
Performance,"test_adata,; by=[; ""patient_id"",; ""external_batch_id"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69. >>> pb = sc.get.aggregate(; test_adata,; by=[; ""patient_id"",; ""timepoint"",; ],; func=""mean"",; ); pb.obs[""patient_id""].nunique(). 69; ```. ### Error output. ```pytb; So only if using all three variables, some patient IDs are lost. I don't see why this would be happening.; ```. ### Versions. <details>. ```; Package Version Editable project location; ------------------------- --------------- -------------------------------------------------------------------------------------------------------------------------; aiohttp 3.9.3; aiosignal 1.3.1; anndata 0.10.5.post1; anyio 4.3.0; appdirs 1.4.4; argon2-cffi 23.1.0; argon2-cffi-bindings 21.2.0; array_api_compat 1.5; arrow 1.3.0; asciitree 0.3.3; asttokens 2.4.1; async-lru 2.0.4; async-timeout 4.0.3; attrs 23.2.0; Babel 2.14.0; beautifulsoup4 4.12.3; bleach 6.1.0; bokeh 3.3.4; branca 0.7.1; Brotli 1.1.0; cached-property 1.5.2; cachetools 5.3.3; certifi 2024.2.2; cffi 1.16.0; charset-normalizer 3.3.2; click 8.1.7; click-plugins 1.1.1; cligj 0.7.2; cloudpickle 3.0.0; colorama 0.4.6; colorcet 3.1.0; comm 0.2.1; confluent-kafka 1.9.2; contourpy 1.2.0; cubinlinker 0.3.0; cucim 24.2.0; cuda-python 11.8.3; cudf 24.2.2; cudf_kafka 24.2.2; cugraph 24.2.0; cuml 24.2.0; cuproj 24.2.0; cupy 12.2.0; cuspatial 24.2.0; custreamz 24.2.2; cuxfilter 24.2.0; cycler 0.12.1; cytoolz 0.12.3; dask 2024.1.1; dask-cuda 24.2.0; dask-cudf 24.2.2; datashader 0.16.0; debugpy 1.8.1; decorator 5.1.1; decoupler 1.6.0; defusedxml 0.7.1; distributed 2024.1.1; docrep 0.3.2; entrypoints 0.4; et-xmlfile 1.1.0; exceptiongroup 1.2.0; executing 2.0.1; fa2 0.3.5; fasteners 0.19; fastjsonschema 2.19.1; fastrlock 0.8.2; fcsparser 0.2.8; filelock 3.13.1; fiona 1.9.5; folium 0.16.0; fonttools 4.49.0; fqdn 1.5.1; frozenlist 1.4.1; fsspec 2024.2.0; GDAL 3.8.1; gdown 5.1.0; geopandas 0.14.3; h11 0.14.0; h2 4.1.0; h5py 3.10.0; harmonypy 0.0.9; holoviews 1.18.3;",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2964:2108,cache,cachetools,2108,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2964,1,['cache'],['cachetools']
Performance,"theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set of information this wouldn't capture?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:1642,cache,cache,1642,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,2,['cache'],['cache']
Performance,"thout proving which one is ground truth, we don't know for sure which one is true. At least initially, I would think that the CLR plots look more plausible. ![image](https://user-images.githubusercontent.com/20694664/83360046-51985080-a34c-11ea-9ec0-2057301ae4fc.png). ![image](https://user-images.githubusercontent.com/20694664/83360065-74c30000-a34c-11ea-9e0b-d28cea53993e.png). ![image](https://user-images.githubusercontent.com/20694664/83360079-84dadf80-a34c-11ea-9026-4256d8a3199b.png). I used a neutral word earlier: that CLR ""injects"" additional changes, but now it seems that may be a positive thing because many of these empirical cases seem believable from a biological standpoint -- a more systematic validation/comparison might conclude that it ""corrects"" some aspect of the signal acquisition (e.g. combats protein differences simply due to cell size). Again, this is because by design, CLR isn't just a rescaling: it performs cell-specific centering relative to all markers in a relative ratio way, so doesn't preserve a 1-to-1 monotonic mapping as a rescaling function like log, asinh, biexponential/logicle/vlog would. But without having tested it in all cases, it's not clear that it will *always* be better with this kind of assumption for other types of markers that may have different fundamental characteristics. I would recommend that people plot both ways and decide on a case-by-case basis for each marker. . EDIT: I looked around a bit more in the literature and do think that the absolute count based transforms (i.e. all the ones not the CLRatio based), do seem to represent physical reality more: cell size (as one explanation). For example, the CD4intermediate/CD3up_skewed include classical monocytes (and might be larger than the CD4negative/CD3negative); while the CD16high/CD3down_skewed include a nonclassical monocyte subset (and might be smaller cellsize, one example in mouse [Fig2](https://www.nature.com/articles/s41467-019-11843-0)). While CLR makes it easier ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215:1941,perform,performs,1941,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1117#issuecomment-636513215,1,['perform'],['performs']
Performance,"tically. However, I'm wary of abandoning a critical discussion of imputation methods in this space because other portions of the typical workflow have issues as well. Further, I think there are important distinctions to be made between different classes of methodology that are (mis)used in this problem space. I. Methods that are fundamentally flawed by their assumptions or algorithm. These should obviously be avoided.; II. Methods that are fundamentally sound but are not sufficiently validated, e.g. the validation doesn't exist in this problem space, isn't sufficiently comprehensive/relevant, performs poorly against other fundamentally sound methodologies, or has such restrictive assumptions it isn't broadly useful/applicable.; III. Methods that are fundamentally sound in assumption/algorithm and can be used by a competent practitioner but still have the potential to be abused through applying it to data that violate those assumptions. I'd consider t-SNE and a great deal of the clustering algorithms to be in class III for the reasons you said; they're valid, functional tools but can be applied in assumption-violating or quasi-valid ways. I'm pretty sure that scImpute, for example, belongs in class I because its description of dropout and simulated test cases are inappropriate. I'd put MAGIC and several other currently available imputation methods in class II as they've got strong foundations but currently insufficient validation IMO. I'm not trying to pick on MAGIC or any specific imputation method. Instead I'd like to have an open discussion about the benefits, limitations, and relative performance of the various imputation methods available with the goal leading to something like @gokceneraslan suggested. Well, and since you brought it up, batch correction and multimodal integration methods are in definite need of the same open discussion, which I'd be happy to have, and I think they should have the same disclaimer regarding their limitations in the documentation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893:1791,perform,performance,1791,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/189#issuecomment-417692893,1,['perform'],['performance']
Performance,"tl.dpt with no branching events works:; ```; sc.tl.dpt(adata, n_branchings=0, n_dcs=10, min_group_size=0.01, allow_kendall_tau_shift=True); yields; performing Diffusion Pseudotime analysis; initialized `.distances` `.connectivities` `.eigen_values` `.eigen_basis` `.distances_dpt`; eigenvalues of transition matrix; [1. 0.87799305 0.74851424 0.7235198 0.5982796 0.5652917; 0.45321003 0.35327435 0.33786523 0.29598442]; finished (0:01:09.57) --> added; 'dpt_pseudotime', the pseudotime (adata.obs); ```. But calling pl.dpt on this object yields an error, looking at the above outout of tl.dpt it seems that dpt_groups was not created in adata.obs if n_branchings=0?. ```; ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2524 try:; -> 2525 return self._engine.get_loc(key); 2526 except KeyError:. pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'dpt_groups'. During handling of the above exception, another exception occurred:. KeyError Traceback (most recent call last); <ipython-input-102-eb7d1d859c99> in <module>(); ----> 1 sc.pl.dpt(adata, groups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:148,perform,performing,148,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['perform'],['performing']
Performance,"to;; 	mso-protection:locked visible;; 	white-space:nowrap;; 	mso-rotate:0;}; .xl65; 	{text-align:center;}; .xl66; 	{text-align:center;; 	border:.5pt solid windowtext;}; .xl67; 	{color:red;; 	text-align:center;; 	border:.5pt solid windowtext;}; .xl68; 	{font-weight:700;; 	text-align:center;; 	border:.5pt solid windowtext;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">.   | Right UMAP | Wrong UMAP; -- | -- | --; Package | Version | Version; Anaconda | 2.1.0 | 2.1.0; Python | 3.6.13 | 3.6.13; anndata | 0.7.6 | 0.7.6; anyio | 2.2.0 | 2.2.0; argon2-cffi | 20.1.0 | 20.1.0; async-generator | 1.1 | 1.1; attrs | 21.2.0 | 21.2.0; Babel | 2.9.1 | 2.9.1; backcall | 0.2.0 | 0.2.0; bleach | 4.0.0 | 4.0.0; brotlipy | 0.7.0 | 0.7.0; cached-property | 1.5.2 | 1.5.2; certifi | 2021.5.30 | 2021.5.30; cffi | 1.14.6 | 1.14.6; charset-normalizer | 2.0.4 | 2.0.4; colorama | 0.4.4 | 0.4.4; contextvars | 2.4 | 2.4; **cryptography | 3.4.7 | 35.0.0**; cycler | 0.10.0 | 0.11.0; dataclasses | 0.8 | 0.8; decorator | 4.4.2 | 4.4.2; defusedxml | 0.7.1 | 0.7.1; entrypoints | 0.3 | 0.3; get-version | 2.1 | 2.1; h5py | 3.1.0 | 3.1.0; idna | 3.2 | 3.2; igraph | 0.9.8 | 0.9.8; immutables | 0.16 | 0.16; importlib-metadata | 4.8.1 | 4.8.1; ipykernel | 5.3.4 | 5.3.4; ipython | 7.16.1 | 7.16.1; ipython-genutils | 0.2.0 | 0.2.0; jedi | 0.17.0 | 0.17.0; **Jinja2 | 3.0.1 | 3.0.2**; joblib | 1.1.0 | 1.1.0; json5 | 0.9.6 | 0.9.6; jsonschema | 3.2.0 | 3.2.0; jupyter-client | 7.0.1 | 7.0.1; jupyter-core | 4.8.1 | 4.8.1; jupyter-server | 1.4.1 | 1.4.1; **jupyterlab | 3.1.7 | 3.2.1**; jupyterlab-pygments | 0.1.2 | 0.1.2; jupyterlab-server | 2.8.2 | 2.8.2; kiwisolver | 1.3.1 | 1.3.1; legacy-api-wrap | 1.2 | 1.2; leidenalg | 0.8.8 | 0.8.8; llvmlite | 0.36.0 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045:3432,cache,cached-property,3432,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045,1,['cache'],['cached-property']
Performance,"trix""]); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/scanpy/readwrite.py"", line 256, in _collect_datasets; dsets[k] = v[:]; File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/dataset.py"", line 738, in __getitem__; selection = sel2.select_read(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 101, in select_read; return ScalarReadSelection(fspace, args); File ""/sc/arion/work/gujarh01/software/anaconda3/lib/python3.9/site-packages/h5py/_hl/selections2.py"", line 86, in __init__; raise ValueError(""Illegal slicing argument for scalar dataspace""). > **ValueError: Illegal slicing argument for scalar dataspace**; ```. `>>> scanpy.logging.print_versions()`. anndata 0.8.0; scanpy 1.9.1. PIL 8.4.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.2; cffi 1.14.6; cloudpickle 2.0.0; colorama 0.4.4; concurrent NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.10.0; dateutil 2.8.2; defusedxml 0.7.1; encodings NA; fsspec 2021.08.1; genericpath NA; h5py 3.3.0; igraph 0.9.6; jinja2 2.11.3; joblib 1.1.0; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.37.0; markupsafe 1.1.1; matplotlib 3.4.3; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; nbinom_ufunc NA; ntpath NA; numba 0.54.1; numexpr 2.7.3; numpy 1.20.3; opcode NA; packaging 21.0; pandas 1.3.4; pkg_resources NA; posixpath NA; psutil 5.8.0; pyexpat NA; pyparsing 3.0.4; pytz 2021.3; scipy 1.7.1; scrublet NA; session_info 1.0.0; six 1.16.0; sklearn 0.24.2; sphinxcontrib NA; sre_compile NA; sre_constants NA; sre_parse NA; tblib 1.7.0; texttable 1.6.4; tlz 0.11.0; toolz 0.11.1; typing_extensions NA; wcwidth 0.2.5; yaml 6.0; zope NA. Python 3.9.7 (default, Sep 16 2021, 13:09:58) [GCC 7.5.0]; Linux-3.10.0-957.10.1.el7.x86_64-x86_64-with-glibc2.17. Session ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572:1692,bottleneck,bottleneck,1692,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2203#issuecomment-1129213572,1,['bottleneck'],['bottleneck']
Performance,"ts.py in umap(adata, **kwargs); 27 If `show==False` a `matplotlib.Axis` or a list of it.; 28 """"""; ---> 29 return plot_scatter(adata, basis='umap', **kwargs); 30 ; 31 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/scanpy/plotting/tools/scatterplots.py in plot_scatter(adata, color, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, basis, groups, components, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs); 280 if sort_order is True and value_to_plot is not None and categorical is False:; 281 order = np.argsort(color_vector); --> 282 color_vector = color_vector[order]; 283 _data_points = data_points[component_idx][order, :]; 284 . h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). h5py/_objects.pyx in h5py._objects.with_phil.wrapper(). ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/dataset.py in __getitem__(self, args); 474 ; 475 # Perform the dataspace selection.; --> 476 selection = sel.select(self.shape, args, dsid=self.id); 477 ; 478 if selection.nselect == 0:. ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in select(shape, args, dsid); 70 elif isinstance(arg, np.ndarray):; 71 sel = PointSelection(shape); ---> 72 sel[arg]; 73 return sel; 74 . ~/miniconda3/envs/scRNA/lib/python3.6/site-packages/h5py/_hl/selections.py in __getitem__(self, arg); 210 """""" Perform point-wise selection from a NumPy boolean array """"""; 211 if not (isinstance(arg, np.ndarray) and arg.dtype.kind == 'b'):; --> 212 raise TypeError(""PointSelection __getitem__ only works with bool arrays""); 213 if not arg.shape == self.shape:; 214 raise TypeError(""Boolean indexing array has incompatible shape""). TypeError: PointSelection __getitem__ only works with bool arrays. <Figure size 1978.56x288 with 0 Axes>; ``` . Is it something implicit in the format of the backed file that cannot be solved? Also, do you think the ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/440:1951,Perform,Perform,1951,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/440,1,['Perform'],['Perform']
Performance,"ues, cast_type, column); 1807 values = astype_nansafe(values, cast_type,; -> 1808 copy=True, skipna=True); 1809 except ValueError:. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/pandas/core/dtypes/cast.py in astype_nansafe(arr, dtype, copy, skipna); 701 # Explicit copy, or required since NumPy can't view from / to object.; --> 702 return arr.astype(dtype, copy=True); 703 . ValueError: could not convert string to float: '4SFGA6_247'. During handling of the above exception, another exception occurred:. ValueError Traceback (most recent call last); <ipython-input-3-bf986d1f9b8c> in <module>; 1 import scanpy as sc; ----> 2 sc.datasets.moignard15(). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/datasets/__init__.py in moignard15(); 104 filename = 'data/moignard15/nbt.3154-S3.xlsx'; 105 backup_url = 'http://www.nature.com/nbt/journal/v33/n3/extref/nbt.3154-S3.xlsx'; --> 106 adata = sc.read(filename, sheet='dCt_values.txt', cache=True, backup_url=backup_url); 107 # filter out 4 genes as in Haghverdi et al. (2016); 108 gene_subset = ~np.in1d(adata.var_names, ['Eif2b1', 'Mrpl19', 'Polr2a', 'Ubc']). ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs); 76 return _read(filename, backed=backed, sheet=sheet, ext=ext,; 77 delimiter=delimiter, first_column_names=first_column_names,; ---> 78 backup_url=backup_url, cache=cache, **kwargs); 79 # generate filename and read to dict; 80 filekey = filename. ~/miniconda3/envs/spols190117/lib/python3.6/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs); 458 'Provide `sheet` parameter when reading \'.xlsx\' files.'); 459 else:; --> 460 adata = read_excel(filename, sheet); 461 elif ext in {'mtx', 'mtx.gz'}:; 462 adata = read_mtx(filename). ~/miniconda3/envs/spols190117/lib/python3.6/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/547:1270,cache,cache,1270,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/547,1,['cache'],['cache']
Performance,"uild_py.py"", line 107, in build_package_data; for package, src_dir, build_dir, filenames in self.data_files:; File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 65, in __getattr__; self.data_files = self._get_data_files(); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 79, in _get_data_files; return list(map(self._get_pkg_data_files, self.packages or ())); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 91, in _get_pkg_data_files; for file in self.find_data_files(package, src_dir); File ""/usr/lib/python3/dist-packages/setuptools/command/build_py.py"", line 98, in find_data_files; + self.package_data.get(package, [])); TypeError: Can't convert 'list' object to str implicitly; ; ----------------------------------------; Failed building wheel for scanpy; Running setup.py clean for scanpy; Running setup.py bdist_wheel for anndata ... done; Stored in directory: /root/.cache/pip/wheels/f1/f0/02/ea67db3107825884bae91e3806e425718f10062c631e2b1367; Running setup.py bdist_wheel for networkx ... done; Stored in directory: /root/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91; Successfully built anndata networkx; Failed to build scanpy; Installing collected packages: six, python-dateutil, pytz, numpy, pandas, scipy, h5py, natsort, anndata, pyparsing, cycler, kiwisolver, matplotlib, seaborn, numexpr, tables, scikit-learn, patsy, statsmodels, decorator, networkx, joblib, llvmlite, numba, scanpy; Running setup.py install for scanpy ... error; Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-33o4crd7/scanpy/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-65l8zi0l-record/install-record.txt --single-version-externally-managed --compile:; /usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'python_re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/355:2789,cache,cache,2789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/355,1,['cache'],['cache']
Performance,"ule>; from . import notation; File ""/opt/conda/lib/python3.7/site-packages/librosa/core/notation.py"", line 8, in <module>; from ..util.exceptions import ParameterError; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/__init__.py"", line 83, in <module>; from .utils import * # pylint: disable=wildcard-import; File ""/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py"", line 1848, in <module>; def __shear_dense(X, factor=+1, axis=-1):; File ""/opt/conda/lib/python3.7/site-packages/numba/core/decorators.py"", line 214, in wrapper; disp.enable_caching(); File ""/opt/conda/lib/python3.7/site-packages/numba/core/dispatcher.py"", line 812, in enable_caching; self._cache = FunctionCache(self.py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 610, in __init__; self._impl = self._impl_class(py_func); File ""/opt/conda/lib/python3.7/site-packages/numba/core/caching.py"", line 348, in __init__; ""for file %r"" % (qualname, source_path)); RuntimeError: cannot cache function '__shear_dense': no locator available for file '/opt/conda/lib/python3.7/site-packages/librosa/util/utils.py'; 1.10.1+cu102; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/opt/conda/lib/python3.7/site-packages/scanpy/__init__.py"", line 14, in <module>; from . import tools as tl; File ""/opt/conda/lib/python3.7/site-packages/scanpy/tools/__init__.py"", line 1, in <module>; from ..preprocessing import pca; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/__init__.py"", line 1, in <module>; from ._recipes import recipe_zheng17, recipe_weinreb17, recipe_seurat; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_recipes.py"", line 7, in <module>; from ._deprecated.highly_variable_genes import (; File ""/opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_deprecated/highly_variable_genes.py"", line 11, in <module>; from .._utils import _get_mean_var; File ""/opt/conda/lib/python3.7/site-packages/scanpy/prepr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2113:2299,cache,cache,2299,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2113,1,['cache'],['cache']
Performance,"un_setup(); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/build_meta.py"", line 145, in run_setup; exec(compile(code, __file__, 'exec'), locals()) File ""setup.py"", line 17, in <module>; setup(; File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/__init__.py"", line 153, in setup; return distutils.core.setup(**attrs); File ""/usr/lib/python3.8/distutils/core.py"", line 108, in setup; _setup_distribution = dist = klass(attrs); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 423, in __init__; _Distribution.__init__(self, {; File ""/usr/lib/python3.8/distutils/dist.py"", line 292, in __init__; self.finalize_options() File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 695, in finalize_options; ep(self); File ""/tmp/pip-build-env-wb9dh0v3/overlay/lib/python3.8/site-packages/setuptools/dist.py"", line 702, in _finalize_setup_keywords; ep.load()(self, ep.name, value); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/integration.py"", line 17, in version_keyword; dist.metadata.version = _get_version(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 148, in _get_version; parsed_version = _do_parse(config); File ""/usr/local/lib/python3.8/dist-packages/setuptools_scm/__init__.py"", line 110, in _do_parse raise LookupError(; LookupError: setuptools-scm was unable to detect version for '/home/ubuntu/code/scanpy'.; Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work.; ; For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj```; ```; #### Versions. <details>. scanpy; problem is with installation, so scanpy.logging.print_versions()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1496:2155,load,load,2155,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1496,1,['load'],['load']
Performance,"up and it does look very nice. Well, I learned a lot from `scanpy` here ;) . > tcellmatch's primary purpose is specificity prediction, this could be easily added ontop of this,. Scirpy currently supports the construction of clonotype similarity networks based on Levenshtein distance and BLOSUM62 pairwise sequence alignments. With these networks, we, indeed, had in mind, that clonotypes forming a connected subgraph should recognize the same antigen. Supporting `tcellmatchs`'s learned embedding distances would be a great addition. Dou you think this could be implemented as a subclass of the `_DistanceCalculator` [here](https://github.com/icbi-lab/scirpy/blob/master/scirpy/_preprocessing/_tcr_dist.py#L20)? Feel free to open an issue in `scirpy` for that! . I'd also be curious how the BLOSUM embedding relates to our alignment distance. (How) does the embedding handle gaps?. > Integration with epitope data bases: I have data loaders for IEDB and VDJdb downloads, can you be a bit more specific how you would integrate that with exploratorive single-cell studies? I can only imagine searching for similar TCRs?. Exactly! I think it would be helpful if we could find a way to automatically annotate clonotypes with known epitopes (e.g. to identify clonotypes that are specific to common viral antigens which could represent ""bystander T-cells"" in cancer). I believe using our alignment-based approach or `tcellmatch` could improve over the existing database-queries that rely on Levenshtein distance. We can continue a more in-depth discussion in https://github.com/icbi-lab/scirpy/issues/54. > An integration with dextramer counts to ""stain"" TCR specificity? . Interesting! Do you have an example where this was used with single cells? . > Could you add a brief summary of how you use anndata to store the TCR data in the docs? That would be very helpful to design extension or custom workflows. Great docs otherwise though!. There's already some information [at the beginning of the tutorial]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910:1045,load,loaders,1045,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1163#issuecomment-613394910,1,['load'],['loaders']
Performance,updated Scanpy to support Weighted sampled data to perform clustering…,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/630:51,perform,perform,51,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630,1,['perform'],['perform']
Performance,"ups will be used. But you have to pass something. This means you can't just forget to pass a parameter and then open a bug report about how genes are showing up multiple times in your DE results. You had to opt in to either behavior. OK, sounds good. Done. > ; > ## New column name; > I wasn't clear here. We should definitely include these values. I just think the names could be better and was wondering what other packages use as column names for these values.; > ; > AFAICT there is no agreed upon way to name these. Seems weird, since you'd think there'd be a technical name for ""when logFC is positive the xxxx group had higher expression"".; > ; > I would go for `f""fraction_{reference}""`, but then you can't pass the output directly to a plotting function without also passing the value for `reference`.; > ; > How about:; > ; > `pct_nz_group` and `pct_nz_reference`/ `pct_nz_ref`? I could also go for `lhs`/ `rhs` instead of `group`/ `reference`, and `fraction` instead of `pct`. But `group`/`reference` is consistent with `rank_genes_groups` and `pct` is consistent with `calculate_qc_metrics`. I like having `nz` in there since otherwise it's not super clear what fraction we're talking about. Could be fraction of total expression, or something about proportion of the dataset? This way it's more clear in the table you show to a collaborator. Sounds good, done. > ; > I agree `score` is a bit weird. Maybe `statistic` is a better choice? @davidsebfischer could probably be more authoritative on this. And yeah, we should change those `z-score` docs. Shall we change this in this function or in sc.tl.rank_genes_groups? I feel like renaming it here is not the best way. > ; > ### Performance; > General question about performance. Is this faster than calling the previous function separately on each group, then concatenating the results?. I think so:. <img width=""737"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/101388354-d29d4b00-388d-11eb-98b3-f91cf03ac84c.png"">",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654:2139,Perform,Performance,2139,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1388#issuecomment-740092654,2,"['Perform', 'perform']","['Performance', 'performance']"
Performance,"ups=None). ~/gitDevelopment/scanpy/scanpy/plotting/tools/__init__.py in dpt(adata, basis, color, alpha, groups, components, projection, legend_loc, legend_fontsize, legend_fontweight, color_map, palette, right_margin, size, title, show, save); 677 """"""; 678 colors = ['dpt_pseudotime']; --> 679 if len(np.unique(adata.obs['dpt_groups'].values)) > 1: colors += ['dpt_groups']; 680 if color is not None: colors = color; 681 dpt_scatter(. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in __getitem__(self, key); 2137 return self._getitem_multilevel(key); 2138 else:; -> 2139 return self._getitem_column(key); 2140 ; 2141 def _getitem_column(self, key):. /usr/local/lib/python3.6/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2144 # get column; 2145 if self.columns.is_unique:; -> 2146 return self._get_item_cache(key); 2147 ; 2148 # duplicate columns & possible reduce dimensionality. /usr/local/lib/python3.6/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 1840 res = cache.get(item); 1841 if res is None:; -> 1842 values = self._data.get(item); 1843 res = self._box_item_values(item, values); 1844 cache[item] = res. /usr/local/lib/python3.6/site-packages/pandas/core/internals.py in get(self, item, fastpath); 3841 ; 3842 if not isna(item):; -> 3843 loc = self.items.get_loc(item); 3844 else:; 3845 indexer = np.arange(len(self.items))[isna(self.items)]. /usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 2525 return self._engine.get_loc(key); 2526 except KeyError:; -> 2527 return self._engine.get_loc(self._maybe_cast_indexer(key)); 2528 ; 2529 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item(). pandas/_libs/hashtable_class_helper.p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/129:2538,cache,cache,2538,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/129,1,['cache'],['cache']
Performance,"users\yuanjian\.conda\envs\py363636\lib\site-packages (from importlib-metadata>=0.7->scanpy[leiden]) (3.6.0); Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (3.0.4); Collecting kiwisolver>=1.0.1; Using cached kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB); Requirement already satisfied: python-dateutil>=2.1 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from matplotlib>=3.1.2->scanpy[leiden]) (2.8.2); Collecting cycler>=0.10; Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB); Collecting pillow>=6.2.0; Using cached Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB); Collecting decorator<5,>=4.3; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Requirement already satisfied: setuptools in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from numba>=0.41.0->scanpy[leiden]) (58.0.4); Collecting llvmlite<0.37,>=0.36.0rc1; Using cached llvmlite-0.36.0-cp36-cp36m-win_amd64.whl (16.0 MB); Requirement already satisfied: pytz>=2017.2 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from pandas>=0.21->scanpy[leiden]) (2021.3); Requirement already satisfied: six>=1.5 in c:\users\yuanjian\.conda\envs\py363636\lib\site-packages (from python-dateutil>=2.1->matplotlib>=3.1.2->scanpy[leiden]) (1.16.0); Collecting threadpoolctl>=2.0.0; Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB); Collecting pynndescent>=0.5; Using cached pynndescent-0.5.5-py3-none-any.whl; Collecting get-version>=2.0.4; Using cached get_version-2.1-py3-none-any.whl (43 kB); Collecting igraph==0.9.8; Using cached igraph-0.9.8-cp36-cp36m-win_amd64.whl (2.7 MB); Collecting texttable>=1.6.2; Using cached texttable-1.6.4-py2.py3-none-any.whl (10 kB); Collecting stdlib-list; Using cached stdlib_list-0.8.0-py3-none-any.whl (63 kB); Collecting numexpr>=2.6.2; Using cached numexpr-2.7.3-cp36-cp36m-win_amd64.whl (93 kB); Requirement already s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955:3608,cache,cached,3608,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2045#issuecomment-962562955,1,['cache'],['cached']
Performance,"velope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:566) prefix=prefix,; [567](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:567) is_legacy=is_legacy,; [568](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:568) ); [569](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:569) if is_legacy or not gex_only:; [570](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/rea",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:5675,cache,cache,5675,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,1,['cache'],['cache']
Performance,"veshell.py"", line 3442, in run_code; exec(code_obj, self.user_global_ns, self.user_ns); File ""<ipython-input-8-1616c771d0da>"", line 1, in <module>; sc.tl.dendrogram(adata, 'cat_key', use_rep='X'); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scanpy/tools/_dendrogram.py"", line 139, in dendrogram; corr_condensed = distance.squareform(1 - corr_matrix); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scipy/spatial/distance.py"", line 2354, in squareform; is_valid_dm(X, throw=True, name='X'); File ""/Applications/miniconda3/envs/gep-dynamics/lib/python3.9/site-packages/scipy/spatial/distance.py"", line 2429, in is_valid_dm; raise ValueError(('Distance matrix \'%s\' must be '; ValueError: Distance matrix 'X' must be symmetric.```. #### Versions. <details>. sc.logging.print_versions() ; -----; anndata 0.8.0; scanpy 1.9.1; -----; PIL 9.4.0; PyQt5 NA; anyio NA; asttokens NA; attr 22.2.0; babel 2.11.0; backcall 0.2.0; beta_ufunc NA; binom_ufunc NA; bottleneck 1.3.6; brotli NA; certifi 2022.12.07; cffi 1.15.1; charset_normalizer 2.1.1; cloudpickle 2.2.1; colorama 0.4.6; comm 0.1.2; cycler 0.10.0; cython_runtime NA; cytoolz 0.12.0; dask 2023.1.1; dateutil 2.8.2; decorator 5.1.1; defusedxml 0.7.1; executing 1.2.0; fastcluster 1.2.6; fastjsonschema NA; gepdynamics NA; h5py 3.8.0; hypergeom_ufunc NA; idna 3.4; igraph 0.10.4; invgauss_ufunc NA; ipykernel 6.21.1; ipython_genutils 0.2.0; ipywidgets 8.0.4; jedi 0.18.2; jinja2 3.1.2; joblib 1.2.0; json5 NA; jsonschema 4.17.3; jupyter_events 0.5.0; jupyter_server 2.2.1; jupyterlab_server 2.19.0; kiwisolver 1.4.4; leidenalg 0.9.1; llvmlite 0.39.1; louvain 0.8.0; markupsafe 2.1.2; matplotlib 3.6.3; mpl_toolkits NA; natsort 8.2.0; nbformat 5.7.3; nbinom_ufunc NA; ncf_ufunc NA; nct_ufunc NA; ncx2_ufunc NA; numba 0.56.4; numexpr 2.8.3; numpy 1.23.5; packaging 23.0; pandas 1.5.3; parso 0.8.3; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; platformdirs 2.6.2; prometheus_client NA; p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2418:2261,bottleneck,bottleneck,2261,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2418,1,['bottleneck'],['bottleneck']
Performance,"write.py:570) return adata. File ~\AppData\Roaming\Python\Python312\site-packages\scanpy\readwrite.py:594, in _read_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix, is_legacy); [588](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:588) suffix = """" if is_legacy else "".gz""; [589](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:589) adata = read(; [590](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:590) path / f""{prefix}matrix.mtx{suffix}"",; [591](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:591) cache=cache,; [592](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:592) cache_compression=cache_compression,; [593](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:593) ).T # transpose the data; --> [594](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:594) genes = pd.read_csv(; [595](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-packages/scanpy/readwrite.py:595) path / f""{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}"",; [596](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/pmbm1/MEGA/PhD/PipelineDevelope/scRNA/~/AppData/Roaming/Python/Python312/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3214:6508,cache,cache,6508,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3214,2,['cache'],['cache']
Performance,"x([ b'0', b'1', b'2', b'3', b'4', b'5', b'6', b'7', b'8', ..., dtype='object', length=640); ```; When using `h5py==2.10.0`, it works as expected (i.e. the index type is str). The same happens for `.var_names` (I haven't check further). I'd recommend updating the requirements.txt oto `h5py<=2.10.0` for now. Related h5py issue: https://github.com/h5py/h5py/issues/1732. Also, I've encoutered this bug when coming up with example (seems unrelated):; ```python; import scanpy as sc. adata = sc.datasets.paul15(). sc.read('data/paul15/paul15.h5'); ```; The last line raises:. ```pytb; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-24-3d2f3a02bf09> in <module>; ----> 1 sc.read('data/paul15/paul15.h5'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,. ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs); 698 if ext in {'h5', 'h5ad'}:; 699 if sheet is None:; --> 700 return read_h5ad(filename, backed=backed); 701 else:; 702 logg.debug(f'reading sheet {sheet} from file {filename}'). ~/.miniconda3/envs/cellrank2/lib/python3.8/site-packages/anndata/_io/h5ad.py in read_h5ad(filename, backed, as_sparse, as_sparse_fmt, chunk_size); 427 _clean_uns(d) # backwards compat; 428 ; --> 429 return AnnData(**d); 430 ; 431 . TypeError: __init__() got an unexpected keyword argument 'batch.names'. ```. #### Versions. <details>. -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.1; -----; PIL 8.0.1; absl NA; anndata 0.7.4; autoreload NA; backcall 0.2.0; cellrank",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1480:1613,cache,cache,1613,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1480,1,['cache'],['cache']
Performance,"x.gz file. . ### Minimal code sample. ```python; sc.read_10x_mtx(""GSE123366_Combined""); ```. ### Error output. ```pytb; FileNotFoundError Traceback (most recent call last); Cell In[31], line 1; ----> 1 sc.read_10x_mtx(""GSE123366_Combined"", cache=True). File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:490, in read_10x_mtx(path, var_names, make_unique, cache, cache_compression, gex_only, prefix); 488 genefile_exists = (path / f'{prefix}genes.tsv').is_file(); 489 read = _read_legacy_10x_mtx if genefile_exists else _read_v3_10x_mtx; --> 490 adata = read(; 491 str(path),; 492 var_names=var_names,; 493 make_unique=make_unique,; 494 cache=cache,; 495 cache_compression=cache_compression,; 496 prefix=prefix,; 497 ); 498 if genefile_exists or not gex_only:; 499 return adata. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:554, in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression, prefix); 550 """"""; 551 Read mtx from output from Cell Ranger v3 or later versions; 552 """"""; 553 path = Path(path); --> 554 adata = read(; 555 path / f'{prefix}matrix.mtx.gz',; 556 cache=cache,; 557 cache_compression=cache_compression,; 558 ).T # transpose the data; 559 genes = pd.read_csv(path / f'{prefix}features.tsv.gz', header=None, sep='\t'); 560 if var_names == 'gene_symbols':. File ~/virtualenvs/scellai2/lib/python3.9/site-packages/scanpy/readwrite.py:112, in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs); 110 filename = Path(filename) # allow passing strings; 111 if is_valid_filename(filename):; --> 112 return _read(; 113 filename,; 114 backed=backed,; 115 sheet=sheet,; 116 ext=ext,; 117 delimiter=delimiter,; 118 first_column_names=first_column_names,; 119 backup_url=backup_url,; 120 cache=cache,; 121 cache_compression=cache_compression,; 122 **kwargs,; 123 ); 124 # generate filename and read to dict; 125 filekey = str(filename). File ~/virtualenvs/scell",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2570:1446,cache,cache,1446,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2570,1,['cache'],['cache']
Performance,"xists or not gex_only:. ~/.local/lib/python3.7/site-packages/scanpy/readwrite.py in _read_v3_10x_mtx(path, var_names, make_unique, cache, cache_compression); 371 else:; 372 raise ValueError(""`var_names` needs to be 'gene_symbols' or 'gene_ids'""); --> 373 adata.var['feature_types'] = genes[2].values; 374 adata.obs_names = pd.read_csv(path / 'barcodes.tsv.gz', header=None)[0]; 375 return adata. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key); 2686 return self._getitem_multilevel(key); 2687 else:; -> 2688 return self._getitem_column(key); 2689 ; 2690 def _getitem_column(self, key):. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _getitem_column(self, key); 2693 # get column; 2694 if self.columns.is_unique:; -> 2695 return self._get_item_cache(key); 2696 ; 2697 # duplicate columns & possible reduce dimensionality. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _get_item_cache(self, item); 2487 res = cache.get(item); 2488 if res is None:; -> 2489 values = self._data.get(item); 2490 res = self._box_item_values(item, values); 2491 cache[item] = res. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py in get(self, item, fastpath); 4113 ; 4114 if not isna(item):; -> 4115 loc = self.items.get_loc(item); 4116 else:; 4117 indexer = np.arange(len(self.items))[isna(self.items)]. /Applications/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance); 3078 return self._engine.get_loc(key); 3079 except KeyError:; -> 3080 return self._engine.get_loc(self._maybe_cast_indexer(key)); 3081 ; 3082 indexer = self.get_indexer([key], method=method, tolerance=tolerance). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc(). pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item(). pandas/_libs/h",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1408:2650,cache,cache,2650,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1408,1,['cache'],['cache']
Performance,"y variable genes](https://github.com/theislab/scanpy/blob/d4a7a2d98c1ea219c93d798170a2ca31d208cdbf/scanpy/preprocessing/_highly_variable_genes.py#L81)), but it's not that widely used. I think this is because it has to be implemented manually in the code (not sure if this is what you mean by ""intrinsic""?), which makes it take some effort to implement and not all contributors are aware of. I think using a decorator would be nice for abstracting out the process. This would have benefits of consistency of usage by making it easy, consistency of logged messages, and separation of concerns between computation and tracking. I also think you'd be able to know the exact set of operations from this approach. Assuming all top level functions have been wrapped with a decorator like the one I presented above, this code:. ```python; adata = sc.read_10x_h5(""./10x_run/outs/filtered_gene_matrix.h5""); sc.pp.normalize_per_cell(adata, 1000); sc.pp.log1p(adata); sc.pp.pca(adata); adata.write(""./cache/01_simple_process.h5ad""); ```. Should result in a set of (psuedo-)records like:. ```; # Where id(1) is a stand in for value like `id(adata)`; {""call"": ""read_10x_h5"", ""args"": {""filename"": ""./10x_run/outs/filtered_gene_matrix.h5""}, ""returned_adata"": id(1)}; {""call"": ""normalize_per_cell"", ""args"": {""counts_per_cell_after"": 1000}, ""adata_id"": id(1)}; {""call"": ""log1p"", ""adata_id"": id(1)}; {""call"": ""pca"", ""adata_id"": id(1)}; {""call"": ""write"", ""args"" : {""filename"": ""./cache/01_simple_process.h5ad""}, ""adata_id"": id(1)}; ```. It's pretty trivial to go through these logs and figure out what happened to the AnnData, and made accessible through helper functions. Maybe they'd look like `sc.logging.get_operations(adata_id=id(adata))` or `sc.logging.get_operations(written_to=""./cache/01_simple_process.h5ad"")`. There could also be a helper function to add the relevant records to some field in `.uns` of the relevant AnnData object or a setting which has a log handler do that automatically. Is there some set o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063:1171,cache,cache,1171,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/472#issuecomment-464575063,1,['cache'],['cache']
Performance,"yhd3eb1b0_0; - prompt-toolkit=3.0.8=py_0; - prompt_toolkit=3.0.8=0; - ptyprocess=0.7.0=pyhd3eb1b0_2; - pycparser=2.20=py_2; - pygments=2.7.4=pyhd3eb1b0_0; - pyparsing=2.4.7=pyhd3eb1b0_0; - pyqt=5.9.2=py37h05f1152_2; - pyrsistent=0.17.3=py37h7b6447c_0; - python=3.7.9=h7579374_0; - python-dateutil=2.8.1=pyhd3eb1b0_0; - pyzmq=20.0.0=py37h2531618_1; - qt=5.9.7=h5867ecd_1; - qtconsole=4.7.7=py_0; - qtpy=1.9.0=py_0; - readline=8.1=h27cfd23_0; - send2trash=1.5.0=pyhd3eb1b0_1; - setuptools=52.0.0=py37h06a4308_0; - sip=4.19.8=py37hf484d3e_0; - six=1.15.0=py37h06a4308_0; - sqlite=3.33.0=h62c20be_0; - terminado=0.9.2=py37h06a4308_0; - testpath=0.4.4=pyhd3eb1b0_0; - tk=8.6.10=hbc83047_0; - tornado=6.1=py37h27cfd23_0; - traitlets=5.0.5=pyhd3eb1b0_0; - wcwidth=0.2.5=py_0; - webencodings=0.5.1=py37_1; - wheel=0.36.2=pyhd3eb1b0_0; - widgetsnbextension=3.5.1=py37_0; - xz=5.2.5=h7b6447c_0; - zeromq=4.3.3=he6710b0_3; - zipp=3.4.0=pyhd3eb1b0_0; - zlib=1.2.11=h7b6447c_3; - pip:; - anndata==0.7.5; - cached-property==1.5.2; - click==7.1.2; - cycler==0.10.0; - get-version==2.1; - h5py==3.1.0; - importlib-metadata==3.4.0; - joblib==1.0.0; - kiwisolver==1.3.1; - legacy-api-wrap==1.2; - leidenalg==0.8.3; - llvmlite==0.35.0; - loompy==3.0.6; - louvain==0.7.0; - matplotlib==3.3.4; - natsort==7.1.1; - networkx==2.5; - numba==0.52.0; - numexpr==2.7.2; - numpy==1.20.0; - numpy-groupies==0.9.13; - pandas==1.2.1; - patsy==0.5.1; - pillow==8.1.0; - python-igraph==0.8.3; - pytz==2021.1; - scanpy==1.6.1; - scikit-learn==0.24.1; - scipy==1.6.0; - scvelo==0.2.2; - seaborn==0.11.1; - setuptools-scm==5.0.1; - sinfo==0.3.1; - statsmodels==0.12.1; - stdlib-list==0.8.0; - tables==3.6.1; - texttable==1.6.3; - threadpoolctl==2.1.0; - tqdm==4.56.0; - typing-extensions==3.7.4.3; - umap-learn==0.4.6; ```. I can reproduce the issue with a Docker container that only has the minimal conda environment above. Additionally, I already tried installing the exact same dependency versions in the new environment, but got the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1625:4448,cache,cached-property,4448,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625,1,['cache'],['cached-property']
Performance,you can check here that the log is actually performed as you suggest: https://github.com/theislab/scanpy/blob/5533b644e796379fd146bf8e659fd49f92f718cd/scanpy/preprocessing/_recipes.py#L66-L95. I'll close this for now,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149:44,perform,performed,44,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1251#issuecomment-702373149,1,['perform'],['performed']
Performance,ypi; ipykernel 6.29.3 pypi_0 pypi; ipython 8.22.2 pypi_0 pypi; ipywidgets 8.1.2 pypi_0 pypi; isoduration 20.11.0 pypi_0 pypi; jedi 0.19.1 pypi_0 pypi; jinja2 3.1.3 py311haa95532_0; joblib 1.3.2 pypi_0 pypi; json5 0.9.22 pypi_0 pypi; jsonpointer 2.4 pypi_0 pypi; jsonschema 4.21.1 pypi_0 pypi; jsonschema-specifications 2023.12.1 pypi_0 pypi; jupyter-client 8.6.1 pypi_0 pypi; jupyter-core 5.7.2 pypi_0 pypi; jupyter-events 0.9.1 pypi_0 pypi; jupyter-lsp 2.2.4 pypi_0 pypi; jupyter-server 2.13.0 pypi_0 pypi; jupyter-server-terminals 0.5.3 pypi_0 pypi; jupyter_client 8.6.0 py311haa95532_0; jupyter_core 5.5.0 py311haa95532_0; jupyter_events 0.8.0 py311haa95532_0; jupyter_server 2.10.0 py311haa95532_0; jupyter_server_terminals 0.4.4 py311haa95532_1; jupyterlab 4.1.5 pypi_0 pypi; jupyterlab-pygments 0.3.0 pypi_0 pypi; jupyterlab-server 2.25.4 pypi_0 pypi; jupyterlab-widgets 3.0.10 pypi_0 pypi; jupyterlab_pygments 0.1.2 py_0; jupyterlab_server 2.25.1 py311haa95532_0; kiwisolver 1.4.5 pypi_0 pypi; lazy-loader 0.3 pypi_0 pypi; legacy-api-wrap 1.4 pypi_0 pypi; leidenalg 0.10.2 pypi_0 pypi; libffi 3.4.4 hd77b12b_0; libsodium 1.0.18 h62dcd97_0; llvmlite 0.42.0 pypi_0 pypi; m2w64-bwidget 1.9.10 2; m2w64-bzip2 1.0.6 6; m2w64-expat 2.1.1 2; m2w64-fftw 3.3.4 6; m2w64-flac 1.3.1 # Name Version Build Channel; _r-mutex 1.0.0 anacondar_1 ; anndata 0.10.6 pypi_0 pypi; anyio 4.3.0 pypi_0 pypi; argon2-cffi 23.1.0 pypi_0 pypi; argon2-cffi-bindings 21.2.0 py311h2bbff1b_0 ; array-api-compat 1.5.1 pypi_0 pypi; arrow 1.3.0 pypi_0 pypi; asttokens 2.4.1 pypi_0 pypi; async-lru 2.0.4 py311haa95532_0 ; attrs 23.2.0 pypi_0 pypi; babel 2.14.0 pypi_0 pypi; beautifulsoup4 4.12.3 pypi_0 pypi; bleach 6.1.0 pypi_0 pypi; brotli-python 1.0.9 py311hd77b12b_7 ; bzip2 1.0.8 h2bbff1b_5 ; ca-certificates 2023.12.12 haa95532_0 ; certifi 2024.2.2 py311haa95532_0 ; cffi 1.16.0 py311h2bbff1b_0 ; chardet 5.2.0 pypi_0 pypi; charset-normalizer 3.3.2 pypi_0 pypi; colorama 0.4.6 py311haa95532_0 ; comm 0.2.2 pypi_0 pypi; conto,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2969:4477,load,loader,4477,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2969,1,['load'],['loader']
Performance,"ysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at doublet score = 0.42; Detected doublet rate = 0.3%; Estimated detectable doublet fraction = 5.2%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 6.6%; Scrublet finished (0:00:14); ```. I'm still not sure what actually caused the problem, but it seems that some dependency inconsistency occurred when performing PCA within the pipeline. Perhaps some package required for the `sc.pp.scrublet()` pipeline needs to be updated to a newer version?. Here are the details of the packages in the virtual environment when I ran the code on my desktop (failed case):; ```; channels:; - pytorch; - plotly; - conda-forge; - bioconda; - defaults; dependencies:; - anndata=0.10.7; - anyio=4.4.0; - appnope=0.1.4; - argcomplete=3.3.0; - argh=0.31.2; - argon2-cffi=23.1.0; - argon2-cffi-bindings=21.2.0; - arpack=3.8.0; - array-api-compat=1.7.1; - arrow=1.3.0; - asttokens=2.4.1; - async-lru=2.0.4; - attrs=23.2.0; - babel=2.14.0; - beautifulsoup4=4.12.3; - biopython=1.83; - blas=2.120; - blas-devel=3.9.0; - bleach=6.1.0; - blosc=1.21.5; - brotli=1.1.0; - brotli-bin=1.1.0; - brotli-python=1.1.0; - bzip2=1.0.8; - c-ares=1.28.1; - c-blosc2=2.14.4; - ca-certificates=2024.6.2; - cached-property=1.5.2; - cached_property=1.5.2; - certifi=2024.6.2; - cffi=1.16.0; - charset-norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:2306,perform,performing,2306,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['perform'],['performing']
Safety," a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_scipy_sparse.py) around `scipy.sparse` to implement NumPy's `__array_function__` protocol. This allows sparse arrays to be chunks in a Dask array. This approach seemed promising, with basic operations able to take take advantage of multiple cores and run faster than regular `scipy.sparse`. However, when I first tried running the whole Zheng17 recipe, `scipy.sparse` was always faster than Dask with `scipy.sparse`, even with many cores (e.g. 64). It turned out that by using Anndata arrays, Dask has to materialize intermediate data more than is necessary in order to populate the Anndata metadata. This is because the way Anndata works means that its metadata must be computed eagerly after each operation in the Zheng17 recipe, rather than lazily for the whole computation (which is the way Dask works). To avoid this complication I rewrote the Zheng17 recipe to do all the NumPy array computations and then construct an Anndata representation at the end,; to take advantage of Dask's deferred processing of lazy values. (See https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/preprocessing/_dask_optimized.py#L115 for the code.). With this change, running on the 1M neurons dataset with 64 cores `scipy.sparse` takes 334s, while Dask with `scipy.sparse` takes 138s, a 2.4x speedup. That's a significant speedup, but I'm not sure that it justifies the code overhead. I'd be interested to hear what others think. . ### Other notes. #### Code; See this branch: https://github.com/theislab/scanpy/compare/master...tomwhite:sparse-dask. #### CuPy and GPUs; I also wrote a [wrapper](https://github.com/tomwhite/scanpy/blob/sparse-dask/scanpy/sparsearray/_cupy_sparse.py) around the GPU equivalent of `scipy.sparse`, [`cupyx.scipy.sparse`](https://docs-cupy.chainer.org/en/stable/reference/sparse.html). Many operations work, however `cupyx.scipy.sparse` has a number of missing features that mean it can’t be ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/921:1585,avoid,avoid,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/921,1,['avoid'],['avoid']
Safety," genes that are detected in less than 5 cells; filtered out 5905 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6390 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5974 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 7136 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5971 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6234 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6442 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5593 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6412 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6662 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6039 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6036 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5923 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_u",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2080:2404,detect,detected,2404,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2080,1,['detect'],['detected']
Safety," is well defined; 53 return self.accessor_cls; ---> 54 return self.construct_accessor(instance); 55 ; 56 def __set__(self, instance, value):. ~/anaconda3/lib/python3.6/site-packages/pandas/core/categorical.py in _make_accessor(cls, data); 2209 def _make_accessor(cls, data):; 2210 if not is_categorical_dtype(data.dtype):; -> 2211 raise AttributeError(""Can only use .cat accessor with a ""; 2212 ""'category' dtype""); 2213 return CategoricalAccessor(data.values, data.index,. AttributeError: Can only use .cat accessor with a 'category' dtype; ```. Then, I comment out the respective line of code, run the whole thing again, and it works. And when I uncomment the line it works fine again. When I comment the line for the first time, I get a couple of lines displayed in the output saying:; > ... 'donor' was turned into a categorical variable; > ... 'gene_symbols' was turned into a categorical variable. or something like that... My theory is that sanitize_anndata() detects that these variables should be categorical variables and tries to convert them into categoricals. As this sc.pl.scatter call is the first time sanitize_anndata() is called after the variables are read in, this is the first time this conversion would take place. However, I am calling the sc.pl.scatter() on a subsetted anndata object, so it somehow cannot do the conversion. Once I call sc.pl.scatter on a non-subsetted anndata object once, the conversion can take place and I can subsequently call sc.pl.scatter also on a subsetted anndata object. If this is true, I can see why this is happening. However I feel this behaviour will be quite puzzling to a typical user. Maybe sanitize_anndata() should be called before plotting (probably hard to implement), or the plotting functions should have a parameter to plot only a subset of the data. That way sanitize_anndata can be called on the whole anndata object every time as there is no longer a reason to pass a view of the object. You could then test if a view is being pas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/166:3471,detect,detects,3471,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/166,1,['detect'],['detects']
Safety," line 48, in _guard_py_ver; raise RuntimeError(msg.format(cur_py, min_py, max_py)); RuntimeError: Cannot install on Python version 3.11.0; only versions >=3.7,<3.11 are supported.; error: subprocess-exited-with-error; ; × python setup.py egg_info did not run successfully.; │ exit code: 1; ╰─> See above for output.; ; note: This error originates from a subprocess, and is likely not a problem with pip.; full command: /Users/dang/opt/miniconda3/envs2/test/bin/python3.11 -c '; exec(compile('""'""''""'""''""'""'; # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py; #; # - It imports setuptools before invoking setup.py, to enable projects that directly; # import from `distutils.core` to work with newer packaging standards.; # - It provides a clear error message when setuptools is not installed.; # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so; # setuptools doesn'""'""'t think the script is `-c`. This avoids the following warning:; # manifest_maker: standard file '""'""'-c'""'""' not found"".; # - It generates a shim setup.py, for handling setup.cfg-only projects.; import os, sys, tokenize; ; try:; import setuptools; except ImportError as error:; print(; ""ERROR: Can not execute `setup.py` since setuptools is not available in ""; ""the build environment."",; file=sys.stderr,; ); sys.exit(1); ; __file__ = %r; sys.argv[0] = __file__; ; if os.path.exists(__file__):; filename = __file__; with tokenize.open(__file__) as f:; setup_py_code = f.read(); else:; filename = ""<auto-generated setuptools caller>""; setup_py_code = ""from setuptools import setup; setup()""; ; exec(compile(setup_py_code, filename, ""exec"")); '""'""''""'""''""'""' % ('""'""'/private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-install-3aknwjnh/numba_c251d9588484449eb116f16ee1b89979/setup.py'""'""',), ""<pip-setuptools-caller>"", ""exec""))' egg_info --egg-base /private/var/folders/8z/k5cyvf4j5kl0mzc9vn1gf_2h0000gq/T/pip-pip-egg-info-tlduu_0q; cwd: /private/var/folders/8z/k5cyv",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209:3465,avoid,avoids,3465,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2369#issuecomment-1332434209,1,['avoid'],['avoids']
Safety," of the other tools... Simply click on the GitHub links in the Scanpy docs... If your package works reliably, both the restrictions you mention should in principle not prevent adding your package. Of course, in the future, we want all elements of Scanpy to scale to millions of cells, not just the core tools. But for a lot of people, it's right now helpful to have a large number of tools available also for relatively small datasets. The only problem is to avoid cluttering the Scanpy API with virtually any tool there is. Tools in the API should have passed a certain quality check. Doublet detection is a difficult problem. Already last autumn, we played around with @swolock 's tool but didn't end up using it - it was good, but in our situation, it didn't seem to apply (are you eventually going to distribute a package for it @swolock ?). I myself quickly wrote a tool, too, but it didn't work well. Just yesterday, [this](https://www.biorxiv.org/content/early/2018/06/20/352484) appeared. Then there is also [this](https://www.biorxiv.org/content/early/2018/04/04/234872) on ""empty cell detection"". There are more tools out there, I think... What I mean is: computationally detecting doublets is still something where the field has not agreed on a consensus. Just like batch correction. Therefore, I would not add a tool `tl.doublet_detection` or `tl.detect_doublets` to the API at this stage. There are two options. Either we create a `.beta` module of the API for tools that don't even have a preprint and add your tool and similar cases in the future there. We could make a separate page for that entitled *Cutting Edge Beta Tools* which advertises these tools for people to try out and play around with it. When you have a solid preprint and/or publication or if you think that your tool should go in the main API anyways 😄, we should add your package as `tl.detect_doublets_ONEWORDDESCRIBGINGYOURALGORITHM`... . @flying-sheep @gokceneraslan @fidelram @dawe anyone opinions on such cases?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409:1527,detect,detection,1527,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/173#issuecomment-399367409,2,['detect'],"['detecting', 'detection']"
Safety," vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. [1] During: lowering ""$80for_iter.1 = iternext(value=$phi80.0)"" at /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py (107); @jit(float32[:, :](float32[:, :], float32[:, :], int32[:], int32[:], float32[:, :], float32)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:178: NumbaWarning: Function ""compute_correction"" was compiled in object mode without forceobj=True. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/numba/object_mode_passes.py:188: NumbaDeprecationWarning: ; Fall-back from the nopython compilation path to the object mode compilation path has been detected, this is deprecated behaviour. For more information visit http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit. File ""../../../anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py"", line 107:; def compute_correction(data1, data2, mnn1, mnn2, data2_or_raw2, sigma):; <source elided>; vect_reduced = np.zeros((data2.shape[0], vect.shape[1]), dtype=np.float32); for index, ve in zip(mnn2, vect):; ^. state.func_ir.loc)); Adjusting variance...; Applying correction...; Step 2 of 11: processing batch 2; Looking for MNNs...; Computing correction vectors...; /home/auesro/anaconda3/envs/Scanpy/lib/python3.7/site-packages/mnnpy/utils.py:102: NumbaWarning: ; Compilation is falling back to object mode WITHOUT looplifting enabled because Function ""compute_correction"" failed type inference due to: non-precise type pyobject; [1] During: typing of argument at /home/a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1167:24774,detect,detected,24774,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1167,1,['detect'],['detected']
Safety," we stick with the underlined sections? `:Parameters:` is a lot less pretty than the underlined counterpart.; - Why do we indent? Jupyter's typical help box is very narrow and the output really gets more squashed. Also, there seem to be a lot of unnecessary newlines. Pasting `tl.tsne` here looks somewhat acceptable (though not nice). But invoking it in a Jupyter notebook doesn't look nice...; ```; Signature: sc.tl.tsne(adata, n_pcs=None, use_rep=None, perplexity=30, early_exaggeration=12, learning_rate=1000, random_state=0, use_fast_tsne=True, n_jobs=None, copy=False); Docstring:; t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_. t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been; proposed for visualizating single-cell data by [Amir13]_. Here, by default,; we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve; a huge speedup and better convergence if you install `Multicore-tSNE; <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which; will be automatically detected by Scanpy. :Parameters:. **adata** : :class:`~anndata.AnnData`. Annotated data matrix. **n_pcs** : `int` or `None`, optional (default: `None`). Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`. **use_rep** : \{`None`, 'X'\} or any key for `.obsm`, optional (default: `None`). Use the indicated representation. If `None`, the representation is chosen; automatically: for `.n_vars` < 50, `.X` is used, otherwise 'X_pca' is used.; If 'X_pca' is not present, it's computed with default parameters. **perplexity** : `float`, optional (default: 30). The perplexity is related to the number of nearest neighbors that; is used in other manifold learning algorithms. Larger datasets; usually require a larger perplexity. Consider selecting a value; between 5 and 50. The choice is not extremely critical since t-SNE; is quite insensitive to this parameter. **early_exaggeration** : `float`, optional (default: 12.0). Controls how tight natural clusters i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:1319,detect,detected,1319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['detect'],['detected']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Dear `scanpy` developers, . I was exploring the new features in the latest version of Scanpy, but encountered a prolonged pause when running the `sc.pp.scrublet(adata)`. Initially I thought the problem was due to the large size (~100k cells) of the dataset I was exploring (I let it run for almost a whole week and nothing changed). However, even if I switched to my own dataset (unpublished, around 5k celIs), it paused at the same step. ; ```; Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; ```. I was running this analysis on my Intel-core iMac. Surprisingly, when I ran the same line of code (under a similar virtual environment) on my M2-chip laptop, it finished in a flash of time.; ```Running Scrublet; filtered out 1419 genes that are detected in less than 3 cells; normalizing counts per cell; finished (0:00:00); extracting highly variable genes; finished (0:00:00); --> added; 'highly_variable', boolean vector (adata.var); 'means', float vector (adata.var); 'dispersions', float vector (adata.var); 'dispersions_norm', float vector (adata.var); normalizing counts per cell; finished (0:00:00); normalizing counts per cell; finished (0:00:00); Embedding transcriptomes using PCA...; using data matrix X directly; Automatically set threshold at do",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3116:786,detect,detected,786,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3116,1,['detect'],['detected']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. I am working with a set of 2 10x scRNA samples. I read them, concatenated them and then I did basic filtering. I then used ""adata.raw = adata"" to freeze the counts on adata.raw before proceding. Then I ran: ; ```; sc.pp.normalize_total(adata, target_sum=1e4); sc.pp.log1p(adata); ```. To my surprise, when I check the adata.raw I see that the values have been also lognormized (and not only adata). ; Is that how it is supposed to be? Is there any way to avoid this behavior ? I know I can store the raw counts in layers, I just want to understand how it works. . To check the data I used : ; `print(adata.raw.X[1:10,1:10]) `. ### Minimal code sample. ```python; #read the data; Data1_adata= sc.read_10x_mtx(; '/Data_1/filtered_feature_bc_matrix', ; var_names='gene_symbols', index); cache=True) ; #concatenate; adata = Data1_adata.concatenate(Data2_adata); # save raw counts in raw slot.; adata.raw = adata ; # normalize to depth 10 000; sc.pp.normalize_total(adata, target_sum=1e4). # logaritmize; sc.pp.log1p(adata). #check adata.raw ; print(adata.raw.X[1:10,1:10]); ```. ### Error output. _No response_. ### Versions. <details>. ```; anndata 0.10.7; scanpy 1.10.0; -----; PIL 8.4.0; anyio NA; arrow 1.3.0; asttokens NA; attr 23.2.0; attrs 23.2.0; babel 2.14.0; backcall 0.2.0; bottleneck 1.3.7; brotli NA; certifi 2024.02.02; cffi 1.16.0; chardet 5.2.0; charset_normalizer 3.3.2; cloudpickle 3.0.0; colorama 0.4.6; comm 0.2.1; cycler 0.12.1; cython_runtime NA; cytoolz 0.12.3; dask 2024.2.0; dateutil 2.8.2; debugpy 1.8.1; decorator 5.1.1; defusedxml 0.7.1; exceptiongroup 1.2.0; executing 2.0.1; fastjsonschema NA; fqdn NA; h5py 3.7.0; idna 3.6; igraph 0.11.4; importlib_resources NA; ipykernel 6.29.2; ipyw",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3073:744,avoid,avoid,744,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3073,1,['avoid'],['avoid']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ### What happened?. While I am using `sc.pp.calculate_qc_metrics(ad, inplace=True)` to get QC metrics, its reported that a error occoured. ; Error message as below. ; ﻿﻿; It might just be because there's something wrong with my data. Does anyone else have a similar situation?. ### Minimal code sample. ```python; import scanpy as sc; import numpy as np; import anndata. # ad = anndata.read_h5ad('mypath'). def scrublet_by_sample(ad, key='samplename'):; """""" do doublet prediction by batch/sample """"""; """""" ad = anndata object """"""; """""" key = sample or batch in ad.obs""""""; sc.pp.calculate_qc_metrics(ad, inplace=True); ads = []; samplenames = ad.obs[key].unique(); for i in samplenames:; adx = ad[ad.obs[key].isin([i])].copy(); print(i,adx.n_obs); sc.external.pp.scrublet(adx,n_prin_comps=min(30,adx.shape[0]-1)); ads.append(adx); adata = ads[0].concatenate(tuple(ads[1:]), join='outer'); return adata. if np.array_equal(arr, np.round(arr)):; ad = scrublet_by_sample(ad, 'sample_ID'); ad.write(qc_h5); qc_md5 = generate_file_md5(qc_h5); print(""QC MD5 Hash:"", qc_md5); ```. ### Error output. ```pytb; Traceback (most recent call last):; File ""<stdin>"", line 2, in <module>; File ""<stdin>"", line 5, in scrublet_by_sample; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 306, in calculate_qc_metrics; obs_metrics = describe_obs(; File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 116, in describe_obs; proportions = top_segment_proportions(X, percent_top); File ""/home/sunhao/.conda/envs/h5ad/lib/python3.9/site-packages/scanpy/preprocessing/_qc.py"", line 401, in top_segment_proportions; return top_segment_proportions_sparse_csr(mtx.d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2758:740,predict,prediction,740,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2758,1,['predict'],['prediction']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. **In fact, that's not a real bug report, but a suggestion about safe design to plot with scanpy.**. It may happen when our celltype name is too long to show in the plotting figure.; The problem happens here:. https://github.com/scverse/scanpy/blob/a20334f02e6f2a0b56dd6dd862b07d5bdd4d879e/scanpy/plotting/_baseplot_class.py#L1059-L1061. It cut off the string in functions `dotplot`/`matrixplot(var_group_labels=)` , as the function `_plot_var_groups_brackets(group_labels=)`. So, when we use code like the sample, the veryvery long labels will be cut off & we got an Error; Because Var `celltype_order` is a list, and function `_plot_var_groups_brackets` (PATH: [scanpy/plotting/_baseplot_class.py](https://github.com/scverse/scanpy/blob/main/scanpy/plotting/_baseplot_class.py)) will affect the string in list,; then if affect the `celltype_order` itself, so. when called with the `categories_order`, it has been changed by `_plot_var_groups_brackets`, then the Error happened. I suggest to add one copy, for parameter `group_labels` in function `_plot_var_groups_brackets`; It may help someone are not so skillful on coding, to solve the problem maybe happen. For example: add the code `group_labels = copy.deepcopy(group_labels)` at the top of function `_plot_var_groups_brackets`. Thank you very much for your attention. ### Minimal code sample. ```python; adata: any anndata; markers: gene list include in var_names; group: obs key; celltype_order = ['short', 'veryveryverylong_name', 'others', ...]. sc.pl.dotplot(; 	adata, markers, group, show=False, swap_axes=True,; 	categories_order=celltype_order, var_group_labels=celltype_order, var_group_positions=pos_markers,; ); ```. ### Error output. ```pytb; K",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3081:353,safe,safe,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3081,1,['safe'],['safe']
Safety,"### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. Hi,; since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught.; Cheers,; Jesko. ### Minimal code sample. ```python; import anndata as ad; import scanpy as sc; import numpy as np; import pandas as pd; adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})); sc.pp.regress_out(adata, ""a""); ```. ### Error output. ```pytb; .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified; warnings.warn(msg, category=PerfectSeparationWarning); ```. ### Versions. <details>. ```; anndata 0.10.4; scanpy 1.9.6; statsmodels 0.14.0; ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3260:1439,predict,prediction,1439,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260,2,"['detect', 'predict']","['detected', 'prediction']"
Safety,"### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened?. There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:; ```py; adata = sc.datasets.pbmc3k_processed(); sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)); ``` ; ```pytb; ---------------------------------------------------------------------------; ValueError Traceback (most recent call last); Cell In[15], line 1; ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw); 325 if keys:; 326 df = df[keys]; --> 328 for k, idx in obsm_keys:; 329 added_k = f""{k}-{idx}""; 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2); ```; The function works if you pass a list of Tuples:; ```; sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]); ```; So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3310:380,avoid,avoid,380,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310,1,['avoid'],['avoid']
Safety,"### What kind of feature would you like to request?. Additional function parameters / changed functionality / changed defaults?. ### Please describe your wishes. The sc.tl.rank_gene_groups function needs additional arguments to filter some genes before running a test on them, just like Seurat's `FindMarkers` function. . 1. `minc_pct`: Only test genes detected in a minimum fraction of min_pct cells in either of the two comparison groups. It is beneficial for comparing cluster A to cluster B.; 2. `min_cells`: Minimum number of cells expressing the feature in at least one of the two comparison groups.; 3. `min_pct_difference`: Only test genes showing a minimum difference in the detection fraction between the two comparison groups. I will also try to implement these changes.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/3159:353,detect,detected,353,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3159,2,['detect'],"['detected', 'detection']"
Safety,"(). ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/indexes/base.py in _isnan(self); 2172 """"""; 2173 if self._can_hold_na:; -> 2174 return isna(self); 2175 else:; 2176 # shouldn't reach to this condition by checking hasnans beforehand. ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in isna(obj); 125 Name: 1, dtype: bool; 126 """"""; --> 127 return _isna(obj); 128 ; 129 . ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex; ```. </details>. I don't get an error from this on master, but I do get these warnings. ```; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; ```. No differences between pandas, seaborn, or matplotlib versions between these environments. Seems like it's sorta fixed on mas",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:5449,avoid,avoided,5449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['avoid'],['avoided']
Safety,"([], [])), shape=(n_obs, 1)); --> 390 connectivities = fuzzy_simplicial_set(; 391 X,; 392 n_neighbors,. /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/umap/umap_.py in fuzzy_simplicial_set(X, n_neighbors, random_state, metric, metric_kwds, knn_indices, knn_dists, angular, set_op_mix_ratio, local_connectivity, apply_set_operations, verbose); 600 knn_dists = knn_dists.astype(np.float32); 601 ; --> 602 sigmas, rhos = smooth_knn_dist(; 603 knn_dists, float(n_neighbors), local_connectivity=float(local_connectivity),; 604 ). SystemError: CPUDispatcher(<function smooth_knn_dist at 0x14a113bac160>) returned a result with an error set. time: 4.73 s (started: 2021-08-18 11:47:40 +01:00); ```. #### Versions. <details>. ```pytb; WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; autotime 0.3.1; backcall 0.2.0; bottleneck 1.3.2; cffi 1.14.6; cycler 0.10.0; cython_runtime NA; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; h5py 2.10.0; igraph 0.9.6; ipykernel 6.0.3; ipython_genutils 0.2.0; jedi 0.18.0; joblib 1.0.1; kiwisolver 1.3.1; leidenalg 0.8.7; llvmlite 0.33.0; loompy 3.0.6; louvain 0.7.0; matplotlib 3.4.2; matplotlib_inline NA; mkl 2.4.0; mpl_toolkits NA; natsort 7.1.1; numba 0.50.1; numexpr 2.7.3; numpy 1.20.3; numpy_groupies 0.9.13; packaging 21.0; pandas 1.3.0; parso 0.8.2; pexpect 4.8.0; pickleshare 0.7.5; pkg_resources NA; prompt_toolkit 3.0.19; ptyprocess 0.7.0; pycparser",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:3967,avoid,avoid,3967,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['avoid'],['avoid']
Safety,(fix): conditional imports to avoid `anndata.io` warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/3289:30,avoid,avoid,30,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289,1,['avoid'],['avoid']
Safety,"(sorry, I messed up my branch, so sending a new PR). I added a new batch_key option to HVG function. If specified, it runs the HVG selection in every batch separately and then merges the list in order to reduce the batch effects by avoiding the selection of batch-specific genes. This doesn't fully correct the batch effect but reduces it. Running the function for each batch is trivial but merging is trickier than I thought. How I do it now is as follows:. - hvg is run on each batch and resulting hvg lists are first concatenated into a single dataframe.; The data frame is grouped by genes. mean, dispersion and normalized dispersion values are aggregated via `np.nanmean`. Two new columns are created 1) ""in how many batches a gene is detected as hvg"". 2) intersection of all HVGs across batches. - if n_top_genes is given, combined hvg lists are sorted by 1) in how many batches a gene is detected as hvg 2) normalized dispersion. normalized dispersion is used to break the ties. Then top n genes are selected as the final hvg list. - if n_top_genes is not given, same mean and dispersion thresholds are applied to the combined hvg list. Here is the code to see the improvement of this approach:. ```python; import scanpy as sc; import numpy as np; import pandas as pd. ad = sc.read(""pancreas.h5ad"", backup_url=""https://goo.gl/V29FNk"") # adapted from scGen repo; ad.obs[""cell_type""] = ad.obs[""celltype""].tolist(). ad = sc.AnnData(ad.raw.X, var=ad.raw.var, obs=ad.obs); sc.pp.normalize_per_cell(ad); sc.pp.log1p(ad). sc.pp.highly_variable_genes(ad, batch_key='batch'); sc.pp.pca(ad); sc.pp.neighbors(ad); sc.tl.umap(ad); sc.pl.umap(ad, color=[""batch"", ""cell_type""]); ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/622:232,avoid,avoiding,232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622,3,"['avoid', 'detect']","['avoiding', 'detected']"
Safety,"**The following is what this function does (we can see it with ?sc.pp.recipe_zheng17):**; ```; sc.pp.filter_genes(adata, min_counts=1) # only consider genes with more than 1 count; sc.pp.normalize_per_cell( # normalize with total UMI count per cell; adata, key_n_counts='n_counts_all'); filter_result = sc.pp.filter_genes_dispersion( # select highly-variable genes; adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False); adata = adata[:, filter_result.gene_subset] # subset the genes; sc.pp.normalize_per_cell(adata) # renormalize after filtering; if log: sc.pp.log1p(adata) # log transform: adata.X = log(adata.X + 1); sc.pp.scale(adata) # scale to unit variance and shift to zero mean; ```. **But in the original paper Zheng et al. (2017 (https://www.nature.com/articles/ncomms14049#Sec11), it said:**; Only genes with at least one UMI count detected in at least one cell are used. UMI normalization was performed by first dividing UMI counts by the total UMI counts in each cell, **followed by multiplication with the median of the total UMI counts across cells**. Then, we took the natural log of the UMI counts. Finally, each gene was normalized such that the mean signal for each gene is 0, and standard deviation is 1. **So, comparing these two pipelines, the pipeline implemented in scanpy is not the same with the method described in the original paper, in the paper, there is a step**: _multiplication with the median of the total UMI counts across cells_, but this step was skipped inside the function sc.pp.recipe_zheng17. **Is there anyone who can tell me why they are different?** @flying-sheep",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/905:857,detect,detected,857,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/905,1,['detect'],['detected']
Safety,"- [X] I have checked that this issue has not already been reported.; - [X] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. This is probably a bug in my thinking, but naively I thought that `sc.pp.normalize_total()` normalizes counts per cell, thus allowing comparison of different cells by correcting for variable sequencing depth. However, the log transformation applied after normalisation seems to upset this relationship, example below. Why is this not problematic?. Incidentally, I first noticed this on my real biological dataset, not the toy example below. Edit: [relevant paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6215955/). > We can show, mathematically, that if we normalize expression profiles to have the same mean across cells, the mean after the equation [log] transformation used for RNA-Seq data will not be the same, and it will depend on the detection rate... And this [one](https://www.biorxiv.org/content/10.1101/404962v1.full):. > One issue of particular interest is that the mean of the log-counts is not generally the same as the log-mean count [1]. This is problematic in scRNA-seq contexts where the log-transformation is applied to normalized expression data. ---. ### Minimal code sample. ```python; >>> from anndata import AnnData; >>> import scanpy as sc; >>> import numpy as np; >>> adata = AnnData(np.array([[3, 3, 3, 6, 6],[1, 1, 1, 2, 2],[1, 22, 1, 2, 2], ])); >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']; >>> X_norm_log = np.log1p(X_norm); >>> X_norm_again = np.expm1(X_norm_log); >>> adata.X.sum(axis=1); array([21., 7., 28.], dtype=float32) # Different counts for each cell; >>> X_norm.sum(axis=1); array([1., 1., 1.], dtype=float32) # Normalisation means same counts for each cell; >>> X_norm_log.sum(axis=1); array([0.90322304, 0.90322304, 0.7879869 ], dtype=float32) # <<< Interested in this! Different counts for each ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1364:971,detect,detection,971,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1364,1,['detect'],['detection']
Safety,"- [x ] I have checked that this issue has not already been reported.; - [ x] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Selection of highly variable genes works fine in default settings, but I get an error when I try to use seurat_v3 flavor. ```python; adata2.layers[""counts""] = adata2.X.copy(); adata2.raw = adata2 # keep full dimension safe; sc.pp.normalize_total(adata2, target_sum=1e4); sc.pp.log1p(adata2); sc.pp.highly_variable_genes(; adata2,; flavor=""seurat_v3"",; n_top_genes=3000,; layer=""counts"",; batch_key=""Sample"",; subset=True; ); ```. ```pytb; ValueError Traceback (most recent call last); <ipython-input-18-64d280f5029c> in <module>; 3 sc.pp.normalize_total(adata2, target_sum=1e4); 4 sc.pp.log1p(adata2); ----> 5 sc.pp.highly_variable_genes(; 6 adata2,; 7 flavor=""seurat_v3"",. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, layer, n_top_genes, min_disp, max_disp, min_mean, max_mean, span, n_bins, flavor, subset, inplace, batch_key, check_values); 417 ; 418 if flavor == 'seurat_v3':; --> 419 return _highly_variable_genes_seurat_v3(; 420 adata,; 421 layer=layer,. /data04/projects04/MarianaBoroni/lbbc_members/lib/conda_envs/diogoamb/lib/python3.9/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_seurat_v3(adata, layer, n_top_genes, batch_key, check_values, span, subset, inplace); 83 x = np.log10(mean[not_const]); 84 model = loess(x, y, span=span, degree=2); ---> 85 model.fit(); 86 estimat_var[not_const] = model.outputs.fitted_values; 87 reg_std = np.sqrt(10 ** estimat_var). _loess.pyx in _loess.loess.fit(). ValueError: b'svddc failed in l2fit.'; ```. #### Versions; 0.10.00",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2034:449,safe,safe,449,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2034,1,['safe'],['safe']
Safety,"- [x ] I have checked that this issue has not already been reported.; - [x ] I have confirmed this bug exists on the latest version of scanpy.; - [ ] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. **Note**: Please read [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing how to provide the necessary information for us to reproduce your bug. ### Minimal code sample (that we can copy&paste without having any data). ```python; # create new env; conda install -c pytorch pytorch; conda install -c pytorch cudatoolkit=11.3. conda install -c bioconda scanpy; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versionsThe following specifications were found to be incompatible with your system:. - feature:/linux-64::__glibc==2.31=0; - feature:|@/linux-64::__glibc==2.31=0. Your installed version is: 2.31; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2282:1145,abort,abort,1145,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2282,1,['abort'],['abort']
Safety,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. ### Minimal code sample (that we can copy&paste without having any data). ```python; import scanpy as sc; adata = sc.datasets.pbmc3k(); sc.external.pp.scrublet(adata, threshold='I am ignored'); ```. ```pytb; /opt/conda/lib/python3.7/site-packages/scanpy/preprocessing/_normalization.py:138: UserWarning: Revieved a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.27; Detected doublet rate = 1.5%; Estimated detectable doublet fraction = 44.3%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 3.4%. ```. #### Versions. <details>. scanpy==1.7.0 anndata==0.7.5 umap==0.5.1 numpy==1.20.0 scipy==1.6.0 pandas==1.2.1 scikit-learn==0.24.1 statsmodels==0.12.2 python-igraph==0.8.3 louvain==0.7.0 leidenalg==0.8.3. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1644:655,Detect,Detected,655,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644,2,"['Detect', 'detect']","['Detected', 'detectable']"
Safety,"- [x] I have checked that this issue has not already been reported.; - [x] I have confirmed this bug exists on the latest version of scanpy.; - [x] (optional) I have confirmed this bug exists on the master branch of scanpy. ---. Found while investigating #1477. `scanpy.set_figure_parms` detects whether we're in an IPython session (or at least tries to #1841), and sets the default output format. The way it sets the format [is deprecated](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.set_matplotlib_formats), as it looks like the `matplotlib_inline` backend has moved to a separate package. . The new way to call this is: . ```python; import matplotlib_inline.backend_inline. if isinstance(ipython_format, str):; ipython_format = [ipython_format]; matplotlib_inline.backend_inline.set_matplotlib_formats(*ipython_format); ```. It may take some investigation to figure out how to do this in a backwards compatible way. I would also note the current backend format that we are using (`""png2x""`) is undocumented. It is equivalent to the documented format `""retina""`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1842:288,detect,detects,288,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1842,1,['detect'],['detects']
Safety,"---. Apologies for all the edits, but I'm stuck on this so have been playing around with it. Basically I'm getting weird errors when running scanpy on scvelo objects. I first had this issue when running `sc.pp.pca(adata_panc, n_comps=50)`, but managed to solve it by previously setting `adata_panc.X = np.array(adata_panc.X.todense())`. However, I'm now getting the exact same error when running `sc.pp.neighbors(adata_panc)` and I'm not sure which matrix to test. Any advice would be very much appreciated!. ### Minimal code sample (that we can copy&paste without having any data). ```python; adata_panc = scv.datasets.pancreas(); scv.pp.filter_and_normalize(adata_panc, n_top_genes=3000, min_shared_counts=20); del adata_panc.obsm['X_pca']; del adata_panc.obsm['X_umap']; del adata_panc.obsp['distances']; del adata_panc.obsp['connectivities']; adata_panc.X = np.array(adata_panc.X.todense()); sc.pp.pca(adata_panc, n_comps=50); sc.pp.neighbors(adata_panc); ```. ```pytb; Filtered out 20801 genes that are detected 20 counts (shared).; Normalized count data: X, spliced, unspliced.; Extracted 3000 highly variable genes.; Logarithmized X.; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); TypeError: expected dtype object, got 'numpy.dtype[float32]'. The above exception was the direct cause of the following exception:. SystemError Traceback (most recent call last); /hps/scratch/lsf_tmpdir/hl-codon-10-04/ipykernel_2322052/531027197.py in <module>; 7 adata_panc.X = np.array(adata_panc.X.todense()); 8 sc.pp.pca(adata_panc, n_comps=50); ----> 9 sc.pp.neighbors(adata_panc). /hps/software/users/marioni/Leah/miniconda3/envs/scvelo/lib/python3.8/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, key_added, copy); 137 adata._init_as_actual(adata.copy()); 138 neighbors = Neighbors(adata); --> 139 neighbors.compute_neighbors(; 140 n_ne",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1983:1232,detect,detected,1232,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1983,1,['detect'],['detected']
Safety,"-serif;; 	mso-font-charset:0;; 	text-align:center;}; .xl68; 	{text-align:center;}; ruby; 	{ruby-align:left;}; rt; 	{color:windowtext;; 	font-size:9.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:等线;; 	mso-generic-font-family:auto;; 	mso-font-charset:134;; 	mso-char-type:none;; 	display:none;}; -->; </style>; </head>. <body link=""#0563C1"" vlink=""#954F72"">. Windows PC1 |   | Windows PC2 |   | Windows PC3 |  ; -- | -- | -- | -- | -- | --; adjustText | 0.7.3 | adjustText | 0.7.3 | adjustText | 0.7.3; aiohttp | 3.8.1 | aiohttp | 3.8.1 | aiohttp | 3.8.1; aiosignal | 1.2.0 | aiosignal | 1.2.0 | aiosignal | 1.2.0; anndata | 0.7.8 | anndata | 0.7.8 | anndata | 0.7.8; anyio | 2.2.0 | anyio | 2.2.0 | anyio | 2.2.0; arboreto | 0.1.6 | arboreto | 0.1.6 | arboreto | 0.1.6; argon2-cffi | 20.1.0 | argon2-cffi | 20.1.0 | argon2-cffi | 20.1.0; async-generator | 1.1 | async-generator | 1.1 | async-generator | 1.1; async-timeout | 4.0.2 | async-timeout | 4.0.2 | async-timeout | 4.0.2; attrs | 21.4.0 | attrs | 21.2.0 | attrs | 21.4.0; Babel | 2.9.1 | Babel | 2.9.1 | Babel | 2.9.1; backcall | 0.2.0 | backcall | 0.2.0 | backcall | 0.2.0; bleach | 4.1.0 | bleach | 4.1.0 | bleach | 4.1.0; bokeh | 2.4.2 | bokeh | 2.4.2 | bokeh | 2.4.2; boltons | 21.0.0 | boltons | 21.0.0 | boltons | 21.0.0; brotlipy | 0.7.0 | brotlipy | 0.7.0 | brotlipy | 0.7.0; cellrank | 1.5.1 | cellrank | 1.5.1 | cellrank | 1.5.1; certifi | 2020.6.20 | certifi | 2020.6.20 | certifi | 2020.6.20; cffi | 1.15.0 | cffi | 1.15.0 | cffi | 1.15.0; charset-normalizer | 2.0.4 | charset-normalizer | 2.0.4 | charset-normalizer | 2.0.4; click | 8.0.3 | click | 8.0.3 | click | 8.0.3; cloudpickle | 2.0.0 | cloudpickle | 2.0.0 | cloudpickle | 2.0.0; colorama | 0.4.4 | colorama | 0.4.4 | colorama | 0.4.4; cryptography | 36.0.0 | cryptography | 36.0.0 | cryptography | 36.0.0; ctxcore | 0.1.1 | ctxcore | 0.1.1 | ctxcore | 0.1.1; cycler | 0.11.0 | cycler | 0.11.0 | cycler | 0.11.0; cytoolz | 0.11.0 | cyto",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2114:6319,timeout,timeout,6319,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2114,1,['timeout'],['timeout']
Safety,". **random_state** : typing.Union[int, mtrand.RandomState, NoneType]. A numpy random seed. **method** : {'umap', 'gauss', `None`} (default: `'umap'`). Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_; with adaptive width [Haghverdi16]_) for computing connectivities. **metric** : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], float]], optional (default: 'euclidean'). A known metric’s name or a callable that returns a distance. **metric_kwds** : Mapping. Options for the metric. **copy** : bool. Return a copy instead of writing to adata. :Returns:. Depending on `copy`, updates or returns `adata` with the following:. . **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Weighted adjacency matrix of the neighborhood graph of data; points. Weights should be interpreted as connectivities. **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`). Instead of decaying weights, this stores distances for each pair of; neighbors.; File: ~/_hholtz/01_projects/1512_scanpy/scanpy/scanpy/neighbors/__init__.py; Type: function; ```. PS: ; - Already the [docs](http://scanpy.readthedocs.io/en/latest/api/scanpy.api.Neighbors.compute_neighbors.html) show that `Neighbors.compute_neighbors` has invalid numpydoc... this was the case in several instances and I'm slowly fixing all of them... It's just a matter of adding `\` at the line breaks.; - I completely agree that the redundency between signature and docstring information lead to a a very small number of errors in the docstrings. However, in several instances, I'm setting the default value in the signature to `None`. But in the docstring, I'm giving the value to which this `None` evaluates in the default case (depending on what is passed)... There is quite a number of such cases. Clearly, one could replace all of them with `'auto'` parameters, which is probably the better way of doing this. As the whole thing is backwards compat, this is not an immediate problem",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999:7602,redund,redundency,7602,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/192#issuecomment-404108999,1,['redund'],['redundency']
Safety,". File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:485, in Axes3D.draw(self, renderer); 480 # Calculate projection of collections and patches and zorder them.; 481 # Make sure they are drawn above the grids.; 482 zorder_offset = max(axis.get_zorder(); 483 for axis in self._get_axis_list()) + 1; 484 for i, col in enumerate(; --> 485 sorted(self.collections,; 486 key=do_3d_projection,; 487 reverse=True)):; 488 col.zorder = zorder_offset + i; 489 for i, patch in enumerate(; 490 sorted(self.patches,; 491 key=do_3d_projection,; 492 reverse=True)):. File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/axes3d.py:471, in Axes3D.draw.<locals>.do_3d_projection(artist); 458 """"""; 459 Call `do_3d_projection` on an *artist*, and warn if passing; 460 *renderer*.; (...); 464 *renderer* and raise a warning.; 465 """"""; 467 if artist.__module__ == 'mpl_toolkits.mplot3d.art3d':; 468 # Our 3D Artists have deprecated the renderer parameter, so; 469 # avoid passing it to them; call this directly once the; 470 # deprecation has expired.; --> 471 return artist.do_3d_projection(); 473 _api.warn_deprecated(; 474 ""3.4"",; 475 message=""The 'renderer' parameter of ""; 476 ""do_3d_projection() was deprecated in Matplotlib ""; 477 ""%(since)s and will be removed %(removal)s.""); 478 return artist.do_3d_projection(renderer). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/matplotlib/_api/deprecation.py:431, in delete_parameter.<locals>.wrapper(*inner_args, **inner_kwargs); 421 deprecation_addendum = (; 422 f""If any parameter follows {name!r}, they should be passed as ""; 423 f""keyword, not positionally.""); 424 warn_deprecated(; 425 since,; 426 name=repr(name),; (...); 429 else deprecation_addendum,; 430 **kwargs); --> 431 return func(*inner_args, **inner_kwargs). File ~/anaconda3/envs/ml/lib/python3.9/site-packages/mpl_toolkits/mplot3d/art3d.py:599, in Path3DCollection.do_3d_projection(self, renderer); 597 @_api.delete_parameter('3.4', 'renderer')",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2285:8946,avoid,avoid,8946,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2285,1,['avoid'],['avoid']
Safety,". ~/miniconda3/envs/scanpy-forge/lib/python3.8/site-packages/pandas/core/dtypes/missing.py in _isna(obj, inf_as_na); 154 # hack (for now) because MI registers as ndarray; 155 elif isinstance(obj, ABCMultiIndex):; --> 156 raise NotImplementedError(""isna is not defined for MultiIndex""); 157 elif isinstance(obj, type):; 158 return False. NotImplementedError: isna is not defined for MultiIndex; ```. </details>. I don't get an error from this on master, but I do get these warnings. ```; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*. Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.; ```. No differences between pandas, seaborn, or matplotlib versions between these environments. Seems like it's sorta fixed on master. I think this might be a cause:. ```python; import scanpy as sc; import seaborn as sns; sns.set() # <--- here. pbmc = sc.datasets.pbmc68k_reduced(); sc.pl.umap(pbmc, color = 'phase') # This errors. pbmc.uns[""phase_colors""]; ```. ```; [(0.2980392156862745, 0.4470588235294118, 0.6901960784313725),; (0.8666666666666667, 0.5176470588235295, 0.3215686274509804),; (0.3333333333333333, 0.6588235294117647, 0.40784313725490196)]; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1885:6083,avoid,avoided,6083,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1885,1,['avoid'],['avoided']
Safety,"/github.com/scverse/scanpy/blob/master/scanpy/external/pp/_scrublet.py#L252) results in a key error since the scrubbed data contains fewer cells than the original adata. ### Minimal code sample (that we can copy&paste without having any data); I do not have a code sample that does not require data, but any anndata that contains cells with fewer than 3 genes should trigger the error.; ```python; import scanpy; adata = scanpy.read(""test.h5ad"") # e.g. contains cells with fewer than 3 genes; sc.external.pp.scrublet(test_adata, batch_key = ""label""); ```. ```pytb; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:251: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; adata.var['n_cells'] = number; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:170: UserWarning: Received a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.16; Detected doublet rate = 6.4%; Estimated detectable doublet fraction = 61.7%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 10.4%. ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:251: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; adata.var['n_cells'] = number; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:170: UserWarning: Received a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.17; Detected doublet rate = 5.8%; Estimated detectable doublet fraction = 55.7%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 10.5%. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /tmp/ipykernel_2434661/1304834185.py in <module>; ----> 1 sc.external.pp.scrublet(test_adata, batch_key = ""label""). ~/.local/lib/python3.9/site-packages/scanpy/external/pp/_scrublet.py in sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2377:1585,Detect,Detected,1585,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2377,1,['Detect'],['Detected']
Safety,"/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/__init__.py in <module>; 89 ; ---> 90 __version__ = get_version(root="".."", relative_to=__file__); 91 del get_version. ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in get_version(root, version_scheme, local_scheme, write_to, write_to_template, relative_to, tag_regex, fallback_version, fallback_root, parse, git_describe_command); 142 config = Configuration(**locals()); --> 143 return _get_version(config); 144 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _get_version(config); 146 def _get_version(config):; --> 147 parsed_version = _do_parse(config); 148 . ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/setuptools_scm/__init__.py in _do_parse(config); 117 ""https://github.com/user/proj/archive/master.zip ""; --> 118 ""use git+https://github.com/user/proj.git#egg=proj"" % config.absolute_root; 119 ). LookupError: setuptools-scm was unable to detect version for '/Users/kabitabaral/miniconda3/envs/scanpy/lib/python3.6/site-packages'. Make sure you're either building from a fully intact git repository or PyPI tarballs. Most other sources (such as GitHub's tarballs, a git checkout without the .git folder) don't contain the necessary metadata and will not work. For example, if you're using pip, instead of https://github.com/user/proj/archive/master.zip use git+https://github.com/user/proj.git#egg=proj. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); ~/miniconda3/envs/scanpy/lib/python3.6/site-packages/anndata/compat.py in pkg_version(package); 56 try:; ---> 57 from importlib.metadata import version as v; 58 except ImportError:. ModuleNotFoundError: No module named 'importlib.metadata'. During handling of the above exception, another exception occurred:. ModuleNotFoundError Traceback (most recent call last); <ipython-input-11-495a6d84c058> in <module>; 1 import os; ----",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845:1153,detect,detect,1153,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1154#issuecomment-611202845,1,['detect'],['detect']
Safety,"/scanpy/external/pp/_scrublet.py#L252) results in a key error since the scrubbed data contains fewer cells than the original adata. ### Minimal code sample (that we can copy&paste without having any data); I do not have a code sample that does not require data, but any anndata that contains cells with fewer than 3 genes should trigger the error.; ```python; import scanpy; adata = scanpy.read(""test.h5ad"") # e.g. contains cells with fewer than 3 genes; sc.external.pp.scrublet(test_adata, batch_key = ""label""); ```. ```pytb; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:251: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; adata.var['n_cells'] = number; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:170: UserWarning: Received a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.16; Detected doublet rate = 6.4%; Estimated detectable doublet fraction = 61.7%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 10.4%. ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_simple.py:251: ImplicitModificationWarning: Trying to modify attribute `.var` of view, initializing view as actual.; adata.var['n_cells'] = number; ~/.local/lib/python3.9/site-packages/scanpy/preprocessing/_normalization.py:170: UserWarning: Received a view of an AnnData. Making a copy.; view_to_actual(adata). Automatically set threshold at doublet score = 0.17; Detected doublet rate = 5.8%; Estimated detectable doublet fraction = 55.7%; Overall doublet rate:; 	Expected = 5.0%; 	Estimated = 10.5%. ---------------------------------------------------------------------------; KeyError Traceback (most recent call last); /tmp/ipykernel_2434661/1304834185.py in <module>; ----> 1 sc.external.pp.scrublet(test_adata, batch_key = ""label""). ~/.local/lib/python3.9/site-packages/scanpy/external/pp/_scrublet.py in scrublet(adata, adata_sim, batch_key, sim",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2377:1625,detect,detectable,1625,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2377,1,['detect'],['detectable']
Safety,0.13; pydeseq2 0.4.1; PyDispatcher 2.0.5; pydocstyle 6.3.0; pyerfa 2.0.0.3; pyflakes 3.0.1; Pygments 2.16.1; PyJWT 2.8.0; pylint 2.17.5; pylint-venv 3.0.2; pyls-spyder 0.4.0; pynndescent 0.5.10; pyodbc 4.0.39; pyOpenSSL 23.2.0; pyparsing 3.1.1; PyQt5-sip 12.11.0; PySocks 1.7.1; pytest 7.4.2; python-dateutil 2.8.2; python-dotenv 1.0.0; python-json-logger 2.0.7; python-lsp-black 1.3.0; python-lsp-jsonrpc 1.1.2; python-lsp-server 1.7.2; python-slugify 8.0.1; pytoolconfig 1.2.5; pytz 2023.3.post1; pyviz_comms 3.0.0; PyWavelets 1.4.1; pyxdg 0.28; PyYAML 6.0.1; pyzmq 25.1.1; QDarkStyle 3.1; qstylizer 0.2.2; QtAwesome 1.2.3; qtconsole 5.4.4; QtPy 2.4.0; queuelib 1.6.2; referencing 0.30.2; regex 2023.10.3; requests 2.31.0; requests-file 1.5.1; requests-toolbelt 1.0.0; reretry 0.11.8; rfc3339-validator 0.1.4; rfc3986-validator 0.1.1; rich 13.6.0; rope 1.10.0; rpds-py 0.10.4; Rtree 1.0.1; ruamel.yaml 0.17.35; ruamel.yaml.clib 0.2.7; ruamel-yaml-conda 0.15.80; s3fs 0.5.1; sacremoses 0.0.53; safetensors 0.3.3; scanpy 1.9.5; scikit-image 0.21.0; scikit-learn 1.3.1; scikit-learn-intelex 20230725.122106; scipy 1.11.3; Scrapy 2.11.0; scrublet 0.2.3; scTE 1.0; scTE 1.0; seaborn 0.13.0; SecretStorage 3.3.3; semver 3.0.1; Send2Trash 1.8.2; service-identity 18.1.0; session-info 1.0.0; setuptools 68.0.0; sip 6.6.2; six 1.16.0; smart-open 6.4.0; smmap 5.0.0; snakemake 7.32.3; sniffio 1.3.0; snowballstemmer 2.2.0; sortedcontainers 2.4.0; soupsieve 2.5; Sphinx 7.2.6; sphinxcontrib-applehelp 1.0.7; sphinxcontrib-devhelp 1.0.5; sphinxcontrib-htmlhelp 2.0.4; sphinxcontrib-jsmath 1.0.1; sphinxcontrib-qthelp 1.0.6; sphinxcontrib-serializinghtml 1.1.9; spyder 5.4.3; spyder-kernels 2.4.4; SQLAlchemy 2.0.21; stack-data 0.6.2; statsmodels 0.14.0; stdlib-list 0.8.0; stopit 1.1.2; sympy 1.12; tables 3.9.1; tabulate 0.9.0; TBB 0.2; tblib 2.0.0; tenacity 8.2.3; terminado 0.17.1; text-unidecode 1.3; textdistance 4.5.0; texttable 1.7.0; threadpoolctl 3.2.0; three-merge 0.1.1; throttler 1.2.2; tifffile 20,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2680:8959,safe,safetensors,8959,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2680,1,['safe'],['safetensors']
Safety,"14 with self.builtin_trap:; -> 2115 result = fn(magic_arg_s, cell); 2116 return result; 2117 . <decorator-gen-59> in time(self, line, cell, local_ns). /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magic.py in <lambda>(f, *a, **k); 186 # but it's overkill for just that one bit of state.; 187 def magic_deco(arg):; --> 188 call = lambda f, *a, **k: f(*a, **k); 189 ; 190 if callable(arg):. /public/bioapps/ana/anaconda3/envs/python35/lib/python3.5/site-packages/IPython/core/magics/execution.py in time(self, line, cell, local_ns); 1178 else:; 1179 st = clock2(); -> 1180 exec(code, glob, local_ns); 1181 end = clock2(); 1182 out = None. <timed exec> in <module>(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in dpt(adata, n_branchings, n_neighbors, knn, n_pcs, n_dcs, min_group_size, n_jobs, recompute_graph, recompute_pca, allow_kendall_tau_shift, flavor, copy); 127 adata.smp['dpt_pseudotime'] = dpt.pseudotime; 128 # detect branchings and partition the data into segments; --> 129 dpt.branchings_segments(); 130 # vector of length n_groups; 131 adata.add['dpt_groups_order'] = [str(n) for n in dpt.segs_names_unique]. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in branchings_segments(self); 188 for each segment.; 189 """"""; --> 190 self.detect_branchings(); 191 self.postprocess_segments(); 192 self.set_segs_names(). /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branchings(self); 258 segs_connects,; 259 segs_undecided,; --> 260 segs_adjacency, iseg, tips3); 261 # store as class members; 262 self.segs = segs. /public/workspace/jiping/scanpy-master/scanpy/tools/dpt.py in detect_branching(self, segs, segs_tips, segs_connects, segs_undecided, segs_adjacency, iseg, tips3); 464 # branching on the segment, return the list ssegs of segments that; 465 # are defined by splitting this segment; --> 466 result = self._detect_branching(Dseg, tips3, seg); 467 ssegs, ssegs_tips, ssegs_adjacency, ssegs_connec",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/33:2545,detect,detect,2545,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/33,1,['detect'],['detect']
Safety,"18:; ```; conda create -n temp_env_scanpy; conda activate temp_env_scanpy; (temp_env_scanpy) giov@vm:~$ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: -; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/linux-64::__cuda==9.1=0. Your installed CUDA driver is: 9.1; ```; Interestingly, this error is not thrown all the time, e.g. in a VM centos 7 without cuda:; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: \; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed. UnsatisfiableError:; ```; Another student working with me had the same issue in windows. His error was:; ```; UnsatisfiableError: The following specifications were found to be incompatible with your CUDA driver:. - feature:/win-64::__cuda==10.2=0. Your installed CUDA driver is: 10.2; ```; But on a mac, no problem at all. In all situations, I have at least another environment with scanpy installed.; In all cases, conda was `4.8.3`.; I cannot rule out completely the possibility that my conda in those 2 vms are messed up.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1142:1551,abort,abort,1551,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1142,1,['abort'],['abort']
Safety,"2 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5593 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6412 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6662 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6039 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6036 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 5923 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6767 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 7077 genes that are detected in less than 5 cells; Observation names are not unique. To make them unique, call `.obs_names_make_unique`.; filtered out 6080 genes that are detected in less than 5 cells; Traceback (most recent call last):; File ""/mypath/runMulti_scanpy.py"", line 161, in main; adata = sc.concat([adata,adata_tmp],join='outer'); File ""/mypath/scanpy1.7/lib/python3.8/site-packages/anndata/_core/merge.py"", line 818, in concat; X = concat_arrays(; File ""/mypath/scanpy1.7/lib/python3.8/site-packages/anndata/_core/merge.py"", line 424, in concat_arrays; return np.concatenate(; File ""<__array_function__ internals>"", line 5, in concatenate; numpy.core._exceptions.MemoryError: Unable to allocate 15.9 GiB for an array with shape (180000, 23752) and data type float32; ```. Thanks !!!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2080:3461,detect,detected,3461,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2080,3,['detect'],['detected']
Safety,"6 elif isinstance(data, ma.MaskedArray):; 437 import numpy.ma.mrecords as mrecords. /usr/local/lib/python3.8/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype); 237 else:; 238 nan_dtype = dtype; --> 239 val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype); 240 arrays.loc[missing] = [val] * missing.sum(); 241 . /usr/local/lib/python3.8/site-packages/pandas/core/dtypes/cast.py in construct_1d_arraylike_from_scalar(value, length, dtype); 1438 else:; 1439 if not isinstance(dtype, (np.dtype, type(np.dtype))):; -> 1440 dtype = dtype.dtype; 1441 ; 1442 if length and is_integer_dtype(dtype) and isna(value):. AttributeError: type object 'object' has no attribute 'dtype'; ```. #### Versions. <details>. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.4; scanpy 1.6.0; sinfo 0.3.4; -----; MulticoreTSNE NA; PIL 8.0.1; appnope 0.1.2; attr 20.3.0; backcall 0.2.0; cffi 1.14.4; cloudpickle 2.0.0; colorama 0.4.4; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.2; dask 2022.01.0; dateutil 2.8.1; decorator 4.4.2; dunamai 1.7.0; fsspec 2022.01.0; get_version 3.5.3; google NA; h5py 2.10.0; idna 2.10; igraph 0.9.6; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.5; jedi 0.17.2; jinja2 2.11.2; joblib 1.0.1; jsonschema 3.2.0; jupyter_server 1.13.3; kiwisolver 1.3.1; legacy_api_wrap 1.2; leidenalg 0.8.0; llvmlite 0.38.0; loompy 3.0.6; louvain 0.7.0; markupsafe 1.1.1; matplotlib 3.3.3; mpl_toolkits NA; natsort 7.1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/2121:4282,avoid,avoid,4282,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/2121,1,['avoid'],['avoid']
Safety,"; 310 show = settings.autoshow if show is None else show; 311 if save:; --> 312 savefig(writekey, dpi=dpi, ext=ext); 313 if show:; 314 pl.show(). ~/opt/anaconda3/lib/python3.8/site-packages/scanpy/plotting/_utils.py in savefig(writekey, dpi, ext); 280 else:; 281 dpi = rcParams['savefig.dpi']; --> 282 settings.figdir.mkdir(parents=True, exist_ok=True); 283 if ext is None:; 284 ext = settings.file_format_figs. AttributeError: 'str' object has no attribute 'mkdir'; ```. #### Versions. scanpy==1.8.1 anndata==0.7.6 umap==0.5.1 numpy==1.18.5 scipy==1.6.2 pandas==1.1.5 scikit-learn==0.24.2 statsmodels==0.12.2 python-igraph==0.9.4 louvain==0.7.0 pynndescent==0.5.2. [Paste the output of scanpy.logging.print_versions() leaving a blank line after the details tag]. WARNING: If you miss a compact list, please try `print_header`!; The `sinfo` package has changed name and is now called `session_info` to become more discoverable and self-explanatory. The `sinfo` PyPI package will be kept around to avoid breaking old installs and you can downgrade to 0.3.2 if you want to use it without seeing this message. For the latest features and bug fixes, please install `session_info` instead. The usage and defaults also changed slightly, so please review the latest README at https://gitlab.com/joelostblom/session_info.; -----; anndata 0.7.6; scanpy 1.8.1; sinfo 0.3.4; -----; PIL 8.3.1; anyio NA; appdirs 1.4.4; appnope 0.1.2; attr 21.2.0; babel 2.9.1; backcall 0.2.0; bioservices 1.7.12; bottleneck 1.3.2; brotli NA; bs4 4.9.3; certifi 2021.05.30; cffi 1.14.6; chardet 4.0.0; cloudpickle 1.6.0; colorama 0.4.4; colorlog NA; cycler 0.10.0; cython_runtime NA; cytoolz 0.11.0; dask 2021.07.2; dateutil 2.8.2; decorator 5.0.9; defusedxml 0.7.1; docutils 0.17.1; easydev 0.11.1; fsspec 2021.07.0; gseapy 0.10.5; h5py 2.10.0; html5lib 1.1; idna 2.10; igraph 0.9.4; ipykernel 5.3.4; ipython_genutils 0.2.0; ipywidgets 7.6.3; jedi 0.17.2; jinja2 2.11.3; joblib 1.0.1; json5 NA; jsonschema 3.2.0; jupyter_server 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1981:3457,avoid,avoid,3457,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1981,1,['avoid'],['avoid']
Safety,"<!-- Please give a clear and concise description of what the bug is: -->; I am calculating custom connectivities using hsnw on rep 'X', I don't want to calculate PCA, I want to compute UMAP using these connectivities. ; sc.tl.umap falls back to pca in:; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_umap.py#L153; https://github.com/theislab/scanpy/blob/5bc37a2b10f40463f1d90ea1d61dc599bbea2cd0/scanpy/tools/_utils.py#L23. how to get sc.tl.umap to run on the precomputed 'X 'connectivities?; ... <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```python; import scanpy as sc; from scvelo.pp import neighbors; adata; #AnnData object with n_obs × n_vars = 4329 × 192; #obs: 'BARCODE', 'sample', 'detectable.features'; #var: 'gene_ids', 'feature_types'; #layers: 'normalized.counts'. neighbors(adata, n_neighbors = 20, use_rep = ""X"",knn = True,random_state = 0,method = 'hnsw',metric = ""euclidean"",metric_kwds = {""M"":20,""ef"":200,""ef_construction"":200},num_threads=1). adata.uns[""neighbors""]['params']; #{'n_neighbors': 20, 'method': 'hnsw', 'metric': 'euclidean', 'n_pcs': None}. sc.tl.umap(adata). #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. <!-- Put your Error output in this code block (if applicable, else delete the block): -->; ```pytb; #WARNING: .obsp[""connectivities""] have not been computed using umap; #WARNING: You’re trying to run this on 192 dimensions of `.X`, if you really want this, set `use_rep='X'`.; # Falling back to preprocessing with `sc.pp.pca` and default params. ...; ```. #### Versions:; <!-- Output of scanpy.logging.print_versions() -->; scanpy==1.5.1 anndata==0.7.3 umap==0.4.4 numpy==1.19.0 scipy==1.5.0 pandas==1.0.5 scikit-learn==0.23.1 statsmodels==0.11.1 python-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1318:789,detect,detectable,789,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1318,1,['detect'],['detectable']
Safety,"<!-- Please give a clear and concise description of what the bug is: -->; Not able to install with conda and no info about the source of error.; <!-- Put a minimal reproducible example that reproduces the bug in the code block below: -->; ```bash; (scrna) $ conda install -c bioconda scanpy. Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort.; failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions; ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1190:808,abort,abort,808,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1190,1,['abort'],['abort']
Safety,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [X] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; ...; `sc.tl.score_genes_cell_cycle` calculates scores via `sc.tl.score_genes` but also assigns a categorical label of cell cycle phase. Given lists of marker genes of cell types, can a similar approach be used to potentially annotate putative cell types? Maybe it is too naive? My main concern is that cells that do not fit any cell type for which there are markers will be mis-assigned. I am aware that there are tons of automated cell type prediction tools for scRNA-seq, but not found anything directly supported by scanpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1863:911,predict,prediction,911,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1863,1,['predict'],['prediction']
Safety,"<!-- What kind of feature would you like to request? -->; - [ ] Additional function parameters / changed functionality / changed defaults?; - [x] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->; Adding multiplex community detection from Leiden: https://leidenalg.readthedocs.io/en/stable/multiplex.html#layer-multiplex. It seems very straightforward and would be the most simple way to integrate two modalities on the graph. We would make great use of it in Squidpy (rna counts+image), but I think it should live in Scanpy becasue it could be useful for other multi-modal data. This is a duplicate of #1107 and it has been extensively discussed in #1117 . In the latter however, lots of thought went into normalization/processing which is superfluous for this case as it is only specific for CITE-seq data. Here we'd just want to allow users to get partitions out of multiple graphs. This could be done in two ways:; - adding arguments to existing `tl.leiden`, so that it accepts multiple graphs and multiple resolutions params per graph.; - creating a separate function `sc.tl.leiden_multiplex`.; Any thoughts on this @ivirshup @Koncopd ?. I think @WeilerP also had some thoughts along these lines. Have you ever tried this out? is there any other analysis tool you explored with a simlar purpose? Would be interested to hear your thoughts!; worth mentioning that another approach, the WNN from seurat, was also mentioned here: https://github.com/theislab/scanpy/pull/1117#issuecomment-777020580; although am not sure how much work that requries.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1818:496,detect,detection,496,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1818,1,['detect'],['detection']
Safety,"<!-- What kind of feature would you like to request? -->; - [x] Additional function parameters / changed functionality / changed defaults?; - [ ] New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`?; - [ ] New plotting function: A kind of plot you would like to seein `sc.pl`?; - [ ] External tools: Do you know an existing package that should go into `sc.external.*`?; - [ ] Other?. <!-- Please describe your wishes below: -->. Especially when we visualize large datasets with multiple categorical variables (e.g. patient, disease, cell type) using `sc.pl.dotplot`, and we use a sequence in the `groupby` argument (`e.g. sc.pl.dotplot(ad, 'genex', groupby=['individual', 'disease_status', 'cell type'])`), sometimes we end up with too few cells in some rows, in which summary statistics like fraction of nonzero expressors or mean expression are not very robust. To avoid that, I think it'd be cool to have a minimum observation cutoff in the function, where e.g. `min_cells=5` would show `groupby` combinations with at least 5 cells. Without this option, this sort of filtering becomes an annoying pandas exercise (which some might enjoy but possibly not everyone).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://github.com/scverse/scanpy/issues/1829:907,avoid,avoid,907,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1829,1,['avoid'],['avoid']
